{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"WCrrJRzdYD4l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a00e6d87-1f20-4aad-a560-9d8fb6dfdbec","executionInfo":{"status":"ok","timestamp":1555616152166,"user_tz":-180,"elapsed":874,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":58,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"u4wZPCMoYrrm","colab_type":"code","colab":{}},"cell_type":"code","source":["path='drive/My Drive/CNN/Images/Subject3_Figures/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-bWOxl86i248","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","import sys,os\n","import random\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"metadata":{"id":"820nIGaZsqNt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e989a637-a49b-4752-da7e-38d766c369dd","executionInfo":{"status":"ok","timestamp":1555616152555,"user_tz":-180,"elapsed":1216,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}}},"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"execution_count":61,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n"],"name":"stdout"}]},{"metadata":{"id":"muHUxcbmm3ID","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"aed9e0bc-3a09-4592-afdf-78a409c5ea1f","executionInfo":{"status":"ok","timestamp":1555616152556,"user_tz":-180,"elapsed":1200,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}}},"cell_type":"code","source":["from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 16\n","# percentage of training set to use as validation\n","test_size = 0.2\n","valid_size = 0.2\n","train_size = 0.8\n","\n","# convert data to a normalized torch.FloatTensor\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","dataset = datasets.ImageFolder(path, transform = transform)\n","dataloader = torch.utils.data.DataLoader(dataset, shuffle = True)\n","\n","# specify the image classes\n","classes = ['L1_Standing_still','L2_Sitting_and_relaxing', 'L3_Lying_down', 'L4_Walking', 'L5_Climbing_stairs', 'L9_Cycling', 'L10_Jogging', 'L11_Running']\n","\n","# obtain training indices that will be used for validation\n","num_data = len(dataset)\n","indices = list(range(num_data))\n","np.random.shuffle(indices)\n","split = int(np.floor(test_size * num_data))\n","train_idx1, test_idx = indices[split:], indices[:split]\n","\n","num_train_data = len(dataset) - split\n","split2 = int(np.floor(valid_size * num_train_data))\n","train_idx, valid_idx = train_idx1[split2:], train_idx1[:split2]\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","test_sampler = SubsetRandomSampler(test_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n","test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers)\n","valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n","print(len(train_idx))\n","print(len(test_idx))\n","print(len(valid_idx))"],"execution_count":62,"outputs":[{"output_type":"stream","text":["304\n","95\n","76\n"],"name":"stdout"}]},{"metadata":{"id":"X_abZZXgrW9O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"outputId":"5c173845-f346-44f8-9aa8-b0b045b1624e","executionInfo":{"status":"ok","timestamp":1555616152861,"user_tz":-180,"elapsed":1485,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}}},"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","#35\n","# define the CNN architecture\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # convolutional layer (sees 144x32x4 image tensor)\n","        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n","\n","        # convolutional layer (sees 72x16x16 tensor)\n","        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n","        \n","        # convolutional layer (sees 36x8x32 tensor)\n","        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n","        # convolutional layer (sees 18x8x32 tensor)\n","        self.conv4 = nn.Conv2d(32, 64, 3, padding=1)\n","        # max pooling layer\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.pool1 = nn.MaxPool2d(3, 3)\n","        # linear layer (64 * 9 * 9 -> 500)\n","        self.fc1 = nn.Linear(64 * 9 * 9, 3500)\n","        # linear layer (500 -> 10)\n","        self.fc2 = nn.Linear(3500, 1500)\n","       # self.fc3 = nn.Linear(1500,700)\n","       # self.fc4 = nn.Linear(700, 300)\n","        self.fc3 = nn.Linear(1500, 700)\n","        self.fc4 = nn.Linear(700, 11)\n","        # dropout layer (p=0.25)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        # add sequence of convolutional and max pooling layers\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = self.pool(F.relu(self.conv4(x)))\n","        # flatten image input\n","        x = x.view(-1, 64 * 9 * 9)\n","        # add dropout layer\n","        #x = self.dropout(x)\n","        # add 1st hidden layer, with relu activation function\n","        x = F.relu(self.fc1(x))\n","        # add dropout layer\n","        x = self.dropout(x)\n","        # add 2nd hidden layer, with relu activation function\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc3(x))\n","        x = self.dropout(x)\n","       # x = F.relu(self.fc4(x))\n","       # x = self.dropout(x)\n","        x = self.fc4(x)\n","        return x\n","# create a complete CNN\n","model = Net()\n","print(model)\n","# move tensors to GPU if CUDA is available\n","if train_on_gpu:\n","    model.cuda()"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (pool1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=5184, out_features=3500, bias=True)\n","  (fc2): Linear(in_features=3500, out_features=1500, bias=True)\n","  (fc3): Linear(in_features=1500, out_features=700, bias=True)\n","  (fc4): Linear(in_features=700, out_features=11, bias=True)\n","  (dropout): Dropout(p=0.2)\n",")\n"],"name":"stdout"}]},{"metadata":{"id":"W6D2ECmUm6s7","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch.optim as optim\n","\n","# specify loss function (categorical cross-entropy)\n","criterion = nn.CrossEntropyLoss()\n","\n","# specify optimizer\n","optimizer = optim.SGD(model.parameters(), lr=0.01)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wBHGXGW-m9mQ","colab_type":"code","outputId":"57da367a-5976-46f4-fdaf-83e1f25807b1","executionInfo":{"status":"error","timestamp":1555616528884,"user_tz":-180,"elapsed":377489,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}},"colab":{"base_uri":"https://localhost:8080/","height":7524}},"cell_type":"code","source":["# number of epochs to train the model\n","n_epochs = 1000\n","\n","valid_loss_min = np.Inf # track change in validation loss\n","\n","for epoch in range(1, n_epochs+1):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    \n","    ###################\n","    # train the model #\n","    ###################\n","    model.train()\n","    for data, target in train_loader:\n","        # move tensors to GPU if CUDA is available\n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","        \n","    ######################    \n","    # validate the model #\n","    ######################\n","    model.eval()\n","    for data, target in valid_loader:\n","        # move tensors to GPU if CUDA is available\n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # update average validation loss \n","        valid_loss += loss.item()*data.size(0)\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(train_loader.dataset)\n","    valid_loss = valid_loss/len(valid_loader.dataset)\n","        \n","    # print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        epoch, train_loss, valid_loss))\n","    \n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","       # torch.save(model.state_dict(), 'model_cifar.pt')\n","        torch.save(model.state_dict(), 'drive/My Drive/CNN/model_augmented.pt')\n","        valid_loss_min = valid_loss\n"],"execution_count":65,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 1.530800 \tValidation Loss: 0.382047\n","Validation loss decreased (inf --> 0.382047).  Saving model ...\n","Epoch: 2 \tTraining Loss: 1.525568 \tValidation Loss: 0.380834\n","Validation loss decreased (0.382047 --> 0.380834).  Saving model ...\n","Epoch: 3 \tTraining Loss: 1.520601 \tValidation Loss: 0.379676\n","Validation loss decreased (0.380834 --> 0.379676).  Saving model ...\n","Epoch: 4 \tTraining Loss: 1.515215 \tValidation Loss: 0.378568\n","Validation loss decreased (0.379676 --> 0.378568).  Saving model ...\n","Epoch: 5 \tTraining Loss: 1.510747 \tValidation Loss: 0.377502\n","Validation loss decreased (0.378568 --> 0.377502).  Saving model ...\n","Epoch: 6 \tTraining Loss: 1.505762 \tValidation Loss: 0.376473\n","Validation loss decreased (0.377502 --> 0.376473).  Saving model ...\n","Epoch: 7 \tTraining Loss: 1.501675 \tValidation Loss: 0.375483\n","Validation loss decreased (0.376473 --> 0.375483).  Saving model ...\n","Epoch: 8 \tTraining Loss: 1.497353 \tValidation Loss: 0.374526\n","Validation loss decreased (0.375483 --> 0.374526).  Saving model ...\n","Epoch: 9 \tTraining Loss: 1.493097 \tValidation Loss: 0.373595\n","Validation loss decreased (0.374526 --> 0.373595).  Saving model ...\n","Epoch: 10 \tTraining Loss: 1.488879 \tValidation Loss: 0.372694\n","Validation loss decreased (0.373595 --> 0.372694).  Saving model ...\n","Epoch: 11 \tTraining Loss: 1.485188 \tValidation Loss: 0.371817\n","Validation loss decreased (0.372694 --> 0.371817).  Saving model ...\n","Epoch: 12 \tTraining Loss: 1.481228 \tValidation Loss: 0.370962\n","Validation loss decreased (0.371817 --> 0.370962).  Saving model ...\n","Epoch: 13 \tTraining Loss: 1.477671 \tValidation Loss: 0.370123\n","Validation loss decreased (0.370962 --> 0.370123).  Saving model ...\n","Epoch: 14 \tTraining Loss: 1.473895 \tValidation Loss: 0.369302\n","Validation loss decreased (0.370123 --> 0.369302).  Saving model ...\n","Epoch: 15 \tTraining Loss: 1.470326 \tValidation Loss: 0.368493\n","Validation loss decreased (0.369302 --> 0.368493).  Saving model ...\n","Epoch: 16 \tTraining Loss: 1.466718 \tValidation Loss: 0.367697\n","Validation loss decreased (0.368493 --> 0.367697).  Saving model ...\n","Epoch: 17 \tTraining Loss: 1.463179 \tValidation Loss: 0.366909\n","Validation loss decreased (0.367697 --> 0.366909).  Saving model ...\n","Epoch: 18 \tTraining Loss: 1.459425 \tValidation Loss: 0.366126\n","Validation loss decreased (0.366909 --> 0.366126).  Saving model ...\n","Epoch: 19 \tTraining Loss: 1.455969 \tValidation Loss: 0.365344\n","Validation loss decreased (0.366126 --> 0.365344).  Saving model ...\n","Epoch: 20 \tTraining Loss: 1.452541 \tValidation Loss: 0.364551\n","Validation loss decreased (0.365344 --> 0.364551).  Saving model ...\n","Epoch: 21 \tTraining Loss: 1.448873 \tValidation Loss: 0.363746\n","Validation loss decreased (0.364551 --> 0.363746).  Saving model ...\n","Epoch: 22 \tTraining Loss: 1.445397 \tValidation Loss: 0.362925\n","Validation loss decreased (0.363746 --> 0.362925).  Saving model ...\n","Epoch: 23 \tTraining Loss: 1.441611 \tValidation Loss: 0.362064\n","Validation loss decreased (0.362925 --> 0.362064).  Saving model ...\n","Epoch: 24 \tTraining Loss: 1.437326 \tValidation Loss: 0.361145\n","Validation loss decreased (0.362064 --> 0.361145).  Saving model ...\n","Epoch: 25 \tTraining Loss: 1.432761 \tValidation Loss: 0.360123\n","Validation loss decreased (0.361145 --> 0.360123).  Saving model ...\n","Epoch: 26 \tTraining Loss: 1.428419 \tValidation Loss: 0.358978\n","Validation loss decreased (0.360123 --> 0.358978).  Saving model ...\n","Epoch: 27 \tTraining Loss: 1.422463 \tValidation Loss: 0.357626\n","Validation loss decreased (0.358978 --> 0.357626).  Saving model ...\n","Epoch: 28 \tTraining Loss: 1.416082 \tValidation Loss: 0.355921\n","Validation loss decreased (0.357626 --> 0.355921).  Saving model ...\n","Epoch: 29 \tTraining Loss: 1.408010 \tValidation Loss: 0.353692\n","Validation loss decreased (0.355921 --> 0.353692).  Saving model ...\n","Epoch: 30 \tTraining Loss: 1.394675 \tValidation Loss: 0.350653\n","Validation loss decreased (0.353692 --> 0.350653).  Saving model ...\n","Epoch: 31 \tTraining Loss: 1.381838 \tValidation Loss: 0.347335\n","Validation loss decreased (0.350653 --> 0.347335).  Saving model ...\n","Epoch: 32 \tTraining Loss: 1.362759 \tValidation Loss: 0.343937\n","Validation loss decreased (0.347335 --> 0.343937).  Saving model ...\n","Epoch: 33 \tTraining Loss: 1.355783 \tValidation Loss: 0.340825\n","Validation loss decreased (0.343937 --> 0.340825).  Saving model ...\n","Epoch: 34 \tTraining Loss: 1.348399 \tValidation Loss: 0.338735\n","Validation loss decreased (0.340825 --> 0.338735).  Saving model ...\n","Epoch: 35 \tTraining Loss: 1.347615 \tValidation Loss: 0.338092\n","Validation loss decreased (0.338735 --> 0.338092).  Saving model ...\n","Epoch: 36 \tTraining Loss: 1.347675 \tValidation Loss: 0.337076\n","Validation loss decreased (0.338092 --> 0.337076).  Saving model ...\n","Epoch: 37 \tTraining Loss: 1.342091 \tValidation Loss: 0.338776\n","Epoch: 38 \tTraining Loss: 1.335325 \tValidation Loss: 0.336347\n","Validation loss decreased (0.337076 --> 0.336347).  Saving model ...\n","Epoch: 39 \tTraining Loss: 1.342727 \tValidation Loss: 0.336963\n","Epoch: 40 \tTraining Loss: 1.345097 \tValidation Loss: 0.336427\n","Epoch: 41 \tTraining Loss: 1.344208 \tValidation Loss: 0.335279\n","Validation loss decreased (0.336347 --> 0.335279).  Saving model ...\n","Epoch: 42 \tTraining Loss: 1.344130 \tValidation Loss: 0.337291\n","Epoch: 43 \tTraining Loss: 1.342707 \tValidation Loss: 0.337557\n","Epoch: 44 \tTraining Loss: 1.342901 \tValidation Loss: 0.335932\n","Epoch: 45 \tTraining Loss: 1.349299 \tValidation Loss: 0.337197\n","Epoch: 46 \tTraining Loss: 1.342980 \tValidation Loss: 0.336414\n","Epoch: 47 \tTraining Loss: 1.342973 \tValidation Loss: 0.337644\n","Epoch: 48 \tTraining Loss: 1.342488 \tValidation Loss: 0.335944\n","Epoch: 49 \tTraining Loss: 1.337328 \tValidation Loss: 0.337278\n","Epoch: 50 \tTraining Loss: 1.342967 \tValidation Loss: 0.334343\n","Validation loss decreased (0.335279 --> 0.334343).  Saving model ...\n","Epoch: 51 \tTraining Loss: 1.342574 \tValidation Loss: 0.336123\n","Epoch: 52 \tTraining Loss: 1.341354 \tValidation Loss: 0.337102\n","Epoch: 53 \tTraining Loss: 1.337620 \tValidation Loss: 0.335801\n","Epoch: 54 \tTraining Loss: 1.339607 \tValidation Loss: 0.335830\n","Epoch: 55 \tTraining Loss: 1.346975 \tValidation Loss: 0.336235\n","Epoch: 56 \tTraining Loss: 1.345461 \tValidation Loss: 0.336355\n","Epoch: 57 \tTraining Loss: 1.342147 \tValidation Loss: 0.337041\n","Epoch: 58 \tTraining Loss: 1.342941 \tValidation Loss: 0.335885\n","Epoch: 59 \tTraining Loss: 1.341343 \tValidation Loss: 0.337285\n","Epoch: 60 \tTraining Loss: 1.341393 \tValidation Loss: 0.336645\n","Epoch: 61 \tTraining Loss: 1.340467 \tValidation Loss: 0.336546\n","Epoch: 62 \tTraining Loss: 1.340514 \tValidation Loss: 0.336928\n","Epoch: 63 \tTraining Loss: 1.336828 \tValidation Loss: 0.336683\n","Epoch: 64 \tTraining Loss: 1.344046 \tValidation Loss: 0.336183\n","Epoch: 65 \tTraining Loss: 1.342661 \tValidation Loss: 0.336562\n","Epoch: 66 \tTraining Loss: 1.337155 \tValidation Loss: 0.336208\n","Epoch: 67 \tTraining Loss: 1.341912 \tValidation Loss: 0.336481\n","Epoch: 68 \tTraining Loss: 1.342933 \tValidation Loss: 0.336981\n","Epoch: 69 \tTraining Loss: 1.341027 \tValidation Loss: 0.336244\n","Epoch: 70 \tTraining Loss: 1.338098 \tValidation Loss: 0.336806\n","Epoch: 71 \tTraining Loss: 1.340815 \tValidation Loss: 0.338274\n","Epoch: 72 \tTraining Loss: 1.335191 \tValidation Loss: 0.335929\n","Epoch: 73 \tTraining Loss: 1.341486 \tValidation Loss: 0.335892\n","Epoch: 74 \tTraining Loss: 1.341992 \tValidation Loss: 0.337054\n","Epoch: 75 \tTraining Loss: 1.340687 \tValidation Loss: 0.337399\n","Epoch: 76 \tTraining Loss: 1.340501 \tValidation Loss: 0.336728\n","Epoch: 77 \tTraining Loss: 1.335069 \tValidation Loss: 0.335031\n","Epoch: 78 \tTraining Loss: 1.342007 \tValidation Loss: 0.335166\n","Epoch: 79 \tTraining Loss: 1.341873 \tValidation Loss: 0.336134\n","Epoch: 80 \tTraining Loss: 1.337242 \tValidation Loss: 0.336743\n","Epoch: 81 \tTraining Loss: 1.344591 \tValidation Loss: 0.336683\n","Epoch: 82 \tTraining Loss: 1.338847 \tValidation Loss: 0.336073\n","Epoch: 83 \tTraining Loss: 1.338578 \tValidation Loss: 0.335459\n","Epoch: 84 \tTraining Loss: 1.337060 \tValidation Loss: 0.336345\n","Epoch: 85 \tTraining Loss: 1.340114 \tValidation Loss: 0.336885\n","Epoch: 86 \tTraining Loss: 1.335984 \tValidation Loss: 0.336018\n","Epoch: 87 \tTraining Loss: 1.342410 \tValidation Loss: 0.336534\n","Epoch: 88 \tTraining Loss: 1.335993 \tValidation Loss: 0.336427\n","Epoch: 89 \tTraining Loss: 1.331558 \tValidation Loss: 0.335109\n","Epoch: 90 \tTraining Loss: 1.336824 \tValidation Loss: 0.335824\n","Epoch: 91 \tTraining Loss: 1.336560 \tValidation Loss: 0.335736\n","Epoch: 92 \tTraining Loss: 1.335611 \tValidation Loss: 0.336345\n","Epoch: 93 \tTraining Loss: 1.334679 \tValidation Loss: 0.336071\n","Epoch: 94 \tTraining Loss: 1.337054 \tValidation Loss: 0.337113\n","Epoch: 95 \tTraining Loss: 1.337570 \tValidation Loss: 0.336262\n","Epoch: 96 \tTraining Loss: 1.333869 \tValidation Loss: 0.335882\n","Epoch: 97 \tTraining Loss: 1.342050 \tValidation Loss: 0.336305\n","Epoch: 98 \tTraining Loss: 1.334177 \tValidation Loss: 0.335781\n","Epoch: 99 \tTraining Loss: 1.335832 \tValidation Loss: 0.335578\n","Epoch: 100 \tTraining Loss: 1.340502 \tValidation Loss: 0.335487\n","Epoch: 101 \tTraining Loss: 1.335103 \tValidation Loss: 0.335455\n","Epoch: 102 \tTraining Loss: 1.336749 \tValidation Loss: 0.335513\n","Epoch: 103 \tTraining Loss: 1.335914 \tValidation Loss: 0.336510\n","Epoch: 104 \tTraining Loss: 1.335033 \tValidation Loss: 0.335120\n","Epoch: 105 \tTraining Loss: 1.338745 \tValidation Loss: 0.335727\n","Epoch: 106 \tTraining Loss: 1.338247 \tValidation Loss: 0.334671\n","Epoch: 107 \tTraining Loss: 1.338351 \tValidation Loss: 0.335978\n","Epoch: 108 \tTraining Loss: 1.331390 \tValidation Loss: 0.337140\n","Epoch: 109 \tTraining Loss: 1.337571 \tValidation Loss: 0.336423\n","Epoch: 110 \tTraining Loss: 1.336380 \tValidation Loss: 0.336119\n","Epoch: 111 \tTraining Loss: 1.336020 \tValidation Loss: 0.335934\n","Epoch: 112 \tTraining Loss: 1.335534 \tValidation Loss: 0.335871\n","Epoch: 113 \tTraining Loss: 1.338207 \tValidation Loss: 0.335404\n","Epoch: 114 \tTraining Loss: 1.335476 \tValidation Loss: 0.336190\n","Epoch: 115 \tTraining Loss: 1.334562 \tValidation Loss: 0.335308\n","Epoch: 116 \tTraining Loss: 1.337671 \tValidation Loss: 0.336226\n","Epoch: 117 \tTraining Loss: 1.335879 \tValidation Loss: 0.335544\n","Epoch: 118 \tTraining Loss: 1.332831 \tValidation Loss: 0.335019\n","Epoch: 119 \tTraining Loss: 1.333708 \tValidation Loss: 0.334443\n","Epoch: 120 \tTraining Loss: 1.332862 \tValidation Loss: 0.335121\n","Epoch: 121 \tTraining Loss: 1.334226 \tValidation Loss: 0.336383\n","Epoch: 122 \tTraining Loss: 1.334746 \tValidation Loss: 0.335565\n","Epoch: 123 \tTraining Loss: 1.328999 \tValidation Loss: 0.335751\n","Epoch: 124 \tTraining Loss: 1.332726 \tValidation Loss: 0.334512\n","Epoch: 125 \tTraining Loss: 1.335023 \tValidation Loss: 0.334152\n","Validation loss decreased (0.334343 --> 0.334152).  Saving model ...\n","Epoch: 126 \tTraining Loss: 1.330186 \tValidation Loss: 0.334045\n","Validation loss decreased (0.334152 --> 0.334045).  Saving model ...\n","Epoch: 127 \tTraining Loss: 1.330390 \tValidation Loss: 0.334044\n","Validation loss decreased (0.334045 --> 0.334044).  Saving model ...\n","Epoch: 128 \tTraining Loss: 1.330902 \tValidation Loss: 0.334524\n","Epoch: 129 \tTraining Loss: 1.333113 \tValidation Loss: 0.335755\n","Epoch: 130 \tTraining Loss: 1.328169 \tValidation Loss: 0.335804\n","Epoch: 131 \tTraining Loss: 1.333372 \tValidation Loss: 0.334739\n","Epoch: 132 \tTraining Loss: 1.331468 \tValidation Loss: 0.334537\n","Epoch: 133 \tTraining Loss: 1.325693 \tValidation Loss: 0.333478\n","Validation loss decreased (0.334044 --> 0.333478).  Saving model ...\n","Epoch: 134 \tTraining Loss: 1.322941 \tValidation Loss: 0.333565\n","Epoch: 135 \tTraining Loss: 1.327943 \tValidation Loss: 0.332566\n","Validation loss decreased (0.333478 --> 0.332566).  Saving model ...\n","Epoch: 136 \tTraining Loss: 1.322551 \tValidation Loss: 0.332869\n","Epoch: 137 \tTraining Loss: 1.316796 \tValidation Loss: 0.332021\n","Validation loss decreased (0.332566 --> 0.332021).  Saving model ...\n","Epoch: 138 \tTraining Loss: 1.320199 \tValidation Loss: 0.332081\n","Epoch: 139 \tTraining Loss: 1.315436 \tValidation Loss: 0.330446\n","Validation loss decreased (0.332021 --> 0.330446).  Saving model ...\n","Epoch: 140 \tTraining Loss: 1.307920 \tValidation Loss: 0.328102\n","Validation loss decreased (0.330446 --> 0.328102).  Saving model ...\n","Epoch: 141 \tTraining Loss: 1.302065 \tValidation Loss: 0.326982\n","Validation loss decreased (0.328102 --> 0.326982).  Saving model ...\n","Epoch: 142 \tTraining Loss: 1.287800 \tValidation Loss: 0.324001\n","Validation loss decreased (0.326982 --> 0.324001).  Saving model ...\n","Epoch: 143 \tTraining Loss: 1.264857 \tValidation Loss: 0.317582\n","Validation loss decreased (0.324001 --> 0.317582).  Saving model ...\n","Epoch: 144 \tTraining Loss: 1.239509 \tValidation Loss: 0.307412\n","Validation loss decreased (0.317582 --> 0.307412).  Saving model ...\n","Epoch: 145 \tTraining Loss: 1.181898 \tValidation Loss: 0.291877\n","Validation loss decreased (0.307412 --> 0.291877).  Saving model ...\n","Epoch: 146 \tTraining Loss: 1.117501 \tValidation Loss: 0.277945\n","Validation loss decreased (0.291877 --> 0.277945).  Saving model ...\n","Epoch: 147 \tTraining Loss: 1.038421 \tValidation Loss: 0.258997\n","Validation loss decreased (0.277945 --> 0.258997).  Saving model ...\n","Epoch: 148 \tTraining Loss: 1.011749 \tValidation Loss: 0.258448\n","Validation loss decreased (0.258997 --> 0.258448).  Saving model ...\n","Epoch: 149 \tTraining Loss: 0.989007 \tValidation Loss: 0.254814\n","Validation loss decreased (0.258448 --> 0.254814).  Saving model ...\n","Epoch: 150 \tTraining Loss: 0.943974 \tValidation Loss: 0.278918\n","Epoch: 151 \tTraining Loss: 0.968033 \tValidation Loss: 0.256058\n","Epoch: 152 \tTraining Loss: 0.942185 \tValidation Loss: 0.237579\n","Validation loss decreased (0.254814 --> 0.237579).  Saving model ...\n","Epoch: 153 \tTraining Loss: 0.936561 \tValidation Loss: 0.297592\n","Epoch: 154 \tTraining Loss: 0.959323 \tValidation Loss: 0.237143\n","Validation loss decreased (0.237579 --> 0.237143).  Saving model ...\n","Epoch: 155 \tTraining Loss: 0.905835 \tValidation Loss: 0.248017\n","Epoch: 156 \tTraining Loss: 0.907015 \tValidation Loss: 0.240878\n","Epoch: 157 \tTraining Loss: 0.898973 \tValidation Loss: 0.261033\n","Epoch: 158 \tTraining Loss: 0.914382 \tValidation Loss: 0.234699\n","Validation loss decreased (0.237143 --> 0.234699).  Saving model ...\n","Epoch: 159 \tTraining Loss: 0.890817 \tValidation Loss: 0.291880\n","Epoch: 160 \tTraining Loss: 0.872561 \tValidation Loss: 0.306297\n","Epoch: 161 \tTraining Loss: 0.890997 \tValidation Loss: 0.234475\n","Validation loss decreased (0.234699 --> 0.234475).  Saving model ...\n","Epoch: 162 \tTraining Loss: 0.837568 \tValidation Loss: 0.244738\n","Epoch: 163 \tTraining Loss: 0.807150 \tValidation Loss: 0.214963\n","Validation loss decreased (0.234475 --> 0.214963).  Saving model ...\n","Epoch: 164 \tTraining Loss: 0.844892 \tValidation Loss: 0.252124\n","Epoch: 165 \tTraining Loss: 0.884287 \tValidation Loss: 0.226300\n","Epoch: 166 \tTraining Loss: 0.842676 \tValidation Loss: 0.224111\n","Epoch: 167 \tTraining Loss: 0.827745 \tValidation Loss: 0.225179\n","Epoch: 168 \tTraining Loss: 0.815473 \tValidation Loss: 0.226369\n","Epoch: 169 \tTraining Loss: 0.790810 \tValidation Loss: 0.204477\n","Validation loss decreased (0.214963 --> 0.204477).  Saving model ...\n","Epoch: 170 \tTraining Loss: 0.786753 \tValidation Loss: 0.212432\n","Epoch: 171 \tTraining Loss: 0.827385 \tValidation Loss: 0.212721\n","Epoch: 172 \tTraining Loss: 0.799153 \tValidation Loss: 0.211629\n","Epoch: 173 \tTraining Loss: 0.798039 \tValidation Loss: 0.220860\n","Epoch: 174 \tTraining Loss: 0.721369 \tValidation Loss: 0.236296\n","Epoch: 175 \tTraining Loss: 0.800267 \tValidation Loss: 0.209765\n","Epoch: 176 \tTraining Loss: 0.716855 \tValidation Loss: 0.201525\n","Validation loss decreased (0.204477 --> 0.201525).  Saving model ...\n","Epoch: 177 \tTraining Loss: 0.734333 \tValidation Loss: 0.197428\n","Validation loss decreased (0.201525 --> 0.197428).  Saving model ...\n","Epoch: 178 \tTraining Loss: 0.699905 \tValidation Loss: 0.196548\n","Validation loss decreased (0.197428 --> 0.196548).  Saving model ...\n","Epoch: 179 \tTraining Loss: 0.774123 \tValidation Loss: 0.196355\n","Validation loss decreased (0.196548 --> 0.196355).  Saving model ...\n","Epoch: 180 \tTraining Loss: 0.711352 \tValidation Loss: 0.196407\n","Epoch: 181 \tTraining Loss: 0.638747 \tValidation Loss: 0.270978\n","Epoch: 182 \tTraining Loss: 0.696538 \tValidation Loss: 0.195914\n","Validation loss decreased (0.196355 --> 0.195914).  Saving model ...\n","Epoch: 183 \tTraining Loss: 0.684546 \tValidation Loss: 0.183370\n","Validation loss decreased (0.195914 --> 0.183370).  Saving model ...\n","Epoch: 184 \tTraining Loss: 0.658693 \tValidation Loss: 0.189962\n","Epoch: 185 \tTraining Loss: 0.667140 \tValidation Loss: 0.245814\n","Epoch: 186 \tTraining Loss: 0.661931 \tValidation Loss: 0.226900\n","Epoch: 187 \tTraining Loss: 0.519989 \tValidation Loss: 0.216911\n","Epoch: 188 \tTraining Loss: 0.606744 \tValidation Loss: 0.166124\n","Validation loss decreased (0.183370 --> 0.166124).  Saving model ...\n","Epoch: 189 \tTraining Loss: 0.620671 \tValidation Loss: 0.222538\n","Epoch: 190 \tTraining Loss: 0.577747 \tValidation Loss: 0.213274\n","Epoch: 191 \tTraining Loss: 0.579888 \tValidation Loss: 0.162043\n","Validation loss decreased (0.166124 --> 0.162043).  Saving model ...\n","Epoch: 192 \tTraining Loss: 0.513470 \tValidation Loss: 0.196935\n","Epoch: 193 \tTraining Loss: 0.513590 \tValidation Loss: 0.215481\n","Epoch: 194 \tTraining Loss: 0.487898 \tValidation Loss: 0.178090\n","Epoch: 195 \tTraining Loss: 0.485878 \tValidation Loss: 0.246093\n","Epoch: 196 \tTraining Loss: 0.473117 \tValidation Loss: 0.178568\n","Epoch: 197 \tTraining Loss: 0.404910 \tValidation Loss: 0.176296\n","Epoch: 198 \tTraining Loss: 0.483729 \tValidation Loss: 0.158185\n","Validation loss decreased (0.162043 --> 0.158185).  Saving model ...\n","Epoch: 199 \tTraining Loss: 0.429107 \tValidation Loss: 0.149906\n","Validation loss decreased (0.158185 --> 0.149906).  Saving model ...\n","Epoch: 200 \tTraining Loss: 0.334439 \tValidation Loss: 0.185230\n","Epoch: 201 \tTraining Loss: 0.453287 \tValidation Loss: 0.146681\n","Validation loss decreased (0.149906 --> 0.146681).  Saving model ...\n","Epoch: 202 \tTraining Loss: 0.315320 \tValidation Loss: 0.142350\n","Validation loss decreased (0.146681 --> 0.142350).  Saving model ...\n","Epoch: 203 \tTraining Loss: 0.464167 \tValidation Loss: 0.196439\n","Epoch: 204 \tTraining Loss: 0.307103 \tValidation Loss: 0.154251\n","Epoch: 205 \tTraining Loss: 0.275147 \tValidation Loss: 0.148060\n","Epoch: 206 \tTraining Loss: 0.264463 \tValidation Loss: 0.135320\n","Validation loss decreased (0.142350 --> 0.135320).  Saving model ...\n","Epoch: 207 \tTraining Loss: 0.315782 \tValidation Loss: 0.131474\n","Validation loss decreased (0.135320 --> 0.131474).  Saving model ...\n","Epoch: 208 \tTraining Loss: 0.329574 \tValidation Loss: 0.142257\n","Epoch: 209 \tTraining Loss: 0.223283 \tValidation Loss: 0.140481\n","Epoch: 210 \tTraining Loss: 0.326775 \tValidation Loss: 0.127107\n","Validation loss decreased (0.131474 --> 0.127107).  Saving model ...\n","Epoch: 211 \tTraining Loss: 0.274760 \tValidation Loss: 0.159267\n","Epoch: 212 \tTraining Loss: 0.248431 \tValidation Loss: 0.117373\n","Validation loss decreased (0.127107 --> 0.117373).  Saving model ...\n","Epoch: 213 \tTraining Loss: 0.240335 \tValidation Loss: 0.125983\n","Epoch: 214 \tTraining Loss: 0.275589 \tValidation Loss: 0.206886\n","Epoch: 215 \tTraining Loss: 0.270392 \tValidation Loss: 0.124795\n","Epoch: 216 \tTraining Loss: 0.203378 \tValidation Loss: 0.132676\n","Epoch: 217 \tTraining Loss: 0.208701 \tValidation Loss: 0.162863\n","Epoch: 218 \tTraining Loss: 0.167342 \tValidation Loss: 0.142386\n","Epoch: 219 \tTraining Loss: 0.202798 \tValidation Loss: 0.161699\n","Epoch: 220 \tTraining Loss: 0.169928 \tValidation Loss: 0.357144\n","Epoch: 221 \tTraining Loss: 0.195218 \tValidation Loss: 0.144021\n","Epoch: 222 \tTraining Loss: 0.170175 \tValidation Loss: 0.158449\n","Epoch: 223 \tTraining Loss: 0.115080 \tValidation Loss: 0.143282\n","Epoch: 224 \tTraining Loss: 0.113915 \tValidation Loss: 0.181462\n","Epoch: 225 \tTraining Loss: 0.285307 \tValidation Loss: 0.140382\n","Epoch: 226 \tTraining Loss: 0.110451 \tValidation Loss: 0.154001\n","Epoch: 227 \tTraining Loss: 0.078796 \tValidation Loss: 0.150503\n","Epoch: 228 \tTraining Loss: 0.089928 \tValidation Loss: 0.141387\n","Epoch: 229 \tTraining Loss: 0.127570 \tValidation Loss: 0.169800\n","Epoch: 230 \tTraining Loss: 0.097775 \tValidation Loss: 0.214662\n","Epoch: 231 \tTraining Loss: 0.116749 \tValidation Loss: 0.139524\n","Epoch: 232 \tTraining Loss: 0.103720 \tValidation Loss: 0.160673\n","Epoch: 233 \tTraining Loss: 0.070012 \tValidation Loss: 0.158380\n","Epoch: 234 \tTraining Loss: 0.223249 \tValidation Loss: 0.256525\n","Epoch: 235 \tTraining Loss: 0.088150 \tValidation Loss: 0.177862\n","Epoch: 236 \tTraining Loss: 0.057476 \tValidation Loss: 0.155844\n","Epoch: 237 \tTraining Loss: 0.060846 \tValidation Loss: 0.214545\n","Epoch: 238 \tTraining Loss: 0.321866 \tValidation Loss: 0.158598\n","Epoch: 239 \tTraining Loss: 0.061138 \tValidation Loss: 0.162278\n","Epoch: 240 \tTraining Loss: 0.036860 \tValidation Loss: 0.166921\n","Epoch: 241 \tTraining Loss: 0.091410 \tValidation Loss: 0.178848\n","Epoch: 242 \tTraining Loss: 0.029008 \tValidation Loss: 0.195358\n","Epoch: 243 \tTraining Loss: 0.016425 \tValidation Loss: 0.185721\n","Epoch: 244 \tTraining Loss: 0.043887 \tValidation Loss: 0.201250\n","Epoch: 245 \tTraining Loss: 0.112651 \tValidation Loss: 0.194507\n","Epoch: 246 \tTraining Loss: 0.040202 \tValidation Loss: 0.169008\n","Epoch: 247 \tTraining Loss: 0.016965 \tValidation Loss: 0.171192\n","Epoch: 248 \tTraining Loss: 0.016207 \tValidation Loss: 0.173387\n","Epoch: 249 \tTraining Loss: 0.062148 \tValidation Loss: 0.163421\n","Epoch: 250 \tTraining Loss: 0.015212 \tValidation Loss: 0.207706\n","Epoch: 251 \tTraining Loss: 0.014503 \tValidation Loss: 0.206275\n","Epoch: 252 \tTraining Loss: 0.013674 \tValidation Loss: 0.216898\n","Epoch: 253 \tTraining Loss: 0.015714 \tValidation Loss: 0.214527\n","Epoch: 254 \tTraining Loss: 0.006492 \tValidation Loss: 0.209580\n","Epoch: 255 \tTraining Loss: 0.032526 \tValidation Loss: 0.252189\n","Epoch: 256 \tTraining Loss: 0.019027 \tValidation Loss: 0.222899\n","Epoch: 257 \tTraining Loss: 0.015003 \tValidation Loss: 0.228270\n","Epoch: 258 \tTraining Loss: 0.012766 \tValidation Loss: 0.237021\n","Epoch: 259 \tTraining Loss: 0.032372 \tValidation Loss: 0.209577\n","Epoch: 260 \tTraining Loss: 0.004079 \tValidation Loss: 0.218431\n","Epoch: 261 \tTraining Loss: 0.006888 \tValidation Loss: 0.222170\n","Epoch: 262 \tTraining Loss: 0.003053 \tValidation Loss: 0.238126\n","Epoch: 263 \tTraining Loss: 0.002867 \tValidation Loss: 0.242081\n","Epoch: 264 \tTraining Loss: 0.003357 \tValidation Loss: 0.250227\n","Epoch: 265 \tTraining Loss: 0.003740 \tValidation Loss: 0.237867\n","Epoch: 266 \tTraining Loss: 0.004099 \tValidation Loss: 0.235398\n","Epoch: 267 \tTraining Loss: 0.004026 \tValidation Loss: 0.228809\n","Epoch: 268 \tTraining Loss: 0.002556 \tValidation Loss: 0.249050\n","Epoch: 269 \tTraining Loss: 0.002353 \tValidation Loss: 0.257196\n","Epoch: 270 \tTraining Loss: 0.361195 \tValidation Loss: 0.160737\n","Epoch: 271 \tTraining Loss: 0.045865 \tValidation Loss: 0.187211\n","Epoch: 272 \tTraining Loss: 0.014493 \tValidation Loss: 0.188949\n","Epoch: 273 \tTraining Loss: 0.010134 \tValidation Loss: 0.200611\n","Epoch: 274 \tTraining Loss: 0.007054 \tValidation Loss: 0.199829\n","Epoch: 275 \tTraining Loss: 0.006003 \tValidation Loss: 0.220549\n","Epoch: 276 \tTraining Loss: 0.004894 \tValidation Loss: 0.233751\n","Epoch: 277 \tTraining Loss: 0.003653 \tValidation Loss: 0.221557\n","Epoch: 278 \tTraining Loss: 0.003949 \tValidation Loss: 0.239126\n","Epoch: 279 \tTraining Loss: 0.004710 \tValidation Loss: 0.243622\n","Epoch: 280 \tTraining Loss: 0.002172 \tValidation Loss: 0.237734\n","Epoch: 281 \tTraining Loss: 0.002257 \tValidation Loss: 0.237249\n","Epoch: 282 \tTraining Loss: 0.002311 \tValidation Loss: 0.251427\n","Epoch: 283 \tTraining Loss: 0.002241 \tValidation Loss: 0.256012\n","Epoch: 284 \tTraining Loss: 0.002614 \tValidation Loss: 0.253852\n","Epoch: 285 \tTraining Loss: 0.002965 \tValidation Loss: 0.248352\n","Epoch: 286 \tTraining Loss: 0.002224 \tValidation Loss: 0.256360\n","Epoch: 287 \tTraining Loss: 0.001723 \tValidation Loss: 0.263948\n","Epoch: 288 \tTraining Loss: 0.001417 \tValidation Loss: 0.263924\n","Epoch: 289 \tTraining Loss: 0.001129 \tValidation Loss: 0.263236\n","Epoch: 290 \tTraining Loss: 0.001854 \tValidation Loss: 0.269892\n","Epoch: 291 \tTraining Loss: 0.001539 \tValidation Loss: 0.272647\n","Epoch: 292 \tTraining Loss: 0.001918 \tValidation Loss: 0.281482\n","Epoch: 293 \tTraining Loss: 0.001739 \tValidation Loss: 0.273703\n","Epoch: 294 \tTraining Loss: 0.001213 \tValidation Loss: 0.277642\n","Epoch: 295 \tTraining Loss: 0.001040 \tValidation Loss: 0.282380\n","Epoch: 296 \tTraining Loss: 0.001330 \tValidation Loss: 0.289669\n","Epoch: 297 \tTraining Loss: 0.002274 \tValidation Loss: 0.278374\n","Epoch: 298 \tTraining Loss: 0.000955 \tValidation Loss: 0.283546\n","Epoch: 299 \tTraining Loss: 0.001059 \tValidation Loss: 0.282857\n","Epoch: 300 \tTraining Loss: 0.000636 \tValidation Loss: 0.282308\n","Epoch: 301 \tTraining Loss: 0.000889 \tValidation Loss: 0.283963\n","Epoch: 302 \tTraining Loss: 0.000852 \tValidation Loss: 0.287609\n","Epoch: 303 \tTraining Loss: 0.001102 \tValidation Loss: 0.298314\n","Epoch: 304 \tTraining Loss: 0.000718 \tValidation Loss: 0.291343\n","Epoch: 305 \tTraining Loss: 0.001245 \tValidation Loss: 0.304634\n","Epoch: 306 \tTraining Loss: 0.000804 \tValidation Loss: 0.306172\n","Epoch: 307 \tTraining Loss: 0.000880 \tValidation Loss: 0.306678\n","Epoch: 308 \tTraining Loss: 0.000569 \tValidation Loss: 0.303406\n","Epoch: 309 \tTraining Loss: 0.000528 \tValidation Loss: 0.309260\n","Epoch: 310 \tTraining Loss: 0.000560 \tValidation Loss: 0.309833\n","Epoch: 311 \tTraining Loss: 0.000867 \tValidation Loss: 0.307412\n","Epoch: 312 \tTraining Loss: 0.001034 \tValidation Loss: 0.303438\n","Epoch: 313 \tTraining Loss: 0.000592 \tValidation Loss: 0.306928\n","Epoch: 314 \tTraining Loss: 0.000726 \tValidation Loss: 0.312425\n","Epoch: 315 \tTraining Loss: 0.000958 \tValidation Loss: 0.318960\n","Epoch: 316 \tTraining Loss: 0.000646 \tValidation Loss: 0.306060\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-dc7800b86891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# move tensors to GPU if CUDA is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain_on_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \"\"\"\n\u001b[1;32m    131\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"xJNV380i3RaL","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load the Model with the Lowest Validation Loss\n","model.load_state_dict(torch.load('drive/My Drive/CNN/model_augmented.pt'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3iPqrZ44tjyW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"26b327bb-1c82-4857-ccf2-7d859f16816e","executionInfo":{"status":"ok","timestamp":1555616542787,"user_tz":-180,"elapsed":805,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}}},"cell_type":"code","source":["# track test loss\n","test_loss = 0.0\n","class_correct = list(0. for i in range(8))\n","class_total = list(0. for i in range(8))\n","\n","model.eval()\n","# iterate over test data\n","for data, target in test_loader:\n","    # move tensors to GPU if CUDA is available\n","    if train_on_gpu:\n","        data, target = data.cuda(), target.cuda()\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    output = model(data)\n","    # calculate the batch loss\n","    loss = criterion(output, target)\n","    # update test loss \n","    test_loss += loss.item()*data.size(0)\n","    # convert output probabilities to predicted class\n","    _, pred = torch.max(output, 1)    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(target.data.view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    # calculate test accuracy for each object class\n","   # for i in range(batch_size):\n","    for i in range(len(target)):  \n","        label = target.data[i]\n","        class_correct[label] += correct[i].item()\n","        class_total[label] += 1\n","\n","# average test loss\n","test_loss = test_loss/len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","for i in range(8):\n","    if class_total[i] > 0:\n","        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n","            classes[i], 100 * class_correct[i] / class_total[i],\n","            np.sum(class_correct[i]), np.sum(class_total[i])))\n","    else:\n","        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n","\n","print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","    100. * np.sum(class_correct) / np.sum(class_total),\n","    np.sum(class_correct), np.sum(class_total)))\n"],"execution_count":67,"outputs":[{"output_type":"stream","text":["Test Loss: 0.109141\n","\n","Test Accuracy of L1_Standing_still: 88% ( 8/ 9)\n","Test Accuracy of L2_Sitting_and_relaxing: 77% ( 7/ 9)\n","Test Accuracy of L3_Lying_down: 93% (14/15)\n","Test Accuracy of L4_Walking: 80% ( 8/10)\n","Test Accuracy of L5_Climbing_stairs: 70% ( 7/10)\n","Test Accuracy of L9_Cycling: 50% ( 6/12)\n","Test Accuracy of L10_Jogging: 75% (12/16)\n","Test Accuracy of L11_Running: 71% (10/14)\n","\n","Test Accuracy (Overall): 75% (72/95)\n"],"name":"stdout"}]}]}
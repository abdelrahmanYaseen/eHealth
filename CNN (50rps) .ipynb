{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of CNN (50rps).ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"WCrrJRzdYD4l","colab_type":"code","outputId":"29f7f92e-700b-42a0-f847-2729ba150509","executionInfo":{"status":"ok","timestamp":1556808387087,"user_tz":-180,"elapsed":896,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"u4wZPCMoYrrm","colab_type":"code","colab":{}},"cell_type":"code","source":["path='drive/My Drive/CNN without cycling/Images/Subject3_Figures_50rps/'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-bWOxl86i248","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import preprocessing\n","import sys,os\n","import random\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"metadata":{"id":"820nIGaZsqNt","colab_type":"code","outputId":"53b1802b-47f5-472b-a727-e72a9be24f0a","executionInfo":{"status":"ok","timestamp":1556808387453,"user_tz":-180,"elapsed":1141,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# check if CUDA is available\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('CUDA is available!  Training on GPU ...')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CUDA is available!  Training on GPU ...\n"],"name":"stdout"}]},{"metadata":{"id":"muHUxcbmm3ID","colab_type":"code","outputId":"4d83f89c-ae6f-411f-8bdf-6a66b98e0dc0","executionInfo":{"status":"ok","timestamp":1556808387845,"user_tz":-180,"elapsed":1496,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","# number of subprocesses to use for data loading\n","num_workers = 0\n","# how many samples per batch to load\n","batch_size = 16\n","# percentage of training set to use as validation\n","test_size = 0.2\n","valid_size = 0.2\n","train_size = 0.8\n","\n","# convert data to a normalized torch.FloatTensor\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","dataset = datasets.ImageFolder(path, transform = transform)\n","dataloader = torch.utils.data.DataLoader(dataset, shuffle = True)\n","\n","# specify the image classes\n","classes = ['L1_Standing_still','L2_Sitting_and_relaxing', 'L3_Lying_down', 'L4_Walking', 'L5_Climbing_stairs','L9_Cycling', 'L10_Jogging', 'L11_Running']\n","\n","# obtain training indices that will be used for validation\n","num_data = len(dataset)\n","indices = list(range(num_data))\n","np.random.seed(3)\n","np.random.shuffle(indices)\n","split = int(np.floor(test_size * num_data))\n","train_idx1, test_idx = indices[split:], indices[:split]\n","\n","num_train_data = len(dataset) - split\n","split2 = int(np.floor(valid_size * num_train_data))\n","train_idx, valid_idx = train_idx1[split2:], train_idx1[:split2]\n","\n","# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_idx)\n","test_sampler = SubsetRandomSampler(test_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","# prepare data loaders (combine dataset and sampler)\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n","test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=num_workers)\n","valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n","print(len(train_idx))\n","print(len(test_idx))\n","print(len(valid_idx))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["615\n","192\n","153\n"],"name":"stdout"}]},{"metadata":{"id":"X_abZZXgrW9O","colab_type":"code","outputId":"098d97bf-bb6c-4180-eaee-e0a056c691ec","executionInfo":{"status":"ok","timestamp":1556808387847,"user_tz":-180,"elapsed":1461,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","#35\n","# define the CNN architecture\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # convolutional layer (sees 144x32x4 image tensor)\n","        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n","\n","        # convolutional layer (sees 72x16x16 tensor)\n","        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n","        \n","        # convolutional layer (sees 36x8x32 tensor)\n","        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n","        # convolutional layer (sees 18x8x32 tensor)\n","        self.conv4 = nn.Conv2d(32, 64, 3, padding=1)\n","        # max pooling layer\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.pool1 = nn.MaxPool2d(3, 3)\n","        # linear layer (64 * 9 * 9 -> 500)\n","        self.fc1 = nn.Linear(64 * 9 * 9, 3500)\n","        # linear layer (500 -> 10)\n","        self.fc2 = nn.Linear(3500, 1500)\n","       # self.fc3 = nn.Linear(1500,700)\n","       # self.fc4 = nn.Linear(700, 300)\n","        self.fc3 = nn.Linear(1500, 700)\n","        self.fc4 = nn.Linear(700, 8)\n","        # dropout layer (p=0.25)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        # add sequence of convolutional and max pooling layers\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = self.pool(F.relu(self.conv4(x)))\n","        # flatten image input\n","        x = x.view(-1, 64 * 9 * 9)\n","        # add dropout layer\n","        #x = self.dropout(x)\n","        # add 1st hidden layer, with relu activation function\n","        x = F.relu(self.fc1(x))\n","        # add dropout layer\n","        x = self.dropout(x)\n","        # add 2nd hidden layer, with relu activation function\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc3(x))\n","        x = self.dropout(x)\n","       # x = F.relu(self.fc4(x))\n","       # x = self.dropout(x)\n","        x = self.fc4(x)\n","        return x\n","# create a complete CNN\n","model = Net()\n","print(model)\n","# move tensors to GPU if CUDA is available\n","if train_on_gpu:\n","    model.cuda()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (pool1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=5184, out_features=3500, bias=True)\n","  (fc2): Linear(in_features=3500, out_features=1500, bias=True)\n","  (fc3): Linear(in_features=1500, out_features=700, bias=True)\n","  (fc4): Linear(in_features=700, out_features=8, bias=True)\n","  (dropout): Dropout(p=0.2)\n",")\n"],"name":"stdout"}]},{"metadata":{"id":"W6D2ECmUm6s7","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch.optim as optim\n","\n","# specify loss function (categorical cross-entropy)\n","criterion = nn.CrossEntropyLoss()\n","\n","# specify optimizer\n","optimizer = optim.SGD(model.parameters(), lr=0.01)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wBHGXGW-m9mQ","colab_type":"code","outputId":"e55f25f4-48c0-462f-9193-de131cbabe35","executionInfo":{"status":"error","timestamp":1556808841001,"user_tz":-180,"elapsed":454374,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}},"colab":{"base_uri":"https://localhost:8080/","height":2730}},"cell_type":"code","source":["# number of epochs to train the model\n","n_epochs = 1000\n","\n","valid_loss_min = np.Inf # track change in validation loss\n","\n","for epoch in range(1, n_epochs+1):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    \n","    ###################\n","    # train the model #\n","    ###################\n","    model.train()\n","    for data, target in train_loader:\n","        # move tensors to GPU if CUDA is available\n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","        \n","    ######################    \n","    # validate the model #\n","    ######################\n","    model.eval()\n","    for data, target in valid_loader:\n","        # move tensors to GPU if CUDA is available\n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # update average validation loss \n","        valid_loss += loss.item()*data.size(0)\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(train_loader.dataset)\n","    valid_loss = valid_loss/len(valid_loader.dataset)\n","        \n","    # print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        epoch, train_loss, valid_loss))\n","    \n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","       # torch.save(model.state_dict(), 'model_cifar.pt')\n","        torch.save(model.state_dict(), 'drive/My Drive/CNN without cycling/model_augmented2.pt')\n","        valid_loss_min = valid_loss\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 1.332583 \tValidation Loss: 0.331547\n","Validation loss decreased (inf --> 0.331547).  Saving model ...\n","Epoch: 2 \tTraining Loss: 1.332160 \tValidation Loss: 0.331636\n","Epoch: 3 \tTraining Loss: 1.331860 \tValidation Loss: 0.331734\n","Epoch: 4 \tTraining Loss: 1.331540 \tValidation Loss: 0.331817\n","Epoch: 5 \tTraining Loss: 1.331185 \tValidation Loss: 0.331906\n","Epoch: 6 \tTraining Loss: 1.331154 \tValidation Loss: 0.331971\n","Epoch: 7 \tTraining Loss: 1.330937 \tValidation Loss: 0.332047\n","Epoch: 8 \tTraining Loss: 1.330832 \tValidation Loss: 0.332116\n","Epoch: 9 \tTraining Loss: 1.330603 \tValidation Loss: 0.332169\n","Epoch: 10 \tTraining Loss: 1.330338 \tValidation Loss: 0.332240\n","Epoch: 11 \tTraining Loss: 1.330459 \tValidation Loss: 0.332301\n","Epoch: 12 \tTraining Loss: 1.330314 \tValidation Loss: 0.332377\n","Epoch: 13 \tTraining Loss: 1.330116 \tValidation Loss: 0.332446\n","Epoch: 14 \tTraining Loss: 1.330099 \tValidation Loss: 0.332503\n","Epoch: 15 \tTraining Loss: 1.329876 \tValidation Loss: 0.332533\n","Epoch: 16 \tTraining Loss: 1.329813 \tValidation Loss: 0.332567\n","Epoch: 17 \tTraining Loss: 1.329614 \tValidation Loss: 0.332633\n","Epoch: 18 \tTraining Loss: 1.329813 \tValidation Loss: 0.332691\n","Epoch: 19 \tTraining Loss: 1.329425 \tValidation Loss: 0.332727\n","Epoch: 20 \tTraining Loss: 1.329727 \tValidation Loss: 0.332767\n","Epoch: 21 \tTraining Loss: 1.329545 \tValidation Loss: 0.332815\n","Epoch: 22 \tTraining Loss: 1.329409 \tValidation Loss: 0.332866\n","Epoch: 23 \tTraining Loss: 1.329378 \tValidation Loss: 0.332892\n","Epoch: 24 \tTraining Loss: 1.329228 \tValidation Loss: 0.332908\n","Epoch: 25 \tTraining Loss: 1.329534 \tValidation Loss: 0.332933\n","Epoch: 26 \tTraining Loss: 1.329404 \tValidation Loss: 0.332940\n","Epoch: 27 \tTraining Loss: 1.329372 \tValidation Loss: 0.332960\n","Epoch: 28 \tTraining Loss: 1.329069 \tValidation Loss: 0.332979\n","Epoch: 29 \tTraining Loss: 1.329213 \tValidation Loss: 0.332990\n","Epoch: 30 \tTraining Loss: 1.328915 \tValidation Loss: 0.332990\n","Epoch: 31 \tTraining Loss: 1.328975 \tValidation Loss: 0.333021\n","Epoch: 32 \tTraining Loss: 1.328898 \tValidation Loss: 0.333018\n","Epoch: 33 \tTraining Loss: 1.328795 \tValidation Loss: 0.333005\n","Epoch: 34 \tTraining Loss: 1.328751 \tValidation Loss: 0.333035\n","Epoch: 35 \tTraining Loss: 1.328741 \tValidation Loss: 0.333026\n","Epoch: 36 \tTraining Loss: 1.328667 \tValidation Loss: 0.333026\n","Epoch: 37 \tTraining Loss: 1.328431 \tValidation Loss: 0.332999\n","Epoch: 38 \tTraining Loss: 1.328491 \tValidation Loss: 0.332992\n","Epoch: 39 \tTraining Loss: 1.328310 \tValidation Loss: 0.332960\n","Epoch: 40 \tTraining Loss: 1.328216 \tValidation Loss: 0.332932\n","Epoch: 41 \tTraining Loss: 1.328171 \tValidation Loss: 0.332898\n","Epoch: 42 \tTraining Loss: 1.327913 \tValidation Loss: 0.332870\n","Epoch: 43 \tTraining Loss: 1.327907 \tValidation Loss: 0.332812\n","Epoch: 44 \tTraining Loss: 1.327843 \tValidation Loss: 0.332777\n","Epoch: 45 \tTraining Loss: 1.327445 \tValidation Loss: 0.332736\n","Epoch: 46 \tTraining Loss: 1.327319 \tValidation Loss: 0.332664\n","Epoch: 47 \tTraining Loss: 1.327045 \tValidation Loss: 0.332583\n","Epoch: 48 \tTraining Loss: 1.326696 \tValidation Loss: 0.332497\n","Epoch: 49 \tTraining Loss: 1.326478 \tValidation Loss: 0.332396\n","Epoch: 50 \tTraining Loss: 1.326010 \tValidation Loss: 0.332275\n","Epoch: 51 \tTraining Loss: 1.325672 \tValidation Loss: 0.332150\n","Epoch: 52 \tTraining Loss: 1.325157 \tValidation Loss: 0.331967\n","Epoch: 53 \tTraining Loss: 1.324672 \tValidation Loss: 0.331781\n","Epoch: 54 \tTraining Loss: 1.323712 \tValidation Loss: 0.331479\n","Validation loss decreased (0.331547 --> 0.331479).  Saving model ...\n","Epoch: 55 \tTraining Loss: 1.322625 \tValidation Loss: 0.331160\n","Validation loss decreased (0.331479 --> 0.331160).  Saving model ...\n","Epoch: 56 \tTraining Loss: 1.321596 \tValidation Loss: 0.330687\n","Validation loss decreased (0.331160 --> 0.330687).  Saving model ...\n","Epoch: 57 \tTraining Loss: 1.319954 \tValidation Loss: 0.330195\n","Validation loss decreased (0.330687 --> 0.330195).  Saving model ...\n","Epoch: 58 \tTraining Loss: 1.318413 \tValidation Loss: 0.329535\n","Validation loss decreased (0.330195 --> 0.329535).  Saving model ...\n","Epoch: 59 \tTraining Loss: 1.316036 \tValidation Loss: 0.328561\n","Validation loss decreased (0.329535 --> 0.328561).  Saving model ...\n","Epoch: 60 \tTraining Loss: 1.312481 \tValidation Loss: 0.327264\n","Validation loss decreased (0.328561 --> 0.327264).  Saving model ...\n","Epoch: 61 \tTraining Loss: 1.306683 \tValidation Loss: 0.325332\n","Validation loss decreased (0.327264 --> 0.325332).  Saving model ...\n","Epoch: 62 \tTraining Loss: 1.299524 \tValidation Loss: 0.322085\n","Validation loss decreased (0.325332 --> 0.322085).  Saving model ...\n","Epoch: 63 \tTraining Loss: 1.285073 \tValidation Loss: 0.316114\n","Validation loss decreased (0.322085 --> 0.316114).  Saving model ...\n","Epoch: 64 \tTraining Loss: 1.255668 \tValidation Loss: 0.302000\n","Validation loss decreased (0.316114 --> 0.302000).  Saving model ...\n","Epoch: 65 \tTraining Loss: 1.186300 \tValidation Loss: 0.273868\n","Validation loss decreased (0.302000 --> 0.273868).  Saving model ...\n","Epoch: 66 \tTraining Loss: 1.102611 \tValidation Loss: 0.255105\n","Validation loss decreased (0.273868 --> 0.255105).  Saving model ...\n","Epoch: 67 \tTraining Loss: 1.050261 \tValidation Loss: 0.245822\n","Validation loss decreased (0.255105 --> 0.245822).  Saving model ...\n","Epoch: 68 \tTraining Loss: 1.023478 \tValidation Loss: 0.253504\n","Epoch: 69 \tTraining Loss: 1.009630 \tValidation Loss: 0.266908\n","Epoch: 70 \tTraining Loss: 1.016534 \tValidation Loss: 0.234497\n","Validation loss decreased (0.245822 --> 0.234497).  Saving model ...\n","Epoch: 71 \tTraining Loss: 0.971720 \tValidation Loss: 0.248367\n","Epoch: 72 \tTraining Loss: 0.949780 \tValidation Loss: 0.240140\n","Epoch: 73 \tTraining Loss: 0.971191 \tValidation Loss: 0.220329\n","Validation loss decreased (0.234497 --> 0.220329).  Saving model ...\n","Epoch: 74 \tTraining Loss: 0.918953 \tValidation Loss: 0.250818\n","Epoch: 75 \tTraining Loss: 0.928340 \tValidation Loss: 0.272938\n","Epoch: 76 \tTraining Loss: 0.897502 \tValidation Loss: 0.214924\n","Validation loss decreased (0.220329 --> 0.214924).  Saving model ...\n","Epoch: 77 \tTraining Loss: 0.837645 \tValidation Loss: 0.205239\n","Validation loss decreased (0.214924 --> 0.205239).  Saving model ...\n","Epoch: 78 \tTraining Loss: 0.844544 \tValidation Loss: 0.221356\n","Epoch: 79 \tTraining Loss: 0.787316 \tValidation Loss: 0.225565\n","Epoch: 80 \tTraining Loss: 0.793110 \tValidation Loss: 0.174685\n","Validation loss decreased (0.205239 --> 0.174685).  Saving model ...\n","Epoch: 81 \tTraining Loss: 0.685195 \tValidation Loss: 0.198517\n","Epoch: 82 \tTraining Loss: 0.660632 \tValidation Loss: 0.282661\n","Epoch: 83 \tTraining Loss: 0.638133 \tValidation Loss: 0.150258\n","Validation loss decreased (0.174685 --> 0.150258).  Saving model ...\n","Epoch: 84 \tTraining Loss: 0.612397 \tValidation Loss: 0.152657\n","Epoch: 85 \tTraining Loss: 0.588452 \tValidation Loss: 0.155699\n","Epoch: 86 \tTraining Loss: 0.528084 \tValidation Loss: 0.150028\n","Validation loss decreased (0.150258 --> 0.150028).  Saving model ...\n","Epoch: 87 \tTraining Loss: 0.514121 \tValidation Loss: 0.144885\n","Validation loss decreased (0.150028 --> 0.144885).  Saving model ...\n","Epoch: 88 \tTraining Loss: 0.520446 \tValidation Loss: 0.137262\n","Validation loss decreased (0.144885 --> 0.137262).  Saving model ...\n","Epoch: 89 \tTraining Loss: 0.461210 \tValidation Loss: 0.136697\n","Validation loss decreased (0.137262 --> 0.136697).  Saving model ...\n","Epoch: 90 \tTraining Loss: 0.472788 \tValidation Loss: 0.136987\n","Epoch: 91 \tTraining Loss: 0.446052 \tValidation Loss: 0.139052\n","Epoch: 92 \tTraining Loss: 0.438315 \tValidation Loss: 0.133916\n","Validation loss decreased (0.136697 --> 0.133916).  Saving model ...\n","Epoch: 93 \tTraining Loss: 0.392261 \tValidation Loss: 0.145415\n","Epoch: 94 \tTraining Loss: 0.391178 \tValidation Loss: 0.137725\n","Epoch: 95 \tTraining Loss: 0.390119 \tValidation Loss: 0.153511\n","Epoch: 96 \tTraining Loss: 0.366534 \tValidation Loss: 0.210118\n","Epoch: 97 \tTraining Loss: 0.366690 \tValidation Loss: 0.138522\n","Epoch: 98 \tTraining Loss: 0.338538 \tValidation Loss: 0.131472\n","Validation loss decreased (0.133916 --> 0.131472).  Saving model ...\n","Epoch: 99 \tTraining Loss: 0.338640 \tValidation Loss: 0.170510\n","Epoch: 100 \tTraining Loss: 0.309479 \tValidation Loss: 0.206246\n","Epoch: 101 \tTraining Loss: 0.339754 \tValidation Loss: 0.141187\n","Epoch: 102 \tTraining Loss: 0.294361 \tValidation Loss: 0.156484\n","Epoch: 103 \tTraining Loss: 0.274152 \tValidation Loss: 0.170250\n","Epoch: 104 \tTraining Loss: 0.286002 \tValidation Loss: 0.207916\n","Epoch: 105 \tTraining Loss: 0.267863 \tValidation Loss: 0.117879\n","Validation loss decreased (0.131472 --> 0.117879).  Saving model ...\n","Epoch: 106 \tTraining Loss: 0.237693 \tValidation Loss: 0.116090\n","Validation loss decreased (0.117879 --> 0.116090).  Saving model ...\n","Epoch: 107 \tTraining Loss: 0.267242 \tValidation Loss: 0.143439\n","Epoch: 108 \tTraining Loss: 0.242548 \tValidation Loss: 0.142753\n","Epoch: 109 \tTraining Loss: 0.229199 \tValidation Loss: 0.136090\n","Epoch: 110 \tTraining Loss: 0.237251 \tValidation Loss: 0.175792\n","Epoch: 111 \tTraining Loss: 0.233886 \tValidation Loss: 0.127731\n","Epoch: 112 \tTraining Loss: 0.228713 \tValidation Loss: 0.142178\n","Epoch: 113 \tTraining Loss: 0.190112 \tValidation Loss: 0.143288\n","Epoch: 114 \tTraining Loss: 0.206980 \tValidation Loss: 0.139731\n","Epoch: 115 \tTraining Loss: 0.206888 \tValidation Loss: 0.189974\n","Epoch: 116 \tTraining Loss: 0.186772 \tValidation Loss: 0.171779\n","Epoch: 117 \tTraining Loss: 0.207198 \tValidation Loss: 0.153389\n","Epoch: 118 \tTraining Loss: 0.190019 \tValidation Loss: 0.192619\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-163-584b15923fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# update training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"xJNV380i3RaL","colab_type":"code","colab":{}},"cell_type":"code","source":["#Load the Model with the Lowest Validation Loss\n","model.load_state_dict(torch.load('drive/My Drive/CNN without cycling/model_augmented2.pt'))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w5yhlPXn4dUk","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"3iPqrZ44tjyW","colab_type":"code","outputId":"28b2ba2d-e995-4ffa-b69b-f97cae752447","executionInfo":{"status":"ok","timestamp":1556808881704,"user_tz":-180,"elapsed":1322,"user":{"displayName":"batoul alhassany","photoUrl":"","userId":"07438840849110152279"}},"colab":{"base_uri":"https://localhost:8080/","height":1020}},"cell_type":"code","source":["# track test loss\n","test_loss = 0.0\n","class_correct = list(0. for i in range(8))\n","class_total = list(0. for i in range(8))\n","\n","model.eval()\n","# iterate over test data\n","for data, target in test_loader:\n","    # move tensors to GPU if CUDA is available\n","    if train_on_gpu:\n","        data, target = data.cuda(), target.cuda()\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    output = model(data)\n","    # calculate the batch loss\n","    loss = criterion(output, target)\n","    # update test loss \n","    test_loss += loss.item()*data.size(0)\n","    # convert output probabilities to predicted class\n","    _, pred = torch.max(output, 1)    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(target.data.view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    # calculate test accuracy for each object class\n","   # for i in range(batch_size):\n","    for i in range(len(target)):\n","        label = target.data[i]\n","        if(pred[i] == label):\n","           class_correct[label] += 1\n","        else:\n","           print(label,pred[i]) \n","        class_total[label] += 1\n","\n","# average test loss\n","test_loss = test_loss/len(test_loader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","for i in range(8):\n","    if class_total[i] > 0:\n","        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n","            classes[i], 100 * class_correct[i] / class_total[i],\n","            np.sum(class_correct[i]), np.sum(class_total[i])))\n","    else:\n","        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n","\n","print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","    100. * np.sum(class_correct) / np.sum(class_total),\n","    np.sum(class_correct), np.sum(class_total)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(4, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(0, device='cuda:0') tensor(1, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(2, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(7, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(7, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(7, device='cuda:0')\n","tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n","tensor(7, device='cuda:0') tensor(5, device='cuda:0')\n","tensor(2, device='cuda:0') tensor(4, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(3, device='cuda:0') tensor(2, device='cuda:0')\n","tensor(0, device='cuda:0') tensor(6, device='cuda:0')\n","tensor(2, device='cuda:0') tensor(5, device='cuda:0')\n","tensor(7, device='cuda:0') tensor(2, device='cuda:0')\n","tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n","tensor(4, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(2, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(2, device='cuda:0') tensor(5, device='cuda:0')\n","tensor(2, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(4, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(4, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(7, device='cuda:0') tensor(5, device='cuda:0')\n","tensor(0, device='cuda:0') tensor(1, device='cuda:0')\n","tensor(7, device='cuda:0') tensor(5, device='cuda:0')\n","tensor(6, device='cuda:0') tensor(0, device='cuda:0')\n","tensor(3, device='cuda:0') tensor(4, device='cuda:0')\n","tensor(0, device='cuda:0') tensor(1, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(7, device='cuda:0')\n","tensor(0, device='cuda:0') tensor(1, device='cuda:0')\n","tensor(0, device='cuda:0') tensor(1, device='cuda:0')\n","tensor(7, device='cuda:0') tensor(5, device='cuda:0')\n","tensor(0, device='cuda:0') tensor(1, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(2, device='cuda:0')\n","tensor(4, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(4, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(7, device='cuda:0') tensor(5, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(2, device='cuda:0')\n","tensor(7, device='cuda:0') tensor(5, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(7, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(7, device='cuda:0')\n","tensor(1, device='cuda:0') tensor(0, device='cuda:0')\n","tensor(3, device='cuda:0') tensor(4, device='cuda:0')\n","tensor(0, device='cuda:0') tensor(1, device='cuda:0')\n","tensor(4, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(4, device='cuda:0') tensor(3, device='cuda:0')\n","tensor(5, device='cuda:0') tensor(7, device='cuda:0')\n","Test Loss: 0.114665\n","\n","Test Accuracy of L1_Standing_still: 75% (24/32)\n","Test Accuracy of L2_Sitting_and_relaxing: 90% (27/30)\n","Test Accuracy of L3_Lying_down: 76% (16/21)\n","Test Accuracy of L4_Walking: 84% (16/19)\n","Test Accuracy of L5_Climbing_stairs: 65% (15/23)\n","Test Accuracy of L9_Cycling: 55% (15/27)\n","Test Accuracy of L10_Jogging: 96% (24/25)\n","Test Accuracy of L11_Running: 53% ( 8/15)\n","\n","Test Accuracy (Overall): 75% (145/192)\n"],"name":"stdout"}]}]}
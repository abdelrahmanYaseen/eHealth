{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# #preparing the data : Given a file, the function prepares 2 arrays\n",
    "# \"\"\"\n",
    "# feature_set : each element is an array of 50 readings\n",
    "# targets : each element is a label corrisponding to the readings with the same index in feature_set\n",
    "# \"\"\"\n",
    "# def getSamplesFromFiles(filesNames=[\"mHealth_ECGProcessed_subject3.log\"],readingsPerSample=50):\n",
    "#     targets  = np.array([])\n",
    "#     feature_set= np.array([])\n",
    "#     for fileName in filesNames:\n",
    "#         df = pd.read_csv(fileName, header=None, delim_whitespace=True)\n",
    "#         for i in range(int(df.shape[0]/readingsPerSample)):\n",
    "#             if df[2][i*readingsPerSample:i*readingsPerSample+readingsPerSample].mean() in range(1,13):\n",
    "#                 feature_set=np.append(feature_set,[df[1][i*readingsPerSample:i*readingsPerSample+readingsPerSample]])\n",
    "#                 targets=np.append(targets,df[2][i*readingsPerSample:i*readingsPerSample+readingsPerSample].mean()-1)\n",
    "\n",
    "#     feature_set=feature_set.reshape(int(feature_set.shape[0]/readingsPerSample),readingsPerSample)\n",
    "# #     one_hot_labels=one_hot_labels.reshape(int(one_hot_labels.shape[0]/12),12)\n",
    "#     return feature_set[0:800], targets[0:800], feature_set[800:1000], targets[800:1000], feature_set[1000:1340], targets[1000:1340]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the data : Given a file, the function prepares 2 arrays\n",
    "\"\"\"\n",
    "feature_set : each element is an array of 50 readings\n",
    "targets : each element is a label corrisponding to the readings with the same index in feature_set\n",
    "\"\"\"\n",
    "def getSamplesFromFiles(filesNames=[\"mHealth_ECGProcessed_subject3.log\"],readingsPerSample=50):\n",
    "    targets  = np.array([])\n",
    "    feature_set= np.array([])\n",
    "    for fileName in filesNames:\n",
    "        df = pd.read_csv(fileName, header=None, delim_whitespace=True)\n",
    "        for i in range(int(df.shape[0]/readingsPerSample)):\n",
    "            if df[2][i*readingsPerSample:i*readingsPerSample+readingsPerSample].mean() in range(1,13):\n",
    "                feature_set=np.append(feature_set,[df[1][i*readingsPerSample:i*readingsPerSample+readingsPerSample]])\n",
    "                targets=np.append(targets,df[2][i*readingsPerSample:i*readingsPerSample+readingsPerSample].mean()-1)\n",
    "\n",
    "    feature_set=feature_set.reshape(int(feature_set.shape[0]/readingsPerSample),readingsPerSample)\n",
    "#     one_hot_labels=one_hot_labels.reshape(int(one_hot_labels.shape[0]/12),12)\n",
    "    return feature_set, targets\n",
    "def unison_shuffled_copies(a, b,seed=1):\n",
    "    assert len(a) == len(b)\n",
    "    np.random.seed(seed)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # number of hidden nodes in each layer (512)\n",
    "        hidden_1 = 50\n",
    "#         hidden_2 = 33\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(100, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, 12)\n",
    "#         self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "#         self.fc3 = nn.Linear(hidden_2, 12)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "#         x = x.view(-1, 50)\n",
    "#         print('XXXXXXXXXXXXX',x.shape,'\\n',x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "#         x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "#         x = self.fc3(x)        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This piece of code does the following:\n",
    "    1- retrieves the data from the specified files with the specified grouping number \n",
    "        (this grouping number must match the number of input nodes in the NN)\n",
    "    2- shuffles the data (shuffles both the target and the feature set in the same way)\n",
    "    3- separates the data into Test, Train sets given a percentage\n",
    "    4- separates the Train data into Train and Val\n",
    "\"\"\"\n",
    "\n",
    "feature_set,targets=getSamplesFromFiles([\"mHealth_ECGProcessed_subject3.log\",\"mHealth_ECGProcessed_subject4.log\",\"mHealth_ECGProcessed_subject7.log\",\"mHealth_ECGProcessed_subject10.log\"],100)\n",
    "feature_set,targets=unison_shuffled_copies(feature_set,targets)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_set, targets, test_size=0.33, random_state=42)\n",
    "feature_set, validation_feature_set, targets, validation_targets = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180,)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set.shape\n",
    "validation_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.491971 \tValidation Loss: 2.499911\n",
      "Validation loss decreased (inf --> 2.499911).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 2.487505 \tValidation Loss: 2.499706\n",
      "Validation loss decreased (2.499911 --> 2.499706).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 2.495937 \tValidation Loss: 2.499478\n",
      "Validation loss decreased (2.499706 --> 2.499478).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 2.487893 \tValidation Loss: 2.499262\n",
      "Validation loss decreased (2.499478 --> 2.499262).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 2.488603 \tValidation Loss: 2.498989\n",
      "Validation loss decreased (2.499262 --> 2.498989).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 2.495399 \tValidation Loss: 2.498718\n",
      "Validation loss decreased (2.498989 --> 2.498718).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 2.488120 \tValidation Loss: 2.498438\n",
      "Validation loss decreased (2.498718 --> 2.498438).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 2.474117 \tValidation Loss: 2.498157\n",
      "Validation loss decreased (2.498438 --> 2.498157).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 2.483326 \tValidation Loss: 2.497921\n",
      "Validation loss decreased (2.498157 --> 2.497921).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 2.501931 \tValidation Loss: 2.497670\n",
      "Validation loss decreased (2.497921 --> 2.497670).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 2.492611 \tValidation Loss: 2.497476\n",
      "Validation loss decreased (2.497670 --> 2.497476).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 2.485122 \tValidation Loss: 2.497246\n",
      "Validation loss decreased (2.497476 --> 2.497246).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 2.481594 \tValidation Loss: 2.497080\n",
      "Validation loss decreased (2.497246 --> 2.497080).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 2.488350 \tValidation Loss: 2.496833\n",
      "Validation loss decreased (2.497080 --> 2.496833).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 2.487205 \tValidation Loss: 2.496662\n",
      "Validation loss decreased (2.496833 --> 2.496662).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 2.494393 \tValidation Loss: 2.496492\n",
      "Validation loss decreased (2.496662 --> 2.496492).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 2.478866 \tValidation Loss: 2.496354\n",
      "Validation loss decreased (2.496492 --> 2.496354).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 2.494558 \tValidation Loss: 2.496153\n",
      "Validation loss decreased (2.496354 --> 2.496153).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 2.488978 \tValidation Loss: 2.495941\n",
      "Validation loss decreased (2.496153 --> 2.495941).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 2.479498 \tValidation Loss: 2.495722\n",
      "Validation loss decreased (2.495941 --> 2.495722).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 2.491093 \tValidation Loss: 2.495487\n",
      "Validation loss decreased (2.495722 --> 2.495487).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 2.481487 \tValidation Loss: 2.495325\n",
      "Validation loss decreased (2.495487 --> 2.495325).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 2.490182 \tValidation Loss: 2.495140\n",
      "Validation loss decreased (2.495325 --> 2.495140).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 2.475863 \tValidation Loss: 2.495021\n",
      "Validation loss decreased (2.495140 --> 2.495021).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 2.488946 \tValidation Loss: 2.494843\n",
      "Validation loss decreased (2.495021 --> 2.494843).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 2.481211 \tValidation Loss: 2.494753\n",
      "Validation loss decreased (2.494843 --> 2.494753).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 2.472565 \tValidation Loss: 2.494519\n",
      "Validation loss decreased (2.494753 --> 2.494519).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 2.479938 \tValidation Loss: 2.494360\n",
      "Validation loss decreased (2.494519 --> 2.494360).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 2.480502 \tValidation Loss: 2.494236\n",
      "Validation loss decreased (2.494360 --> 2.494236).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 2.484793 \tValidation Loss: 2.494046\n",
      "Validation loss decreased (2.494236 --> 2.494046).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 2.482288 \tValidation Loss: 2.493897\n",
      "Validation loss decreased (2.494046 --> 2.493897).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 2.473225 \tValidation Loss: 2.493779\n",
      "Validation loss decreased (2.493897 --> 2.493779).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 2.491191 \tValidation Loss: 2.493630\n",
      "Validation loss decreased (2.493779 --> 2.493630).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 2.486330 \tValidation Loss: 2.493479\n",
      "Validation loss decreased (2.493630 --> 2.493479).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 2.476610 \tValidation Loss: 2.493396\n",
      "Validation loss decreased (2.493479 --> 2.493396).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 2.480914 \tValidation Loss: 2.493214\n",
      "Validation loss decreased (2.493396 --> 2.493214).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 2.483775 \tValidation Loss: 2.493008\n",
      "Validation loss decreased (2.493214 --> 2.493008).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 2.478402 \tValidation Loss: 2.492858\n",
      "Validation loss decreased (2.493008 --> 2.492858).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 2.480641 \tValidation Loss: 2.492738\n",
      "Validation loss decreased (2.492858 --> 2.492738).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 2.486860 \tValidation Loss: 2.492580\n",
      "Validation loss decreased (2.492738 --> 2.492580).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 2.483121 \tValidation Loss: 2.492424\n",
      "Validation loss decreased (2.492580 --> 2.492424).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 2.484980 \tValidation Loss: 2.492363\n",
      "Validation loss decreased (2.492424 --> 2.492363).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 2.487599 \tValidation Loss: 2.492292\n",
      "Validation loss decreased (2.492363 --> 2.492292).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 2.484672 \tValidation Loss: 2.492191\n",
      "Validation loss decreased (2.492292 --> 2.492191).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 2.487821 \tValidation Loss: 2.492036\n",
      "Validation loss decreased (2.492191 --> 2.492036).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 2.483007 \tValidation Loss: 2.492037\n",
      "Epoch: 47 \tTraining Loss: 2.484834 \tValidation Loss: 2.491924\n",
      "Validation loss decreased (2.492036 --> 2.491924).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 2.484115 \tValidation Loss: 2.491868\n",
      "Validation loss decreased (2.491924 --> 2.491868).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 2.478400 \tValidation Loss: 2.491782\n",
      "Validation loss decreased (2.491868 --> 2.491782).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 2.477143 \tValidation Loss: 2.491628\n",
      "Validation loss decreased (2.491782 --> 2.491628).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 2.483497 \tValidation Loss: 2.491520\n",
      "Validation loss decreased (2.491628 --> 2.491520).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 2.477969 \tValidation Loss: 2.491430\n",
      "Validation loss decreased (2.491520 --> 2.491430).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 2.480144 \tValidation Loss: 2.491291\n",
      "Validation loss decreased (2.491430 --> 2.491291).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 2.484247 \tValidation Loss: 2.491148\n",
      "Validation loss decreased (2.491291 --> 2.491148).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 2.481225 \tValidation Loss: 2.491035\n",
      "Validation loss decreased (2.491148 --> 2.491035).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 2.472874 \tValidation Loss: 2.490885\n",
      "Validation loss decreased (2.491035 --> 2.490885).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 2.474856 \tValidation Loss: 2.490759\n",
      "Validation loss decreased (2.490885 --> 2.490759).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 2.477102 \tValidation Loss: 2.490687\n",
      "Validation loss decreased (2.490759 --> 2.490687).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 2.480105 \tValidation Loss: 2.490619\n",
      "Validation loss decreased (2.490687 --> 2.490619).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 2.481025 \tValidation Loss: 2.490455\n",
      "Validation loss decreased (2.490619 --> 2.490455).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 2.468923 \tValidation Loss: 2.490305\n",
      "Validation loss decreased (2.490455 --> 2.490305).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 2.480198 \tValidation Loss: 2.490147\n",
      "Validation loss decreased (2.490305 --> 2.490147).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 2.480449 \tValidation Loss: 2.489963\n",
      "Validation loss decreased (2.490147 --> 2.489963).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 2.478254 \tValidation Loss: 2.489824\n",
      "Validation loss decreased (2.489963 --> 2.489824).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 2.481777 \tValidation Loss: 2.489718\n",
      "Validation loss decreased (2.489824 --> 2.489718).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 2.478605 \tValidation Loss: 2.489546\n",
      "Validation loss decreased (2.489718 --> 2.489546).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 2.481001 \tValidation Loss: 2.489419\n",
      "Validation loss decreased (2.489546 --> 2.489419).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 2.482987 \tValidation Loss: 2.489359\n",
      "Validation loss decreased (2.489419 --> 2.489359).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 2.485536 \tValidation Loss: 2.489205\n",
      "Validation loss decreased (2.489359 --> 2.489205).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 2.481979 \tValidation Loss: 2.489153\n",
      "Validation loss decreased (2.489205 --> 2.489153).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 2.480505 \tValidation Loss: 2.489088\n",
      "Validation loss decreased (2.489153 --> 2.489088).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 2.477144 \tValidation Loss: 2.488984\n",
      "Validation loss decreased (2.489088 --> 2.488984).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 2.477455 \tValidation Loss: 2.488889\n",
      "Validation loss decreased (2.488984 --> 2.488889).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 2.478918 \tValidation Loss: 2.488763\n",
      "Validation loss decreased (2.488889 --> 2.488763).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 \tTraining Loss: 2.473979 \tValidation Loss: 2.488647\n",
      "Validation loss decreased (2.488763 --> 2.488647).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 2.472263 \tValidation Loss: 2.488536\n",
      "Validation loss decreased (2.488647 --> 2.488536).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 2.482255 \tValidation Loss: 2.488389\n",
      "Validation loss decreased (2.488536 --> 2.488389).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 2.474040 \tValidation Loss: 2.488345\n",
      "Validation loss decreased (2.488389 --> 2.488345).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 2.477711 \tValidation Loss: 2.488196\n",
      "Validation loss decreased (2.488345 --> 2.488196).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 2.469253 \tValidation Loss: 2.488054\n",
      "Validation loss decreased (2.488196 --> 2.488054).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 2.477071 \tValidation Loss: 2.487851\n",
      "Validation loss decreased (2.488054 --> 2.487851).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 2.474900 \tValidation Loss: 2.487756\n",
      "Validation loss decreased (2.487851 --> 2.487756).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 2.474005 \tValidation Loss: 2.487759\n",
      "Epoch: 84 \tTraining Loss: 2.477226 \tValidation Loss: 2.487649\n",
      "Validation loss decreased (2.487756 --> 2.487649).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 2.474608 \tValidation Loss: 2.487585\n",
      "Validation loss decreased (2.487649 --> 2.487585).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 2.475652 \tValidation Loss: 2.487455\n",
      "Validation loss decreased (2.487585 --> 2.487455).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 2.485442 \tValidation Loss: 2.487389\n",
      "Validation loss decreased (2.487455 --> 2.487389).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 2.479845 \tValidation Loss: 2.487391\n",
      "Epoch: 89 \tTraining Loss: 2.479546 \tValidation Loss: 2.487340\n",
      "Validation loss decreased (2.487389 --> 2.487340).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 2.480569 \tValidation Loss: 2.487227\n",
      "Validation loss decreased (2.487340 --> 2.487227).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 2.477519 \tValidation Loss: 2.487163\n",
      "Validation loss decreased (2.487227 --> 2.487163).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 2.483207 \tValidation Loss: 2.487046\n",
      "Validation loss decreased (2.487163 --> 2.487046).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 2.458062 \tValidation Loss: 2.486968\n",
      "Validation loss decreased (2.487046 --> 2.486968).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 2.476233 \tValidation Loss: 2.486938\n",
      "Validation loss decreased (2.486968 --> 2.486938).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 2.479023 \tValidation Loss: 2.486877\n",
      "Validation loss decreased (2.486938 --> 2.486877).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 2.480405 \tValidation Loss: 2.486848\n",
      "Validation loss decreased (2.486877 --> 2.486848).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 2.470073 \tValidation Loss: 2.486832\n",
      "Validation loss decreased (2.486848 --> 2.486832).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 2.480035 \tValidation Loss: 2.486741\n",
      "Validation loss decreased (2.486832 --> 2.486741).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 2.466918 \tValidation Loss: 2.486637\n",
      "Validation loss decreased (2.486741 --> 2.486637).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 2.472882 \tValidation Loss: 2.486522\n",
      "Validation loss decreased (2.486637 --> 2.486522).  Saving model ...\n",
      "Epoch: 101 \tTraining Loss: 2.481366 \tValidation Loss: 2.486428\n",
      "Validation loss decreased (2.486522 --> 2.486428).  Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 2.483696 \tValidation Loss: 2.486313\n",
      "Validation loss decreased (2.486428 --> 2.486313).  Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 2.484037 \tValidation Loss: 2.486177\n",
      "Validation loss decreased (2.486313 --> 2.486177).  Saving model ...\n",
      "Epoch: 104 \tTraining Loss: 2.468117 \tValidation Loss: 2.486066\n",
      "Validation loss decreased (2.486177 --> 2.486066).  Saving model ...\n",
      "Epoch: 105 \tTraining Loss: 2.478830 \tValidation Loss: 2.485990\n",
      "Validation loss decreased (2.486066 --> 2.485990).  Saving model ...\n",
      "Epoch: 106 \tTraining Loss: 2.470205 \tValidation Loss: 2.485874\n",
      "Validation loss decreased (2.485990 --> 2.485874).  Saving model ...\n",
      "Epoch: 107 \tTraining Loss: 2.480208 \tValidation Loss: 2.485793\n",
      "Validation loss decreased (2.485874 --> 2.485793).  Saving model ...\n",
      "Epoch: 108 \tTraining Loss: 2.474724 \tValidation Loss: 2.485723\n",
      "Validation loss decreased (2.485793 --> 2.485723).  Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 2.482878 \tValidation Loss: 2.485667\n",
      "Validation loss decreased (2.485723 --> 2.485667).  Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 2.469466 \tValidation Loss: 2.485535\n",
      "Validation loss decreased (2.485667 --> 2.485535).  Saving model ...\n",
      "Epoch: 111 \tTraining Loss: 2.480083 \tValidation Loss: 2.485410\n",
      "Validation loss decreased (2.485535 --> 2.485410).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 2.478375 \tValidation Loss: 2.485281\n",
      "Validation loss decreased (2.485410 --> 2.485281).  Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 2.482034 \tValidation Loss: 2.485199\n",
      "Validation loss decreased (2.485281 --> 2.485199).  Saving model ...\n",
      "Epoch: 114 \tTraining Loss: 2.468693 \tValidation Loss: 2.485087\n",
      "Validation loss decreased (2.485199 --> 2.485087).  Saving model ...\n",
      "Epoch: 115 \tTraining Loss: 2.477059 \tValidation Loss: 2.485073\n",
      "Validation loss decreased (2.485087 --> 2.485073).  Saving model ...\n",
      "Epoch: 116 \tTraining Loss: 2.481315 \tValidation Loss: 2.485029\n",
      "Validation loss decreased (2.485073 --> 2.485029).  Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 2.489807 \tValidation Loss: 2.484906\n",
      "Validation loss decreased (2.485029 --> 2.484906).  Saving model ...\n",
      "Epoch: 118 \tTraining Loss: 2.476979 \tValidation Loss: 2.484835\n",
      "Validation loss decreased (2.484906 --> 2.484835).  Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 2.476223 \tValidation Loss: 2.484749\n",
      "Validation loss decreased (2.484835 --> 2.484749).  Saving model ...\n",
      "Epoch: 120 \tTraining Loss: 2.476602 \tValidation Loss: 2.484625\n",
      "Validation loss decreased (2.484749 --> 2.484625).  Saving model ...\n",
      "Epoch: 121 \tTraining Loss: 2.467959 \tValidation Loss: 2.484513\n",
      "Validation loss decreased (2.484625 --> 2.484513).  Saving model ...\n",
      "Epoch: 122 \tTraining Loss: 2.481863 \tValidation Loss: 2.484478\n",
      "Validation loss decreased (2.484513 --> 2.484478).  Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 2.473212 \tValidation Loss: 2.484426\n",
      "Validation loss decreased (2.484478 --> 2.484426).  Saving model ...\n",
      "Epoch: 124 \tTraining Loss: 2.474442 \tValidation Loss: 2.484409\n",
      "Validation loss decreased (2.484426 --> 2.484409).  Saving model ...\n",
      "Epoch: 125 \tTraining Loss: 2.469192 \tValidation Loss: 2.484364\n",
      "Validation loss decreased (2.484409 --> 2.484364).  Saving model ...\n",
      "Epoch: 126 \tTraining Loss: 2.476511 \tValidation Loss: 2.484315\n",
      "Validation loss decreased (2.484364 --> 2.484315).  Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 2.472955 \tValidation Loss: 2.484230\n",
      "Validation loss decreased (2.484315 --> 2.484230).  Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 2.472296 \tValidation Loss: 2.484232\n",
      "Epoch: 129 \tTraining Loss: 2.478690 \tValidation Loss: 2.484173\n",
      "Validation loss decreased (2.484230 --> 2.484173).  Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 2.478152 \tValidation Loss: 2.484219\n",
      "Epoch: 131 \tTraining Loss: 2.472515 \tValidation Loss: 2.484135\n",
      "Validation loss decreased (2.484173 --> 2.484135).  Saving model ...\n",
      "Epoch: 132 \tTraining Loss: 2.474176 \tValidation Loss: 2.484105\n",
      "Validation loss decreased (2.484135 --> 2.484105).  Saving model ...\n",
      "Epoch: 133 \tTraining Loss: 2.477515 \tValidation Loss: 2.484070\n",
      "Validation loss decreased (2.484105 --> 2.484070).  Saving model ...\n",
      "Epoch: 134 \tTraining Loss: 2.477241 \tValidation Loss: 2.484009\n",
      "Validation loss decreased (2.484070 --> 2.484009).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 2.469216 \tValidation Loss: 2.483950\n",
      "Validation loss decreased (2.484009 --> 2.483950).  Saving model ...\n",
      "Epoch: 136 \tTraining Loss: 2.470091 \tValidation Loss: 2.483905\n",
      "Validation loss decreased (2.483950 --> 2.483905).  Saving model ...\n",
      "Epoch: 137 \tTraining Loss: 2.475792 \tValidation Loss: 2.483825\n",
      "Validation loss decreased (2.483905 --> 2.483825).  Saving model ...\n",
      "Epoch: 138 \tTraining Loss: 2.489078 \tValidation Loss: 2.483805\n",
      "Validation loss decreased (2.483825 --> 2.483805).  Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 2.478063 \tValidation Loss: 2.483754\n",
      "Validation loss decreased (2.483805 --> 2.483754).  Saving model ...\n",
      "Epoch: 140 \tTraining Loss: 2.463946 \tValidation Loss: 2.483709\n",
      "Validation loss decreased (2.483754 --> 2.483709).  Saving model ...\n",
      "Epoch: 141 \tTraining Loss: 2.478874 \tValidation Loss: 2.483680\n",
      "Validation loss decreased (2.483709 --> 2.483680).  Saving model ...\n",
      "Epoch: 142 \tTraining Loss: 2.476847 \tValidation Loss: 2.483657\n",
      "Validation loss decreased (2.483680 --> 2.483657).  Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 2.470960 \tValidation Loss: 2.483520\n",
      "Validation loss decreased (2.483657 --> 2.483520).  Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 2.455817 \tValidation Loss: 2.483478\n",
      "Validation loss decreased (2.483520 --> 2.483478).  Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 2.467090 \tValidation Loss: 2.483368\n",
      "Validation loss decreased (2.483478 --> 2.483368).  Saving model ...\n",
      "Epoch: 146 \tTraining Loss: 2.478452 \tValidation Loss: 2.483285\n",
      "Validation loss decreased (2.483368 --> 2.483285).  Saving model ...\n",
      "Epoch: 147 \tTraining Loss: 2.465322 \tValidation Loss: 2.483208\n",
      "Validation loss decreased (2.483285 --> 2.483208).  Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 2.485763 \tValidation Loss: 2.483202\n",
      "Validation loss decreased (2.483208 --> 2.483202).  Saving model ...\n",
      "Epoch: 149 \tTraining Loss: 2.477423 \tValidation Loss: 2.483105\n",
      "Validation loss decreased (2.483202 --> 2.483105).  Saving model ...\n",
      "Epoch: 150 \tTraining Loss: 2.484629 \tValidation Loss: 2.482999\n",
      "Validation loss decreased (2.483105 --> 2.482999).  Saving model ...\n",
      "Epoch: 151 \tTraining Loss: 2.468063 \tValidation Loss: 2.482888\n",
      "Validation loss decreased (2.482999 --> 2.482888).  Saving model ...\n",
      "Epoch: 152 \tTraining Loss: 2.481038 \tValidation Loss: 2.482779\n",
      "Validation loss decreased (2.482888 --> 2.482779).  Saving model ...\n",
      "Epoch: 153 \tTraining Loss: 2.471456 \tValidation Loss: 2.482776\n",
      "Validation loss decreased (2.482779 --> 2.482776).  Saving model ...\n",
      "Epoch: 154 \tTraining Loss: 2.476215 \tValidation Loss: 2.482770\n",
      "Validation loss decreased (2.482776 --> 2.482770).  Saving model ...\n",
      "Epoch: 155 \tTraining Loss: 2.469458 \tValidation Loss: 2.482640\n",
      "Validation loss decreased (2.482770 --> 2.482640).  Saving model ...\n",
      "Epoch: 156 \tTraining Loss: 2.473696 \tValidation Loss: 2.482561\n",
      "Validation loss decreased (2.482640 --> 2.482561).  Saving model ...\n",
      "Epoch: 157 \tTraining Loss: 2.472832 \tValidation Loss: 2.482419\n",
      "Validation loss decreased (2.482561 --> 2.482419).  Saving model ...\n",
      "Epoch: 158 \tTraining Loss: 2.475774 \tValidation Loss: 2.482371\n",
      "Validation loss decreased (2.482419 --> 2.482371).  Saving model ...\n",
      "Epoch: 159 \tTraining Loss: 2.471426 \tValidation Loss: 2.482288\n",
      "Validation loss decreased (2.482371 --> 2.482288).  Saving model ...\n",
      "Epoch: 160 \tTraining Loss: 2.477330 \tValidation Loss: 2.482302\n",
      "Epoch: 161 \tTraining Loss: 2.465329 \tValidation Loss: 2.482227\n",
      "Validation loss decreased (2.482288 --> 2.482227).  Saving model ...\n",
      "Epoch: 162 \tTraining Loss: 2.473266 \tValidation Loss: 2.482086\n",
      "Validation loss decreased (2.482227 --> 2.482086).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 163 \tTraining Loss: 2.472899 \tValidation Loss: 2.482038\n",
      "Validation loss decreased (2.482086 --> 2.482038).  Saving model ...\n",
      "Epoch: 164 \tTraining Loss: 2.473785 \tValidation Loss: 2.482055\n",
      "Epoch: 165 \tTraining Loss: 2.474881 \tValidation Loss: 2.481992\n",
      "Validation loss decreased (2.482038 --> 2.481992).  Saving model ...\n",
      "Epoch: 166 \tTraining Loss: 2.471499 \tValidation Loss: 2.481954\n",
      "Validation loss decreased (2.481992 --> 2.481954).  Saving model ...\n",
      "Epoch: 167 \tTraining Loss: 2.471295 \tValidation Loss: 2.481862\n",
      "Validation loss decreased (2.481954 --> 2.481862).  Saving model ...\n",
      "Epoch: 168 \tTraining Loss: 2.485353 \tValidation Loss: 2.481809\n",
      "Validation loss decreased (2.481862 --> 2.481809).  Saving model ...\n",
      "Epoch: 169 \tTraining Loss: 2.472471 \tValidation Loss: 2.481841\n",
      "Epoch: 170 \tTraining Loss: 2.464840 \tValidation Loss: 2.481734\n",
      "Validation loss decreased (2.481809 --> 2.481734).  Saving model ...\n",
      "Epoch: 171 \tTraining Loss: 2.474538 \tValidation Loss: 2.481660\n",
      "Validation loss decreased (2.481734 --> 2.481660).  Saving model ...\n",
      "Epoch: 172 \tTraining Loss: 2.474817 \tValidation Loss: 2.481599\n",
      "Validation loss decreased (2.481660 --> 2.481599).  Saving model ...\n",
      "Epoch: 173 \tTraining Loss: 2.467429 \tValidation Loss: 2.481541\n",
      "Validation loss decreased (2.481599 --> 2.481541).  Saving model ...\n",
      "Epoch: 174 \tTraining Loss: 2.474655 \tValidation Loss: 2.481406\n",
      "Validation loss decreased (2.481541 --> 2.481406).  Saving model ...\n",
      "Epoch: 175 \tTraining Loss: 2.474953 \tValidation Loss: 2.481382\n",
      "Validation loss decreased (2.481406 --> 2.481382).  Saving model ...\n",
      "Epoch: 176 \tTraining Loss: 2.474854 \tValidation Loss: 2.481407\n",
      "Epoch: 177 \tTraining Loss: 2.471400 \tValidation Loss: 2.481297\n",
      "Validation loss decreased (2.481382 --> 2.481297).  Saving model ...\n",
      "Epoch: 178 \tTraining Loss: 2.466798 \tValidation Loss: 2.481262\n",
      "Validation loss decreased (2.481297 --> 2.481262).  Saving model ...\n",
      "Epoch: 179 \tTraining Loss: 2.477910 \tValidation Loss: 2.481201\n",
      "Validation loss decreased (2.481262 --> 2.481201).  Saving model ...\n",
      "Epoch: 180 \tTraining Loss: 2.473972 \tValidation Loss: 2.481139\n",
      "Validation loss decreased (2.481201 --> 2.481139).  Saving model ...\n",
      "Epoch: 181 \tTraining Loss: 2.471504 \tValidation Loss: 2.481022\n",
      "Validation loss decreased (2.481139 --> 2.481022).  Saving model ...\n",
      "Epoch: 182 \tTraining Loss: 2.462382 \tValidation Loss: 2.480993\n",
      "Validation loss decreased (2.481022 --> 2.480993).  Saving model ...\n",
      "Epoch: 183 \tTraining Loss: 2.472814 \tValidation Loss: 2.480933\n",
      "Validation loss decreased (2.480993 --> 2.480933).  Saving model ...\n",
      "Epoch: 184 \tTraining Loss: 2.465313 \tValidation Loss: 2.480929\n",
      "Validation loss decreased (2.480933 --> 2.480929).  Saving model ...\n",
      "Epoch: 185 \tTraining Loss: 2.467340 \tValidation Loss: 2.480832\n",
      "Validation loss decreased (2.480929 --> 2.480832).  Saving model ...\n",
      "Epoch: 186 \tTraining Loss: 2.473038 \tValidation Loss: 2.480808\n",
      "Validation loss decreased (2.480832 --> 2.480808).  Saving model ...\n",
      "Epoch: 187 \tTraining Loss: 2.468920 \tValidation Loss: 2.480830\n",
      "Epoch: 188 \tTraining Loss: 2.464984 \tValidation Loss: 2.480810\n",
      "Epoch: 189 \tTraining Loss: 2.467668 \tValidation Loss: 2.480734\n",
      "Validation loss decreased (2.480808 --> 2.480734).  Saving model ...\n",
      "Epoch: 190 \tTraining Loss: 2.469799 \tValidation Loss: 2.480690\n",
      "Validation loss decreased (2.480734 --> 2.480690).  Saving model ...\n",
      "Epoch: 191 \tTraining Loss: 2.474444 \tValidation Loss: 2.480650\n",
      "Validation loss decreased (2.480690 --> 2.480650).  Saving model ...\n",
      "Epoch: 192 \tTraining Loss: 2.465910 \tValidation Loss: 2.480609\n",
      "Validation loss decreased (2.480650 --> 2.480609).  Saving model ...\n",
      "Epoch: 193 \tTraining Loss: 2.476006 \tValidation Loss: 2.480577\n",
      "Validation loss decreased (2.480609 --> 2.480577).  Saving model ...\n",
      "Epoch: 194 \tTraining Loss: 2.467451 \tValidation Loss: 2.480505\n",
      "Validation loss decreased (2.480577 --> 2.480505).  Saving model ...\n",
      "Epoch: 195 \tTraining Loss: 2.463433 \tValidation Loss: 2.480422\n",
      "Validation loss decreased (2.480505 --> 2.480422).  Saving model ...\n",
      "Epoch: 196 \tTraining Loss: 2.467617 \tValidation Loss: 2.480471\n",
      "Epoch: 197 \tTraining Loss: 2.471582 \tValidation Loss: 2.480387\n",
      "Validation loss decreased (2.480422 --> 2.480387).  Saving model ...\n",
      "Epoch: 198 \tTraining Loss: 2.483299 \tValidation Loss: 2.480392\n",
      "Epoch: 199 \tTraining Loss: 2.469967 \tValidation Loss: 2.480314\n",
      "Validation loss decreased (2.480387 --> 2.480314).  Saving model ...\n",
      "Epoch: 200 \tTraining Loss: 2.469101 \tValidation Loss: 2.480265\n",
      "Validation loss decreased (2.480314 --> 2.480265).  Saving model ...\n",
      "Epoch: 201 \tTraining Loss: 2.470215 \tValidation Loss: 2.480245\n",
      "Validation loss decreased (2.480265 --> 2.480245).  Saving model ...\n",
      "Epoch: 202 \tTraining Loss: 2.478911 \tValidation Loss: 2.480202\n",
      "Validation loss decreased (2.480245 --> 2.480202).  Saving model ...\n",
      "Epoch: 203 \tTraining Loss: 2.466250 \tValidation Loss: 2.480200\n",
      "Validation loss decreased (2.480202 --> 2.480200).  Saving model ...\n",
      "Epoch: 204 \tTraining Loss: 2.464148 \tValidation Loss: 2.480195\n",
      "Validation loss decreased (2.480200 --> 2.480195).  Saving model ...\n",
      "Epoch: 205 \tTraining Loss: 2.472596 \tValidation Loss: 2.480168\n",
      "Validation loss decreased (2.480195 --> 2.480168).  Saving model ...\n",
      "Epoch: 206 \tTraining Loss: 2.471906 \tValidation Loss: 2.480101\n",
      "Validation loss decreased (2.480168 --> 2.480101).  Saving model ...\n",
      "Epoch: 207 \tTraining Loss: 2.474447 \tValidation Loss: 2.480030\n",
      "Validation loss decreased (2.480101 --> 2.480030).  Saving model ...\n",
      "Epoch: 208 \tTraining Loss: 2.466116 \tValidation Loss: 2.480019\n",
      "Validation loss decreased (2.480030 --> 2.480019).  Saving model ...\n",
      "Epoch: 209 \tTraining Loss: 2.470070 \tValidation Loss: 2.479920\n",
      "Validation loss decreased (2.480019 --> 2.479920).  Saving model ...\n",
      "Epoch: 210 \tTraining Loss: 2.465141 \tValidation Loss: 2.479881\n",
      "Validation loss decreased (2.479920 --> 2.479881).  Saving model ...\n",
      "Epoch: 211 \tTraining Loss: 2.469289 \tValidation Loss: 2.479832\n",
      "Validation loss decreased (2.479881 --> 2.479832).  Saving model ...\n",
      "Epoch: 212 \tTraining Loss: 2.470916 \tValidation Loss: 2.479816\n",
      "Validation loss decreased (2.479832 --> 2.479816).  Saving model ...\n",
      "Epoch: 213 \tTraining Loss: 2.467080 \tValidation Loss: 2.479740\n",
      "Validation loss decreased (2.479816 --> 2.479740).  Saving model ...\n",
      "Epoch: 214 \tTraining Loss: 2.470097 \tValidation Loss: 2.479680\n",
      "Validation loss decreased (2.479740 --> 2.479680).  Saving model ...\n",
      "Epoch: 215 \tTraining Loss: 2.475057 \tValidation Loss: 2.479645\n",
      "Validation loss decreased (2.479680 --> 2.479645).  Saving model ...\n",
      "Epoch: 216 \tTraining Loss: 2.464628 \tValidation Loss: 2.479522\n",
      "Validation loss decreased (2.479645 --> 2.479522).  Saving model ...\n",
      "Epoch: 217 \tTraining Loss: 2.467647 \tValidation Loss: 2.479402\n",
      "Validation loss decreased (2.479522 --> 2.479402).  Saving model ...\n",
      "Epoch: 218 \tTraining Loss: 2.462634 \tValidation Loss: 2.479390\n",
      "Validation loss decreased (2.479402 --> 2.479390).  Saving model ...\n",
      "Epoch: 219 \tTraining Loss: 2.474492 \tValidation Loss: 2.479330\n",
      "Validation loss decreased (2.479390 --> 2.479330).  Saving model ...\n",
      "Epoch: 220 \tTraining Loss: 2.467950 \tValidation Loss: 2.479290\n",
      "Validation loss decreased (2.479330 --> 2.479290).  Saving model ...\n",
      "Epoch: 221 \tTraining Loss: 2.468556 \tValidation Loss: 2.479250\n",
      "Validation loss decreased (2.479290 --> 2.479250).  Saving model ...\n",
      "Epoch: 222 \tTraining Loss: 2.473464 \tValidation Loss: 2.479201\n",
      "Validation loss decreased (2.479250 --> 2.479201).  Saving model ...\n",
      "Epoch: 223 \tTraining Loss: 2.476069 \tValidation Loss: 2.479188\n",
      "Validation loss decreased (2.479201 --> 2.479188).  Saving model ...\n",
      "Epoch: 224 \tTraining Loss: 2.468717 \tValidation Loss: 2.479185\n",
      "Validation loss decreased (2.479188 --> 2.479185).  Saving model ...\n",
      "Epoch: 225 \tTraining Loss: 2.469442 \tValidation Loss: 2.479203\n",
      "Epoch: 226 \tTraining Loss: 2.470718 \tValidation Loss: 2.479247\n",
      "Epoch: 227 \tTraining Loss: 2.472189 \tValidation Loss: 2.479238\n",
      "Epoch: 228 \tTraining Loss: 2.477184 \tValidation Loss: 2.479214\n",
      "Epoch: 229 \tTraining Loss: 2.470084 \tValidation Loss: 2.479204\n",
      "Epoch: 230 \tTraining Loss: 2.466079 \tValidation Loss: 2.479207\n",
      "Epoch: 231 \tTraining Loss: 2.471527 \tValidation Loss: 2.479192\n",
      "Epoch: 232 \tTraining Loss: 2.470011 \tValidation Loss: 2.479178\n",
      "Validation loss decreased (2.479185 --> 2.479178).  Saving model ...\n",
      "Epoch: 233 \tTraining Loss: 2.460106 \tValidation Loss: 2.479110\n",
      "Validation loss decreased (2.479178 --> 2.479110).  Saving model ...\n",
      "Epoch: 234 \tTraining Loss: 2.463313 \tValidation Loss: 2.479125\n",
      "Epoch: 235 \tTraining Loss: 2.473162 \tValidation Loss: 2.479042\n",
      "Validation loss decreased (2.479110 --> 2.479042).  Saving model ...\n",
      "Epoch: 236 \tTraining Loss: 2.467276 \tValidation Loss: 2.479029\n",
      "Validation loss decreased (2.479042 --> 2.479029).  Saving model ...\n",
      "Epoch: 237 \tTraining Loss: 2.472661 \tValidation Loss: 2.478961\n",
      "Validation loss decreased (2.479029 --> 2.478961).  Saving model ...\n",
      "Epoch: 238 \tTraining Loss: 2.468415 \tValidation Loss: 2.478947\n",
      "Validation loss decreased (2.478961 --> 2.478947).  Saving model ...\n",
      "Epoch: 239 \tTraining Loss: 2.475084 \tValidation Loss: 2.478894\n",
      "Validation loss decreased (2.478947 --> 2.478894).  Saving model ...\n",
      "Epoch: 240 \tTraining Loss: 2.470249 \tValidation Loss: 2.478888\n",
      "Validation loss decreased (2.478894 --> 2.478888).  Saving model ...\n",
      "Epoch: 241 \tTraining Loss: 2.471986 \tValidation Loss: 2.478791\n",
      "Validation loss decreased (2.478888 --> 2.478791).  Saving model ...\n",
      "Epoch: 242 \tTraining Loss: 2.466034 \tValidation Loss: 2.478740\n",
      "Validation loss decreased (2.478791 --> 2.478740).  Saving model ...\n",
      "Epoch: 243 \tTraining Loss: 2.478363 \tValidation Loss: 2.478681\n",
      "Validation loss decreased (2.478740 --> 2.478681).  Saving model ...\n",
      "Epoch: 244 \tTraining Loss: 2.469022 \tValidation Loss: 2.478681\n",
      "Validation loss decreased (2.478681 --> 2.478681).  Saving model ...\n",
      "Epoch: 245 \tTraining Loss: 2.465446 \tValidation Loss: 2.478698\n",
      "Epoch: 246 \tTraining Loss: 2.465454 \tValidation Loss: 2.478674\n",
      "Validation loss decreased (2.478681 --> 2.478674).  Saving model ...\n",
      "Epoch: 247 \tTraining Loss: 2.468966 \tValidation Loss: 2.478661\n",
      "Validation loss decreased (2.478674 --> 2.478661).  Saving model ...\n",
      "Epoch: 248 \tTraining Loss: 2.472225 \tValidation Loss: 2.478602\n",
      "Validation loss decreased (2.478661 --> 2.478602).  Saving model ...\n",
      "Epoch: 249 \tTraining Loss: 2.466893 \tValidation Loss: 2.478565\n",
      "Validation loss decreased (2.478602 --> 2.478565).  Saving model ...\n",
      "Epoch: 250 \tTraining Loss: 2.467449 \tValidation Loss: 2.478506\n",
      "Validation loss decreased (2.478565 --> 2.478506).  Saving model ...\n",
      "Epoch: 251 \tTraining Loss: 2.469646 \tValidation Loss: 2.478453\n",
      "Validation loss decreased (2.478506 --> 2.478453).  Saving model ...\n",
      "Epoch: 252 \tTraining Loss: 2.466158 \tValidation Loss: 2.478372\n",
      "Validation loss decreased (2.478453 --> 2.478372).  Saving model ...\n",
      "Epoch: 253 \tTraining Loss: 2.465785 \tValidation Loss: 2.478345\n",
      "Validation loss decreased (2.478372 --> 2.478345).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254 \tTraining Loss: 2.465798 \tValidation Loss: 2.478266\n",
      "Validation loss decreased (2.478345 --> 2.478266).  Saving model ...\n",
      "Epoch: 255 \tTraining Loss: 2.472060 \tValidation Loss: 2.478204\n",
      "Validation loss decreased (2.478266 --> 2.478204).  Saving model ...\n",
      "Epoch: 256 \tTraining Loss: 2.465667 \tValidation Loss: 2.478098\n",
      "Validation loss decreased (2.478204 --> 2.478098).  Saving model ...\n",
      "Epoch: 257 \tTraining Loss: 2.472133 \tValidation Loss: 2.478090\n",
      "Validation loss decreased (2.478098 --> 2.478090).  Saving model ...\n",
      "Epoch: 258 \tTraining Loss: 2.464460 \tValidation Loss: 2.478095\n",
      "Epoch: 259 \tTraining Loss: 2.473789 \tValidation Loss: 2.478118\n",
      "Epoch: 260 \tTraining Loss: 2.468419 \tValidation Loss: 2.478103\n",
      "Epoch: 261 \tTraining Loss: 2.465396 \tValidation Loss: 2.478137\n",
      "Epoch: 262 \tTraining Loss: 2.466135 \tValidation Loss: 2.478079\n",
      "Validation loss decreased (2.478090 --> 2.478079).  Saving model ...\n",
      "Epoch: 263 \tTraining Loss: 2.470169 \tValidation Loss: 2.478072\n",
      "Validation loss decreased (2.478079 --> 2.478072).  Saving model ...\n",
      "Epoch: 264 \tTraining Loss: 2.474519 \tValidation Loss: 2.478065\n",
      "Validation loss decreased (2.478072 --> 2.478065).  Saving model ...\n",
      "Epoch: 265 \tTraining Loss: 2.473688 \tValidation Loss: 2.478039\n",
      "Validation loss decreased (2.478065 --> 2.478039).  Saving model ...\n",
      "Epoch: 266 \tTraining Loss: 2.468848 \tValidation Loss: 2.478025\n",
      "Validation loss decreased (2.478039 --> 2.478025).  Saving model ...\n",
      "Epoch: 267 \tTraining Loss: 2.468726 \tValidation Loss: 2.477963\n",
      "Validation loss decreased (2.478025 --> 2.477963).  Saving model ...\n",
      "Epoch: 268 \tTraining Loss: 2.466947 \tValidation Loss: 2.477922\n",
      "Validation loss decreased (2.477963 --> 2.477922).  Saving model ...\n",
      "Epoch: 269 \tTraining Loss: 2.464520 \tValidation Loss: 2.477881\n",
      "Validation loss decreased (2.477922 --> 2.477881).  Saving model ...\n",
      "Epoch: 270 \tTraining Loss: 2.464641 \tValidation Loss: 2.477824\n",
      "Validation loss decreased (2.477881 --> 2.477824).  Saving model ...\n",
      "Epoch: 271 \tTraining Loss: 2.469451 \tValidation Loss: 2.477795\n",
      "Validation loss decreased (2.477824 --> 2.477795).  Saving model ...\n",
      "Epoch: 272 \tTraining Loss: 2.472802 \tValidation Loss: 2.477771\n",
      "Validation loss decreased (2.477795 --> 2.477771).  Saving model ...\n",
      "Epoch: 273 \tTraining Loss: 2.471505 \tValidation Loss: 2.477735\n",
      "Validation loss decreased (2.477771 --> 2.477735).  Saving model ...\n",
      "Epoch: 274 \tTraining Loss: 2.483516 \tValidation Loss: 2.477727\n",
      "Validation loss decreased (2.477735 --> 2.477727).  Saving model ...\n",
      "Epoch: 275 \tTraining Loss: 2.467829 \tValidation Loss: 2.477651\n",
      "Validation loss decreased (2.477727 --> 2.477651).  Saving model ...\n",
      "Epoch: 276 \tTraining Loss: 2.466593 \tValidation Loss: 2.477633\n",
      "Validation loss decreased (2.477651 --> 2.477633).  Saving model ...\n",
      "Epoch: 277 \tTraining Loss: 2.468575 \tValidation Loss: 2.477574\n",
      "Validation loss decreased (2.477633 --> 2.477574).  Saving model ...\n",
      "Epoch: 278 \tTraining Loss: 2.467759 \tValidation Loss: 2.477574\n",
      "Epoch: 279 \tTraining Loss: 2.460549 \tValidation Loss: 2.477571\n",
      "Validation loss decreased (2.477574 --> 2.477571).  Saving model ...\n",
      "Epoch: 280 \tTraining Loss: 2.474176 \tValidation Loss: 2.477533\n",
      "Validation loss decreased (2.477571 --> 2.477533).  Saving model ...\n",
      "Epoch: 281 \tTraining Loss: 2.461575 \tValidation Loss: 2.477495\n",
      "Validation loss decreased (2.477533 --> 2.477495).  Saving model ...\n",
      "Epoch: 282 \tTraining Loss: 2.475457 \tValidation Loss: 2.477455\n",
      "Validation loss decreased (2.477495 --> 2.477455).  Saving model ...\n",
      "Epoch: 283 \tTraining Loss: 2.470648 \tValidation Loss: 2.477464\n",
      "Epoch: 284 \tTraining Loss: 2.469753 \tValidation Loss: 2.477438\n",
      "Validation loss decreased (2.477455 --> 2.477438).  Saving model ...\n",
      "Epoch: 285 \tTraining Loss: 2.476431 \tValidation Loss: 2.477389\n",
      "Validation loss decreased (2.477438 --> 2.477389).  Saving model ...\n",
      "Epoch: 286 \tTraining Loss: 2.459272 \tValidation Loss: 2.477305\n",
      "Validation loss decreased (2.477389 --> 2.477305).  Saving model ...\n",
      "Epoch: 287 \tTraining Loss: 2.461559 \tValidation Loss: 2.477341\n",
      "Epoch: 288 \tTraining Loss: 2.472177 \tValidation Loss: 2.477334\n",
      "Epoch: 289 \tTraining Loss: 2.463459 \tValidation Loss: 2.477320\n",
      "Epoch: 290 \tTraining Loss: 2.466496 \tValidation Loss: 2.477303\n",
      "Validation loss decreased (2.477305 --> 2.477303).  Saving model ...\n",
      "Epoch: 291 \tTraining Loss: 2.459907 \tValidation Loss: 2.477300\n",
      "Validation loss decreased (2.477303 --> 2.477300).  Saving model ...\n",
      "Epoch: 292 \tTraining Loss: 2.468500 \tValidation Loss: 2.477276\n",
      "Validation loss decreased (2.477300 --> 2.477276).  Saving model ...\n",
      "Epoch: 293 \tTraining Loss: 2.465992 \tValidation Loss: 2.477216\n",
      "Validation loss decreased (2.477276 --> 2.477216).  Saving model ...\n",
      "Epoch: 294 \tTraining Loss: 2.473718 \tValidation Loss: 2.477186\n",
      "Validation loss decreased (2.477216 --> 2.477186).  Saving model ...\n",
      "Epoch: 295 \tTraining Loss: 2.464024 \tValidation Loss: 2.477156\n",
      "Validation loss decreased (2.477186 --> 2.477156).  Saving model ...\n",
      "Epoch: 296 \tTraining Loss: 2.462040 \tValidation Loss: 2.477135\n",
      "Validation loss decreased (2.477156 --> 2.477135).  Saving model ...\n",
      "Epoch: 297 \tTraining Loss: 2.462933 \tValidation Loss: 2.477149\n",
      "Epoch: 298 \tTraining Loss: 2.461181 \tValidation Loss: 2.477095\n",
      "Validation loss decreased (2.477135 --> 2.477095).  Saving model ...\n",
      "Epoch: 299 \tTraining Loss: 2.470292 \tValidation Loss: 2.477060\n",
      "Validation loss decreased (2.477095 --> 2.477060).  Saving model ...\n",
      "Epoch: 300 \tTraining Loss: 2.472716 \tValidation Loss: 2.477019\n",
      "Validation loss decreased (2.477060 --> 2.477019).  Saving model ...\n",
      "Epoch: 301 \tTraining Loss: 2.457046 \tValidation Loss: 2.476965\n",
      "Validation loss decreased (2.477019 --> 2.476965).  Saving model ...\n",
      "Epoch: 302 \tTraining Loss: 2.468683 \tValidation Loss: 2.476902\n",
      "Validation loss decreased (2.476965 --> 2.476902).  Saving model ...\n",
      "Epoch: 303 \tTraining Loss: 2.467635 \tValidation Loss: 2.476866\n",
      "Validation loss decreased (2.476902 --> 2.476866).  Saving model ...\n",
      "Epoch: 304 \tTraining Loss: 2.466475 \tValidation Loss: 2.476836\n",
      "Validation loss decreased (2.476866 --> 2.476836).  Saving model ...\n",
      "Epoch: 305 \tTraining Loss: 2.473501 \tValidation Loss: 2.476832\n",
      "Validation loss decreased (2.476836 --> 2.476832).  Saving model ...\n",
      "Epoch: 306 \tTraining Loss: 2.461263 \tValidation Loss: 2.476818\n",
      "Validation loss decreased (2.476832 --> 2.476818).  Saving model ...\n",
      "Epoch: 307 \tTraining Loss: 2.464180 \tValidation Loss: 2.476785\n",
      "Validation loss decreased (2.476818 --> 2.476785).  Saving model ...\n",
      "Epoch: 308 \tTraining Loss: 2.466916 \tValidation Loss: 2.476779\n",
      "Validation loss decreased (2.476785 --> 2.476779).  Saving model ...\n",
      "Epoch: 309 \tTraining Loss: 2.467772 \tValidation Loss: 2.476776\n",
      "Validation loss decreased (2.476779 --> 2.476776).  Saving model ...\n",
      "Epoch: 310 \tTraining Loss: 2.467074 \tValidation Loss: 2.476716\n",
      "Validation loss decreased (2.476776 --> 2.476716).  Saving model ...\n",
      "Epoch: 311 \tTraining Loss: 2.470148 \tValidation Loss: 2.476680\n",
      "Validation loss decreased (2.476716 --> 2.476680).  Saving model ...\n",
      "Epoch: 312 \tTraining Loss: 2.467261 \tValidation Loss: 2.476638\n",
      "Validation loss decreased (2.476680 --> 2.476638).  Saving model ...\n",
      "Epoch: 313 \tTraining Loss: 2.471442 \tValidation Loss: 2.476601\n",
      "Validation loss decreased (2.476638 --> 2.476601).  Saving model ...\n",
      "Epoch: 314 \tTraining Loss: 2.468021 \tValidation Loss: 2.476635\n",
      "Epoch: 315 \tTraining Loss: 2.471861 \tValidation Loss: 2.476624\n",
      "Epoch: 316 \tTraining Loss: 2.473324 \tValidation Loss: 2.476616\n",
      "Epoch: 317 \tTraining Loss: 2.474788 \tValidation Loss: 2.476579\n",
      "Validation loss decreased (2.476601 --> 2.476579).  Saving model ...\n",
      "Epoch: 318 \tTraining Loss: 2.464180 \tValidation Loss: 2.476524\n",
      "Validation loss decreased (2.476579 --> 2.476524).  Saving model ...\n",
      "Epoch: 319 \tTraining Loss: 2.477554 \tValidation Loss: 2.476502\n",
      "Validation loss decreased (2.476524 --> 2.476502).  Saving model ...\n",
      "Epoch: 320 \tTraining Loss: 2.468711 \tValidation Loss: 2.476463\n",
      "Validation loss decreased (2.476502 --> 2.476463).  Saving model ...\n",
      "Epoch: 321 \tTraining Loss: 2.463017 \tValidation Loss: 2.476435\n",
      "Validation loss decreased (2.476463 --> 2.476435).  Saving model ...\n",
      "Epoch: 322 \tTraining Loss: 2.463286 \tValidation Loss: 2.476379\n",
      "Validation loss decreased (2.476435 --> 2.476379).  Saving model ...\n",
      "Epoch: 323 \tTraining Loss: 2.472012 \tValidation Loss: 2.476324\n",
      "Validation loss decreased (2.476379 --> 2.476324).  Saving model ...\n",
      "Epoch: 324 \tTraining Loss: 2.467864 \tValidation Loss: 2.476351\n",
      "Epoch: 325 \tTraining Loss: 2.472492 \tValidation Loss: 2.476318\n",
      "Validation loss decreased (2.476324 --> 2.476318).  Saving model ...\n",
      "Epoch: 326 \tTraining Loss: 2.464577 \tValidation Loss: 2.476262\n",
      "Validation loss decreased (2.476318 --> 2.476262).  Saving model ...\n",
      "Epoch: 327 \tTraining Loss: 2.463491 \tValidation Loss: 2.476217\n",
      "Validation loss decreased (2.476262 --> 2.476217).  Saving model ...\n",
      "Epoch: 328 \tTraining Loss: 2.466953 \tValidation Loss: 2.476226\n",
      "Epoch: 329 \tTraining Loss: 2.460583 \tValidation Loss: 2.476165\n",
      "Validation loss decreased (2.476217 --> 2.476165).  Saving model ...\n",
      "Epoch: 330 \tTraining Loss: 2.463756 \tValidation Loss: 2.476130\n",
      "Validation loss decreased (2.476165 --> 2.476130).  Saving model ...\n",
      "Epoch: 331 \tTraining Loss: 2.471689 \tValidation Loss: 2.476069\n",
      "Validation loss decreased (2.476130 --> 2.476069).  Saving model ...\n",
      "Epoch: 332 \tTraining Loss: 2.461754 \tValidation Loss: 2.476018\n",
      "Validation loss decreased (2.476069 --> 2.476018).  Saving model ...\n",
      "Epoch: 333 \tTraining Loss: 2.471853 \tValidation Loss: 2.476019\n",
      "Epoch: 334 \tTraining Loss: 2.465532 \tValidation Loss: 2.475976\n",
      "Validation loss decreased (2.476018 --> 2.475976).  Saving model ...\n",
      "Epoch: 335 \tTraining Loss: 2.462836 \tValidation Loss: 2.475981\n",
      "Epoch: 336 \tTraining Loss: 2.454337 \tValidation Loss: 2.475950\n",
      "Validation loss decreased (2.475976 --> 2.475950).  Saving model ...\n",
      "Epoch: 337 \tTraining Loss: 2.459941 \tValidation Loss: 2.475877\n",
      "Validation loss decreased (2.475950 --> 2.475877).  Saving model ...\n",
      "Epoch: 338 \tTraining Loss: 2.473271 \tValidation Loss: 2.475820\n",
      "Validation loss decreased (2.475877 --> 2.475820).  Saving model ...\n",
      "Epoch: 339 \tTraining Loss: 2.463581 \tValidation Loss: 2.475762\n",
      "Validation loss decreased (2.475820 --> 2.475762).  Saving model ...\n",
      "Epoch: 340 \tTraining Loss: 2.467640 \tValidation Loss: 2.475722\n",
      "Validation loss decreased (2.475762 --> 2.475722).  Saving model ...\n",
      "Epoch: 341 \tTraining Loss: 2.468053 \tValidation Loss: 2.475654\n",
      "Validation loss decreased (2.475722 --> 2.475654).  Saving model ...\n",
      "Epoch: 342 \tTraining Loss: 2.468044 \tValidation Loss: 2.475586\n",
      "Validation loss decreased (2.475654 --> 2.475586).  Saving model ...\n",
      "Epoch: 343 \tTraining Loss: 2.464285 \tValidation Loss: 2.475552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (2.475586 --> 2.475552).  Saving model ...\n",
      "Epoch: 344 \tTraining Loss: 2.469914 \tValidation Loss: 2.475540\n",
      "Validation loss decreased (2.475552 --> 2.475540).  Saving model ...\n",
      "Epoch: 345 \tTraining Loss: 2.465830 \tValidation Loss: 2.475509\n",
      "Validation loss decreased (2.475540 --> 2.475509).  Saving model ...\n",
      "Epoch: 346 \tTraining Loss: 2.477671 \tValidation Loss: 2.475491\n",
      "Validation loss decreased (2.475509 --> 2.475491).  Saving model ...\n",
      "Epoch: 347 \tTraining Loss: 2.468657 \tValidation Loss: 2.475481\n",
      "Validation loss decreased (2.475491 --> 2.475481).  Saving model ...\n",
      "Epoch: 348 \tTraining Loss: 2.468502 \tValidation Loss: 2.475483\n",
      "Epoch: 349 \tTraining Loss: 2.472094 \tValidation Loss: 2.475449\n",
      "Validation loss decreased (2.475481 --> 2.475449).  Saving model ...\n",
      "Epoch: 350 \tTraining Loss: 2.461321 \tValidation Loss: 2.475487\n",
      "Epoch: 351 \tTraining Loss: 2.456950 \tValidation Loss: 2.475407\n",
      "Validation loss decreased (2.475449 --> 2.475407).  Saving model ...\n",
      "Epoch: 352 \tTraining Loss: 2.463751 \tValidation Loss: 2.475367\n",
      "Validation loss decreased (2.475407 --> 2.475367).  Saving model ...\n",
      "Epoch: 353 \tTraining Loss: 2.457222 \tValidation Loss: 2.475323\n",
      "Validation loss decreased (2.475367 --> 2.475323).  Saving model ...\n",
      "Epoch: 354 \tTraining Loss: 2.470286 \tValidation Loss: 2.475278\n",
      "Validation loss decreased (2.475323 --> 2.475278).  Saving model ...\n",
      "Epoch: 355 \tTraining Loss: 2.473192 \tValidation Loss: 2.475285\n",
      "Epoch: 356 \tTraining Loss: 2.476965 \tValidation Loss: 2.475266\n",
      "Validation loss decreased (2.475278 --> 2.475266).  Saving model ...\n",
      "Epoch: 357 \tTraining Loss: 2.461898 \tValidation Loss: 2.475260\n",
      "Validation loss decreased (2.475266 --> 2.475260).  Saving model ...\n",
      "Epoch: 358 \tTraining Loss: 2.462997 \tValidation Loss: 2.475240\n",
      "Validation loss decreased (2.475260 --> 2.475240).  Saving model ...\n",
      "Epoch: 359 \tTraining Loss: 2.464700 \tValidation Loss: 2.475240\n",
      "Epoch: 360 \tTraining Loss: 2.467334 \tValidation Loss: 2.475225\n",
      "Validation loss decreased (2.475240 --> 2.475225).  Saving model ...\n",
      "Epoch: 361 \tTraining Loss: 2.467613 \tValidation Loss: 2.475188\n",
      "Validation loss decreased (2.475225 --> 2.475188).  Saving model ...\n",
      "Epoch: 362 \tTraining Loss: 2.467736 \tValidation Loss: 2.475135\n",
      "Validation loss decreased (2.475188 --> 2.475135).  Saving model ...\n",
      "Epoch: 363 \tTraining Loss: 2.465445 \tValidation Loss: 2.475078\n",
      "Validation loss decreased (2.475135 --> 2.475078).  Saving model ...\n",
      "Epoch: 364 \tTraining Loss: 2.463108 \tValidation Loss: 2.475104\n",
      "Epoch: 365 \tTraining Loss: 2.467798 \tValidation Loss: 2.475148\n",
      "Epoch: 366 \tTraining Loss: 2.468733 \tValidation Loss: 2.475123\n",
      "Epoch: 367 \tTraining Loss: 2.463885 \tValidation Loss: 2.475115\n",
      "Epoch: 368 \tTraining Loss: 2.460291 \tValidation Loss: 2.475059\n",
      "Validation loss decreased (2.475078 --> 2.475059).  Saving model ...\n",
      "Epoch: 369 \tTraining Loss: 2.455662 \tValidation Loss: 2.475031\n",
      "Validation loss decreased (2.475059 --> 2.475031).  Saving model ...\n",
      "Epoch: 370 \tTraining Loss: 2.459918 \tValidation Loss: 2.475053\n",
      "Epoch: 371 \tTraining Loss: 2.462154 \tValidation Loss: 2.475006\n",
      "Validation loss decreased (2.475031 --> 2.475006).  Saving model ...\n",
      "Epoch: 372 \tTraining Loss: 2.471185 \tValidation Loss: 2.475031\n",
      "Epoch: 373 \tTraining Loss: 2.459858 \tValidation Loss: 2.475057\n",
      "Epoch: 374 \tTraining Loss: 2.471853 \tValidation Loss: 2.475028\n",
      "Epoch: 375 \tTraining Loss: 2.455492 \tValidation Loss: 2.475016\n",
      "Epoch: 376 \tTraining Loss: 2.469655 \tValidation Loss: 2.475031\n",
      "Epoch: 377 \tTraining Loss: 2.460205 \tValidation Loss: 2.474990\n",
      "Validation loss decreased (2.475006 --> 2.474990).  Saving model ...\n",
      "Epoch: 378 \tTraining Loss: 2.456826 \tValidation Loss: 2.474994\n",
      "Epoch: 379 \tTraining Loss: 2.462139 \tValidation Loss: 2.474907\n",
      "Validation loss decreased (2.474990 --> 2.474907).  Saving model ...\n",
      "Epoch: 380 \tTraining Loss: 2.466811 \tValidation Loss: 2.474896\n",
      "Validation loss decreased (2.474907 --> 2.474896).  Saving model ...\n",
      "Epoch: 381 \tTraining Loss: 2.465655 \tValidation Loss: 2.474904\n",
      "Epoch: 382 \tTraining Loss: 2.470345 \tValidation Loss: 2.474948\n",
      "Epoch: 383 \tTraining Loss: 2.466917 \tValidation Loss: 2.474900\n",
      "Epoch: 384 \tTraining Loss: 2.457545 \tValidation Loss: 2.474890\n",
      "Validation loss decreased (2.474896 --> 2.474890).  Saving model ...\n",
      "Epoch: 385 \tTraining Loss: 2.476131 \tValidation Loss: 2.474880\n",
      "Validation loss decreased (2.474890 --> 2.474880).  Saving model ...\n",
      "Epoch: 386 \tTraining Loss: 2.462090 \tValidation Loss: 2.474843\n",
      "Validation loss decreased (2.474880 --> 2.474843).  Saving model ...\n",
      "Epoch: 387 \tTraining Loss: 2.465586 \tValidation Loss: 2.474848\n",
      "Epoch: 388 \tTraining Loss: 2.466716 \tValidation Loss: 2.474835\n",
      "Validation loss decreased (2.474843 --> 2.474835).  Saving model ...\n",
      "Epoch: 389 \tTraining Loss: 2.467437 \tValidation Loss: 2.474770\n",
      "Validation loss decreased (2.474835 --> 2.474770).  Saving model ...\n",
      "Epoch: 390 \tTraining Loss: 2.465682 \tValidation Loss: 2.474689\n",
      "Validation loss decreased (2.474770 --> 2.474689).  Saving model ...\n",
      "Epoch: 391 \tTraining Loss: 2.463333 \tValidation Loss: 2.474692\n",
      "Epoch: 392 \tTraining Loss: 2.465995 \tValidation Loss: 2.474628\n",
      "Validation loss decreased (2.474689 --> 2.474628).  Saving model ...\n",
      "Epoch: 393 \tTraining Loss: 2.460359 \tValidation Loss: 2.474588\n",
      "Validation loss decreased (2.474628 --> 2.474588).  Saving model ...\n",
      "Epoch: 394 \tTraining Loss: 2.461522 \tValidation Loss: 2.474557\n",
      "Validation loss decreased (2.474588 --> 2.474557).  Saving model ...\n",
      "Epoch: 395 \tTraining Loss: 2.463819 \tValidation Loss: 2.474523\n",
      "Validation loss decreased (2.474557 --> 2.474523).  Saving model ...\n",
      "Epoch: 396 \tTraining Loss: 2.463078 \tValidation Loss: 2.474480\n",
      "Validation loss decreased (2.474523 --> 2.474480).  Saving model ...\n",
      "Epoch: 397 \tTraining Loss: 2.468365 \tValidation Loss: 2.474440\n",
      "Validation loss decreased (2.474480 --> 2.474440).  Saving model ...\n",
      "Epoch: 398 \tTraining Loss: 2.467549 \tValidation Loss: 2.474385\n",
      "Validation loss decreased (2.474440 --> 2.474385).  Saving model ...\n",
      "Epoch: 399 \tTraining Loss: 2.474814 \tValidation Loss: 2.474381\n",
      "Validation loss decreased (2.474385 --> 2.474381).  Saving model ...\n",
      "Epoch: 400 \tTraining Loss: 2.462898 \tValidation Loss: 2.474353\n",
      "Validation loss decreased (2.474381 --> 2.474353).  Saving model ...\n",
      "Epoch: 401 \tTraining Loss: 2.467235 \tValidation Loss: 2.474320\n",
      "Validation loss decreased (2.474353 --> 2.474320).  Saving model ...\n",
      "Epoch: 402 \tTraining Loss: 2.465148 \tValidation Loss: 2.474311\n",
      "Validation loss decreased (2.474320 --> 2.474311).  Saving model ...\n",
      "Epoch: 403 \tTraining Loss: 2.470282 \tValidation Loss: 2.474322\n",
      "Epoch: 404 \tTraining Loss: 2.457926 \tValidation Loss: 2.474263\n",
      "Validation loss decreased (2.474311 --> 2.474263).  Saving model ...\n",
      "Epoch: 405 \tTraining Loss: 2.465472 \tValidation Loss: 2.474257\n",
      "Validation loss decreased (2.474263 --> 2.474257).  Saving model ...\n",
      "Epoch: 406 \tTraining Loss: 2.464631 \tValidation Loss: 2.474222\n",
      "Validation loss decreased (2.474257 --> 2.474222).  Saving model ...\n",
      "Epoch: 407 \tTraining Loss: 2.466949 \tValidation Loss: 2.474221\n",
      "Validation loss decreased (2.474222 --> 2.474221).  Saving model ...\n",
      "Epoch: 408 \tTraining Loss: 2.461801 \tValidation Loss: 2.474209\n",
      "Validation loss decreased (2.474221 --> 2.474209).  Saving model ...\n",
      "Epoch: 409 \tTraining Loss: 2.460873 \tValidation Loss: 2.474152\n",
      "Validation loss decreased (2.474209 --> 2.474152).  Saving model ...\n",
      "Epoch: 410 \tTraining Loss: 2.467185 \tValidation Loss: 2.474121\n",
      "Validation loss decreased (2.474152 --> 2.474121).  Saving model ...\n",
      "Epoch: 411 \tTraining Loss: 2.467176 \tValidation Loss: 2.474094\n",
      "Validation loss decreased (2.474121 --> 2.474094).  Saving model ...\n",
      "Epoch: 412 \tTraining Loss: 2.457377 \tValidation Loss: 2.474042\n",
      "Validation loss decreased (2.474094 --> 2.474042).  Saving model ...\n",
      "Epoch: 413 \tTraining Loss: 2.453627 \tValidation Loss: 2.474006\n",
      "Validation loss decreased (2.474042 --> 2.474006).  Saving model ...\n",
      "Epoch: 414 \tTraining Loss: 2.466360 \tValidation Loss: 2.474021\n",
      "Epoch: 415 \tTraining Loss: 2.471551 \tValidation Loss: 2.474049\n",
      "Epoch: 416 \tTraining Loss: 2.467531 \tValidation Loss: 2.474030\n",
      "Epoch: 417 \tTraining Loss: 2.461343 \tValidation Loss: 2.474006\n",
      "Validation loss decreased (2.474006 --> 2.474006).  Saving model ...\n",
      "Epoch: 418 \tTraining Loss: 2.465530 \tValidation Loss: 2.474030\n",
      "Epoch: 419 \tTraining Loss: 2.455945 \tValidation Loss: 2.473990\n",
      "Validation loss decreased (2.474006 --> 2.473990).  Saving model ...\n",
      "Epoch: 420 \tTraining Loss: 2.466872 \tValidation Loss: 2.473974\n",
      "Validation loss decreased (2.473990 --> 2.473974).  Saving model ...\n",
      "Epoch: 421 \tTraining Loss: 2.464719 \tValidation Loss: 2.474010\n",
      "Epoch: 422 \tTraining Loss: 2.464534 \tValidation Loss: 2.473991\n",
      "Epoch: 423 \tTraining Loss: 2.456193 \tValidation Loss: 2.473954\n",
      "Validation loss decreased (2.473974 --> 2.473954).  Saving model ...\n",
      "Epoch: 424 \tTraining Loss: 2.454988 \tValidation Loss: 2.473922\n",
      "Validation loss decreased (2.473954 --> 2.473922).  Saving model ...\n",
      "Epoch: 425 \tTraining Loss: 2.467444 \tValidation Loss: 2.473870\n",
      "Validation loss decreased (2.473922 --> 2.473870).  Saving model ...\n",
      "Epoch: 426 \tTraining Loss: 2.458734 \tValidation Loss: 2.473854\n",
      "Validation loss decreased (2.473870 --> 2.473854).  Saving model ...\n",
      "Epoch: 427 \tTraining Loss: 2.453552 \tValidation Loss: 2.473810\n",
      "Validation loss decreased (2.473854 --> 2.473810).  Saving model ...\n",
      "Epoch: 428 \tTraining Loss: 2.461344 \tValidation Loss: 2.473843\n",
      "Epoch: 429 \tTraining Loss: 2.459657 \tValidation Loss: 2.473768\n",
      "Validation loss decreased (2.473810 --> 2.473768).  Saving model ...\n",
      "Epoch: 430 \tTraining Loss: 2.469523 \tValidation Loss: 2.473764\n",
      "Validation loss decreased (2.473768 --> 2.473764).  Saving model ...\n",
      "Epoch: 431 \tTraining Loss: 2.467633 \tValidation Loss: 2.473714\n",
      "Validation loss decreased (2.473764 --> 2.473714).  Saving model ...\n",
      "Epoch: 432 \tTraining Loss: 2.461549 \tValidation Loss: 2.473658\n",
      "Validation loss decreased (2.473714 --> 2.473658).  Saving model ...\n",
      "Epoch: 433 \tTraining Loss: 2.456673 \tValidation Loss: 2.473580\n",
      "Validation loss decreased (2.473658 --> 2.473580).  Saving model ...\n",
      "Epoch: 434 \tTraining Loss: 2.457681 \tValidation Loss: 2.473553\n",
      "Validation loss decreased (2.473580 --> 2.473553).  Saving model ...\n",
      "Epoch: 435 \tTraining Loss: 2.465348 \tValidation Loss: 2.473529\n",
      "Validation loss decreased (2.473553 --> 2.473529).  Saving model ...\n",
      "Epoch: 436 \tTraining Loss: 2.460944 \tValidation Loss: 2.473507\n",
      "Validation loss decreased (2.473529 --> 2.473507).  Saving model ...\n",
      "Epoch: 437 \tTraining Loss: 2.456172 \tValidation Loss: 2.473485\n",
      "Validation loss decreased (2.473507 --> 2.473485).  Saving model ...\n",
      "Epoch: 438 \tTraining Loss: 2.457042 \tValidation Loss: 2.473445\n",
      "Validation loss decreased (2.473485 --> 2.473445).  Saving model ...\n",
      "Epoch: 439 \tTraining Loss: 2.465234 \tValidation Loss: 2.473409\n",
      "Validation loss decreased (2.473445 --> 2.473409).  Saving model ...\n",
      "Epoch: 440 \tTraining Loss: 2.460361 \tValidation Loss: 2.473372\n",
      "Validation loss decreased (2.473409 --> 2.473372).  Saving model ...\n",
      "Epoch: 441 \tTraining Loss: 2.454487 \tValidation Loss: 2.473319\n",
      "Validation loss decreased (2.473372 --> 2.473319).  Saving model ...\n",
      "Epoch: 442 \tTraining Loss: 2.471866 \tValidation Loss: 2.473282\n",
      "Validation loss decreased (2.473319 --> 2.473282).  Saving model ...\n",
      "Epoch: 443 \tTraining Loss: 2.467027 \tValidation Loss: 2.473243\n",
      "Validation loss decreased (2.473282 --> 2.473243).  Saving model ...\n",
      "Epoch: 444 \tTraining Loss: 2.462249 \tValidation Loss: 2.473200\n",
      "Validation loss decreased (2.473243 --> 2.473200).  Saving model ...\n",
      "Epoch: 445 \tTraining Loss: 2.457669 \tValidation Loss: 2.473142\n",
      "Validation loss decreased (2.473200 --> 2.473142).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 446 \tTraining Loss: 2.466679 \tValidation Loss: 2.473127\n",
      "Validation loss decreased (2.473142 --> 2.473127).  Saving model ...\n",
      "Epoch: 447 \tTraining Loss: 2.452701 \tValidation Loss: 2.473094\n",
      "Validation loss decreased (2.473127 --> 2.473094).  Saving model ...\n",
      "Epoch: 448 \tTraining Loss: 2.457352 \tValidation Loss: 2.473065\n",
      "Validation loss decreased (2.473094 --> 2.473065).  Saving model ...\n",
      "Epoch: 449 \tTraining Loss: 2.461504 \tValidation Loss: 2.473040\n",
      "Validation loss decreased (2.473065 --> 2.473040).  Saving model ...\n",
      "Epoch: 450 \tTraining Loss: 2.462683 \tValidation Loss: 2.472968\n",
      "Validation loss decreased (2.473040 --> 2.472968).  Saving model ...\n",
      "Epoch: 451 \tTraining Loss: 2.477254 \tValidation Loss: 2.472972\n",
      "Epoch: 452 \tTraining Loss: 2.461592 \tValidation Loss: 2.472972\n",
      "Epoch: 453 \tTraining Loss: 2.462793 \tValidation Loss: 2.472931\n",
      "Validation loss decreased (2.472968 --> 2.472931).  Saving model ...\n",
      "Epoch: 454 \tTraining Loss: 2.462206 \tValidation Loss: 2.472886\n",
      "Validation loss decreased (2.472931 --> 2.472886).  Saving model ...\n",
      "Epoch: 455 \tTraining Loss: 2.458328 \tValidation Loss: 2.472817\n",
      "Validation loss decreased (2.472886 --> 2.472817).  Saving model ...\n",
      "Epoch: 456 \tTraining Loss: 2.466676 \tValidation Loss: 2.472793\n",
      "Validation loss decreased (2.472817 --> 2.472793).  Saving model ...\n",
      "Epoch: 457 \tTraining Loss: 2.460214 \tValidation Loss: 2.472784\n",
      "Validation loss decreased (2.472793 --> 2.472784).  Saving model ...\n",
      "Epoch: 458 \tTraining Loss: 2.467191 \tValidation Loss: 2.472816\n",
      "Epoch: 459 \tTraining Loss: 2.462668 \tValidation Loss: 2.472781\n",
      "Validation loss decreased (2.472784 --> 2.472781).  Saving model ...\n",
      "Epoch: 460 \tTraining Loss: 2.455883 \tValidation Loss: 2.472753\n",
      "Validation loss decreased (2.472781 --> 2.472753).  Saving model ...\n",
      "Epoch: 461 \tTraining Loss: 2.454761 \tValidation Loss: 2.472728\n",
      "Validation loss decreased (2.472753 --> 2.472728).  Saving model ...\n",
      "Epoch: 462 \tTraining Loss: 2.463728 \tValidation Loss: 2.472712\n",
      "Validation loss decreased (2.472728 --> 2.472712).  Saving model ...\n",
      "Epoch: 463 \tTraining Loss: 2.456579 \tValidation Loss: 2.472672\n",
      "Validation loss decreased (2.472712 --> 2.472672).  Saving model ...\n",
      "Epoch: 464 \tTraining Loss: 2.466033 \tValidation Loss: 2.472672\n",
      "Epoch: 465 \tTraining Loss: 2.455983 \tValidation Loss: 2.472622\n",
      "Validation loss decreased (2.472672 --> 2.472622).  Saving model ...\n",
      "Epoch: 466 \tTraining Loss: 2.453757 \tValidation Loss: 2.472617\n",
      "Validation loss decreased (2.472622 --> 2.472617).  Saving model ...\n",
      "Epoch: 467 \tTraining Loss: 2.457906 \tValidation Loss: 2.472601\n",
      "Validation loss decreased (2.472617 --> 2.472601).  Saving model ...\n",
      "Epoch: 468 \tTraining Loss: 2.461855 \tValidation Loss: 2.472597\n",
      "Validation loss decreased (2.472601 --> 2.472597).  Saving model ...\n",
      "Epoch: 469 \tTraining Loss: 2.464023 \tValidation Loss: 2.472568\n",
      "Validation loss decreased (2.472597 --> 2.472568).  Saving model ...\n",
      "Epoch: 470 \tTraining Loss: 2.452495 \tValidation Loss: 2.472536\n",
      "Validation loss decreased (2.472568 --> 2.472536).  Saving model ...\n",
      "Epoch: 471 \tTraining Loss: 2.468969 \tValidation Loss: 2.472510\n",
      "Validation loss decreased (2.472536 --> 2.472510).  Saving model ...\n",
      "Epoch: 472 \tTraining Loss: 2.463424 \tValidation Loss: 2.472465\n",
      "Validation loss decreased (2.472510 --> 2.472465).  Saving model ...\n",
      "Epoch: 473 \tTraining Loss: 2.464311 \tValidation Loss: 2.472388\n",
      "Validation loss decreased (2.472465 --> 2.472388).  Saving model ...\n",
      "Epoch: 474 \tTraining Loss: 2.456029 \tValidation Loss: 2.472392\n",
      "Epoch: 475 \tTraining Loss: 2.464866 \tValidation Loss: 2.472382\n",
      "Validation loss decreased (2.472388 --> 2.472382).  Saving model ...\n",
      "Epoch: 476 \tTraining Loss: 2.458495 \tValidation Loss: 2.472323\n",
      "Validation loss decreased (2.472382 --> 2.472323).  Saving model ...\n",
      "Epoch: 477 \tTraining Loss: 2.465036 \tValidation Loss: 2.472336\n",
      "Epoch: 478 \tTraining Loss: 2.462350 \tValidation Loss: 2.472366\n",
      "Epoch: 479 \tTraining Loss: 2.450674 \tValidation Loss: 2.472390\n",
      "Epoch: 480 \tTraining Loss: 2.453828 \tValidation Loss: 2.472336\n",
      "Epoch: 481 \tTraining Loss: 2.468256 \tValidation Loss: 2.472317\n",
      "Validation loss decreased (2.472323 --> 2.472317).  Saving model ...\n",
      "Epoch: 482 \tTraining Loss: 2.464890 \tValidation Loss: 2.472273\n",
      "Validation loss decreased (2.472317 --> 2.472273).  Saving model ...\n",
      "Epoch: 483 \tTraining Loss: 2.456495 \tValidation Loss: 2.472235\n",
      "Validation loss decreased (2.472273 --> 2.472235).  Saving model ...\n",
      "Epoch: 484 \tTraining Loss: 2.452574 \tValidation Loss: 2.472149\n",
      "Validation loss decreased (2.472235 --> 2.472149).  Saving model ...\n",
      "Epoch: 485 \tTraining Loss: 2.455498 \tValidation Loss: 2.472141\n",
      "Validation loss decreased (2.472149 --> 2.472141).  Saving model ...\n",
      "Epoch: 486 \tTraining Loss: 2.456482 \tValidation Loss: 2.472097\n",
      "Validation loss decreased (2.472141 --> 2.472097).  Saving model ...\n",
      "Epoch: 487 \tTraining Loss: 2.473046 \tValidation Loss: 2.472054\n",
      "Validation loss decreased (2.472097 --> 2.472054).  Saving model ...\n",
      "Epoch: 488 \tTraining Loss: 2.461414 \tValidation Loss: 2.472018\n",
      "Validation loss decreased (2.472054 --> 2.472018).  Saving model ...\n",
      "Epoch: 489 \tTraining Loss: 2.463116 \tValidation Loss: 2.472015\n",
      "Validation loss decreased (2.472018 --> 2.472015).  Saving model ...\n",
      "Epoch: 490 \tTraining Loss: 2.465693 \tValidation Loss: 2.472016\n",
      "Epoch: 491 \tTraining Loss: 2.464749 \tValidation Loss: 2.471996\n",
      "Validation loss decreased (2.472015 --> 2.471996).  Saving model ...\n",
      "Epoch: 492 \tTraining Loss: 2.464804 \tValidation Loss: 2.471911\n",
      "Validation loss decreased (2.471996 --> 2.471911).  Saving model ...\n",
      "Epoch: 493 \tTraining Loss: 2.457456 \tValidation Loss: 2.471886\n",
      "Validation loss decreased (2.471911 --> 2.471886).  Saving model ...\n",
      "Epoch: 494 \tTraining Loss: 2.454674 \tValidation Loss: 2.471857\n",
      "Validation loss decreased (2.471886 --> 2.471857).  Saving model ...\n",
      "Epoch: 495 \tTraining Loss: 2.459251 \tValidation Loss: 2.471861\n",
      "Epoch: 496 \tTraining Loss: 2.454010 \tValidation Loss: 2.471817\n",
      "Validation loss decreased (2.471857 --> 2.471817).  Saving model ...\n",
      "Epoch: 497 \tTraining Loss: 2.448661 \tValidation Loss: 2.471842\n",
      "Epoch: 498 \tTraining Loss: 2.467449 \tValidation Loss: 2.471835\n",
      "Epoch: 499 \tTraining Loss: 2.464102 \tValidation Loss: 2.471819\n",
      "Epoch: 500 \tTraining Loss: 2.470058 \tValidation Loss: 2.471859\n",
      "Epoch: 501 \tTraining Loss: 2.468249 \tValidation Loss: 2.471842\n",
      "Epoch: 502 \tTraining Loss: 2.467491 \tValidation Loss: 2.471809\n",
      "Validation loss decreased (2.471817 --> 2.471809).  Saving model ...\n",
      "Epoch: 503 \tTraining Loss: 2.464795 \tValidation Loss: 2.471799\n",
      "Validation loss decreased (2.471809 --> 2.471799).  Saving model ...\n",
      "Epoch: 504 \tTraining Loss: 2.459340 \tValidation Loss: 2.471765\n",
      "Validation loss decreased (2.471799 --> 2.471765).  Saving model ...\n",
      "Epoch: 505 \tTraining Loss: 2.462539 \tValidation Loss: 2.471761\n",
      "Validation loss decreased (2.471765 --> 2.471761).  Saving model ...\n",
      "Epoch: 506 \tTraining Loss: 2.460009 \tValidation Loss: 2.471742\n",
      "Validation loss decreased (2.471761 --> 2.471742).  Saving model ...\n",
      "Epoch: 507 \tTraining Loss: 2.447075 \tValidation Loss: 2.471697\n",
      "Validation loss decreased (2.471742 --> 2.471697).  Saving model ...\n",
      "Epoch: 508 \tTraining Loss: 2.458961 \tValidation Loss: 2.471682\n",
      "Validation loss decreased (2.471697 --> 2.471682).  Saving model ...\n",
      "Epoch: 509 \tTraining Loss: 2.458335 \tValidation Loss: 2.471647\n",
      "Validation loss decreased (2.471682 --> 2.471647).  Saving model ...\n",
      "Epoch: 510 \tTraining Loss: 2.465259 \tValidation Loss: 2.471621\n",
      "Validation loss decreased (2.471647 --> 2.471621).  Saving model ...\n",
      "Epoch: 511 \tTraining Loss: 2.467133 \tValidation Loss: 2.471609\n",
      "Validation loss decreased (2.471621 --> 2.471609).  Saving model ...\n",
      "Epoch: 512 \tTraining Loss: 2.453927 \tValidation Loss: 2.471581\n",
      "Validation loss decreased (2.471609 --> 2.471581).  Saving model ...\n",
      "Epoch: 513 \tTraining Loss: 2.457165 \tValidation Loss: 2.471558\n",
      "Validation loss decreased (2.471581 --> 2.471558).  Saving model ...\n",
      "Epoch: 514 \tTraining Loss: 2.456437 \tValidation Loss: 2.471552\n",
      "Validation loss decreased (2.471558 --> 2.471552).  Saving model ...\n",
      "Epoch: 515 \tTraining Loss: 2.456221 \tValidation Loss: 2.471540\n",
      "Validation loss decreased (2.471552 --> 2.471540).  Saving model ...\n",
      "Epoch: 516 \tTraining Loss: 2.463041 \tValidation Loss: 2.471537\n",
      "Validation loss decreased (2.471540 --> 2.471537).  Saving model ...\n",
      "Epoch: 517 \tTraining Loss: 2.463796 \tValidation Loss: 2.471539\n",
      "Epoch: 518 \tTraining Loss: 2.458323 \tValidation Loss: 2.471507\n",
      "Validation loss decreased (2.471537 --> 2.471507).  Saving model ...\n",
      "Epoch: 519 \tTraining Loss: 2.465281 \tValidation Loss: 2.471471\n",
      "Validation loss decreased (2.471507 --> 2.471471).  Saving model ...\n",
      "Epoch: 520 \tTraining Loss: 2.461249 \tValidation Loss: 2.471427\n",
      "Validation loss decreased (2.471471 --> 2.471427).  Saving model ...\n",
      "Epoch: 521 \tTraining Loss: 2.463845 \tValidation Loss: 2.471436\n",
      "Epoch: 522 \tTraining Loss: 2.465462 \tValidation Loss: 2.471412\n",
      "Validation loss decreased (2.471427 --> 2.471412).  Saving model ...\n",
      "Epoch: 523 \tTraining Loss: 2.464182 \tValidation Loss: 2.471380\n",
      "Validation loss decreased (2.471412 --> 2.471380).  Saving model ...\n",
      "Epoch: 524 \tTraining Loss: 2.463349 \tValidation Loss: 2.471355\n",
      "Validation loss decreased (2.471380 --> 2.471355).  Saving model ...\n",
      "Epoch: 525 \tTraining Loss: 2.453536 \tValidation Loss: 2.471301\n",
      "Validation loss decreased (2.471355 --> 2.471301).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 526 \tTraining Loss: 2.461933 \tValidation Loss: 2.471290\n",
      "Validation loss decreased (2.471301 --> 2.471290).  Saving model ...\n",
      "Epoch: 527 \tTraining Loss: 2.452029 \tValidation Loss: 2.471255\n",
      "Validation loss decreased (2.471290 --> 2.471255).  Saving model ...\n",
      "Epoch: 528 \tTraining Loss: 2.458251 \tValidation Loss: 2.471264\n",
      "Epoch: 529 \tTraining Loss: 2.459132 \tValidation Loss: 2.471225\n",
      "Validation loss decreased (2.471255 --> 2.471225).  Saving model ...\n",
      "Epoch: 530 \tTraining Loss: 2.457732 \tValidation Loss: 2.471215\n",
      "Validation loss decreased (2.471225 --> 2.471215).  Saving model ...\n",
      "Epoch: 531 \tTraining Loss: 2.456793 \tValidation Loss: 2.471177\n",
      "Validation loss decreased (2.471215 --> 2.471177).  Saving model ...\n",
      "Epoch: 532 \tTraining Loss: 2.462934 \tValidation Loss: 2.471129\n",
      "Validation loss decreased (2.471177 --> 2.471129).  Saving model ...\n",
      "Epoch: 533 \tTraining Loss: 2.462792 \tValidation Loss: 2.471122\n",
      "Validation loss decreased (2.471129 --> 2.471122).  Saving model ...\n",
      "Epoch: 534 \tTraining Loss: 2.453598 \tValidation Loss: 2.471106\n",
      "Validation loss decreased (2.471122 --> 2.471106).  Saving model ...\n",
      "Epoch: 535 \tTraining Loss: 2.459641 \tValidation Loss: 2.471082\n",
      "Validation loss decreased (2.471106 --> 2.471082).  Saving model ...\n",
      "Epoch: 536 \tTraining Loss: 2.462155 \tValidation Loss: 2.471082\n",
      "Validation loss decreased (2.471082 --> 2.471082).  Saving model ...\n",
      "Epoch: 537 \tTraining Loss: 2.453921 \tValidation Loss: 2.471056\n",
      "Validation loss decreased (2.471082 --> 2.471056).  Saving model ...\n",
      "Epoch: 538 \tTraining Loss: 2.460070 \tValidation Loss: 2.471012\n",
      "Validation loss decreased (2.471056 --> 2.471012).  Saving model ...\n",
      "Epoch: 539 \tTraining Loss: 2.452853 \tValidation Loss: 2.470979\n",
      "Validation loss decreased (2.471012 --> 2.470979).  Saving model ...\n",
      "Epoch: 540 \tTraining Loss: 2.451235 \tValidation Loss: 2.470911\n",
      "Validation loss decreased (2.470979 --> 2.470911).  Saving model ...\n",
      "Epoch: 541 \tTraining Loss: 2.457505 \tValidation Loss: 2.470924\n",
      "Epoch: 542 \tTraining Loss: 2.460449 \tValidation Loss: 2.470957\n",
      "Epoch: 543 \tTraining Loss: 2.458689 \tValidation Loss: 2.470923\n",
      "Epoch: 544 \tTraining Loss: 2.459838 \tValidation Loss: 2.470896\n",
      "Validation loss decreased (2.470911 --> 2.470896).  Saving model ...\n",
      "Epoch: 545 \tTraining Loss: 2.451411 \tValidation Loss: 2.470866\n",
      "Validation loss decreased (2.470896 --> 2.470866).  Saving model ...\n",
      "Epoch: 546 \tTraining Loss: 2.454937 \tValidation Loss: 2.470882\n",
      "Epoch: 547 \tTraining Loss: 2.457329 \tValidation Loss: 2.470881\n",
      "Epoch: 548 \tTraining Loss: 2.455692 \tValidation Loss: 2.470865\n",
      "Validation loss decreased (2.470866 --> 2.470865).  Saving model ...\n",
      "Epoch: 549 \tTraining Loss: 2.455310 \tValidation Loss: 2.470845\n",
      "Validation loss decreased (2.470865 --> 2.470845).  Saving model ...\n",
      "Epoch: 550 \tTraining Loss: 2.458359 \tValidation Loss: 2.470810\n",
      "Validation loss decreased (2.470845 --> 2.470810).  Saving model ...\n",
      "Epoch: 551 \tTraining Loss: 2.465399 \tValidation Loss: 2.470781\n",
      "Validation loss decreased (2.470810 --> 2.470781).  Saving model ...\n",
      "Epoch: 552 \tTraining Loss: 2.455296 \tValidation Loss: 2.470760\n",
      "Validation loss decreased (2.470781 --> 2.470760).  Saving model ...\n",
      "Epoch: 553 \tTraining Loss: 2.449369 \tValidation Loss: 2.470691\n",
      "Validation loss decreased (2.470760 --> 2.470691).  Saving model ...\n",
      "Epoch: 554 \tTraining Loss: 2.464118 \tValidation Loss: 2.470659\n",
      "Validation loss decreased (2.470691 --> 2.470659).  Saving model ...\n",
      "Epoch: 555 \tTraining Loss: 2.449265 \tValidation Loss: 2.470613\n",
      "Validation loss decreased (2.470659 --> 2.470613).  Saving model ...\n",
      "Epoch: 556 \tTraining Loss: 2.452019 \tValidation Loss: 2.470580\n",
      "Validation loss decreased (2.470613 --> 2.470580).  Saving model ...\n",
      "Epoch: 557 \tTraining Loss: 2.453705 \tValidation Loss: 2.470572\n",
      "Validation loss decreased (2.470580 --> 2.470572).  Saving model ...\n",
      "Epoch: 558 \tTraining Loss: 2.459182 \tValidation Loss: 2.470579\n",
      "Epoch: 559 \tTraining Loss: 2.465447 \tValidation Loss: 2.470599\n",
      "Epoch: 560 \tTraining Loss: 2.452392 \tValidation Loss: 2.470594\n",
      "Epoch: 561 \tTraining Loss: 2.464747 \tValidation Loss: 2.470550\n",
      "Validation loss decreased (2.470572 --> 2.470550).  Saving model ...\n",
      "Epoch: 562 \tTraining Loss: 2.450498 \tValidation Loss: 2.470496\n",
      "Validation loss decreased (2.470550 --> 2.470496).  Saving model ...\n",
      "Epoch: 563 \tTraining Loss: 2.463165 \tValidation Loss: 2.470526\n",
      "Epoch: 564 \tTraining Loss: 2.461922 \tValidation Loss: 2.470528\n",
      "Epoch: 565 \tTraining Loss: 2.452567 \tValidation Loss: 2.470506\n",
      "Epoch: 566 \tTraining Loss: 2.463377 \tValidation Loss: 2.470500\n",
      "Epoch: 567 \tTraining Loss: 2.459282 \tValidation Loss: 2.470482\n",
      "Validation loss decreased (2.470496 --> 2.470482).  Saving model ...\n",
      "Epoch: 568 \tTraining Loss: 2.460102 \tValidation Loss: 2.470501\n",
      "Epoch: 569 \tTraining Loss: 2.447706 \tValidation Loss: 2.470510\n",
      "Epoch: 570 \tTraining Loss: 2.463531 \tValidation Loss: 2.470534\n",
      "Epoch: 571 \tTraining Loss: 2.460388 \tValidation Loss: 2.470519\n",
      "Epoch: 572 \tTraining Loss: 2.460623 \tValidation Loss: 2.470464\n",
      "Validation loss decreased (2.470482 --> 2.470464).  Saving model ...\n",
      "Epoch: 573 \tTraining Loss: 2.465859 \tValidation Loss: 2.470409\n",
      "Validation loss decreased (2.470464 --> 2.470409).  Saving model ...\n",
      "Epoch: 574 \tTraining Loss: 2.460299 \tValidation Loss: 2.470399\n",
      "Validation loss decreased (2.470409 --> 2.470399).  Saving model ...\n",
      "Epoch: 575 \tTraining Loss: 2.454977 \tValidation Loss: 2.470320\n",
      "Validation loss decreased (2.470399 --> 2.470320).  Saving model ...\n",
      "Epoch: 576 \tTraining Loss: 2.452182 \tValidation Loss: 2.470250\n",
      "Validation loss decreased (2.470320 --> 2.470250).  Saving model ...\n",
      "Epoch: 577 \tTraining Loss: 2.450989 \tValidation Loss: 2.470211\n",
      "Validation loss decreased (2.470250 --> 2.470211).  Saving model ...\n",
      "Epoch: 578 \tTraining Loss: 2.453217 \tValidation Loss: 2.470178\n",
      "Validation loss decreased (2.470211 --> 2.470178).  Saving model ...\n",
      "Epoch: 579 \tTraining Loss: 2.453396 \tValidation Loss: 2.470173\n",
      "Validation loss decreased (2.470178 --> 2.470173).  Saving model ...\n",
      "Epoch: 580 \tTraining Loss: 2.457282 \tValidation Loss: 2.470167\n",
      "Validation loss decreased (2.470173 --> 2.470167).  Saving model ...\n",
      "Epoch: 581 \tTraining Loss: 2.452569 \tValidation Loss: 2.470126\n",
      "Validation loss decreased (2.470167 --> 2.470126).  Saving model ...\n",
      "Epoch: 582 \tTraining Loss: 2.450208 \tValidation Loss: 2.470082\n",
      "Validation loss decreased (2.470126 --> 2.470082).  Saving model ...\n",
      "Epoch: 583 \tTraining Loss: 2.461325 \tValidation Loss: 2.470042\n",
      "Validation loss decreased (2.470082 --> 2.470042).  Saving model ...\n",
      "Epoch: 584 \tTraining Loss: 2.448246 \tValidation Loss: 2.469977\n",
      "Validation loss decreased (2.470042 --> 2.469977).  Saving model ...\n",
      "Epoch: 585 \tTraining Loss: 2.462404 \tValidation Loss: 2.469951\n",
      "Validation loss decreased (2.469977 --> 2.469951).  Saving model ...\n",
      "Epoch: 586 \tTraining Loss: 2.464352 \tValidation Loss: 2.469923\n",
      "Validation loss decreased (2.469951 --> 2.469923).  Saving model ...\n",
      "Epoch: 587 \tTraining Loss: 2.458810 \tValidation Loss: 2.469921\n",
      "Validation loss decreased (2.469923 --> 2.469921).  Saving model ...\n",
      "Epoch: 588 \tTraining Loss: 2.453164 \tValidation Loss: 2.469878\n",
      "Validation loss decreased (2.469921 --> 2.469878).  Saving model ...\n",
      "Epoch: 589 \tTraining Loss: 2.460115 \tValidation Loss: 2.469865\n",
      "Validation loss decreased (2.469878 --> 2.469865).  Saving model ...\n",
      "Epoch: 590 \tTraining Loss: 2.461074 \tValidation Loss: 2.469855\n",
      "Validation loss decreased (2.469865 --> 2.469855).  Saving model ...\n",
      "Epoch: 591 \tTraining Loss: 2.453532 \tValidation Loss: 2.469841\n",
      "Validation loss decreased (2.469855 --> 2.469841).  Saving model ...\n",
      "Epoch: 592 \tTraining Loss: 2.454024 \tValidation Loss: 2.469839\n",
      "Validation loss decreased (2.469841 --> 2.469839).  Saving model ...\n",
      "Epoch: 593 \tTraining Loss: 2.444850 \tValidation Loss: 2.469762\n",
      "Validation loss decreased (2.469839 --> 2.469762).  Saving model ...\n",
      "Epoch: 594 \tTraining Loss: 2.449253 \tValidation Loss: 2.469734\n",
      "Validation loss decreased (2.469762 --> 2.469734).  Saving model ...\n",
      "Epoch: 595 \tTraining Loss: 2.453615 \tValidation Loss: 2.469696\n",
      "Validation loss decreased (2.469734 --> 2.469696).  Saving model ...\n",
      "Epoch: 596 \tTraining Loss: 2.461845 \tValidation Loss: 2.469641\n",
      "Validation loss decreased (2.469696 --> 2.469641).  Saving model ...\n",
      "Epoch: 597 \tTraining Loss: 2.452129 \tValidation Loss: 2.469629\n",
      "Validation loss decreased (2.469641 --> 2.469629).  Saving model ...\n",
      "Epoch: 598 \tTraining Loss: 2.461661 \tValidation Loss: 2.469578\n",
      "Validation loss decreased (2.469629 --> 2.469578).  Saving model ...\n",
      "Epoch: 599 \tTraining Loss: 2.465089 \tValidation Loss: 2.469581\n",
      "Epoch: 600 \tTraining Loss: 2.452597 \tValidation Loss: 2.469545\n",
      "Validation loss decreased (2.469578 --> 2.469545).  Saving model ...\n",
      "Epoch: 601 \tTraining Loss: 2.460895 \tValidation Loss: 2.469520\n",
      "Validation loss decreased (2.469545 --> 2.469520).  Saving model ...\n",
      "Epoch: 602 \tTraining Loss: 2.456569 \tValidation Loss: 2.469507\n",
      "Validation loss decreased (2.469520 --> 2.469507).  Saving model ...\n",
      "Epoch: 603 \tTraining Loss: 2.456290 \tValidation Loss: 2.469480\n",
      "Validation loss decreased (2.469507 --> 2.469480).  Saving model ...\n",
      "Epoch: 604 \tTraining Loss: 2.455877 \tValidation Loss: 2.469468\n",
      "Validation loss decreased (2.469480 --> 2.469468).  Saving model ...\n",
      "Epoch: 605 \tTraining Loss: 2.462484 \tValidation Loss: 2.469435\n",
      "Validation loss decreased (2.469468 --> 2.469435).  Saving model ...\n",
      "Epoch: 606 \tTraining Loss: 2.453960 \tValidation Loss: 2.469399\n",
      "Validation loss decreased (2.469435 --> 2.469399).  Saving model ...\n",
      "Epoch: 607 \tTraining Loss: 2.465233 \tValidation Loss: 2.469413\n",
      "Epoch: 608 \tTraining Loss: 2.458656 \tValidation Loss: 2.469380\n",
      "Validation loss decreased (2.469399 --> 2.469380).  Saving model ...\n",
      "Epoch: 609 \tTraining Loss: 2.449836 \tValidation Loss: 2.469346\n",
      "Validation loss decreased (2.469380 --> 2.469346).  Saving model ...\n",
      "Epoch: 610 \tTraining Loss: 2.463751 \tValidation Loss: 2.469346\n",
      "Epoch: 611 \tTraining Loss: 2.455953 \tValidation Loss: 2.469353\n",
      "Epoch: 612 \tTraining Loss: 2.456458 \tValidation Loss: 2.469295\n",
      "Validation loss decreased (2.469346 --> 2.469295).  Saving model ...\n",
      "Epoch: 613 \tTraining Loss: 2.450929 \tValidation Loss: 2.469260\n",
      "Validation loss decreased (2.469295 --> 2.469260).  Saving model ...\n",
      "Epoch: 614 \tTraining Loss: 2.458146 \tValidation Loss: 2.469218\n",
      "Validation loss decreased (2.469260 --> 2.469218).  Saving model ...\n",
      "Epoch: 615 \tTraining Loss: 2.463639 \tValidation Loss: 2.469225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 616 \tTraining Loss: 2.457808 \tValidation Loss: 2.469184\n",
      "Validation loss decreased (2.469218 --> 2.469184).  Saving model ...\n",
      "Epoch: 617 \tTraining Loss: 2.455171 \tValidation Loss: 2.469183\n",
      "Validation loss decreased (2.469184 --> 2.469183).  Saving model ...\n",
      "Epoch: 618 \tTraining Loss: 2.453059 \tValidation Loss: 2.469107\n",
      "Validation loss decreased (2.469183 --> 2.469107).  Saving model ...\n",
      "Epoch: 619 \tTraining Loss: 2.444969 \tValidation Loss: 2.469045\n",
      "Validation loss decreased (2.469107 --> 2.469045).  Saving model ...\n",
      "Epoch: 620 \tTraining Loss: 2.453737 \tValidation Loss: 2.468992\n",
      "Validation loss decreased (2.469045 --> 2.468992).  Saving model ...\n",
      "Epoch: 621 \tTraining Loss: 2.452703 \tValidation Loss: 2.468988\n",
      "Validation loss decreased (2.468992 --> 2.468988).  Saving model ...\n",
      "Epoch: 622 \tTraining Loss: 2.454526 \tValidation Loss: 2.468986\n",
      "Validation loss decreased (2.468988 --> 2.468986).  Saving model ...\n",
      "Epoch: 623 \tTraining Loss: 2.458351 \tValidation Loss: 2.469009\n",
      "Epoch: 624 \tTraining Loss: 2.465930 \tValidation Loss: 2.469006\n",
      "Epoch: 625 \tTraining Loss: 2.461173 \tValidation Loss: 2.468950\n",
      "Validation loss decreased (2.468986 --> 2.468950).  Saving model ...\n",
      "Epoch: 626 \tTraining Loss: 2.451730 \tValidation Loss: 2.468941\n",
      "Validation loss decreased (2.468950 --> 2.468941).  Saving model ...\n",
      "Epoch: 627 \tTraining Loss: 2.450209 \tValidation Loss: 2.468931\n",
      "Validation loss decreased (2.468941 --> 2.468931).  Saving model ...\n",
      "Epoch: 628 \tTraining Loss: 2.457571 \tValidation Loss: 2.468924\n",
      "Validation loss decreased (2.468931 --> 2.468924).  Saving model ...\n",
      "Epoch: 629 \tTraining Loss: 2.451694 \tValidation Loss: 2.468848\n",
      "Validation loss decreased (2.468924 --> 2.468848).  Saving model ...\n",
      "Epoch: 630 \tTraining Loss: 2.468161 \tValidation Loss: 2.468855\n",
      "Epoch: 631 \tTraining Loss: 2.454294 \tValidation Loss: 2.468829\n",
      "Validation loss decreased (2.468848 --> 2.468829).  Saving model ...\n",
      "Epoch: 632 \tTraining Loss: 2.458265 \tValidation Loss: 2.468800\n",
      "Validation loss decreased (2.468829 --> 2.468800).  Saving model ...\n",
      "Epoch: 633 \tTraining Loss: 2.444267 \tValidation Loss: 2.468733\n",
      "Validation loss decreased (2.468800 --> 2.468733).  Saving model ...\n",
      "Epoch: 634 \tTraining Loss: 2.457500 \tValidation Loss: 2.468668\n",
      "Validation loss decreased (2.468733 --> 2.468668).  Saving model ...\n",
      "Epoch: 635 \tTraining Loss: 2.463605 \tValidation Loss: 2.468661\n",
      "Validation loss decreased (2.468668 --> 2.468661).  Saving model ...\n",
      "Epoch: 636 \tTraining Loss: 2.458354 \tValidation Loss: 2.468653\n",
      "Validation loss decreased (2.468661 --> 2.468653).  Saving model ...\n",
      "Epoch: 637 \tTraining Loss: 2.453863 \tValidation Loss: 2.468659\n",
      "Epoch: 638 \tTraining Loss: 2.455776 \tValidation Loss: 2.468618\n",
      "Validation loss decreased (2.468653 --> 2.468618).  Saving model ...\n",
      "Epoch: 639 \tTraining Loss: 2.458531 \tValidation Loss: 2.468549\n",
      "Validation loss decreased (2.468618 --> 2.468549).  Saving model ...\n",
      "Epoch: 640 \tTraining Loss: 2.451835 \tValidation Loss: 2.468475\n",
      "Validation loss decreased (2.468549 --> 2.468475).  Saving model ...\n",
      "Epoch: 641 \tTraining Loss: 2.463701 \tValidation Loss: 2.468500\n",
      "Epoch: 642 \tTraining Loss: 2.460404 \tValidation Loss: 2.468467\n",
      "Validation loss decreased (2.468475 --> 2.468467).  Saving model ...\n",
      "Epoch: 643 \tTraining Loss: 2.456940 \tValidation Loss: 2.468479\n",
      "Epoch: 644 \tTraining Loss: 2.464123 \tValidation Loss: 2.468496\n",
      "Epoch: 645 \tTraining Loss: 2.450479 \tValidation Loss: 2.468468\n",
      "Epoch: 646 \tTraining Loss: 2.458657 \tValidation Loss: 2.468397\n",
      "Validation loss decreased (2.468467 --> 2.468397).  Saving model ...\n",
      "Epoch: 647 \tTraining Loss: 2.453588 \tValidation Loss: 2.468388\n",
      "Validation loss decreased (2.468397 --> 2.468388).  Saving model ...\n",
      "Epoch: 648 \tTraining Loss: 2.452052 \tValidation Loss: 2.468346\n",
      "Validation loss decreased (2.468388 --> 2.468346).  Saving model ...\n",
      "Epoch: 649 \tTraining Loss: 2.457660 \tValidation Loss: 2.468339\n",
      "Validation loss decreased (2.468346 --> 2.468339).  Saving model ...\n",
      "Epoch: 650 \tTraining Loss: 2.457089 \tValidation Loss: 2.468316\n",
      "Validation loss decreased (2.468339 --> 2.468316).  Saving model ...\n",
      "Epoch: 651 \tTraining Loss: 2.446790 \tValidation Loss: 2.468300\n",
      "Validation loss decreased (2.468316 --> 2.468300).  Saving model ...\n",
      "Epoch: 652 \tTraining Loss: 2.455093 \tValidation Loss: 2.468327\n",
      "Epoch: 653 \tTraining Loss: 2.454685 \tValidation Loss: 2.468330\n",
      "Epoch: 654 \tTraining Loss: 2.455275 \tValidation Loss: 2.468287\n",
      "Validation loss decreased (2.468300 --> 2.468287).  Saving model ...\n",
      "Epoch: 655 \tTraining Loss: 2.457386 \tValidation Loss: 2.468234\n",
      "Validation loss decreased (2.468287 --> 2.468234).  Saving model ...\n",
      "Epoch: 656 \tTraining Loss: 2.454315 \tValidation Loss: 2.468210\n",
      "Validation loss decreased (2.468234 --> 2.468210).  Saving model ...\n",
      "Epoch: 657 \tTraining Loss: 2.452995 \tValidation Loss: 2.468179\n",
      "Validation loss decreased (2.468210 --> 2.468179).  Saving model ...\n",
      "Epoch: 658 \tTraining Loss: 2.458066 \tValidation Loss: 2.468134\n",
      "Validation loss decreased (2.468179 --> 2.468134).  Saving model ...\n",
      "Epoch: 659 \tTraining Loss: 2.456011 \tValidation Loss: 2.468117\n",
      "Validation loss decreased (2.468134 --> 2.468117).  Saving model ...\n",
      "Epoch: 660 \tTraining Loss: 2.450473 \tValidation Loss: 2.468091\n",
      "Validation loss decreased (2.468117 --> 2.468091).  Saving model ...\n",
      "Epoch: 661 \tTraining Loss: 2.456257 \tValidation Loss: 2.468054\n",
      "Validation loss decreased (2.468091 --> 2.468054).  Saving model ...\n",
      "Epoch: 662 \tTraining Loss: 2.446167 \tValidation Loss: 2.468051\n",
      "Validation loss decreased (2.468054 --> 2.468051).  Saving model ...\n",
      "Epoch: 663 \tTraining Loss: 2.462401 \tValidation Loss: 2.468069\n",
      "Epoch: 664 \tTraining Loss: 2.458057 \tValidation Loss: 2.468061\n",
      "Epoch: 665 \tTraining Loss: 2.456797 \tValidation Loss: 2.468068\n",
      "Epoch: 666 \tTraining Loss: 2.452544 \tValidation Loss: 2.468066\n",
      "Epoch: 667 \tTraining Loss: 2.449588 \tValidation Loss: 2.468028\n",
      "Validation loss decreased (2.468051 --> 2.468028).  Saving model ...\n",
      "Epoch: 668 \tTraining Loss: 2.456404 \tValidation Loss: 2.467955\n",
      "Validation loss decreased (2.468028 --> 2.467955).  Saving model ...\n",
      "Epoch: 669 \tTraining Loss: 2.454433 \tValidation Loss: 2.467932\n",
      "Validation loss decreased (2.467955 --> 2.467932).  Saving model ...\n",
      "Epoch: 670 \tTraining Loss: 2.455583 \tValidation Loss: 2.467880\n",
      "Validation loss decreased (2.467932 --> 2.467880).  Saving model ...\n",
      "Epoch: 671 \tTraining Loss: 2.459260 \tValidation Loss: 2.467851\n",
      "Validation loss decreased (2.467880 --> 2.467851).  Saving model ...\n",
      "Epoch: 672 \tTraining Loss: 2.459043 \tValidation Loss: 2.467849\n",
      "Validation loss decreased (2.467851 --> 2.467849).  Saving model ...\n",
      "Epoch: 673 \tTraining Loss: 2.459563 \tValidation Loss: 2.467832\n",
      "Validation loss decreased (2.467849 --> 2.467832).  Saving model ...\n",
      "Epoch: 674 \tTraining Loss: 2.455480 \tValidation Loss: 2.467796\n",
      "Validation loss decreased (2.467832 --> 2.467796).  Saving model ...\n",
      "Epoch: 675 \tTraining Loss: 2.452208 \tValidation Loss: 2.467754\n",
      "Validation loss decreased (2.467796 --> 2.467754).  Saving model ...\n",
      "Epoch: 676 \tTraining Loss: 2.451199 \tValidation Loss: 2.467764\n",
      "Epoch: 677 \tTraining Loss: 2.461607 \tValidation Loss: 2.467764\n",
      "Epoch: 678 \tTraining Loss: 2.461370 \tValidation Loss: 2.467789\n",
      "Epoch: 679 \tTraining Loss: 2.447674 \tValidation Loss: 2.467767\n",
      "Epoch: 680 \tTraining Loss: 2.450492 \tValidation Loss: 2.467744\n",
      "Validation loss decreased (2.467754 --> 2.467744).  Saving model ...\n",
      "Epoch: 681 \tTraining Loss: 2.460704 \tValidation Loss: 2.467724\n",
      "Validation loss decreased (2.467744 --> 2.467724).  Saving model ...\n",
      "Epoch: 682 \tTraining Loss: 2.448404 \tValidation Loss: 2.467632\n",
      "Validation loss decreased (2.467724 --> 2.467632).  Saving model ...\n",
      "Epoch: 683 \tTraining Loss: 2.447248 \tValidation Loss: 2.467609\n",
      "Validation loss decreased (2.467632 --> 2.467609).  Saving model ...\n",
      "Epoch: 684 \tTraining Loss: 2.456862 \tValidation Loss: 2.467534\n",
      "Validation loss decreased (2.467609 --> 2.467534).  Saving model ...\n",
      "Epoch: 685 \tTraining Loss: 2.446619 \tValidation Loss: 2.467483\n",
      "Validation loss decreased (2.467534 --> 2.467483).  Saving model ...\n",
      "Epoch: 686 \tTraining Loss: 2.455855 \tValidation Loss: 2.467483\n",
      "Validation loss decreased (2.467483 --> 2.467483).  Saving model ...\n",
      "Epoch: 687 \tTraining Loss: 2.448802 \tValidation Loss: 2.467424\n",
      "Validation loss decreased (2.467483 --> 2.467424).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 688 \tTraining Loss: 2.455640 \tValidation Loss: 2.467395\n",
      "Validation loss decreased (2.467424 --> 2.467395).  Saving model ...\n",
      "Epoch: 689 \tTraining Loss: 2.454761 \tValidation Loss: 2.467359\n",
      "Validation loss decreased (2.467395 --> 2.467359).  Saving model ...\n",
      "Epoch: 690 \tTraining Loss: 2.445654 \tValidation Loss: 2.467330\n",
      "Validation loss decreased (2.467359 --> 2.467330).  Saving model ...\n",
      "Epoch: 691 \tTraining Loss: 2.452201 \tValidation Loss: 2.467299\n",
      "Validation loss decreased (2.467330 --> 2.467299).  Saving model ...\n",
      "Epoch: 692 \tTraining Loss: 2.450642 \tValidation Loss: 2.467311\n",
      "Epoch: 693 \tTraining Loss: 2.465527 \tValidation Loss: 2.467356\n",
      "Epoch: 694 \tTraining Loss: 2.456906 \tValidation Loss: 2.467346\n",
      "Epoch: 695 \tTraining Loss: 2.459692 \tValidation Loss: 2.467356\n",
      "Epoch: 696 \tTraining Loss: 2.461097 \tValidation Loss: 2.467358\n",
      "Epoch: 697 \tTraining Loss: 2.449476 \tValidation Loss: 2.467345\n",
      "Epoch: 698 \tTraining Loss: 2.451959 \tValidation Loss: 2.467352\n",
      "Epoch: 699 \tTraining Loss: 2.448833 \tValidation Loss: 2.467264\n",
      "Validation loss decreased (2.467299 --> 2.467264).  Saving model ...\n",
      "Epoch: 700 \tTraining Loss: 2.447437 \tValidation Loss: 2.467250\n",
      "Validation loss decreased (2.467264 --> 2.467250).  Saving model ...\n",
      "Epoch: 701 \tTraining Loss: 2.443368 \tValidation Loss: 2.467179\n",
      "Validation loss decreased (2.467250 --> 2.467179).  Saving model ...\n",
      "Epoch: 702 \tTraining Loss: 2.462703 \tValidation Loss: 2.467132\n",
      "Validation loss decreased (2.467179 --> 2.467132).  Saving model ...\n",
      "Epoch: 703 \tTraining Loss: 2.462973 \tValidation Loss: 2.467115\n",
      "Validation loss decreased (2.467132 --> 2.467115).  Saving model ...\n",
      "Epoch: 704 \tTraining Loss: 2.448789 \tValidation Loss: 2.467080\n",
      "Validation loss decreased (2.467115 --> 2.467080).  Saving model ...\n",
      "Epoch: 705 \tTraining Loss: 2.444104 \tValidation Loss: 2.467036\n",
      "Validation loss decreased (2.467080 --> 2.467036).  Saving model ...\n",
      "Epoch: 706 \tTraining Loss: 2.446877 \tValidation Loss: 2.466973\n",
      "Validation loss decreased (2.467036 --> 2.466973).  Saving model ...\n",
      "Epoch: 707 \tTraining Loss: 2.462843 \tValidation Loss: 2.466967\n",
      "Validation loss decreased (2.466973 --> 2.466967).  Saving model ...\n",
      "Epoch: 708 \tTraining Loss: 2.461429 \tValidation Loss: 2.466968\n",
      "Epoch: 709 \tTraining Loss: 2.456448 \tValidation Loss: 2.466916\n",
      "Validation loss decreased (2.466967 --> 2.466916).  Saving model ...\n",
      "Epoch: 710 \tTraining Loss: 2.454570 \tValidation Loss: 2.466972\n",
      "Epoch: 711 \tTraining Loss: 2.455551 \tValidation Loss: 2.466941\n",
      "Epoch: 712 \tTraining Loss: 2.455733 \tValidation Loss: 2.467022\n",
      "Epoch: 713 \tTraining Loss: 2.445109 \tValidation Loss: 2.466976\n",
      "Epoch: 714 \tTraining Loss: 2.448746 \tValidation Loss: 2.466969\n",
      "Epoch: 715 \tTraining Loss: 2.469867 \tValidation Loss: 2.466994\n",
      "Epoch: 716 \tTraining Loss: 2.460021 \tValidation Loss: 2.467014\n",
      "Epoch: 717 \tTraining Loss: 2.450674 \tValidation Loss: 2.467018\n",
      "Epoch: 718 \tTraining Loss: 2.461411 \tValidation Loss: 2.466988\n",
      "Epoch: 719 \tTraining Loss: 2.454925 \tValidation Loss: 2.466970\n",
      "Epoch: 720 \tTraining Loss: 2.456433 \tValidation Loss: 2.466938\n",
      "Epoch: 721 \tTraining Loss: 2.448077 \tValidation Loss: 2.466925\n",
      "Epoch: 722 \tTraining Loss: 2.462072 \tValidation Loss: 2.466901\n",
      "Validation loss decreased (2.466916 --> 2.466901).  Saving model ...\n",
      "Epoch: 723 \tTraining Loss: 2.450050 \tValidation Loss: 2.466891\n",
      "Validation loss decreased (2.466901 --> 2.466891).  Saving model ...\n",
      "Epoch: 724 \tTraining Loss: 2.458340 \tValidation Loss: 2.466806\n",
      "Validation loss decreased (2.466891 --> 2.466806).  Saving model ...\n",
      "Epoch: 725 \tTraining Loss: 2.444179 \tValidation Loss: 2.466751\n",
      "Validation loss decreased (2.466806 --> 2.466751).  Saving model ...\n",
      "Epoch: 726 \tTraining Loss: 2.449052 \tValidation Loss: 2.466732\n",
      "Validation loss decreased (2.466751 --> 2.466732).  Saving model ...\n",
      "Epoch: 727 \tTraining Loss: 2.442284 \tValidation Loss: 2.466707\n",
      "Validation loss decreased (2.466732 --> 2.466707).  Saving model ...\n",
      "Epoch: 728 \tTraining Loss: 2.447749 \tValidation Loss: 2.466721\n",
      "Epoch: 729 \tTraining Loss: 2.461310 \tValidation Loss: 2.466756\n",
      "Epoch: 730 \tTraining Loss: 2.450857 \tValidation Loss: 2.466751\n",
      "Epoch: 731 \tTraining Loss: 2.454399 \tValidation Loss: 2.466722\n",
      "Epoch: 732 \tTraining Loss: 2.454898 \tValidation Loss: 2.466700\n",
      "Validation loss decreased (2.466707 --> 2.466700).  Saving model ...\n",
      "Epoch: 733 \tTraining Loss: 2.447129 \tValidation Loss: 2.466705\n",
      "Epoch: 734 \tTraining Loss: 2.455992 \tValidation Loss: 2.466668\n",
      "Validation loss decreased (2.466700 --> 2.466668).  Saving model ...\n",
      "Epoch: 735 \tTraining Loss: 2.465665 \tValidation Loss: 2.466696\n",
      "Epoch: 736 \tTraining Loss: 2.456460 \tValidation Loss: 2.466655\n",
      "Validation loss decreased (2.466668 --> 2.466655).  Saving model ...\n",
      "Epoch: 737 \tTraining Loss: 2.447829 \tValidation Loss: 2.466581\n",
      "Validation loss decreased (2.466655 --> 2.466581).  Saving model ...\n",
      "Epoch: 738 \tTraining Loss: 2.459809 \tValidation Loss: 2.466525\n",
      "Validation loss decreased (2.466581 --> 2.466525).  Saving model ...\n",
      "Epoch: 739 \tTraining Loss: 2.451067 \tValidation Loss: 2.466482\n",
      "Validation loss decreased (2.466525 --> 2.466482).  Saving model ...\n",
      "Epoch: 740 \tTraining Loss: 2.445234 \tValidation Loss: 2.466420\n",
      "Validation loss decreased (2.466482 --> 2.466420).  Saving model ...\n",
      "Epoch: 741 \tTraining Loss: 2.451535 \tValidation Loss: 2.466383\n",
      "Validation loss decreased (2.466420 --> 2.466383).  Saving model ...\n",
      "Epoch: 742 \tTraining Loss: 2.456233 \tValidation Loss: 2.466349\n",
      "Validation loss decreased (2.466383 --> 2.466349).  Saving model ...\n",
      "Epoch: 743 \tTraining Loss: 2.465581 \tValidation Loss: 2.466398\n",
      "Epoch: 744 \tTraining Loss: 2.448252 \tValidation Loss: 2.466331\n",
      "Validation loss decreased (2.466349 --> 2.466331).  Saving model ...\n",
      "Epoch: 745 \tTraining Loss: 2.455738 \tValidation Loss: 2.466381\n",
      "Epoch: 746 \tTraining Loss: 2.451572 \tValidation Loss: 2.466354\n",
      "Epoch: 747 \tTraining Loss: 2.439836 \tValidation Loss: 2.466294\n",
      "Validation loss decreased (2.466331 --> 2.466294).  Saving model ...\n",
      "Epoch: 748 \tTraining Loss: 2.451874 \tValidation Loss: 2.466269\n",
      "Validation loss decreased (2.466294 --> 2.466269).  Saving model ...\n",
      "Epoch: 749 \tTraining Loss: 2.447505 \tValidation Loss: 2.466228\n",
      "Validation loss decreased (2.466269 --> 2.466228).  Saving model ...\n",
      "Epoch: 750 \tTraining Loss: 2.456231 \tValidation Loss: 2.466150\n",
      "Validation loss decreased (2.466228 --> 2.466150).  Saving model ...\n",
      "Epoch: 751 \tTraining Loss: 2.445259 \tValidation Loss: 2.466069\n",
      "Validation loss decreased (2.466150 --> 2.466069).  Saving model ...\n",
      "Epoch: 752 \tTraining Loss: 2.456899 \tValidation Loss: 2.466006\n",
      "Validation loss decreased (2.466069 --> 2.466006).  Saving model ...\n",
      "Epoch: 753 \tTraining Loss: 2.451292 \tValidation Loss: 2.465980\n",
      "Validation loss decreased (2.466006 --> 2.465980).  Saving model ...\n",
      "Epoch: 754 \tTraining Loss: 2.456672 \tValidation Loss: 2.465941\n",
      "Validation loss decreased (2.465980 --> 2.465941).  Saving model ...\n",
      "Epoch: 755 \tTraining Loss: 2.441961 \tValidation Loss: 2.465873\n",
      "Validation loss decreased (2.465941 --> 2.465873).  Saving model ...\n",
      "Epoch: 756 \tTraining Loss: 2.456809 \tValidation Loss: 2.465894\n",
      "Epoch: 757 \tTraining Loss: 2.442399 \tValidation Loss: 2.465845\n",
      "Validation loss decreased (2.465873 --> 2.465845).  Saving model ...\n",
      "Epoch: 758 \tTraining Loss: 2.452591 \tValidation Loss: 2.465835\n",
      "Validation loss decreased (2.465845 --> 2.465835).  Saving model ...\n",
      "Epoch: 759 \tTraining Loss: 2.449609 \tValidation Loss: 2.465847\n",
      "Epoch: 760 \tTraining Loss: 2.445216 \tValidation Loss: 2.465780\n",
      "Validation loss decreased (2.465835 --> 2.465780).  Saving model ...\n",
      "Epoch: 761 \tTraining Loss: 2.450368 \tValidation Loss: 2.465812\n",
      "Epoch: 762 \tTraining Loss: 2.452980 \tValidation Loss: 2.465745\n",
      "Validation loss decreased (2.465780 --> 2.465745).  Saving model ...\n",
      "Epoch: 763 \tTraining Loss: 2.458368 \tValidation Loss: 2.465741\n",
      "Validation loss decreased (2.465745 --> 2.465741).  Saving model ...\n",
      "Epoch: 764 \tTraining Loss: 2.444902 \tValidation Loss: 2.465670\n",
      "Validation loss decreased (2.465741 --> 2.465670).  Saving model ...\n",
      "Epoch: 765 \tTraining Loss: 2.448146 \tValidation Loss: 2.465626\n",
      "Validation loss decreased (2.465670 --> 2.465626).  Saving model ...\n",
      "Epoch: 766 \tTraining Loss: 2.442017 \tValidation Loss: 2.465575\n",
      "Validation loss decreased (2.465626 --> 2.465575).  Saving model ...\n",
      "Epoch: 767 \tTraining Loss: 2.443684 \tValidation Loss: 2.465559\n",
      "Validation loss decreased (2.465575 --> 2.465559).  Saving model ...\n",
      "Epoch: 768 \tTraining Loss: 2.443866 \tValidation Loss: 2.465436\n",
      "Validation loss decreased (2.465559 --> 2.465436).  Saving model ...\n",
      "Epoch: 769 \tTraining Loss: 2.449559 \tValidation Loss: 2.465397\n",
      "Validation loss decreased (2.465436 --> 2.465397).  Saving model ...\n",
      "Epoch: 770 \tTraining Loss: 2.452742 \tValidation Loss: 2.465379\n",
      "Validation loss decreased (2.465397 --> 2.465379).  Saving model ...\n",
      "Epoch: 771 \tTraining Loss: 2.447989 \tValidation Loss: 2.465305\n",
      "Validation loss decreased (2.465379 --> 2.465305).  Saving model ...\n",
      "Epoch: 772 \tTraining Loss: 2.449690 \tValidation Loss: 2.465285\n",
      "Validation loss decreased (2.465305 --> 2.465285).  Saving model ...\n",
      "Epoch: 773 \tTraining Loss: 2.457533 \tValidation Loss: 2.465293\n",
      "Epoch: 774 \tTraining Loss: 2.446519 \tValidation Loss: 2.465212\n",
      "Validation loss decreased (2.465285 --> 2.465212).  Saving model ...\n",
      "Epoch: 775 \tTraining Loss: 2.463761 \tValidation Loss: 2.465168\n",
      "Validation loss decreased (2.465212 --> 2.465168).  Saving model ...\n",
      "Epoch: 776 \tTraining Loss: 2.451261 \tValidation Loss: 2.465156\n",
      "Validation loss decreased (2.465168 --> 2.465156).  Saving model ...\n",
      "Epoch: 777 \tTraining Loss: 2.450197 \tValidation Loss: 2.465137\n",
      "Validation loss decreased (2.465156 --> 2.465137).  Saving model ...\n",
      "Epoch: 778 \tTraining Loss: 2.448834 \tValidation Loss: 2.465034\n",
      "Validation loss decreased (2.465137 --> 2.465034).  Saving model ...\n",
      "Epoch: 779 \tTraining Loss: 2.443206 \tValidation Loss: 2.465023\n",
      "Validation loss decreased (2.465034 --> 2.465023).  Saving model ...\n",
      "Epoch: 780 \tTraining Loss: 2.443779 \tValidation Loss: 2.464965\n",
      "Validation loss decreased (2.465023 --> 2.464965).  Saving model ...\n",
      "Epoch: 781 \tTraining Loss: 2.444627 \tValidation Loss: 2.464956\n",
      "Validation loss decreased (2.464965 --> 2.464956).  Saving model ...\n",
      "Epoch: 782 \tTraining Loss: 2.434761 \tValidation Loss: 2.464882\n",
      "Validation loss decreased (2.464956 --> 2.464882).  Saving model ...\n",
      "Epoch: 783 \tTraining Loss: 2.459193 \tValidation Loss: 2.464860\n",
      "Validation loss decreased (2.464882 --> 2.464860).  Saving model ...\n",
      "Epoch: 784 \tTraining Loss: 2.458563 \tValidation Loss: 2.464828\n",
      "Validation loss decreased (2.464860 --> 2.464828).  Saving model ...\n",
      "Epoch: 785 \tTraining Loss: 2.451917 \tValidation Loss: 2.464816\n",
      "Validation loss decreased (2.464828 --> 2.464816).  Saving model ...\n",
      "Epoch: 786 \tTraining Loss: 2.454729 \tValidation Loss: 2.464835\n",
      "Epoch: 787 \tTraining Loss: 2.455931 \tValidation Loss: 2.464826\n",
      "Epoch: 788 \tTraining Loss: 2.440436 \tValidation Loss: 2.464837\n",
      "Epoch: 789 \tTraining Loss: 2.445899 \tValidation Loss: 2.464818\n",
      "Epoch: 790 \tTraining Loss: 2.457809 \tValidation Loss: 2.464813\n",
      "Validation loss decreased (2.464816 --> 2.464813).  Saving model ...\n",
      "Epoch: 791 \tTraining Loss: 2.448993 \tValidation Loss: 2.464821\n",
      "Epoch: 792 \tTraining Loss: 2.460419 \tValidation Loss: 2.464782\n",
      "Validation loss decreased (2.464813 --> 2.464782).  Saving model ...\n",
      "Epoch: 793 \tTraining Loss: 2.453520 \tValidation Loss: 2.464746\n",
      "Validation loss decreased (2.464782 --> 2.464746).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 794 \tTraining Loss: 2.449122 \tValidation Loss: 2.464741\n",
      "Validation loss decreased (2.464746 --> 2.464741).  Saving model ...\n",
      "Epoch: 795 \tTraining Loss: 2.447284 \tValidation Loss: 2.464713\n",
      "Validation loss decreased (2.464741 --> 2.464713).  Saving model ...\n",
      "Epoch: 796 \tTraining Loss: 2.447491 \tValidation Loss: 2.464721\n",
      "Epoch: 797 \tTraining Loss: 2.455405 \tValidation Loss: 2.464663\n",
      "Validation loss decreased (2.464713 --> 2.464663).  Saving model ...\n",
      "Epoch: 798 \tTraining Loss: 2.456422 \tValidation Loss: 2.464681\n",
      "Epoch: 799 \tTraining Loss: 2.453112 \tValidation Loss: 2.464686\n",
      "Epoch: 800 \tTraining Loss: 2.444151 \tValidation Loss: 2.464650\n",
      "Validation loss decreased (2.464663 --> 2.464650).  Saving model ...\n",
      "Epoch: 801 \tTraining Loss: 2.451254 \tValidation Loss: 2.464612\n",
      "Validation loss decreased (2.464650 --> 2.464612).  Saving model ...\n",
      "Epoch: 802 \tTraining Loss: 2.455548 \tValidation Loss: 2.464591\n",
      "Validation loss decreased (2.464612 --> 2.464591).  Saving model ...\n",
      "Epoch: 803 \tTraining Loss: 2.451956 \tValidation Loss: 2.464594\n",
      "Epoch: 804 \tTraining Loss: 2.456739 \tValidation Loss: 2.464594\n",
      "Epoch: 805 \tTraining Loss: 2.442658 \tValidation Loss: 2.464531\n",
      "Validation loss decreased (2.464591 --> 2.464531).  Saving model ...\n",
      "Epoch: 806 \tTraining Loss: 2.452010 \tValidation Loss: 2.464516\n",
      "Validation loss decreased (2.464531 --> 2.464516).  Saving model ...\n",
      "Epoch: 807 \tTraining Loss: 2.450473 \tValidation Loss: 2.464484\n",
      "Validation loss decreased (2.464516 --> 2.464484).  Saving model ...\n",
      "Epoch: 808 \tTraining Loss: 2.455257 \tValidation Loss: 2.464429\n",
      "Validation loss decreased (2.464484 --> 2.464429).  Saving model ...\n",
      "Epoch: 809 \tTraining Loss: 2.447849 \tValidation Loss: 2.464345\n",
      "Validation loss decreased (2.464429 --> 2.464345).  Saving model ...\n",
      "Epoch: 810 \tTraining Loss: 2.445421 \tValidation Loss: 2.464360\n",
      "Epoch: 811 \tTraining Loss: 2.456383 \tValidation Loss: 2.464322\n",
      "Validation loss decreased (2.464345 --> 2.464322).  Saving model ...\n",
      "Epoch: 812 \tTraining Loss: 2.453855 \tValidation Loss: 2.464312\n",
      "Validation loss decreased (2.464322 --> 2.464312).  Saving model ...\n",
      "Epoch: 813 \tTraining Loss: 2.446991 \tValidation Loss: 2.464272\n",
      "Validation loss decreased (2.464312 --> 2.464272).  Saving model ...\n",
      "Epoch: 814 \tTraining Loss: 2.445249 \tValidation Loss: 2.464210\n",
      "Validation loss decreased (2.464272 --> 2.464210).  Saving model ...\n",
      "Epoch: 815 \tTraining Loss: 2.457238 \tValidation Loss: 2.464181\n",
      "Validation loss decreased (2.464210 --> 2.464181).  Saving model ...\n",
      "Epoch: 816 \tTraining Loss: 2.443005 \tValidation Loss: 2.464138\n",
      "Validation loss decreased (2.464181 --> 2.464138).  Saving model ...\n",
      "Epoch: 817 \tTraining Loss: 2.445619 \tValidation Loss: 2.464106\n",
      "Validation loss decreased (2.464138 --> 2.464106).  Saving model ...\n",
      "Epoch: 818 \tTraining Loss: 2.442832 \tValidation Loss: 2.464049\n",
      "Validation loss decreased (2.464106 --> 2.464049).  Saving model ...\n",
      "Epoch: 819 \tTraining Loss: 2.443985 \tValidation Loss: 2.464060\n",
      "Epoch: 820 \tTraining Loss: 2.441962 \tValidation Loss: 2.463999\n",
      "Validation loss decreased (2.464049 --> 2.463999).  Saving model ...\n",
      "Epoch: 821 \tTraining Loss: 2.458488 \tValidation Loss: 2.463907\n",
      "Validation loss decreased (2.463999 --> 2.463907).  Saving model ...\n",
      "Epoch: 822 \tTraining Loss: 2.448612 \tValidation Loss: 2.463890\n",
      "Validation loss decreased (2.463907 --> 2.463890).  Saving model ...\n",
      "Epoch: 823 \tTraining Loss: 2.442442 \tValidation Loss: 2.463878\n",
      "Validation loss decreased (2.463890 --> 2.463878).  Saving model ...\n",
      "Epoch: 824 \tTraining Loss: 2.450622 \tValidation Loss: 2.463815\n",
      "Validation loss decreased (2.463878 --> 2.463815).  Saving model ...\n",
      "Epoch: 825 \tTraining Loss: 2.455229 \tValidation Loss: 2.463789\n",
      "Validation loss decreased (2.463815 --> 2.463789).  Saving model ...\n",
      "Epoch: 826 \tTraining Loss: 2.455574 \tValidation Loss: 2.463735\n",
      "Validation loss decreased (2.463789 --> 2.463735).  Saving model ...\n",
      "Epoch: 827 \tTraining Loss: 2.447885 \tValidation Loss: 2.463709\n",
      "Validation loss decreased (2.463735 --> 2.463709).  Saving model ...\n",
      "Epoch: 828 \tTraining Loss: 2.448124 \tValidation Loss: 2.463686\n",
      "Validation loss decreased (2.463709 --> 2.463686).  Saving model ...\n",
      "Epoch: 829 \tTraining Loss: 2.437821 \tValidation Loss: 2.463686\n",
      "Validation loss decreased (2.463686 --> 2.463686).  Saving model ...\n",
      "Epoch: 830 \tTraining Loss: 2.444484 \tValidation Loss: 2.463638\n",
      "Validation loss decreased (2.463686 --> 2.463638).  Saving model ...\n",
      "Epoch: 831 \tTraining Loss: 2.448067 \tValidation Loss: 2.463593\n",
      "Validation loss decreased (2.463638 --> 2.463593).  Saving model ...\n",
      "Epoch: 832 \tTraining Loss: 2.443927 \tValidation Loss: 2.463543\n",
      "Validation loss decreased (2.463593 --> 2.463543).  Saving model ...\n",
      "Epoch: 833 \tTraining Loss: 2.443614 \tValidation Loss: 2.463523\n",
      "Validation loss decreased (2.463543 --> 2.463523).  Saving model ...\n",
      "Epoch: 834 \tTraining Loss: 2.442982 \tValidation Loss: 2.463509\n",
      "Validation loss decreased (2.463523 --> 2.463509).  Saving model ...\n",
      "Epoch: 835 \tTraining Loss: 2.449932 \tValidation Loss: 2.463505\n",
      "Validation loss decreased (2.463509 --> 2.463505).  Saving model ...\n",
      "Epoch: 836 \tTraining Loss: 2.443182 \tValidation Loss: 2.463444\n",
      "Validation loss decreased (2.463505 --> 2.463444).  Saving model ...\n",
      "Epoch: 837 \tTraining Loss: 2.448503 \tValidation Loss: 2.463389\n",
      "Validation loss decreased (2.463444 --> 2.463389).  Saving model ...\n",
      "Epoch: 838 \tTraining Loss: 2.440630 \tValidation Loss: 2.463393\n",
      "Epoch: 839 \tTraining Loss: 2.451470 \tValidation Loss: 2.463405\n",
      "Epoch: 840 \tTraining Loss: 2.440547 \tValidation Loss: 2.463373\n",
      "Validation loss decreased (2.463389 --> 2.463373).  Saving model ...\n",
      "Epoch: 841 \tTraining Loss: 2.448675 \tValidation Loss: 2.463368\n",
      "Validation loss decreased (2.463373 --> 2.463368).  Saving model ...\n",
      "Epoch: 842 \tTraining Loss: 2.440474 \tValidation Loss: 2.463309\n",
      "Validation loss decreased (2.463368 --> 2.463309).  Saving model ...\n",
      "Epoch: 843 \tTraining Loss: 2.457367 \tValidation Loss: 2.463276\n",
      "Validation loss decreased (2.463309 --> 2.463276).  Saving model ...\n",
      "Epoch: 844 \tTraining Loss: 2.455011 \tValidation Loss: 2.463273\n",
      "Validation loss decreased (2.463276 --> 2.463273).  Saving model ...\n",
      "Epoch: 845 \tTraining Loss: 2.440380 \tValidation Loss: 2.463268\n",
      "Validation loss decreased (2.463273 --> 2.463268).  Saving model ...\n",
      "Epoch: 846 \tTraining Loss: 2.449811 \tValidation Loss: 2.463228\n",
      "Validation loss decreased (2.463268 --> 2.463228).  Saving model ...\n",
      "Epoch: 847 \tTraining Loss: 2.442882 \tValidation Loss: 2.463237\n",
      "Epoch: 848 \tTraining Loss: 2.448297 \tValidation Loss: 2.463242\n",
      "Epoch: 849 \tTraining Loss: 2.442141 \tValidation Loss: 2.463201\n",
      "Validation loss decreased (2.463228 --> 2.463201).  Saving model ...\n",
      "Epoch: 850 \tTraining Loss: 2.440788 \tValidation Loss: 2.463167\n",
      "Validation loss decreased (2.463201 --> 2.463167).  Saving model ...\n",
      "Epoch: 851 \tTraining Loss: 2.447737 \tValidation Loss: 2.463130\n",
      "Validation loss decreased (2.463167 --> 2.463130).  Saving model ...\n",
      "Epoch: 852 \tTraining Loss: 2.453481 \tValidation Loss: 2.463151\n",
      "Epoch: 853 \tTraining Loss: 2.450374 \tValidation Loss: 2.463137\n",
      "Epoch: 854 \tTraining Loss: 2.450191 \tValidation Loss: 2.463125\n",
      "Validation loss decreased (2.463130 --> 2.463125).  Saving model ...\n",
      "Epoch: 855 \tTraining Loss: 2.445047 \tValidation Loss: 2.463094\n",
      "Validation loss decreased (2.463125 --> 2.463094).  Saving model ...\n",
      "Epoch: 856 \tTraining Loss: 2.449967 \tValidation Loss: 2.463115\n",
      "Epoch: 857 \tTraining Loss: 2.454564 \tValidation Loss: 2.463054\n",
      "Validation loss decreased (2.463094 --> 2.463054).  Saving model ...\n",
      "Epoch: 858 \tTraining Loss: 2.444736 \tValidation Loss: 2.463030\n",
      "Validation loss decreased (2.463054 --> 2.463030).  Saving model ...\n",
      "Epoch: 859 \tTraining Loss: 2.445534 \tValidation Loss: 2.462979\n",
      "Validation loss decreased (2.463030 --> 2.462979).  Saving model ...\n",
      "Epoch: 860 \tTraining Loss: 2.446328 \tValidation Loss: 2.462948\n",
      "Validation loss decreased (2.462979 --> 2.462948).  Saving model ...\n",
      "Epoch: 861 \tTraining Loss: 2.452617 \tValidation Loss: 2.462899\n",
      "Validation loss decreased (2.462948 --> 2.462899).  Saving model ...\n",
      "Epoch: 862 \tTraining Loss: 2.442290 \tValidation Loss: 2.462908\n",
      "Epoch: 863 \tTraining Loss: 2.452723 \tValidation Loss: 2.462912\n",
      "Epoch: 864 \tTraining Loss: 2.450061 \tValidation Loss: 2.462899\n",
      "Epoch: 865 \tTraining Loss: 2.453009 \tValidation Loss: 2.462940\n",
      "Epoch: 866 \tTraining Loss: 2.444092 \tValidation Loss: 2.462929\n",
      "Epoch: 867 \tTraining Loss: 2.448853 \tValidation Loss: 2.462885\n",
      "Validation loss decreased (2.462899 --> 2.462885).  Saving model ...\n",
      "Epoch: 868 \tTraining Loss: 2.436637 \tValidation Loss: 2.462806\n",
      "Validation loss decreased (2.462885 --> 2.462806).  Saving model ...\n",
      "Epoch: 869 \tTraining Loss: 2.451845 \tValidation Loss: 2.462822\n",
      "Epoch: 870 \tTraining Loss: 2.456473 \tValidation Loss: 2.462797\n",
      "Validation loss decreased (2.462806 --> 2.462797).  Saving model ...\n",
      "Epoch: 871 \tTraining Loss: 2.459625 \tValidation Loss: 2.462755\n",
      "Validation loss decreased (2.462797 --> 2.462755).  Saving model ...\n",
      "Epoch: 872 \tTraining Loss: 2.450669 \tValidation Loss: 2.462708\n",
      "Validation loss decreased (2.462755 --> 2.462708).  Saving model ...\n",
      "Epoch: 873 \tTraining Loss: 2.450594 \tValidation Loss: 2.462672\n",
      "Validation loss decreased (2.462708 --> 2.462672).  Saving model ...\n",
      "Epoch: 874 \tTraining Loss: 2.443492 \tValidation Loss: 2.462660\n",
      "Validation loss decreased (2.462672 --> 2.462660).  Saving model ...\n",
      "Epoch: 875 \tTraining Loss: 2.445695 \tValidation Loss: 2.462668\n",
      "Epoch: 876 \tTraining Loss: 2.442009 \tValidation Loss: 2.462625\n",
      "Validation loss decreased (2.462660 --> 2.462625).  Saving model ...\n",
      "Epoch: 877 \tTraining Loss: 2.441649 \tValidation Loss: 2.462540\n",
      "Validation loss decreased (2.462625 --> 2.462540).  Saving model ...\n",
      "Epoch: 878 \tTraining Loss: 2.447824 \tValidation Loss: 2.462484\n",
      "Validation loss decreased (2.462540 --> 2.462484).  Saving model ...\n",
      "Epoch: 879 \tTraining Loss: 2.433788 \tValidation Loss: 2.462458\n",
      "Validation loss decreased (2.462484 --> 2.462458).  Saving model ...\n",
      "Epoch: 880 \tTraining Loss: 2.433706 \tValidation Loss: 2.462379\n",
      "Validation loss decreased (2.462458 --> 2.462379).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 881 \tTraining Loss: 2.440925 \tValidation Loss: 2.462368\n",
      "Validation loss decreased (2.462379 --> 2.462368).  Saving model ...\n",
      "Epoch: 882 \tTraining Loss: 2.440771 \tValidation Loss: 2.462345\n",
      "Validation loss decreased (2.462368 --> 2.462345).  Saving model ...\n",
      "Epoch: 883 \tTraining Loss: 2.436430 \tValidation Loss: 2.462284\n",
      "Validation loss decreased (2.462345 --> 2.462284).  Saving model ...\n",
      "Epoch: 884 \tTraining Loss: 2.449543 \tValidation Loss: 2.462231\n",
      "Validation loss decreased (2.462284 --> 2.462231).  Saving model ...\n",
      "Epoch: 885 \tTraining Loss: 2.450589 \tValidation Loss: 2.462221\n",
      "Validation loss decreased (2.462231 --> 2.462221).  Saving model ...\n",
      "Epoch: 886 \tTraining Loss: 2.458159 \tValidation Loss: 2.462192\n",
      "Validation loss decreased (2.462221 --> 2.462192).  Saving model ...\n",
      "Epoch: 887 \tTraining Loss: 2.447289 \tValidation Loss: 2.462165\n",
      "Validation loss decreased (2.462192 --> 2.462165).  Saving model ...\n",
      "Epoch: 888 \tTraining Loss: 2.442085 \tValidation Loss: 2.462122\n",
      "Validation loss decreased (2.462165 --> 2.462122).  Saving model ...\n",
      "Epoch: 889 \tTraining Loss: 2.445372 \tValidation Loss: 2.462052\n",
      "Validation loss decreased (2.462122 --> 2.462052).  Saving model ...\n",
      "Epoch: 890 \tTraining Loss: 2.449741 \tValidation Loss: 2.462099\n",
      "Epoch: 891 \tTraining Loss: 2.436900 \tValidation Loss: 2.462080\n",
      "Epoch: 892 \tTraining Loss: 2.451751 \tValidation Loss: 2.462099\n",
      "Epoch: 893 \tTraining Loss: 2.448014 \tValidation Loss: 2.462054\n",
      "Epoch: 894 \tTraining Loss: 2.451003 \tValidation Loss: 2.462096\n",
      "Epoch: 895 \tTraining Loss: 2.443927 \tValidation Loss: 2.462058\n",
      "Epoch: 896 \tTraining Loss: 2.446391 \tValidation Loss: 2.462012\n",
      "Validation loss decreased (2.462052 --> 2.462012).  Saving model ...\n",
      "Epoch: 897 \tTraining Loss: 2.441676 \tValidation Loss: 2.461981\n",
      "Validation loss decreased (2.462012 --> 2.461981).  Saving model ...\n",
      "Epoch: 898 \tTraining Loss: 2.453882 \tValidation Loss: 2.461950\n",
      "Validation loss decreased (2.461981 --> 2.461950).  Saving model ...\n",
      "Epoch: 899 \tTraining Loss: 2.451485 \tValidation Loss: 2.461904\n",
      "Validation loss decreased (2.461950 --> 2.461904).  Saving model ...\n",
      "Epoch: 900 \tTraining Loss: 2.447168 \tValidation Loss: 2.461892\n",
      "Validation loss decreased (2.461904 --> 2.461892).  Saving model ...\n",
      "Epoch: 901 \tTraining Loss: 2.444139 \tValidation Loss: 2.461874\n",
      "Validation loss decreased (2.461892 --> 2.461874).  Saving model ...\n",
      "Epoch: 902 \tTraining Loss: 2.447074 \tValidation Loss: 2.461823\n",
      "Validation loss decreased (2.461874 --> 2.461823).  Saving model ...\n",
      "Epoch: 903 \tTraining Loss: 2.441255 \tValidation Loss: 2.461820\n",
      "Validation loss decreased (2.461823 --> 2.461820).  Saving model ...\n",
      "Epoch: 904 \tTraining Loss: 2.448970 \tValidation Loss: 2.461817\n",
      "Validation loss decreased (2.461820 --> 2.461817).  Saving model ...\n",
      "Epoch: 905 \tTraining Loss: 2.463720 \tValidation Loss: 2.461779\n",
      "Validation loss decreased (2.461817 --> 2.461779).  Saving model ...\n",
      "Epoch: 906 \tTraining Loss: 2.441270 \tValidation Loss: 2.461783\n",
      "Epoch: 907 \tTraining Loss: 2.447413 \tValidation Loss: 2.461748\n",
      "Validation loss decreased (2.461779 --> 2.461748).  Saving model ...\n",
      "Epoch: 908 \tTraining Loss: 2.443162 \tValidation Loss: 2.461745\n",
      "Validation loss decreased (2.461748 --> 2.461745).  Saving model ...\n",
      "Epoch: 909 \tTraining Loss: 2.449378 \tValidation Loss: 2.461714\n",
      "Validation loss decreased (2.461745 --> 2.461714).  Saving model ...\n",
      "Epoch: 910 \tTraining Loss: 2.456383 \tValidation Loss: 2.461678\n",
      "Validation loss decreased (2.461714 --> 2.461678).  Saving model ...\n",
      "Epoch: 911 \tTraining Loss: 2.442478 \tValidation Loss: 2.461664\n",
      "Validation loss decreased (2.461678 --> 2.461664).  Saving model ...\n",
      "Epoch: 912 \tTraining Loss: 2.445532 \tValidation Loss: 2.461617\n",
      "Validation loss decreased (2.461664 --> 2.461617).  Saving model ...\n",
      "Epoch: 913 \tTraining Loss: 2.445964 \tValidation Loss: 2.461574\n",
      "Validation loss decreased (2.461617 --> 2.461574).  Saving model ...\n",
      "Epoch: 914 \tTraining Loss: 2.439007 \tValidation Loss: 2.461529\n",
      "Validation loss decreased (2.461574 --> 2.461529).  Saving model ...\n",
      "Epoch: 915 \tTraining Loss: 2.451693 \tValidation Loss: 2.461511\n",
      "Validation loss decreased (2.461529 --> 2.461511).  Saving model ...\n",
      "Epoch: 916 \tTraining Loss: 2.448460 \tValidation Loss: 2.461544\n",
      "Epoch: 917 \tTraining Loss: 2.439867 \tValidation Loss: 2.461576\n",
      "Epoch: 918 \tTraining Loss: 2.440079 \tValidation Loss: 2.461510\n",
      "Validation loss decreased (2.461511 --> 2.461510).  Saving model ...\n",
      "Epoch: 919 \tTraining Loss: 2.452120 \tValidation Loss: 2.461544\n",
      "Epoch: 920 \tTraining Loss: 2.450063 \tValidation Loss: 2.461505\n",
      "Validation loss decreased (2.461510 --> 2.461505).  Saving model ...\n",
      "Epoch: 921 \tTraining Loss: 2.446998 \tValidation Loss: 2.461532\n",
      "Epoch: 922 \tTraining Loss: 2.443308 \tValidation Loss: 2.461511\n",
      "Epoch: 923 \tTraining Loss: 2.446250 \tValidation Loss: 2.461523\n",
      "Epoch: 924 \tTraining Loss: 2.443761 \tValidation Loss: 2.461484\n",
      "Validation loss decreased (2.461505 --> 2.461484).  Saving model ...\n",
      "Epoch: 925 \tTraining Loss: 2.439889 \tValidation Loss: 2.461469\n",
      "Validation loss decreased (2.461484 --> 2.461469).  Saving model ...\n",
      "Epoch: 926 \tTraining Loss: 2.431361 \tValidation Loss: 2.461451\n",
      "Validation loss decreased (2.461469 --> 2.461451).  Saving model ...\n",
      "Epoch: 927 \tTraining Loss: 2.445596 \tValidation Loss: 2.461439\n",
      "Validation loss decreased (2.461451 --> 2.461439).  Saving model ...\n",
      "Epoch: 928 \tTraining Loss: 2.445747 \tValidation Loss: 2.461416\n",
      "Validation loss decreased (2.461439 --> 2.461416).  Saving model ...\n",
      "Epoch: 929 \tTraining Loss: 2.448637 \tValidation Loss: 2.461434\n",
      "Epoch: 930 \tTraining Loss: 2.444232 \tValidation Loss: 2.461416\n",
      "Validation loss decreased (2.461416 --> 2.461416).  Saving model ...\n",
      "Epoch: 931 \tTraining Loss: 2.446333 \tValidation Loss: 2.461407\n",
      "Validation loss decreased (2.461416 --> 2.461407).  Saving model ...\n",
      "Epoch: 932 \tTraining Loss: 2.446453 \tValidation Loss: 2.461373\n",
      "Validation loss decreased (2.461407 --> 2.461373).  Saving model ...\n",
      "Epoch: 933 \tTraining Loss: 2.448911 \tValidation Loss: 2.461363\n",
      "Validation loss decreased (2.461373 --> 2.461363).  Saving model ...\n",
      "Epoch: 934 \tTraining Loss: 2.445426 \tValidation Loss: 2.461317\n",
      "Validation loss decreased (2.461363 --> 2.461317).  Saving model ...\n",
      "Epoch: 935 \tTraining Loss: 2.451281 \tValidation Loss: 2.461291\n",
      "Validation loss decreased (2.461317 --> 2.461291).  Saving model ...\n",
      "Epoch: 936 \tTraining Loss: 2.447664 \tValidation Loss: 2.461287\n",
      "Validation loss decreased (2.461291 --> 2.461287).  Saving model ...\n",
      "Epoch: 937 \tTraining Loss: 2.439733 \tValidation Loss: 2.461282\n",
      "Validation loss decreased (2.461287 --> 2.461282).  Saving model ...\n",
      "Epoch: 938 \tTraining Loss: 2.452870 \tValidation Loss: 2.461265\n",
      "Validation loss decreased (2.461282 --> 2.461265).  Saving model ...\n",
      "Epoch: 939 \tTraining Loss: 2.442823 \tValidation Loss: 2.461221\n",
      "Validation loss decreased (2.461265 --> 2.461221).  Saving model ...\n",
      "Epoch: 940 \tTraining Loss: 2.449602 \tValidation Loss: 2.461208\n",
      "Validation loss decreased (2.461221 --> 2.461208).  Saving model ...\n",
      "Epoch: 941 \tTraining Loss: 2.451504 \tValidation Loss: 2.461254\n",
      "Epoch: 942 \tTraining Loss: 2.442578 \tValidation Loss: 2.461204\n",
      "Validation loss decreased (2.461208 --> 2.461204).  Saving model ...\n",
      "Epoch: 943 \tTraining Loss: 2.448581 \tValidation Loss: 2.461144\n",
      "Validation loss decreased (2.461204 --> 2.461144).  Saving model ...\n",
      "Epoch: 944 \tTraining Loss: 2.443817 \tValidation Loss: 2.461107\n",
      "Validation loss decreased (2.461144 --> 2.461107).  Saving model ...\n",
      "Epoch: 945 \tTraining Loss: 2.446231 \tValidation Loss: 2.461076\n",
      "Validation loss decreased (2.461107 --> 2.461076).  Saving model ...\n",
      "Epoch: 946 \tTraining Loss: 2.443395 \tValidation Loss: 2.461105\n",
      "Epoch: 947 \tTraining Loss: 2.453028 \tValidation Loss: 2.461137\n",
      "Epoch: 948 \tTraining Loss: 2.447411 \tValidation Loss: 2.461128\n",
      "Epoch: 949 \tTraining Loss: 2.446157 \tValidation Loss: 2.461119\n",
      "Epoch: 950 \tTraining Loss: 2.442409 \tValidation Loss: 2.461053\n",
      "Validation loss decreased (2.461076 --> 2.461053).  Saving model ...\n",
      "Epoch: 951 \tTraining Loss: 2.450180 \tValidation Loss: 2.461092\n",
      "Epoch: 952 \tTraining Loss: 2.441317 \tValidation Loss: 2.461047\n",
      "Validation loss decreased (2.461053 --> 2.461047).  Saving model ...\n",
      "Epoch: 953 \tTraining Loss: 2.447765 \tValidation Loss: 2.461048\n",
      "Epoch: 954 \tTraining Loss: 2.439454 \tValidation Loss: 2.460977\n",
      "Validation loss decreased (2.461047 --> 2.460977).  Saving model ...\n",
      "Epoch: 955 \tTraining Loss: 2.432959 \tValidation Loss: 2.460994\n",
      "Epoch: 956 \tTraining Loss: 2.445976 \tValidation Loss: 2.460972\n",
      "Validation loss decreased (2.460977 --> 2.460972).  Saving model ...\n",
      "Epoch: 957 \tTraining Loss: 2.436857 \tValidation Loss: 2.460913\n",
      "Validation loss decreased (2.460972 --> 2.460913).  Saving model ...\n",
      "Epoch: 958 \tTraining Loss: 2.435931 \tValidation Loss: 2.460869\n",
      "Validation loss decreased (2.460913 --> 2.460869).  Saving model ...\n",
      "Epoch: 959 \tTraining Loss: 2.443156 \tValidation Loss: 2.460831\n",
      "Validation loss decreased (2.460869 --> 2.460831).  Saving model ...\n",
      "Epoch: 960 \tTraining Loss: 2.445092 \tValidation Loss: 2.460854\n",
      "Epoch: 961 \tTraining Loss: 2.445647 \tValidation Loss: 2.460837\n",
      "Epoch: 962 \tTraining Loss: 2.441739 \tValidation Loss: 2.460824\n",
      "Validation loss decreased (2.460831 --> 2.460824).  Saving model ...\n",
      "Epoch: 963 \tTraining Loss: 2.450827 \tValidation Loss: 2.460834\n",
      "Epoch: 964 \tTraining Loss: 2.448140 \tValidation Loss: 2.460787\n",
      "Validation loss decreased (2.460824 --> 2.460787).  Saving model ...\n",
      "Epoch: 965 \tTraining Loss: 2.439266 \tValidation Loss: 2.460791\n",
      "Epoch: 966 \tTraining Loss: 2.448407 \tValidation Loss: 2.460736\n",
      "Validation loss decreased (2.460787 --> 2.460736).  Saving model ...\n",
      "Epoch: 967 \tTraining Loss: 2.445699 \tValidation Loss: 2.460715\n",
      "Validation loss decreased (2.460736 --> 2.460715).  Saving model ...\n",
      "Epoch: 968 \tTraining Loss: 2.443025 \tValidation Loss: 2.460737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 969 \tTraining Loss: 2.442541 \tValidation Loss: 2.460732\n",
      "Epoch: 970 \tTraining Loss: 2.451750 \tValidation Loss: 2.460719\n",
      "Epoch: 971 \tTraining Loss: 2.456166 \tValidation Loss: 2.460774\n",
      "Epoch: 972 \tTraining Loss: 2.444519 \tValidation Loss: 2.460735\n",
      "Epoch: 973 \tTraining Loss: 2.444308 \tValidation Loss: 2.460744\n",
      "Epoch: 974 \tTraining Loss: 2.436766 \tValidation Loss: 2.460696\n",
      "Validation loss decreased (2.460715 --> 2.460696).  Saving model ...\n",
      "Epoch: 975 \tTraining Loss: 2.446667 \tValidation Loss: 2.460685\n",
      "Validation loss decreased (2.460696 --> 2.460685).  Saving model ...\n",
      "Epoch: 976 \tTraining Loss: 2.453075 \tValidation Loss: 2.460678\n",
      "Validation loss decreased (2.460685 --> 2.460678).  Saving model ...\n",
      "Epoch: 977 \tTraining Loss: 2.449940 \tValidation Loss: 2.460675\n",
      "Validation loss decreased (2.460678 --> 2.460675).  Saving model ...\n",
      "Epoch: 978 \tTraining Loss: 2.437954 \tValidation Loss: 2.460645\n",
      "Validation loss decreased (2.460675 --> 2.460645).  Saving model ...\n",
      "Epoch: 979 \tTraining Loss: 2.438567 \tValidation Loss: 2.460598\n",
      "Validation loss decreased (2.460645 --> 2.460598).  Saving model ...\n",
      "Epoch: 980 \tTraining Loss: 2.448061 \tValidation Loss: 2.460610\n",
      "Epoch: 981 \tTraining Loss: 2.448317 \tValidation Loss: 2.460578\n",
      "Validation loss decreased (2.460598 --> 2.460578).  Saving model ...\n",
      "Epoch: 982 \tTraining Loss: 2.446673 \tValidation Loss: 2.460556\n",
      "Validation loss decreased (2.460578 --> 2.460556).  Saving model ...\n",
      "Epoch: 983 \tTraining Loss: 2.442361 \tValidation Loss: 2.460516\n",
      "Validation loss decreased (2.460556 --> 2.460516).  Saving model ...\n",
      "Epoch: 984 \tTraining Loss: 2.442878 \tValidation Loss: 2.460463\n",
      "Validation loss decreased (2.460516 --> 2.460463).  Saving model ...\n",
      "Epoch: 985 \tTraining Loss: 2.438154 \tValidation Loss: 2.460422\n",
      "Validation loss decreased (2.460463 --> 2.460422).  Saving model ...\n",
      "Epoch: 986 \tTraining Loss: 2.443688 \tValidation Loss: 2.460368\n",
      "Validation loss decreased (2.460422 --> 2.460368).  Saving model ...\n",
      "Epoch: 987 \tTraining Loss: 2.443741 \tValidation Loss: 2.460364\n",
      "Validation loss decreased (2.460368 --> 2.460364).  Saving model ...\n",
      "Epoch: 988 \tTraining Loss: 2.444564 \tValidation Loss: 2.460345\n",
      "Validation loss decreased (2.460364 --> 2.460345).  Saving model ...\n",
      "Epoch: 989 \tTraining Loss: 2.438211 \tValidation Loss: 2.460345\n",
      "Validation loss decreased (2.460345 --> 2.460345).  Saving model ...\n",
      "Epoch: 990 \tTraining Loss: 2.437884 \tValidation Loss: 2.460362\n",
      "Epoch: 991 \tTraining Loss: 2.433392 \tValidation Loss: 2.460315\n",
      "Validation loss decreased (2.460345 --> 2.460315).  Saving model ...\n",
      "Epoch: 992 \tTraining Loss: 2.445360 \tValidation Loss: 2.460302\n",
      "Validation loss decreased (2.460315 --> 2.460302).  Saving model ...\n",
      "Epoch: 993 \tTraining Loss: 2.448409 \tValidation Loss: 2.460299\n",
      "Validation loss decreased (2.460302 --> 2.460299).  Saving model ...\n",
      "Epoch: 994 \tTraining Loss: 2.446697 \tValidation Loss: 2.460284\n",
      "Validation loss decreased (2.460299 --> 2.460284).  Saving model ...\n",
      "Epoch: 995 \tTraining Loss: 2.445685 \tValidation Loss: 2.460215\n",
      "Validation loss decreased (2.460284 --> 2.460215).  Saving model ...\n",
      "Epoch: 996 \tTraining Loss: 2.445205 \tValidation Loss: 2.460262\n",
      "Epoch: 997 \tTraining Loss: 2.444264 \tValidation Loss: 2.460214\n",
      "Validation loss decreased (2.460215 --> 2.460214).  Saving model ...\n",
      "Epoch: 998 \tTraining Loss: 2.441389 \tValidation Loss: 2.460244\n",
      "Epoch: 999 \tTraining Loss: 2.433831 \tValidation Loss: 2.460209\n",
      "Validation loss decreased (2.460214 --> 2.460209).  Saving model ...\n",
      "Epoch: 1000 \tTraining Loss: 2.437856 \tValidation Loss: 2.460179\n",
      "Validation loss decreased (2.460209 --> 2.460179).  Saving model ...\n",
      "Epoch: 1001 \tTraining Loss: 2.442926 \tValidation Loss: 2.460202\n",
      "Epoch: 1002 \tTraining Loss: 2.433715 \tValidation Loss: 2.460150\n",
      "Validation loss decreased (2.460179 --> 2.460150).  Saving model ...\n",
      "Epoch: 1003 \tTraining Loss: 2.442244 \tValidation Loss: 2.460106\n",
      "Validation loss decreased (2.460150 --> 2.460106).  Saving model ...\n",
      "Epoch: 1004 \tTraining Loss: 2.443424 \tValidation Loss: 2.460066\n",
      "Validation loss decreased (2.460106 --> 2.460066).  Saving model ...\n",
      "Epoch: 1005 \tTraining Loss: 2.446628 \tValidation Loss: 2.460069\n",
      "Epoch: 1006 \tTraining Loss: 2.445134 \tValidation Loss: 2.460041\n",
      "Validation loss decreased (2.460066 --> 2.460041).  Saving model ...\n",
      "Epoch: 1007 \tTraining Loss: 2.444348 \tValidation Loss: 2.460052\n",
      "Epoch: 1008 \tTraining Loss: 2.443558 \tValidation Loss: 2.460064\n",
      "Epoch: 1009 \tTraining Loss: 2.445245 \tValidation Loss: 2.460026\n",
      "Validation loss decreased (2.460041 --> 2.460026).  Saving model ...\n",
      "Epoch: 1010 \tTraining Loss: 2.442332 \tValidation Loss: 2.460001\n",
      "Validation loss decreased (2.460026 --> 2.460001).  Saving model ...\n",
      "Epoch: 1011 \tTraining Loss: 2.457976 \tValidation Loss: 2.460051\n",
      "Epoch: 1012 \tTraining Loss: 2.454523 \tValidation Loss: 2.460036\n",
      "Epoch: 1013 \tTraining Loss: 2.437608 \tValidation Loss: 2.459995\n",
      "Validation loss decreased (2.460001 --> 2.459995).  Saving model ...\n",
      "Epoch: 1014 \tTraining Loss: 2.448002 \tValidation Loss: 2.459965\n",
      "Validation loss decreased (2.459995 --> 2.459965).  Saving model ...\n",
      "Epoch: 1015 \tTraining Loss: 2.441082 \tValidation Loss: 2.459949\n",
      "Validation loss decreased (2.459965 --> 2.459949).  Saving model ...\n",
      "Epoch: 1016 \tTraining Loss: 2.440895 \tValidation Loss: 2.459907\n",
      "Validation loss decreased (2.459949 --> 2.459907).  Saving model ...\n",
      "Epoch: 1017 \tTraining Loss: 2.439969 \tValidation Loss: 2.459882\n",
      "Validation loss decreased (2.459907 --> 2.459882).  Saving model ...\n",
      "Epoch: 1018 \tTraining Loss: 2.429675 \tValidation Loss: 2.459810\n",
      "Validation loss decreased (2.459882 --> 2.459810).  Saving model ...\n",
      "Epoch: 1019 \tTraining Loss: 2.440328 \tValidation Loss: 2.459757\n",
      "Validation loss decreased (2.459810 --> 2.459757).  Saving model ...\n",
      "Epoch: 1020 \tTraining Loss: 2.444645 \tValidation Loss: 2.459703\n",
      "Validation loss decreased (2.459757 --> 2.459703).  Saving model ...\n",
      "Epoch: 1021 \tTraining Loss: 2.448225 \tValidation Loss: 2.459699\n",
      "Validation loss decreased (2.459703 --> 2.459699).  Saving model ...\n",
      "Epoch: 1022 \tTraining Loss: 2.451934 \tValidation Loss: 2.459667\n",
      "Validation loss decreased (2.459699 --> 2.459667).  Saving model ...\n",
      "Epoch: 1023 \tTraining Loss: 2.443080 \tValidation Loss: 2.459650\n",
      "Validation loss decreased (2.459667 --> 2.459650).  Saving model ...\n",
      "Epoch: 1024 \tTraining Loss: 2.435588 \tValidation Loss: 2.459644\n",
      "Validation loss decreased (2.459650 --> 2.459644).  Saving model ...\n",
      "Epoch: 1025 \tTraining Loss: 2.442677 \tValidation Loss: 2.459635\n",
      "Validation loss decreased (2.459644 --> 2.459635).  Saving model ...\n",
      "Epoch: 1026 \tTraining Loss: 2.441194 \tValidation Loss: 2.459621\n",
      "Validation loss decreased (2.459635 --> 2.459621).  Saving model ...\n",
      "Epoch: 1027 \tTraining Loss: 2.443059 \tValidation Loss: 2.459639\n",
      "Epoch: 1028 \tTraining Loss: 2.438443 \tValidation Loss: 2.459654\n",
      "Epoch: 1029 \tTraining Loss: 2.446997 \tValidation Loss: 2.459629\n",
      "Epoch: 1030 \tTraining Loss: 2.450012 \tValidation Loss: 2.459582\n",
      "Validation loss decreased (2.459621 --> 2.459582).  Saving model ...\n",
      "Epoch: 1031 \tTraining Loss: 2.443548 \tValidation Loss: 2.459576\n",
      "Validation loss decreased (2.459582 --> 2.459576).  Saving model ...\n",
      "Epoch: 1032 \tTraining Loss: 2.451854 \tValidation Loss: 2.459590\n",
      "Epoch: 1033 \tTraining Loss: 2.432490 \tValidation Loss: 2.459551\n",
      "Validation loss decreased (2.459576 --> 2.459551).  Saving model ...\n",
      "Epoch: 1034 \tTraining Loss: 2.444361 \tValidation Loss: 2.459514\n",
      "Validation loss decreased (2.459551 --> 2.459514).  Saving model ...\n",
      "Epoch: 1035 \tTraining Loss: 2.436045 \tValidation Loss: 2.459454\n",
      "Validation loss decreased (2.459514 --> 2.459454).  Saving model ...\n",
      "Epoch: 1036 \tTraining Loss: 2.444832 \tValidation Loss: 2.459445\n",
      "Validation loss decreased (2.459454 --> 2.459445).  Saving model ...\n",
      "Epoch: 1037 \tTraining Loss: 2.432850 \tValidation Loss: 2.459383\n",
      "Validation loss decreased (2.459445 --> 2.459383).  Saving model ...\n",
      "Epoch: 1038 \tTraining Loss: 2.445947 \tValidation Loss: 2.459393\n",
      "Epoch: 1039 \tTraining Loss: 2.438186 \tValidation Loss: 2.459363\n",
      "Validation loss decreased (2.459383 --> 2.459363).  Saving model ...\n",
      "Epoch: 1040 \tTraining Loss: 2.441502 \tValidation Loss: 2.459344\n",
      "Validation loss decreased (2.459363 --> 2.459344).  Saving model ...\n",
      "Epoch: 1041 \tTraining Loss: 2.438225 \tValidation Loss: 2.459332\n",
      "Validation loss decreased (2.459344 --> 2.459332).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1042 \tTraining Loss: 2.438640 \tValidation Loss: 2.459284\n",
      "Validation loss decreased (2.459332 --> 2.459284).  Saving model ...\n",
      "Epoch: 1043 \tTraining Loss: 2.445419 \tValidation Loss: 2.459240\n",
      "Validation loss decreased (2.459284 --> 2.459240).  Saving model ...\n",
      "Epoch: 1044 \tTraining Loss: 2.440196 \tValidation Loss: 2.459240\n",
      "Validation loss decreased (2.459240 --> 2.459240).  Saving model ...\n",
      "Epoch: 1045 \tTraining Loss: 2.433087 \tValidation Loss: 2.459255\n",
      "Epoch: 1046 \tTraining Loss: 2.436768 \tValidation Loss: 2.459249\n",
      "Epoch: 1047 \tTraining Loss: 2.440687 \tValidation Loss: 2.459294\n",
      "Epoch: 1048 \tTraining Loss: 2.451479 \tValidation Loss: 2.459250\n",
      "Epoch: 1049 \tTraining Loss: 2.441648 \tValidation Loss: 2.459238\n",
      "Validation loss decreased (2.459240 --> 2.459238).  Saving model ...\n",
      "Epoch: 1050 \tTraining Loss: 2.446881 \tValidation Loss: 2.459221\n",
      "Validation loss decreased (2.459238 --> 2.459221).  Saving model ...\n",
      "Epoch: 1051 \tTraining Loss: 2.434296 \tValidation Loss: 2.459177\n",
      "Validation loss decreased (2.459221 --> 2.459177).  Saving model ...\n",
      "Epoch: 1052 \tTraining Loss: 2.442556 \tValidation Loss: 2.459163\n",
      "Validation loss decreased (2.459177 --> 2.459163).  Saving model ...\n",
      "Epoch: 1053 \tTraining Loss: 2.436813 \tValidation Loss: 2.459164\n",
      "Epoch: 1054 \tTraining Loss: 2.436831 \tValidation Loss: 2.459100\n",
      "Validation loss decreased (2.459163 --> 2.459100).  Saving model ...\n",
      "Epoch: 1055 \tTraining Loss: 2.438975 \tValidation Loss: 2.459120\n",
      "Epoch: 1056 \tTraining Loss: 2.454319 \tValidation Loss: 2.459117\n",
      "Epoch: 1057 \tTraining Loss: 2.447985 \tValidation Loss: 2.459086\n",
      "Validation loss decreased (2.459100 --> 2.459086).  Saving model ...\n",
      "Epoch: 1058 \tTraining Loss: 2.445346 \tValidation Loss: 2.459050\n",
      "Validation loss decreased (2.459086 --> 2.459050).  Saving model ...\n",
      "Epoch: 1059 \tTraining Loss: 2.439436 \tValidation Loss: 2.458999\n",
      "Validation loss decreased (2.459050 --> 2.458999).  Saving model ...\n",
      "Epoch: 1060 \tTraining Loss: 2.438036 \tValidation Loss: 2.458998\n",
      "Validation loss decreased (2.458999 --> 2.458998).  Saving model ...\n",
      "Epoch: 1061 \tTraining Loss: 2.432681 \tValidation Loss: 2.458959\n",
      "Validation loss decreased (2.458998 --> 2.458959).  Saving model ...\n",
      "Epoch: 1062 \tTraining Loss: 2.448649 \tValidation Loss: 2.458968\n",
      "Epoch: 1063 \tTraining Loss: 2.443063 \tValidation Loss: 2.459000\n",
      "Epoch: 1064 \tTraining Loss: 2.446075 \tValidation Loss: 2.459003\n",
      "Epoch: 1065 \tTraining Loss: 2.432751 \tValidation Loss: 2.458976\n",
      "Epoch: 1066 \tTraining Loss: 2.438691 \tValidation Loss: 2.458869\n",
      "Validation loss decreased (2.458959 --> 2.458869).  Saving model ...\n",
      "Epoch: 1067 \tTraining Loss: 2.441641 \tValidation Loss: 2.458805\n",
      "Validation loss decreased (2.458869 --> 2.458805).  Saving model ...\n",
      "Epoch: 1068 \tTraining Loss: 2.439004 \tValidation Loss: 2.458738\n",
      "Validation loss decreased (2.458805 --> 2.458738).  Saving model ...\n",
      "Epoch: 1069 \tTraining Loss: 2.447249 \tValidation Loss: 2.458690\n",
      "Validation loss decreased (2.458738 --> 2.458690).  Saving model ...\n",
      "Epoch: 1070 \tTraining Loss: 2.435720 \tValidation Loss: 2.458694\n",
      "Epoch: 1071 \tTraining Loss: 2.440027 \tValidation Loss: 2.458638\n",
      "Validation loss decreased (2.458690 --> 2.458638).  Saving model ...\n",
      "Epoch: 1072 \tTraining Loss: 2.445762 \tValidation Loss: 2.458600\n",
      "Validation loss decreased (2.458638 --> 2.458600).  Saving model ...\n",
      "Epoch: 1073 \tTraining Loss: 2.435209 \tValidation Loss: 2.458571\n",
      "Validation loss decreased (2.458600 --> 2.458571).  Saving model ...\n",
      "Epoch: 1074 \tTraining Loss: 2.444947 \tValidation Loss: 2.458518\n",
      "Validation loss decreased (2.458571 --> 2.458518).  Saving model ...\n",
      "Epoch: 1075 \tTraining Loss: 2.443538 \tValidation Loss: 2.458493\n",
      "Validation loss decreased (2.458518 --> 2.458493).  Saving model ...\n",
      "Epoch: 1076 \tTraining Loss: 2.442878 \tValidation Loss: 2.458450\n",
      "Validation loss decreased (2.458493 --> 2.458450).  Saving model ...\n",
      "Epoch: 1077 \tTraining Loss: 2.443182 \tValidation Loss: 2.458414\n",
      "Validation loss decreased (2.458450 --> 2.458414).  Saving model ...\n",
      "Epoch: 1078 \tTraining Loss: 2.436824 \tValidation Loss: 2.458362\n",
      "Validation loss decreased (2.458414 --> 2.458362).  Saving model ...\n",
      "Epoch: 1079 \tTraining Loss: 2.447427 \tValidation Loss: 2.458360\n",
      "Validation loss decreased (2.458362 --> 2.458360).  Saving model ...\n",
      "Epoch: 1080 \tTraining Loss: 2.443467 \tValidation Loss: 2.458361\n",
      "Epoch: 1081 \tTraining Loss: 2.444498 \tValidation Loss: 2.458377\n",
      "Epoch: 1082 \tTraining Loss: 2.437717 \tValidation Loss: 2.458447\n",
      "Epoch: 1083 \tTraining Loss: 2.435121 \tValidation Loss: 2.458434\n",
      "Epoch: 1084 \tTraining Loss: 2.448488 \tValidation Loss: 2.458423\n",
      "Epoch: 1085 \tTraining Loss: 2.434490 \tValidation Loss: 2.458397\n",
      "Epoch: 1086 \tTraining Loss: 2.428323 \tValidation Loss: 2.458359\n",
      "Validation loss decreased (2.458360 --> 2.458359).  Saving model ...\n",
      "Epoch: 1087 \tTraining Loss: 2.448055 \tValidation Loss: 2.458372\n",
      "Epoch: 1088 \tTraining Loss: 2.438668 \tValidation Loss: 2.458410\n",
      "Epoch: 1089 \tTraining Loss: 2.428417 \tValidation Loss: 2.458363\n",
      "Epoch: 1090 \tTraining Loss: 2.443723 \tValidation Loss: 2.458330\n",
      "Validation loss decreased (2.458359 --> 2.458330).  Saving model ...\n",
      "Epoch: 1091 \tTraining Loss: 2.437102 \tValidation Loss: 2.458308\n",
      "Validation loss decreased (2.458330 --> 2.458308).  Saving model ...\n",
      "Epoch: 1092 \tTraining Loss: 2.432581 \tValidation Loss: 2.458258\n",
      "Validation loss decreased (2.458308 --> 2.458258).  Saving model ...\n",
      "Epoch: 1093 \tTraining Loss: 2.432743 \tValidation Loss: 2.458194\n",
      "Validation loss decreased (2.458258 --> 2.458194).  Saving model ...\n",
      "Epoch: 1094 \tTraining Loss: 2.439824 \tValidation Loss: 2.458148\n",
      "Validation loss decreased (2.458194 --> 2.458148).  Saving model ...\n",
      "Epoch: 1095 \tTraining Loss: 2.429265 \tValidation Loss: 2.458120\n",
      "Validation loss decreased (2.458148 --> 2.458120).  Saving model ...\n",
      "Epoch: 1096 \tTraining Loss: 2.432763 \tValidation Loss: 2.458082\n",
      "Validation loss decreased (2.458120 --> 2.458082).  Saving model ...\n",
      "Epoch: 1097 \tTraining Loss: 2.443892 \tValidation Loss: 2.458109\n",
      "Epoch: 1098 \tTraining Loss: 2.436045 \tValidation Loss: 2.458127\n",
      "Epoch: 1099 \tTraining Loss: 2.438931 \tValidation Loss: 2.458069\n",
      "Validation loss decreased (2.458082 --> 2.458069).  Saving model ...\n",
      "Epoch: 1100 \tTraining Loss: 2.434870 \tValidation Loss: 2.458069\n",
      "Validation loss decreased (2.458069 --> 2.458069).  Saving model ...\n",
      "Epoch: 1101 \tTraining Loss: 2.443464 \tValidation Loss: 2.458018\n",
      "Validation loss decreased (2.458069 --> 2.458018).  Saving model ...\n",
      "Epoch: 1102 \tTraining Loss: 2.439767 \tValidation Loss: 2.457982\n",
      "Validation loss decreased (2.458018 --> 2.457982).  Saving model ...\n",
      "Epoch: 1103 \tTraining Loss: 2.443389 \tValidation Loss: 2.457948\n",
      "Validation loss decreased (2.457982 --> 2.457948).  Saving model ...\n",
      "Epoch: 1104 \tTraining Loss: 2.440783 \tValidation Loss: 2.457958\n",
      "Epoch: 1105 \tTraining Loss: 2.428566 \tValidation Loss: 2.457903\n",
      "Validation loss decreased (2.457948 --> 2.457903).  Saving model ...\n",
      "Epoch: 1106 \tTraining Loss: 2.439360 \tValidation Loss: 2.457952\n",
      "Epoch: 1107 \tTraining Loss: 2.437242 \tValidation Loss: 2.457947\n",
      "Epoch: 1108 \tTraining Loss: 2.448370 \tValidation Loss: 2.457945\n",
      "Epoch: 1109 \tTraining Loss: 2.439548 \tValidation Loss: 2.457973\n",
      "Epoch: 1110 \tTraining Loss: 2.438032 \tValidation Loss: 2.457935\n",
      "Epoch: 1111 \tTraining Loss: 2.443373 \tValidation Loss: 2.457890\n",
      "Validation loss decreased (2.457903 --> 2.457890).  Saving model ...\n",
      "Epoch: 1112 \tTraining Loss: 2.440320 \tValidation Loss: 2.457897\n",
      "Epoch: 1113 \tTraining Loss: 2.438679 \tValidation Loss: 2.457888\n",
      "Validation loss decreased (2.457890 --> 2.457888).  Saving model ...\n",
      "Epoch: 1114 \tTraining Loss: 2.435282 \tValidation Loss: 2.457896\n",
      "Epoch: 1115 \tTraining Loss: 2.435765 \tValidation Loss: 2.457889\n",
      "Epoch: 1116 \tTraining Loss: 2.440523 \tValidation Loss: 2.457886\n",
      "Validation loss decreased (2.457888 --> 2.457886).  Saving model ...\n",
      "Epoch: 1117 \tTraining Loss: 2.438445 \tValidation Loss: 2.457852\n",
      "Validation loss decreased (2.457886 --> 2.457852).  Saving model ...\n",
      "Epoch: 1118 \tTraining Loss: 2.435107 \tValidation Loss: 2.457821\n",
      "Validation loss decreased (2.457852 --> 2.457821).  Saving model ...\n",
      "Epoch: 1119 \tTraining Loss: 2.439811 \tValidation Loss: 2.457794\n",
      "Validation loss decreased (2.457821 --> 2.457794).  Saving model ...\n",
      "Epoch: 1120 \tTraining Loss: 2.434447 \tValidation Loss: 2.457742\n",
      "Validation loss decreased (2.457794 --> 2.457742).  Saving model ...\n",
      "Epoch: 1121 \tTraining Loss: 2.453202 \tValidation Loss: 2.457759\n",
      "Epoch: 1122 \tTraining Loss: 2.441481 \tValidation Loss: 2.457708\n",
      "Validation loss decreased (2.457742 --> 2.457708).  Saving model ...\n",
      "Epoch: 1123 \tTraining Loss: 2.438215 \tValidation Loss: 2.457631\n",
      "Validation loss decreased (2.457708 --> 2.457631).  Saving model ...\n",
      "Epoch: 1124 \tTraining Loss: 2.446511 \tValidation Loss: 2.457591\n",
      "Validation loss decreased (2.457631 --> 2.457591).  Saving model ...\n",
      "Epoch: 1125 \tTraining Loss: 2.435359 \tValidation Loss: 2.457571\n",
      "Validation loss decreased (2.457591 --> 2.457571).  Saving model ...\n",
      "Epoch: 1126 \tTraining Loss: 2.446736 \tValidation Loss: 2.457547\n",
      "Validation loss decreased (2.457571 --> 2.457547).  Saving model ...\n",
      "Epoch: 1127 \tTraining Loss: 2.437818 \tValidation Loss: 2.457525\n",
      "Validation loss decreased (2.457547 --> 2.457525).  Saving model ...\n",
      "Epoch: 1128 \tTraining Loss: 2.446505 \tValidation Loss: 2.457510\n",
      "Validation loss decreased (2.457525 --> 2.457510).  Saving model ...\n",
      "Epoch: 1129 \tTraining Loss: 2.436952 \tValidation Loss: 2.457451\n",
      "Validation loss decreased (2.457510 --> 2.457451).  Saving model ...\n",
      "Epoch: 1130 \tTraining Loss: 2.437803 \tValidation Loss: 2.457456\n",
      "Epoch: 1131 \tTraining Loss: 2.432375 \tValidation Loss: 2.457454\n",
      "Epoch: 1132 \tTraining Loss: 2.439632 \tValidation Loss: 2.457450\n",
      "Validation loss decreased (2.457451 --> 2.457450).  Saving model ...\n",
      "Epoch: 1133 \tTraining Loss: 2.438205 \tValidation Loss: 2.457421\n",
      "Validation loss decreased (2.457450 --> 2.457421).  Saving model ...\n",
      "Epoch: 1134 \tTraining Loss: 2.441216 \tValidation Loss: 2.457421\n",
      "Validation loss decreased (2.457421 --> 2.457421).  Saving model ...\n",
      "Epoch: 1135 \tTraining Loss: 2.441484 \tValidation Loss: 2.457414\n",
      "Validation loss decreased (2.457421 --> 2.457414).  Saving model ...\n",
      "Epoch: 1136 \tTraining Loss: 2.435063 \tValidation Loss: 2.457361\n",
      "Validation loss decreased (2.457414 --> 2.457361).  Saving model ...\n",
      "Epoch: 1137 \tTraining Loss: 2.448315 \tValidation Loss: 2.457354\n",
      "Validation loss decreased (2.457361 --> 2.457354).  Saving model ...\n",
      "Epoch: 1138 \tTraining Loss: 2.433455 \tValidation Loss: 2.457344\n",
      "Validation loss decreased (2.457354 --> 2.457344).  Saving model ...\n",
      "Epoch: 1139 \tTraining Loss: 2.446619 \tValidation Loss: 2.457343\n",
      "Validation loss decreased (2.457344 --> 2.457343).  Saving model ...\n",
      "Epoch: 1140 \tTraining Loss: 2.431750 \tValidation Loss: 2.457304\n",
      "Validation loss decreased (2.457343 --> 2.457304).  Saving model ...\n",
      "Epoch: 1141 \tTraining Loss: 2.433806 \tValidation Loss: 2.457174\n",
      "Validation loss decreased (2.457304 --> 2.457174).  Saving model ...\n",
      "Epoch: 1142 \tTraining Loss: 2.435406 \tValidation Loss: 2.457134\n",
      "Validation loss decreased (2.457174 --> 2.457134).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1143 \tTraining Loss: 2.429993 \tValidation Loss: 2.457100\n",
      "Validation loss decreased (2.457134 --> 2.457100).  Saving model ...\n",
      "Epoch: 1144 \tTraining Loss: 2.439844 \tValidation Loss: 2.457098\n",
      "Validation loss decreased (2.457100 --> 2.457098).  Saving model ...\n",
      "Epoch: 1145 \tTraining Loss: 2.428458 \tValidation Loss: 2.457046\n",
      "Validation loss decreased (2.457098 --> 2.457046).  Saving model ...\n",
      "Epoch: 1146 \tTraining Loss: 2.437402 \tValidation Loss: 2.457053\n",
      "Epoch: 1147 \tTraining Loss: 2.440196 \tValidation Loss: 2.457095\n",
      "Epoch: 1148 \tTraining Loss: 2.430485 \tValidation Loss: 2.457087\n",
      "Epoch: 1149 \tTraining Loss: 2.442739 \tValidation Loss: 2.457118\n",
      "Epoch: 1150 \tTraining Loss: 2.436708 \tValidation Loss: 2.457088\n",
      "Epoch: 1151 \tTraining Loss: 2.430003 \tValidation Loss: 2.457050\n",
      "Epoch: 1152 \tTraining Loss: 2.430762 \tValidation Loss: 2.456979\n",
      "Validation loss decreased (2.457046 --> 2.456979).  Saving model ...\n",
      "Epoch: 1153 \tTraining Loss: 2.433576 \tValidation Loss: 2.456984\n",
      "Epoch: 1154 \tTraining Loss: 2.433815 \tValidation Loss: 2.456954\n",
      "Validation loss decreased (2.456979 --> 2.456954).  Saving model ...\n",
      "Epoch: 1155 \tTraining Loss: 2.441240 \tValidation Loss: 2.456972\n",
      "Epoch: 1156 \tTraining Loss: 2.448350 \tValidation Loss: 2.456954\n",
      "Validation loss decreased (2.456954 --> 2.456954).  Saving model ...\n",
      "Epoch: 1157 \tTraining Loss: 2.443347 \tValidation Loss: 2.456914\n",
      "Validation loss decreased (2.456954 --> 2.456914).  Saving model ...\n",
      "Epoch: 1158 \tTraining Loss: 2.435756 \tValidation Loss: 2.456904\n",
      "Validation loss decreased (2.456914 --> 2.456904).  Saving model ...\n",
      "Epoch: 1159 \tTraining Loss: 2.440760 \tValidation Loss: 2.456862\n",
      "Validation loss decreased (2.456904 --> 2.456862).  Saving model ...\n",
      "Epoch: 1160 \tTraining Loss: 2.431777 \tValidation Loss: 2.456807\n",
      "Validation loss decreased (2.456862 --> 2.456807).  Saving model ...\n",
      "Epoch: 1161 \tTraining Loss: 2.438947 \tValidation Loss: 2.456835\n",
      "Epoch: 1162 \tTraining Loss: 2.435056 \tValidation Loss: 2.456834\n",
      "Epoch: 1163 \tTraining Loss: 2.440551 \tValidation Loss: 2.456836\n",
      "Epoch: 1164 \tTraining Loss: 2.446112 \tValidation Loss: 2.456858\n",
      "Epoch: 1165 \tTraining Loss: 2.424945 \tValidation Loss: 2.456789\n",
      "Validation loss decreased (2.456807 --> 2.456789).  Saving model ...\n",
      "Epoch: 1166 \tTraining Loss: 2.441231 \tValidation Loss: 2.456772\n",
      "Validation loss decreased (2.456789 --> 2.456772).  Saving model ...\n",
      "Epoch: 1167 \tTraining Loss: 2.437847 \tValidation Loss: 2.456730\n",
      "Validation loss decreased (2.456772 --> 2.456730).  Saving model ...\n",
      "Epoch: 1168 \tTraining Loss: 2.438113 \tValidation Loss: 2.456746\n",
      "Epoch: 1169 \tTraining Loss: 2.446456 \tValidation Loss: 2.456727\n",
      "Validation loss decreased (2.456730 --> 2.456727).  Saving model ...\n",
      "Epoch: 1170 \tTraining Loss: 2.433700 \tValidation Loss: 2.456756\n",
      "Epoch: 1171 \tTraining Loss: 2.434124 \tValidation Loss: 2.456709\n",
      "Validation loss decreased (2.456727 --> 2.456709).  Saving model ...\n",
      "Epoch: 1172 \tTraining Loss: 2.433003 \tValidation Loss: 2.456697\n",
      "Validation loss decreased (2.456709 --> 2.456697).  Saving model ...\n",
      "Epoch: 1173 \tTraining Loss: 2.432914 \tValidation Loss: 2.456678\n",
      "Validation loss decreased (2.456697 --> 2.456678).  Saving model ...\n",
      "Epoch: 1174 \tTraining Loss: 2.445534 \tValidation Loss: 2.456733\n",
      "Epoch: 1175 \tTraining Loss: 2.430127 \tValidation Loss: 2.456695\n",
      "Epoch: 1176 \tTraining Loss: 2.439768 \tValidation Loss: 2.456709\n",
      "Epoch: 1177 \tTraining Loss: 2.438986 \tValidation Loss: 2.456624\n",
      "Validation loss decreased (2.456678 --> 2.456624).  Saving model ...\n",
      "Epoch: 1178 \tTraining Loss: 2.432596 \tValidation Loss: 2.456586\n",
      "Validation loss decreased (2.456624 --> 2.456586).  Saving model ...\n",
      "Epoch: 1179 \tTraining Loss: 2.427365 \tValidation Loss: 2.456544\n",
      "Validation loss decreased (2.456586 --> 2.456544).  Saving model ...\n",
      "Epoch: 1180 \tTraining Loss: 2.434983 \tValidation Loss: 2.456547\n",
      "Epoch: 1181 \tTraining Loss: 2.442431 \tValidation Loss: 2.456534\n",
      "Validation loss decreased (2.456544 --> 2.456534).  Saving model ...\n",
      "Epoch: 1182 \tTraining Loss: 2.440494 \tValidation Loss: 2.456532\n",
      "Validation loss decreased (2.456534 --> 2.456532).  Saving model ...\n",
      "Epoch: 1183 \tTraining Loss: 2.423217 \tValidation Loss: 2.456523\n",
      "Validation loss decreased (2.456532 --> 2.456523).  Saving model ...\n",
      "Epoch: 1184 \tTraining Loss: 2.434477 \tValidation Loss: 2.456528\n",
      "Epoch: 1185 \tTraining Loss: 2.443571 \tValidation Loss: 2.456515\n",
      "Validation loss decreased (2.456523 --> 2.456515).  Saving model ...\n",
      "Epoch: 1186 \tTraining Loss: 2.434350 \tValidation Loss: 2.456523\n",
      "Epoch: 1187 \tTraining Loss: 2.437922 \tValidation Loss: 2.456547\n",
      "Epoch: 1188 \tTraining Loss: 2.433515 \tValidation Loss: 2.456469\n",
      "Validation loss decreased (2.456515 --> 2.456469).  Saving model ...\n",
      "Epoch: 1189 \tTraining Loss: 2.436719 \tValidation Loss: 2.456432\n",
      "Validation loss decreased (2.456469 --> 2.456432).  Saving model ...\n",
      "Epoch: 1190 \tTraining Loss: 2.439401 \tValidation Loss: 2.456432\n",
      "Validation loss decreased (2.456432 --> 2.456432).  Saving model ...\n",
      "Epoch: 1191 \tTraining Loss: 2.441074 \tValidation Loss: 2.456397\n",
      "Validation loss decreased (2.456432 --> 2.456397).  Saving model ...\n",
      "Epoch: 1192 \tTraining Loss: 2.425609 \tValidation Loss: 2.456410\n",
      "Epoch: 1193 \tTraining Loss: 2.432709 \tValidation Loss: 2.456386\n",
      "Validation loss decreased (2.456397 --> 2.456386).  Saving model ...\n",
      "Epoch: 1194 \tTraining Loss: 2.433577 \tValidation Loss: 2.456397\n",
      "Epoch: 1195 \tTraining Loss: 2.434870 \tValidation Loss: 2.456422\n",
      "Epoch: 1196 \tTraining Loss: 2.444960 \tValidation Loss: 2.456415\n",
      "Epoch: 1197 \tTraining Loss: 2.432241 \tValidation Loss: 2.456370\n",
      "Validation loss decreased (2.456386 --> 2.456370).  Saving model ...\n",
      "Epoch: 1198 \tTraining Loss: 2.439461 \tValidation Loss: 2.456360\n",
      "Validation loss decreased (2.456370 --> 2.456360).  Saving model ...\n",
      "Epoch: 1199 \tTraining Loss: 2.434268 \tValidation Loss: 2.456313\n",
      "Validation loss decreased (2.456360 --> 2.456313).  Saving model ...\n",
      "Epoch: 1200 \tTraining Loss: 2.435363 \tValidation Loss: 2.456301\n",
      "Validation loss decreased (2.456313 --> 2.456301).  Saving model ...\n",
      "Epoch: 1201 \tTraining Loss: 2.436931 \tValidation Loss: 2.456342\n",
      "Epoch: 1202 \tTraining Loss: 2.443649 \tValidation Loss: 2.456368\n",
      "Epoch: 1203 \tTraining Loss: 2.440088 \tValidation Loss: 2.456381\n",
      "Epoch: 1204 \tTraining Loss: 2.439555 \tValidation Loss: 2.456393\n",
      "Epoch: 1205 \tTraining Loss: 2.435351 \tValidation Loss: 2.456360\n",
      "Epoch: 1206 \tTraining Loss: 2.442645 \tValidation Loss: 2.456367\n",
      "Epoch: 1207 \tTraining Loss: 2.448939 \tValidation Loss: 2.456405\n",
      "Epoch: 1208 \tTraining Loss: 2.440951 \tValidation Loss: 2.456410\n",
      "Epoch: 1209 \tTraining Loss: 2.430260 \tValidation Loss: 2.456383\n",
      "Epoch: 1210 \tTraining Loss: 2.434670 \tValidation Loss: 2.456358\n",
      "Epoch: 1211 \tTraining Loss: 2.429539 \tValidation Loss: 2.456382\n",
      "Epoch: 1212 \tTraining Loss: 2.437962 \tValidation Loss: 2.456371\n",
      "Epoch: 1213 \tTraining Loss: 2.444802 \tValidation Loss: 2.456387\n",
      "Epoch: 1214 \tTraining Loss: 2.439219 \tValidation Loss: 2.456321\n",
      "Epoch: 1215 \tTraining Loss: 2.434529 \tValidation Loss: 2.456350\n",
      "Epoch: 1216 \tTraining Loss: 2.434059 \tValidation Loss: 2.456339\n",
      "Epoch: 1217 \tTraining Loss: 2.438616 \tValidation Loss: 2.456380\n",
      "Epoch: 1218 \tTraining Loss: 2.432864 \tValidation Loss: 2.456377\n",
      "Epoch: 1219 \tTraining Loss: 2.425809 \tValidation Loss: 2.456339\n",
      "Epoch: 1220 \tTraining Loss: 2.436999 \tValidation Loss: 2.456286\n",
      "Validation loss decreased (2.456301 --> 2.456286).  Saving model ...\n",
      "Epoch: 1221 \tTraining Loss: 2.440769 \tValidation Loss: 2.456233\n",
      "Validation loss decreased (2.456286 --> 2.456233).  Saving model ...\n",
      "Epoch: 1222 \tTraining Loss: 2.431132 \tValidation Loss: 2.456143\n",
      "Validation loss decreased (2.456233 --> 2.456143).  Saving model ...\n",
      "Epoch: 1223 \tTraining Loss: 2.428412 \tValidation Loss: 2.456079\n",
      "Validation loss decreased (2.456143 --> 2.456079).  Saving model ...\n",
      "Epoch: 1224 \tTraining Loss: 2.434853 \tValidation Loss: 2.456084\n",
      "Epoch: 1225 \tTraining Loss: 2.424950 \tValidation Loss: 2.456049\n",
      "Validation loss decreased (2.456079 --> 2.456049).  Saving model ...\n",
      "Epoch: 1226 \tTraining Loss: 2.434287 \tValidation Loss: 2.456014\n",
      "Validation loss decreased (2.456049 --> 2.456014).  Saving model ...\n",
      "Epoch: 1227 \tTraining Loss: 2.424533 \tValidation Loss: 2.455993\n",
      "Validation loss decreased (2.456014 --> 2.455993).  Saving model ...\n",
      "Epoch: 1228 \tTraining Loss: 2.430248 \tValidation Loss: 2.455973\n",
      "Validation loss decreased (2.455993 --> 2.455973).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1229 \tTraining Loss: 2.429274 \tValidation Loss: 2.455953\n",
      "Validation loss decreased (2.455973 --> 2.455953).  Saving model ...\n",
      "Epoch: 1230 \tTraining Loss: 2.440119 \tValidation Loss: 2.455946\n",
      "Validation loss decreased (2.455953 --> 2.455946).  Saving model ...\n",
      "Epoch: 1231 \tTraining Loss: 2.434618 \tValidation Loss: 2.455896\n",
      "Validation loss decreased (2.455946 --> 2.455896).  Saving model ...\n",
      "Epoch: 1232 \tTraining Loss: 2.431947 \tValidation Loss: 2.455877\n",
      "Validation loss decreased (2.455896 --> 2.455877).  Saving model ...\n",
      "Epoch: 1233 \tTraining Loss: 2.442548 \tValidation Loss: 2.455875\n",
      "Validation loss decreased (2.455877 --> 2.455875).  Saving model ...\n",
      "Epoch: 1234 \tTraining Loss: 2.431313 \tValidation Loss: 2.455850\n",
      "Validation loss decreased (2.455875 --> 2.455850).  Saving model ...\n",
      "Epoch: 1235 \tTraining Loss: 2.429235 \tValidation Loss: 2.455827\n",
      "Validation loss decreased (2.455850 --> 2.455827).  Saving model ...\n",
      "Epoch: 1236 \tTraining Loss: 2.436943 \tValidation Loss: 2.455814\n",
      "Validation loss decreased (2.455827 --> 2.455814).  Saving model ...\n",
      "Epoch: 1237 \tTraining Loss: 2.441673 \tValidation Loss: 2.455822\n",
      "Epoch: 1238 \tTraining Loss: 2.429250 \tValidation Loss: 2.455825\n",
      "Epoch: 1239 \tTraining Loss: 2.431368 \tValidation Loss: 2.455820\n",
      "Epoch: 1240 \tTraining Loss: 2.435920 \tValidation Loss: 2.455832\n",
      "Epoch: 1241 \tTraining Loss: 2.422767 \tValidation Loss: 2.455833\n",
      "Epoch: 1242 \tTraining Loss: 2.434459 \tValidation Loss: 2.455762\n",
      "Validation loss decreased (2.455814 --> 2.455762).  Saving model ...\n",
      "Epoch: 1243 \tTraining Loss: 2.443187 \tValidation Loss: 2.455746\n",
      "Validation loss decreased (2.455762 --> 2.455746).  Saving model ...\n",
      "Epoch: 1244 \tTraining Loss: 2.431479 \tValidation Loss: 2.455735\n",
      "Validation loss decreased (2.455746 --> 2.455735).  Saving model ...\n",
      "Epoch: 1245 \tTraining Loss: 2.441695 \tValidation Loss: 2.455709\n",
      "Validation loss decreased (2.455735 --> 2.455709).  Saving model ...\n",
      "Epoch: 1246 \tTraining Loss: 2.428142 \tValidation Loss: 2.455706\n",
      "Validation loss decreased (2.455709 --> 2.455706).  Saving model ...\n",
      "Epoch: 1247 \tTraining Loss: 2.419294 \tValidation Loss: 2.455687\n",
      "Validation loss decreased (2.455706 --> 2.455687).  Saving model ...\n",
      "Epoch: 1248 \tTraining Loss: 2.438332 \tValidation Loss: 2.455703\n",
      "Epoch: 1249 \tTraining Loss: 2.442851 \tValidation Loss: 2.455660\n",
      "Validation loss decreased (2.455687 --> 2.455660).  Saving model ...\n",
      "Epoch: 1250 \tTraining Loss: 2.437348 \tValidation Loss: 2.455641\n",
      "Validation loss decreased (2.455660 --> 2.455641).  Saving model ...\n",
      "Epoch: 1251 \tTraining Loss: 2.432965 \tValidation Loss: 2.455639\n",
      "Validation loss decreased (2.455641 --> 2.455639).  Saving model ...\n",
      "Epoch: 1252 \tTraining Loss: 2.428399 \tValidation Loss: 2.455673\n",
      "Epoch: 1253 \tTraining Loss: 2.428730 \tValidation Loss: 2.455630\n",
      "Validation loss decreased (2.455639 --> 2.455630).  Saving model ...\n",
      "Epoch: 1254 \tTraining Loss: 2.440134 \tValidation Loss: 2.455590\n",
      "Validation loss decreased (2.455630 --> 2.455590).  Saving model ...\n",
      "Epoch: 1255 \tTraining Loss: 2.428545 \tValidation Loss: 2.455561\n",
      "Validation loss decreased (2.455590 --> 2.455561).  Saving model ...\n",
      "Epoch: 1256 \tTraining Loss: 2.428894 \tValidation Loss: 2.455518\n",
      "Validation loss decreased (2.455561 --> 2.455518).  Saving model ...\n",
      "Epoch: 1257 \tTraining Loss: 2.424956 \tValidation Loss: 2.455513\n",
      "Validation loss decreased (2.455518 --> 2.455513).  Saving model ...\n",
      "Epoch: 1258 \tTraining Loss: 2.439834 \tValidation Loss: 2.455530\n",
      "Epoch: 1259 \tTraining Loss: 2.436820 \tValidation Loss: 2.455526\n",
      "Epoch: 1260 \tTraining Loss: 2.436576 \tValidation Loss: 2.455500\n",
      "Validation loss decreased (2.455513 --> 2.455500).  Saving model ...\n",
      "Epoch: 1261 \tTraining Loss: 2.435714 \tValidation Loss: 2.455437\n",
      "Validation loss decreased (2.455500 --> 2.455437).  Saving model ...\n",
      "Epoch: 1262 \tTraining Loss: 2.423712 \tValidation Loss: 2.455413\n",
      "Validation loss decreased (2.455437 --> 2.455413).  Saving model ...\n",
      "Epoch: 1263 \tTraining Loss: 2.427176 \tValidation Loss: 2.455348\n",
      "Validation loss decreased (2.455413 --> 2.455348).  Saving model ...\n",
      "Epoch: 1264 \tTraining Loss: 2.428957 \tValidation Loss: 2.455346\n",
      "Validation loss decreased (2.455348 --> 2.455346).  Saving model ...\n",
      "Epoch: 1265 \tTraining Loss: 2.425729 \tValidation Loss: 2.455309\n",
      "Validation loss decreased (2.455346 --> 2.455309).  Saving model ...\n",
      "Epoch: 1266 \tTraining Loss: 2.439716 \tValidation Loss: 2.455256\n",
      "Validation loss decreased (2.455309 --> 2.455256).  Saving model ...\n",
      "Epoch: 1267 \tTraining Loss: 2.424908 \tValidation Loss: 2.455179\n",
      "Validation loss decreased (2.455256 --> 2.455179).  Saving model ...\n",
      "Epoch: 1268 \tTraining Loss: 2.426936 \tValidation Loss: 2.455140\n",
      "Validation loss decreased (2.455179 --> 2.455140).  Saving model ...\n",
      "Epoch: 1269 \tTraining Loss: 2.439880 \tValidation Loss: 2.455182\n",
      "Epoch: 1270 \tTraining Loss: 2.427392 \tValidation Loss: 2.455179\n",
      "Epoch: 1271 \tTraining Loss: 2.436800 \tValidation Loss: 2.455113\n",
      "Validation loss decreased (2.455140 --> 2.455113).  Saving model ...\n",
      "Epoch: 1272 \tTraining Loss: 2.423462 \tValidation Loss: 2.454982\n",
      "Validation loss decreased (2.455113 --> 2.454982).  Saving model ...\n",
      "Epoch: 1273 \tTraining Loss: 2.439781 \tValidation Loss: 2.455035\n",
      "Epoch: 1274 \tTraining Loss: 2.429297 \tValidation Loss: 2.454937\n",
      "Validation loss decreased (2.454982 --> 2.454937).  Saving model ...\n",
      "Epoch: 1275 \tTraining Loss: 2.432171 \tValidation Loss: 2.454881\n",
      "Validation loss decreased (2.454937 --> 2.454881).  Saving model ...\n",
      "Epoch: 1276 \tTraining Loss: 2.431115 \tValidation Loss: 2.454900\n",
      "Epoch: 1277 \tTraining Loss: 2.431988 \tValidation Loss: 2.454922\n",
      "Epoch: 1278 \tTraining Loss: 2.425509 \tValidation Loss: 2.454945\n",
      "Epoch: 1279 \tTraining Loss: 2.432192 \tValidation Loss: 2.454972\n",
      "Epoch: 1280 \tTraining Loss: 2.437960 \tValidation Loss: 2.454977\n",
      "Epoch: 1281 \tTraining Loss: 2.438357 \tValidation Loss: 2.454985\n",
      "Epoch: 1282 \tTraining Loss: 2.435029 \tValidation Loss: 2.454977\n",
      "Epoch: 1283 \tTraining Loss: 2.430417 \tValidation Loss: 2.454931\n",
      "Epoch: 1284 \tTraining Loss: 2.432594 \tValidation Loss: 2.454918\n",
      "Epoch: 1285 \tTraining Loss: 2.435093 \tValidation Loss: 2.454914\n",
      "Epoch: 1286 \tTraining Loss: 2.432006 \tValidation Loss: 2.454915\n",
      "Epoch: 1287 \tTraining Loss: 2.430583 \tValidation Loss: 2.454905\n",
      "Epoch: 1288 \tTraining Loss: 2.423018 \tValidation Loss: 2.454908\n",
      "Epoch: 1289 \tTraining Loss: 2.426035 \tValidation Loss: 2.454888\n",
      "Epoch: 1290 \tTraining Loss: 2.438214 \tValidation Loss: 2.454857\n",
      "Validation loss decreased (2.454881 --> 2.454857).  Saving model ...\n",
      "Epoch: 1291 \tTraining Loss: 2.444807 \tValidation Loss: 2.454893\n",
      "Epoch: 1292 \tTraining Loss: 2.433526 \tValidation Loss: 2.454844\n",
      "Validation loss decreased (2.454857 --> 2.454844).  Saving model ...\n",
      "Epoch: 1293 \tTraining Loss: 2.429512 \tValidation Loss: 2.454836\n",
      "Validation loss decreased (2.454844 --> 2.454836).  Saving model ...\n",
      "Epoch: 1294 \tTraining Loss: 2.438841 \tValidation Loss: 2.454835\n",
      "Validation loss decreased (2.454836 --> 2.454835).  Saving model ...\n",
      "Epoch: 1295 \tTraining Loss: 2.432324 \tValidation Loss: 2.454827\n",
      "Validation loss decreased (2.454835 --> 2.454827).  Saving model ...\n",
      "Epoch: 1296 \tTraining Loss: 2.437663 \tValidation Loss: 2.454838\n",
      "Epoch: 1297 \tTraining Loss: 2.436468 \tValidation Loss: 2.454891\n",
      "Epoch: 1298 \tTraining Loss: 2.432376 \tValidation Loss: 2.454841\n",
      "Epoch: 1299 \tTraining Loss: 2.421175 \tValidation Loss: 2.454834\n",
      "Epoch: 1300 \tTraining Loss: 2.438391 \tValidation Loss: 2.454856\n",
      "Epoch: 1301 \tTraining Loss: 2.432859 \tValidation Loss: 2.454820\n",
      "Validation loss decreased (2.454827 --> 2.454820).  Saving model ...\n",
      "Epoch: 1302 \tTraining Loss: 2.430951 \tValidation Loss: 2.454797\n",
      "Validation loss decreased (2.454820 --> 2.454797).  Saving model ...\n",
      "Epoch: 1303 \tTraining Loss: 2.430941 \tValidation Loss: 2.454781\n",
      "Validation loss decreased (2.454797 --> 2.454781).  Saving model ...\n",
      "Epoch: 1304 \tTraining Loss: 2.431029 \tValidation Loss: 2.454816\n",
      "Epoch: 1305 \tTraining Loss: 2.432093 \tValidation Loss: 2.454787\n",
      "Epoch: 1306 \tTraining Loss: 2.430779 \tValidation Loss: 2.454787\n",
      "Epoch: 1307 \tTraining Loss: 2.436560 \tValidation Loss: 2.454804\n",
      "Epoch: 1308 \tTraining Loss: 2.425093 \tValidation Loss: 2.454757\n",
      "Validation loss decreased (2.454781 --> 2.454757).  Saving model ...\n",
      "Epoch: 1309 \tTraining Loss: 2.426498 \tValidation Loss: 2.454725\n",
      "Validation loss decreased (2.454757 --> 2.454725).  Saving model ...\n",
      "Epoch: 1310 \tTraining Loss: 2.430274 \tValidation Loss: 2.454697\n",
      "Validation loss decreased (2.454725 --> 2.454697).  Saving model ...\n",
      "Epoch: 1311 \tTraining Loss: 2.428937 \tValidation Loss: 2.454728\n",
      "Epoch: 1312 \tTraining Loss: 2.425093 \tValidation Loss: 2.454703\n",
      "Epoch: 1313 \tTraining Loss: 2.431249 \tValidation Loss: 2.454707\n",
      "Epoch: 1314 \tTraining Loss: 2.434437 \tValidation Loss: 2.454686\n",
      "Validation loss decreased (2.454697 --> 2.454686).  Saving model ...\n",
      "Epoch: 1315 \tTraining Loss: 2.427676 \tValidation Loss: 2.454707\n",
      "Epoch: 1316 \tTraining Loss: 2.435204 \tValidation Loss: 2.454654\n",
      "Validation loss decreased (2.454686 --> 2.454654).  Saving model ...\n",
      "Epoch: 1317 \tTraining Loss: 2.419072 \tValidation Loss: 2.454568\n",
      "Validation loss decreased (2.454654 --> 2.454568).  Saving model ...\n",
      "Epoch: 1318 \tTraining Loss: 2.432719 \tValidation Loss: 2.454577\n",
      "Epoch: 1319 \tTraining Loss: 2.434072 \tValidation Loss: 2.454609\n",
      "Epoch: 1320 \tTraining Loss: 2.434627 \tValidation Loss: 2.454569\n",
      "Epoch: 1321 \tTraining Loss: 2.429285 \tValidation Loss: 2.454590\n",
      "Epoch: 1322 \tTraining Loss: 2.435754 \tValidation Loss: 2.454618\n",
      "Epoch: 1323 \tTraining Loss: 2.431473 \tValidation Loss: 2.454609\n",
      "Epoch: 1324 \tTraining Loss: 2.426519 \tValidation Loss: 2.454589\n",
      "Epoch: 1325 \tTraining Loss: 2.418950 \tValidation Loss: 2.454540\n",
      "Validation loss decreased (2.454568 --> 2.454540).  Saving model ...\n",
      "Epoch: 1326 \tTraining Loss: 2.431260 \tValidation Loss: 2.454500\n",
      "Validation loss decreased (2.454540 --> 2.454500).  Saving model ...\n",
      "Epoch: 1327 \tTraining Loss: 2.435378 \tValidation Loss: 2.454412\n",
      "Validation loss decreased (2.454500 --> 2.454412).  Saving model ...\n",
      "Epoch: 1328 \tTraining Loss: 2.431481 \tValidation Loss: 2.454417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1329 \tTraining Loss: 2.430884 \tValidation Loss: 2.454400\n",
      "Validation loss decreased (2.454412 --> 2.454400).  Saving model ...\n",
      "Epoch: 1330 \tTraining Loss: 2.438027 \tValidation Loss: 2.454370\n",
      "Validation loss decreased (2.454400 --> 2.454370).  Saving model ...\n",
      "Epoch: 1331 \tTraining Loss: 2.424630 \tValidation Loss: 2.454340\n",
      "Validation loss decreased (2.454370 --> 2.454340).  Saving model ...\n",
      "Epoch: 1332 \tTraining Loss: 2.431515 \tValidation Loss: 2.454299\n",
      "Validation loss decreased (2.454340 --> 2.454299).  Saving model ...\n",
      "Epoch: 1333 \tTraining Loss: 2.426928 \tValidation Loss: 2.454247\n",
      "Validation loss decreased (2.454299 --> 2.454247).  Saving model ...\n",
      "Epoch: 1334 \tTraining Loss: 2.428820 \tValidation Loss: 2.454239\n",
      "Validation loss decreased (2.454247 --> 2.454239).  Saving model ...\n",
      "Epoch: 1335 \tTraining Loss: 2.439009 \tValidation Loss: 2.454276\n",
      "Epoch: 1336 \tTraining Loss: 2.437892 \tValidation Loss: 2.454235\n",
      "Validation loss decreased (2.454239 --> 2.454235).  Saving model ...\n",
      "Epoch: 1337 \tTraining Loss: 2.431935 \tValidation Loss: 2.454174\n",
      "Validation loss decreased (2.454235 --> 2.454174).  Saving model ...\n",
      "Epoch: 1338 \tTraining Loss: 2.430209 \tValidation Loss: 2.454145\n",
      "Validation loss decreased (2.454174 --> 2.454145).  Saving model ...\n",
      "Epoch: 1339 \tTraining Loss: 2.423342 \tValidation Loss: 2.454129\n",
      "Validation loss decreased (2.454145 --> 2.454129).  Saving model ...\n",
      "Epoch: 1340 \tTraining Loss: 2.428244 \tValidation Loss: 2.454068\n",
      "Validation loss decreased (2.454129 --> 2.454068).  Saving model ...\n",
      "Epoch: 1341 \tTraining Loss: 2.435809 \tValidation Loss: 2.454062\n",
      "Validation loss decreased (2.454068 --> 2.454062).  Saving model ...\n",
      "Epoch: 1342 \tTraining Loss: 2.431218 \tValidation Loss: 2.454103\n",
      "Epoch: 1343 \tTraining Loss: 2.424683 \tValidation Loss: 2.453977\n",
      "Validation loss decreased (2.454062 --> 2.453977).  Saving model ...\n",
      "Epoch: 1344 \tTraining Loss: 2.428652 \tValidation Loss: 2.453955\n",
      "Validation loss decreased (2.453977 --> 2.453955).  Saving model ...\n",
      "Epoch: 1345 \tTraining Loss: 2.436938 \tValidation Loss: 2.453893\n",
      "Validation loss decreased (2.453955 --> 2.453893).  Saving model ...\n",
      "Epoch: 1346 \tTraining Loss: 2.429245 \tValidation Loss: 2.453852\n",
      "Validation loss decreased (2.453893 --> 2.453852).  Saving model ...\n",
      "Epoch: 1347 \tTraining Loss: 2.432799 \tValidation Loss: 2.453881\n",
      "Epoch: 1348 \tTraining Loss: 2.428291 \tValidation Loss: 2.453881\n",
      "Epoch: 1349 \tTraining Loss: 2.422434 \tValidation Loss: 2.453858\n",
      "Epoch: 1350 \tTraining Loss: 2.425102 \tValidation Loss: 2.453886\n",
      "Epoch: 1351 \tTraining Loss: 2.432895 \tValidation Loss: 2.453845\n",
      "Validation loss decreased (2.453852 --> 2.453845).  Saving model ...\n",
      "Epoch: 1352 \tTraining Loss: 2.431826 \tValidation Loss: 2.453782\n",
      "Validation loss decreased (2.453845 --> 2.453782).  Saving model ...\n",
      "Epoch: 1353 \tTraining Loss: 2.433919 \tValidation Loss: 2.453754\n",
      "Validation loss decreased (2.453782 --> 2.453754).  Saving model ...\n",
      "Epoch: 1354 \tTraining Loss: 2.428961 \tValidation Loss: 2.453708\n",
      "Validation loss decreased (2.453754 --> 2.453708).  Saving model ...\n",
      "Epoch: 1355 \tTraining Loss: 2.435858 \tValidation Loss: 2.453671\n",
      "Validation loss decreased (2.453708 --> 2.453671).  Saving model ...\n",
      "Epoch: 1356 \tTraining Loss: 2.422684 \tValidation Loss: 2.453667\n",
      "Validation loss decreased (2.453671 --> 2.453667).  Saving model ...\n",
      "Epoch: 1357 \tTraining Loss: 2.427533 \tValidation Loss: 2.453650\n",
      "Validation loss decreased (2.453667 --> 2.453650).  Saving model ...\n",
      "Epoch: 1358 \tTraining Loss: 2.428707 \tValidation Loss: 2.453621\n",
      "Validation loss decreased (2.453650 --> 2.453621).  Saving model ...\n",
      "Epoch: 1359 \tTraining Loss: 2.438347 \tValidation Loss: 2.453570\n",
      "Validation loss decreased (2.453621 --> 2.453570).  Saving model ...\n",
      "Epoch: 1360 \tTraining Loss: 2.421611 \tValidation Loss: 2.453529\n",
      "Validation loss decreased (2.453570 --> 2.453529).  Saving model ...\n",
      "Epoch: 1361 \tTraining Loss: 2.427638 \tValidation Loss: 2.453496\n",
      "Validation loss decreased (2.453529 --> 2.453496).  Saving model ...\n",
      "Epoch: 1362 \tTraining Loss: 2.434093 \tValidation Loss: 2.453470\n",
      "Validation loss decreased (2.453496 --> 2.453470).  Saving model ...\n",
      "Epoch: 1363 \tTraining Loss: 2.425579 \tValidation Loss: 2.453403\n",
      "Validation loss decreased (2.453470 --> 2.453403).  Saving model ...\n",
      "Epoch: 1364 \tTraining Loss: 2.433397 \tValidation Loss: 2.453452\n",
      "Epoch: 1365 \tTraining Loss: 2.422487 \tValidation Loss: 2.453414\n",
      "Epoch: 1366 \tTraining Loss: 2.433554 \tValidation Loss: 2.453389\n",
      "Validation loss decreased (2.453403 --> 2.453389).  Saving model ...\n",
      "Epoch: 1367 \tTraining Loss: 2.434830 \tValidation Loss: 2.453398\n",
      "Epoch: 1368 \tTraining Loss: 2.433222 \tValidation Loss: 2.453402\n",
      "Epoch: 1369 \tTraining Loss: 2.425413 \tValidation Loss: 2.453380\n",
      "Validation loss decreased (2.453389 --> 2.453380).  Saving model ...\n",
      "Epoch: 1370 \tTraining Loss: 2.422585 \tValidation Loss: 2.453365\n",
      "Validation loss decreased (2.453380 --> 2.453365).  Saving model ...\n",
      "Epoch: 1371 \tTraining Loss: 2.431244 \tValidation Loss: 2.453333\n",
      "Validation loss decreased (2.453365 --> 2.453333).  Saving model ...\n",
      "Epoch: 1372 \tTraining Loss: 2.441366 \tValidation Loss: 2.453291\n",
      "Validation loss decreased (2.453333 --> 2.453291).  Saving model ...\n",
      "Epoch: 1373 \tTraining Loss: 2.428848 \tValidation Loss: 2.453246\n",
      "Validation loss decreased (2.453291 --> 2.453246).  Saving model ...\n",
      "Epoch: 1374 \tTraining Loss: 2.425535 \tValidation Loss: 2.453235\n",
      "Validation loss decreased (2.453246 --> 2.453235).  Saving model ...\n",
      "Epoch: 1375 \tTraining Loss: 2.428536 \tValidation Loss: 2.453187\n",
      "Validation loss decreased (2.453235 --> 2.453187).  Saving model ...\n",
      "Epoch: 1376 \tTraining Loss: 2.426517 \tValidation Loss: 2.453186\n",
      "Validation loss decreased (2.453187 --> 2.453186).  Saving model ...\n",
      "Epoch: 1377 \tTraining Loss: 2.426388 \tValidation Loss: 2.453173\n",
      "Validation loss decreased (2.453186 --> 2.453173).  Saving model ...\n",
      "Epoch: 1378 \tTraining Loss: 2.436136 \tValidation Loss: 2.453140\n",
      "Validation loss decreased (2.453173 --> 2.453140).  Saving model ...\n",
      "Epoch: 1379 \tTraining Loss: 2.437950 \tValidation Loss: 2.453190\n",
      "Epoch: 1380 \tTraining Loss: 2.439051 \tValidation Loss: 2.453181\n",
      "Epoch: 1381 \tTraining Loss: 2.422977 \tValidation Loss: 2.453181\n",
      "Epoch: 1382 \tTraining Loss: 2.424677 \tValidation Loss: 2.453150\n",
      "Epoch: 1383 \tTraining Loss: 2.435098 \tValidation Loss: 2.453080\n",
      "Validation loss decreased (2.453140 --> 2.453080).  Saving model ...\n",
      "Epoch: 1384 \tTraining Loss: 2.446450 \tValidation Loss: 2.453073\n",
      "Validation loss decreased (2.453080 --> 2.453073).  Saving model ...\n",
      "Epoch: 1385 \tTraining Loss: 2.431314 \tValidation Loss: 2.453043\n",
      "Validation loss decreased (2.453073 --> 2.453043).  Saving model ...\n",
      "Epoch: 1386 \tTraining Loss: 2.438707 \tValidation Loss: 2.453034\n",
      "Validation loss decreased (2.453043 --> 2.453034).  Saving model ...\n",
      "Epoch: 1387 \tTraining Loss: 2.428934 \tValidation Loss: 2.452994\n",
      "Validation loss decreased (2.453034 --> 2.452994).  Saving model ...\n",
      "Epoch: 1388 \tTraining Loss: 2.432411 \tValidation Loss: 2.453003\n",
      "Epoch: 1389 \tTraining Loss: 2.424353 \tValidation Loss: 2.452980\n",
      "Validation loss decreased (2.452994 --> 2.452980).  Saving model ...\n",
      "Epoch: 1390 \tTraining Loss: 2.420836 \tValidation Loss: 2.452935\n",
      "Validation loss decreased (2.452980 --> 2.452935).  Saving model ...\n",
      "Epoch: 1391 \tTraining Loss: 2.433861 \tValidation Loss: 2.452936\n",
      "Epoch: 1392 \tTraining Loss: 2.437247 \tValidation Loss: 2.452961\n",
      "Epoch: 1393 \tTraining Loss: 2.421037 \tValidation Loss: 2.452915\n",
      "Validation loss decreased (2.452935 --> 2.452915).  Saving model ...\n",
      "Epoch: 1394 \tTraining Loss: 2.414782 \tValidation Loss: 2.452905\n",
      "Validation loss decreased (2.452915 --> 2.452905).  Saving model ...\n",
      "Epoch: 1395 \tTraining Loss: 2.421773 \tValidation Loss: 2.452876\n",
      "Validation loss decreased (2.452905 --> 2.452876).  Saving model ...\n",
      "Epoch: 1396 \tTraining Loss: 2.435171 \tValidation Loss: 2.452916\n",
      "Epoch: 1397 \tTraining Loss: 2.417579 \tValidation Loss: 2.452879\n",
      "Epoch: 1398 \tTraining Loss: 2.417349 \tValidation Loss: 2.452825\n",
      "Validation loss decreased (2.452876 --> 2.452825).  Saving model ...\n",
      "Epoch: 1399 \tTraining Loss: 2.438350 \tValidation Loss: 2.452818\n",
      "Validation loss decreased (2.452825 --> 2.452818).  Saving model ...\n",
      "Epoch: 1400 \tTraining Loss: 2.420947 \tValidation Loss: 2.452757\n",
      "Validation loss decreased (2.452818 --> 2.452757).  Saving model ...\n",
      "Epoch: 1401 \tTraining Loss: 2.430516 \tValidation Loss: 2.452655\n",
      "Validation loss decreased (2.452757 --> 2.452655).  Saving model ...\n",
      "Epoch: 1402 \tTraining Loss: 2.432933 \tValidation Loss: 2.452675\n",
      "Epoch: 1403 \tTraining Loss: 2.420173 \tValidation Loss: 2.452689\n",
      "Epoch: 1404 \tTraining Loss: 2.423344 \tValidation Loss: 2.452747\n",
      "Epoch: 1405 \tTraining Loss: 2.432940 \tValidation Loss: 2.452748\n",
      "Epoch: 1406 \tTraining Loss: 2.417789 \tValidation Loss: 2.452724\n",
      "Epoch: 1407 \tTraining Loss: 2.419867 \tValidation Loss: 2.452649\n",
      "Validation loss decreased (2.452655 --> 2.452649).  Saving model ...\n",
      "Epoch: 1408 \tTraining Loss: 2.441577 \tValidation Loss: 2.452705\n",
      "Epoch: 1409 \tTraining Loss: 2.426588 \tValidation Loss: 2.452672\n",
      "Epoch: 1410 \tTraining Loss: 2.424182 \tValidation Loss: 2.452646\n",
      "Validation loss decreased (2.452649 --> 2.452646).  Saving model ...\n",
      "Epoch: 1411 \tTraining Loss: 2.421419 \tValidation Loss: 2.452622\n",
      "Validation loss decreased (2.452646 --> 2.452622).  Saving model ...\n",
      "Epoch: 1412 \tTraining Loss: 2.428348 \tValidation Loss: 2.452599\n",
      "Validation loss decreased (2.452622 --> 2.452599).  Saving model ...\n",
      "Epoch: 1413 \tTraining Loss: 2.430524 \tValidation Loss: 2.452570\n",
      "Validation loss decreased (2.452599 --> 2.452570).  Saving model ...\n",
      "Epoch: 1414 \tTraining Loss: 2.427098 \tValidation Loss: 2.452511\n",
      "Validation loss decreased (2.452570 --> 2.452511).  Saving model ...\n",
      "Epoch: 1415 \tTraining Loss: 2.437033 \tValidation Loss: 2.452477\n",
      "Validation loss decreased (2.452511 --> 2.452477).  Saving model ...\n",
      "Epoch: 1416 \tTraining Loss: 2.433922 \tValidation Loss: 2.452430\n",
      "Validation loss decreased (2.452477 --> 2.452430).  Saving model ...\n",
      "Epoch: 1417 \tTraining Loss: 2.432766 \tValidation Loss: 2.452420\n",
      "Validation loss decreased (2.452430 --> 2.452420).  Saving model ...\n",
      "Epoch: 1418 \tTraining Loss: 2.432135 \tValidation Loss: 2.452424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1419 \tTraining Loss: 2.426270 \tValidation Loss: 2.452402\n",
      "Validation loss decreased (2.452420 --> 2.452402).  Saving model ...\n",
      "Epoch: 1420 \tTraining Loss: 2.416195 \tValidation Loss: 2.452278\n",
      "Validation loss decreased (2.452402 --> 2.452278).  Saving model ...\n",
      "Epoch: 1421 \tTraining Loss: 2.422231 \tValidation Loss: 2.452215\n",
      "Validation loss decreased (2.452278 --> 2.452215).  Saving model ...\n",
      "Epoch: 1422 \tTraining Loss: 2.434151 \tValidation Loss: 2.452257\n",
      "Epoch: 1423 \tTraining Loss: 2.421795 \tValidation Loss: 2.452212\n",
      "Validation loss decreased (2.452215 --> 2.452212).  Saving model ...\n",
      "Epoch: 1424 \tTraining Loss: 2.432178 \tValidation Loss: 2.452184\n",
      "Validation loss decreased (2.452212 --> 2.452184).  Saving model ...\n",
      "Epoch: 1425 \tTraining Loss: 2.424405 \tValidation Loss: 2.452123\n",
      "Validation loss decreased (2.452184 --> 2.452123).  Saving model ...\n",
      "Epoch: 1426 \tTraining Loss: 2.428798 \tValidation Loss: 2.452137\n",
      "Epoch: 1427 \tTraining Loss: 2.428139 \tValidation Loss: 2.452062\n",
      "Validation loss decreased (2.452123 --> 2.452062).  Saving model ...\n",
      "Epoch: 1428 \tTraining Loss: 2.431446 \tValidation Loss: 2.452060\n",
      "Validation loss decreased (2.452062 --> 2.452060).  Saving model ...\n",
      "Epoch: 1429 \tTraining Loss: 2.433411 \tValidation Loss: 2.452072\n",
      "Epoch: 1430 \tTraining Loss: 2.424574 \tValidation Loss: 2.452003\n",
      "Validation loss decreased (2.452060 --> 2.452003).  Saving model ...\n",
      "Epoch: 1431 \tTraining Loss: 2.424941 \tValidation Loss: 2.452014\n",
      "Epoch: 1432 \tTraining Loss: 2.419819 \tValidation Loss: 2.452055\n",
      "Epoch: 1433 \tTraining Loss: 2.428176 \tValidation Loss: 2.451991\n",
      "Validation loss decreased (2.452003 --> 2.451991).  Saving model ...\n",
      "Epoch: 1434 \tTraining Loss: 2.419597 \tValidation Loss: 2.451949\n",
      "Validation loss decreased (2.451991 --> 2.451949).  Saving model ...\n",
      "Epoch: 1435 \tTraining Loss: 2.429265 \tValidation Loss: 2.451935\n",
      "Validation loss decreased (2.451949 --> 2.451935).  Saving model ...\n",
      "Epoch: 1436 \tTraining Loss: 2.418392 \tValidation Loss: 2.451941\n",
      "Epoch: 1437 \tTraining Loss: 2.435128 \tValidation Loss: 2.451921\n",
      "Validation loss decreased (2.451935 --> 2.451921).  Saving model ...\n",
      "Epoch: 1438 \tTraining Loss: 2.435375 \tValidation Loss: 2.451917\n",
      "Validation loss decreased (2.451921 --> 2.451917).  Saving model ...\n",
      "Epoch: 1439 \tTraining Loss: 2.435354 \tValidation Loss: 2.451909\n",
      "Validation loss decreased (2.451917 --> 2.451909).  Saving model ...\n",
      "Epoch: 1440 \tTraining Loss: 2.424899 \tValidation Loss: 2.451947\n",
      "Epoch: 1441 \tTraining Loss: 2.424760 \tValidation Loss: 2.451913\n",
      "Epoch: 1442 \tTraining Loss: 2.421939 \tValidation Loss: 2.451912\n",
      "Epoch: 1443 \tTraining Loss: 2.433287 \tValidation Loss: 2.451979\n",
      "Epoch: 1444 \tTraining Loss: 2.422305 \tValidation Loss: 2.452013\n",
      "Epoch: 1445 \tTraining Loss: 2.432122 \tValidation Loss: 2.451977\n",
      "Epoch: 1446 \tTraining Loss: 2.430209 \tValidation Loss: 2.451941\n",
      "Epoch: 1447 \tTraining Loss: 2.430594 \tValidation Loss: 2.451914\n",
      "Epoch: 1448 \tTraining Loss: 2.424276 \tValidation Loss: 2.451868\n",
      "Validation loss decreased (2.451909 --> 2.451868).  Saving model ...\n",
      "Epoch: 1449 \tTraining Loss: 2.419053 \tValidation Loss: 2.451842\n",
      "Validation loss decreased (2.451868 --> 2.451842).  Saving model ...\n",
      "Epoch: 1450 \tTraining Loss: 2.426574 \tValidation Loss: 2.451827\n",
      "Validation loss decreased (2.451842 --> 2.451827).  Saving model ...\n",
      "Epoch: 1451 \tTraining Loss: 2.424554 \tValidation Loss: 2.451753\n",
      "Validation loss decreased (2.451827 --> 2.451753).  Saving model ...\n",
      "Epoch: 1452 \tTraining Loss: 2.419913 \tValidation Loss: 2.451718\n",
      "Validation loss decreased (2.451753 --> 2.451718).  Saving model ...\n",
      "Epoch: 1453 \tTraining Loss: 2.424314 \tValidation Loss: 2.451648\n",
      "Validation loss decreased (2.451718 --> 2.451648).  Saving model ...\n",
      "Epoch: 1454 \tTraining Loss: 2.424829 \tValidation Loss: 2.451636\n",
      "Validation loss decreased (2.451648 --> 2.451636).  Saving model ...\n",
      "Epoch: 1455 \tTraining Loss: 2.422454 \tValidation Loss: 2.451571\n",
      "Validation loss decreased (2.451636 --> 2.451571).  Saving model ...\n",
      "Epoch: 1456 \tTraining Loss: 2.423959 \tValidation Loss: 2.451544\n",
      "Validation loss decreased (2.451571 --> 2.451544).  Saving model ...\n",
      "Epoch: 1457 \tTraining Loss: 2.434708 \tValidation Loss: 2.451480\n",
      "Validation loss decreased (2.451544 --> 2.451480).  Saving model ...\n",
      "Epoch: 1458 \tTraining Loss: 2.422668 \tValidation Loss: 2.451434\n",
      "Validation loss decreased (2.451480 --> 2.451434).  Saving model ...\n",
      "Epoch: 1459 \tTraining Loss: 2.421837 \tValidation Loss: 2.451423\n",
      "Validation loss decreased (2.451434 --> 2.451423).  Saving model ...\n",
      "Epoch: 1460 \tTraining Loss: 2.418916 \tValidation Loss: 2.451395\n",
      "Validation loss decreased (2.451423 --> 2.451395).  Saving model ...\n",
      "Epoch: 1461 \tTraining Loss: 2.426052 \tValidation Loss: 2.451383\n",
      "Validation loss decreased (2.451395 --> 2.451383).  Saving model ...\n",
      "Epoch: 1462 \tTraining Loss: 2.418926 \tValidation Loss: 2.451333\n",
      "Validation loss decreased (2.451383 --> 2.451333).  Saving model ...\n",
      "Epoch: 1463 \tTraining Loss: 2.432753 \tValidation Loss: 2.451316\n",
      "Validation loss decreased (2.451333 --> 2.451316).  Saving model ...\n",
      "Epoch: 1464 \tTraining Loss: 2.425333 \tValidation Loss: 2.451308\n",
      "Validation loss decreased (2.451316 --> 2.451308).  Saving model ...\n",
      "Epoch: 1465 \tTraining Loss: 2.420264 \tValidation Loss: 2.451246\n",
      "Validation loss decreased (2.451308 --> 2.451246).  Saving model ...\n",
      "Epoch: 1466 \tTraining Loss: 2.428801 \tValidation Loss: 2.451187\n",
      "Validation loss decreased (2.451246 --> 2.451187).  Saving model ...\n",
      "Epoch: 1467 \tTraining Loss: 2.421030 \tValidation Loss: 2.451152\n",
      "Validation loss decreased (2.451187 --> 2.451152).  Saving model ...\n",
      "Epoch: 1468 \tTraining Loss: 2.430680 \tValidation Loss: 2.451083\n",
      "Validation loss decreased (2.451152 --> 2.451083).  Saving model ...\n",
      "Epoch: 1469 \tTraining Loss: 2.427030 \tValidation Loss: 2.451013\n",
      "Validation loss decreased (2.451083 --> 2.451013).  Saving model ...\n",
      "Epoch: 1470 \tTraining Loss: 2.430641 \tValidation Loss: 2.450982\n",
      "Validation loss decreased (2.451013 --> 2.450982).  Saving model ...\n",
      "Epoch: 1471 \tTraining Loss: 2.430693 \tValidation Loss: 2.450982\n",
      "Validation loss decreased (2.450982 --> 2.450982).  Saving model ...\n",
      "Epoch: 1472 \tTraining Loss: 2.416365 \tValidation Loss: 2.450935\n",
      "Validation loss decreased (2.450982 --> 2.450935).  Saving model ...\n",
      "Epoch: 1473 \tTraining Loss: 2.425212 \tValidation Loss: 2.450857\n",
      "Validation loss decreased (2.450935 --> 2.450857).  Saving model ...\n",
      "Epoch: 1474 \tTraining Loss: 2.427801 \tValidation Loss: 2.450896\n",
      "Epoch: 1475 \tTraining Loss: 2.419335 \tValidation Loss: 2.450841\n",
      "Validation loss decreased (2.450857 --> 2.450841).  Saving model ...\n",
      "Epoch: 1476 \tTraining Loss: 2.427238 \tValidation Loss: 2.450882\n",
      "Epoch: 1477 \tTraining Loss: 2.421731 \tValidation Loss: 2.450815\n",
      "Validation loss decreased (2.450841 --> 2.450815).  Saving model ...\n",
      "Epoch: 1478 \tTraining Loss: 2.421147 \tValidation Loss: 2.450707\n",
      "Validation loss decreased (2.450815 --> 2.450707).  Saving model ...\n",
      "Epoch: 1479 \tTraining Loss: 2.428806 \tValidation Loss: 2.450696\n",
      "Validation loss decreased (2.450707 --> 2.450696).  Saving model ...\n",
      "Epoch: 1480 \tTraining Loss: 2.416244 \tValidation Loss: 2.450639\n",
      "Validation loss decreased (2.450696 --> 2.450639).  Saving model ...\n",
      "Epoch: 1481 \tTraining Loss: 2.427618 \tValidation Loss: 2.450609\n",
      "Validation loss decreased (2.450639 --> 2.450609).  Saving model ...\n",
      "Epoch: 1482 \tTraining Loss: 2.420084 \tValidation Loss: 2.450578\n",
      "Validation loss decreased (2.450609 --> 2.450578).  Saving model ...\n",
      "Epoch: 1483 \tTraining Loss: 2.418956 \tValidation Loss: 2.450569\n",
      "Validation loss decreased (2.450578 --> 2.450569).  Saving model ...\n",
      "Epoch: 1484 \tTraining Loss: 2.422239 \tValidation Loss: 2.450524\n",
      "Validation loss decreased (2.450569 --> 2.450524).  Saving model ...\n",
      "Epoch: 1485 \tTraining Loss: 2.433109 \tValidation Loss: 2.450478\n",
      "Validation loss decreased (2.450524 --> 2.450478).  Saving model ...\n",
      "Epoch: 1486 \tTraining Loss: 2.415868 \tValidation Loss: 2.450464\n",
      "Validation loss decreased (2.450478 --> 2.450464).  Saving model ...\n",
      "Epoch: 1487 \tTraining Loss: 2.429629 \tValidation Loss: 2.450457\n",
      "Validation loss decreased (2.450464 --> 2.450457).  Saving model ...\n",
      "Epoch: 1488 \tTraining Loss: 2.418603 \tValidation Loss: 2.450429\n",
      "Validation loss decreased (2.450457 --> 2.450429).  Saving model ...\n",
      "Epoch: 1489 \tTraining Loss: 2.425195 \tValidation Loss: 2.450457\n",
      "Epoch: 1490 \tTraining Loss: 2.430060 \tValidation Loss: 2.450440\n",
      "Epoch: 1491 \tTraining Loss: 2.415186 \tValidation Loss: 2.450397\n",
      "Validation loss decreased (2.450429 --> 2.450397).  Saving model ...\n",
      "Epoch: 1492 \tTraining Loss: 2.430204 \tValidation Loss: 2.450373\n",
      "Validation loss decreased (2.450397 --> 2.450373).  Saving model ...\n",
      "Epoch: 1493 \tTraining Loss: 2.429556 \tValidation Loss: 2.450358\n",
      "Validation loss decreased (2.450373 --> 2.450358).  Saving model ...\n",
      "Epoch: 1494 \tTraining Loss: 2.425907 \tValidation Loss: 2.450333\n",
      "Validation loss decreased (2.450358 --> 2.450333).  Saving model ...\n",
      "Epoch: 1495 \tTraining Loss: 2.429673 \tValidation Loss: 2.450382\n",
      "Epoch: 1496 \tTraining Loss: 2.430314 \tValidation Loss: 2.450397\n",
      "Epoch: 1497 \tTraining Loss: 2.419457 \tValidation Loss: 2.450360\n",
      "Epoch: 1498 \tTraining Loss: 2.421850 \tValidation Loss: 2.450280\n",
      "Validation loss decreased (2.450333 --> 2.450280).  Saving model ...\n",
      "Epoch: 1499 \tTraining Loss: 2.433858 \tValidation Loss: 2.450271\n",
      "Validation loss decreased (2.450280 --> 2.450271).  Saving model ...\n",
      "Epoch: 1500 \tTraining Loss: 2.419873 \tValidation Loss: 2.450298\n",
      "Epoch: 1501 \tTraining Loss: 2.437523 \tValidation Loss: 2.450288\n",
      "Epoch: 1502 \tTraining Loss: 2.425585 \tValidation Loss: 2.450284\n",
      "Epoch: 1503 \tTraining Loss: 2.421543 \tValidation Loss: 2.450248\n",
      "Validation loss decreased (2.450271 --> 2.450248).  Saving model ...\n",
      "Epoch: 1504 \tTraining Loss: 2.424142 \tValidation Loss: 2.450237\n",
      "Validation loss decreased (2.450248 --> 2.450237).  Saving model ...\n",
      "Epoch: 1505 \tTraining Loss: 2.425360 \tValidation Loss: 2.450257\n",
      "Epoch: 1506 \tTraining Loss: 2.423758 \tValidation Loss: 2.450251\n",
      "Epoch: 1507 \tTraining Loss: 2.431910 \tValidation Loss: 2.450202\n",
      "Validation loss decreased (2.450237 --> 2.450202).  Saving model ...\n",
      "Epoch: 1508 \tTraining Loss: 2.426516 \tValidation Loss: 2.450178\n",
      "Validation loss decreased (2.450202 --> 2.450178).  Saving model ...\n",
      "Epoch: 1509 \tTraining Loss: 2.423830 \tValidation Loss: 2.450202\n",
      "Epoch: 1510 \tTraining Loss: 2.428444 \tValidation Loss: 2.450224\n",
      "Epoch: 1511 \tTraining Loss: 2.428386 \tValidation Loss: 2.450211\n",
      "Epoch: 1512 \tTraining Loss: 2.423339 \tValidation Loss: 2.450275\n",
      "Epoch: 1513 \tTraining Loss: 2.421876 \tValidation Loss: 2.450266\n",
      "Epoch: 1514 \tTraining Loss: 2.424288 \tValidation Loss: 2.450219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1515 \tTraining Loss: 2.421371 \tValidation Loss: 2.450158\n",
      "Validation loss decreased (2.450178 --> 2.450158).  Saving model ...\n",
      "Epoch: 1516 \tTraining Loss: 2.423330 \tValidation Loss: 2.450082\n",
      "Validation loss decreased (2.450158 --> 2.450082).  Saving model ...\n",
      "Epoch: 1517 \tTraining Loss: 2.422188 \tValidation Loss: 2.450033\n",
      "Validation loss decreased (2.450082 --> 2.450033).  Saving model ...\n",
      "Epoch: 1518 \tTraining Loss: 2.437730 \tValidation Loss: 2.449991\n",
      "Validation loss decreased (2.450033 --> 2.449991).  Saving model ...\n",
      "Epoch: 1519 \tTraining Loss: 2.426310 \tValidation Loss: 2.450014\n",
      "Epoch: 1520 \tTraining Loss: 2.422789 \tValidation Loss: 2.449957\n",
      "Validation loss decreased (2.449991 --> 2.449957).  Saving model ...\n",
      "Epoch: 1521 \tTraining Loss: 2.423743 \tValidation Loss: 2.449980\n",
      "Epoch: 1522 \tTraining Loss: 2.436382 \tValidation Loss: 2.449969\n",
      "Epoch: 1523 \tTraining Loss: 2.412549 \tValidation Loss: 2.449900\n",
      "Validation loss decreased (2.449957 --> 2.449900).  Saving model ...\n",
      "Epoch: 1524 \tTraining Loss: 2.433897 \tValidation Loss: 2.449889\n",
      "Validation loss decreased (2.449900 --> 2.449889).  Saving model ...\n",
      "Epoch: 1525 \tTraining Loss: 2.435228 \tValidation Loss: 2.449856\n",
      "Validation loss decreased (2.449889 --> 2.449856).  Saving model ...\n",
      "Epoch: 1526 \tTraining Loss: 2.431444 \tValidation Loss: 2.449829\n",
      "Validation loss decreased (2.449856 --> 2.449829).  Saving model ...\n",
      "Epoch: 1527 \tTraining Loss: 2.423985 \tValidation Loss: 2.449814\n",
      "Validation loss decreased (2.449829 --> 2.449814).  Saving model ...\n",
      "Epoch: 1528 \tTraining Loss: 2.423878 \tValidation Loss: 2.449775\n",
      "Validation loss decreased (2.449814 --> 2.449775).  Saving model ...\n",
      "Epoch: 1529 \tTraining Loss: 2.421942 \tValidation Loss: 2.449774\n",
      "Validation loss decreased (2.449775 --> 2.449774).  Saving model ...\n",
      "Epoch: 1530 \tTraining Loss: 2.427955 \tValidation Loss: 2.449810\n",
      "Epoch: 1531 \tTraining Loss: 2.438433 \tValidation Loss: 2.449809\n",
      "Epoch: 1532 \tTraining Loss: 2.431071 \tValidation Loss: 2.449782\n",
      "Epoch: 1533 \tTraining Loss: 2.435372 \tValidation Loss: 2.449746\n",
      "Validation loss decreased (2.449774 --> 2.449746).  Saving model ...\n",
      "Epoch: 1534 \tTraining Loss: 2.426576 \tValidation Loss: 2.449745\n",
      "Validation loss decreased (2.449746 --> 2.449745).  Saving model ...\n",
      "Epoch: 1535 \tTraining Loss: 2.420986 \tValidation Loss: 2.449692\n",
      "Validation loss decreased (2.449745 --> 2.449692).  Saving model ...\n",
      "Epoch: 1536 \tTraining Loss: 2.424080 \tValidation Loss: 2.449670\n",
      "Validation loss decreased (2.449692 --> 2.449670).  Saving model ...\n",
      "Epoch: 1537 \tTraining Loss: 2.428863 \tValidation Loss: 2.449652\n",
      "Validation loss decreased (2.449670 --> 2.449652).  Saving model ...\n",
      "Epoch: 1538 \tTraining Loss: 2.416983 \tValidation Loss: 2.449666\n",
      "Epoch: 1539 \tTraining Loss: 2.411652 \tValidation Loss: 2.449630\n",
      "Validation loss decreased (2.449652 --> 2.449630).  Saving model ...\n",
      "Epoch: 1540 \tTraining Loss: 2.420064 \tValidation Loss: 2.449573\n",
      "Validation loss decreased (2.449630 --> 2.449573).  Saving model ...\n",
      "Epoch: 1541 \tTraining Loss: 2.420320 \tValidation Loss: 2.449543\n",
      "Validation loss decreased (2.449573 --> 2.449543).  Saving model ...\n",
      "Epoch: 1542 \tTraining Loss: 2.425915 \tValidation Loss: 2.449511\n",
      "Validation loss decreased (2.449543 --> 2.449511).  Saving model ...\n",
      "Epoch: 1543 \tTraining Loss: 2.417719 \tValidation Loss: 2.449429\n",
      "Validation loss decreased (2.449511 --> 2.449429).  Saving model ...\n",
      "Epoch: 1544 \tTraining Loss: 2.422730 \tValidation Loss: 2.449355\n",
      "Validation loss decreased (2.449429 --> 2.449355).  Saving model ...\n",
      "Epoch: 1545 \tTraining Loss: 2.423258 \tValidation Loss: 2.449296\n",
      "Validation loss decreased (2.449355 --> 2.449296).  Saving model ...\n",
      "Epoch: 1546 \tTraining Loss: 2.429194 \tValidation Loss: 2.449283\n",
      "Validation loss decreased (2.449296 --> 2.449283).  Saving model ...\n",
      "Epoch: 1547 \tTraining Loss: 2.443861 \tValidation Loss: 2.449328\n",
      "Epoch: 1548 \tTraining Loss: 2.414948 \tValidation Loss: 2.449301\n",
      "Epoch: 1549 \tTraining Loss: 2.425204 \tValidation Loss: 2.449261\n",
      "Validation loss decreased (2.449283 --> 2.449261).  Saving model ...\n",
      "Epoch: 1550 \tTraining Loss: 2.419109 \tValidation Loss: 2.449159\n",
      "Validation loss decreased (2.449261 --> 2.449159).  Saving model ...\n",
      "Epoch: 1551 \tTraining Loss: 2.430464 \tValidation Loss: 2.449169\n",
      "Epoch: 1552 \tTraining Loss: 2.410622 \tValidation Loss: 2.449097\n",
      "Validation loss decreased (2.449159 --> 2.449097).  Saving model ...\n",
      "Epoch: 1553 \tTraining Loss: 2.429283 \tValidation Loss: 2.449059\n",
      "Validation loss decreased (2.449097 --> 2.449059).  Saving model ...\n",
      "Epoch: 1554 \tTraining Loss: 2.429599 \tValidation Loss: 2.449123\n",
      "Epoch: 1555 \tTraining Loss: 2.422781 \tValidation Loss: 2.449055\n",
      "Validation loss decreased (2.449059 --> 2.449055).  Saving model ...\n",
      "Epoch: 1556 \tTraining Loss: 2.425298 \tValidation Loss: 2.449022\n",
      "Validation loss decreased (2.449055 --> 2.449022).  Saving model ...\n",
      "Epoch: 1557 \tTraining Loss: 2.427493 \tValidation Loss: 2.448979\n",
      "Validation loss decreased (2.449022 --> 2.448979).  Saving model ...\n",
      "Epoch: 1558 \tTraining Loss: 2.425471 \tValidation Loss: 2.448823\n",
      "Validation loss decreased (2.448979 --> 2.448823).  Saving model ...\n",
      "Epoch: 1559 \tTraining Loss: 2.423311 \tValidation Loss: 2.448745\n",
      "Validation loss decreased (2.448823 --> 2.448745).  Saving model ...\n",
      "Epoch: 1560 \tTraining Loss: 2.429947 \tValidation Loss: 2.448702\n",
      "Validation loss decreased (2.448745 --> 2.448702).  Saving model ...\n",
      "Epoch: 1561 \tTraining Loss: 2.420806 \tValidation Loss: 2.448644\n",
      "Validation loss decreased (2.448702 --> 2.448644).  Saving model ...\n",
      "Epoch: 1562 \tTraining Loss: 2.423161 \tValidation Loss: 2.448614\n",
      "Validation loss decreased (2.448644 --> 2.448614).  Saving model ...\n",
      "Epoch: 1563 \tTraining Loss: 2.417482 \tValidation Loss: 2.448616\n",
      "Epoch: 1564 \tTraining Loss: 2.426711 \tValidation Loss: 2.448591\n",
      "Validation loss decreased (2.448614 --> 2.448591).  Saving model ...\n",
      "Epoch: 1565 \tTraining Loss: 2.428478 \tValidation Loss: 2.448548\n",
      "Validation loss decreased (2.448591 --> 2.448548).  Saving model ...\n",
      "Epoch: 1566 \tTraining Loss: 2.418857 \tValidation Loss: 2.448528\n",
      "Validation loss decreased (2.448548 --> 2.448528).  Saving model ...\n",
      "Epoch: 1567 \tTraining Loss: 2.422851 \tValidation Loss: 2.448518\n",
      "Validation loss decreased (2.448528 --> 2.448518).  Saving model ...\n",
      "Epoch: 1568 \tTraining Loss: 2.430890 \tValidation Loss: 2.448442\n",
      "Validation loss decreased (2.448518 --> 2.448442).  Saving model ...\n",
      "Epoch: 1569 \tTraining Loss: 2.413338 \tValidation Loss: 2.448333\n",
      "Validation loss decreased (2.448442 --> 2.448333).  Saving model ...\n",
      "Epoch: 1570 \tTraining Loss: 2.416682 \tValidation Loss: 2.448308\n",
      "Validation loss decreased (2.448333 --> 2.448308).  Saving model ...\n",
      "Epoch: 1571 \tTraining Loss: 2.418129 \tValidation Loss: 2.448251\n",
      "Validation loss decreased (2.448308 --> 2.448251).  Saving model ...\n",
      "Epoch: 1572 \tTraining Loss: 2.420779 \tValidation Loss: 2.448214\n",
      "Validation loss decreased (2.448251 --> 2.448214).  Saving model ...\n",
      "Epoch: 1573 \tTraining Loss: 2.421617 \tValidation Loss: 2.448142\n",
      "Validation loss decreased (2.448214 --> 2.448142).  Saving model ...\n",
      "Epoch: 1574 \tTraining Loss: 2.423214 \tValidation Loss: 2.448087\n",
      "Validation loss decreased (2.448142 --> 2.448087).  Saving model ...\n",
      "Epoch: 1575 \tTraining Loss: 2.422656 \tValidation Loss: 2.448086\n",
      "Validation loss decreased (2.448087 --> 2.448086).  Saving model ...\n",
      "Epoch: 1576 \tTraining Loss: 2.414715 \tValidation Loss: 2.448015\n",
      "Validation loss decreased (2.448086 --> 2.448015).  Saving model ...\n",
      "Epoch: 1577 \tTraining Loss: 2.433154 \tValidation Loss: 2.448014\n",
      "Validation loss decreased (2.448015 --> 2.448014).  Saving model ...\n",
      "Epoch: 1578 \tTraining Loss: 2.420933 \tValidation Loss: 2.448002\n",
      "Validation loss decreased (2.448014 --> 2.448002).  Saving model ...\n",
      "Epoch: 1579 \tTraining Loss: 2.430496 \tValidation Loss: 2.448062\n",
      "Epoch: 1580 \tTraining Loss: 2.422562 \tValidation Loss: 2.448080\n",
      "Epoch: 1581 \tTraining Loss: 2.410207 \tValidation Loss: 2.447981\n",
      "Validation loss decreased (2.448002 --> 2.447981).  Saving model ...\n",
      "Epoch: 1582 \tTraining Loss: 2.420609 \tValidation Loss: 2.448003\n",
      "Epoch: 1583 \tTraining Loss: 2.414551 \tValidation Loss: 2.447959\n",
      "Validation loss decreased (2.447981 --> 2.447959).  Saving model ...\n",
      "Epoch: 1584 \tTraining Loss: 2.428170 \tValidation Loss: 2.447929\n",
      "Validation loss decreased (2.447959 --> 2.447929).  Saving model ...\n",
      "Epoch: 1585 \tTraining Loss: 2.424779 \tValidation Loss: 2.447950\n",
      "Epoch: 1586 \tTraining Loss: 2.426377 \tValidation Loss: 2.447951\n",
      "Epoch: 1587 \tTraining Loss: 2.420825 \tValidation Loss: 2.447933\n",
      "Epoch: 1588 \tTraining Loss: 2.414690 \tValidation Loss: 2.447915\n",
      "Validation loss decreased (2.447929 --> 2.447915).  Saving model ...\n",
      "Epoch: 1589 \tTraining Loss: 2.417662 \tValidation Loss: 2.447856\n",
      "Validation loss decreased (2.447915 --> 2.447856).  Saving model ...\n",
      "Epoch: 1590 \tTraining Loss: 2.419298 \tValidation Loss: 2.447798\n",
      "Validation loss decreased (2.447856 --> 2.447798).  Saving model ...\n",
      "Epoch: 1591 \tTraining Loss: 2.417543 \tValidation Loss: 2.447753\n",
      "Validation loss decreased (2.447798 --> 2.447753).  Saving model ...\n",
      "Epoch: 1592 \tTraining Loss: 2.427742 \tValidation Loss: 2.447768\n",
      "Epoch: 1593 \tTraining Loss: 2.416664 \tValidation Loss: 2.447737\n",
      "Validation loss decreased (2.447753 --> 2.447737).  Saving model ...\n",
      "Epoch: 1594 \tTraining Loss: 2.431423 \tValidation Loss: 2.447780\n",
      "Epoch: 1595 \tTraining Loss: 2.423466 \tValidation Loss: 2.447834\n",
      "Epoch: 1596 \tTraining Loss: 2.412817 \tValidation Loss: 2.447767\n",
      "Epoch: 1597 \tTraining Loss: 2.431973 \tValidation Loss: 2.447712\n",
      "Validation loss decreased (2.447737 --> 2.447712).  Saving model ...\n",
      "Epoch: 1598 \tTraining Loss: 2.412359 \tValidation Loss: 2.447587\n",
      "Validation loss decreased (2.447712 --> 2.447587).  Saving model ...\n",
      "Epoch: 1599 \tTraining Loss: 2.424618 \tValidation Loss: 2.447661\n",
      "Epoch: 1600 \tTraining Loss: 2.427731 \tValidation Loss: 2.447615\n",
      "Epoch: 1601 \tTraining Loss: 2.410923 \tValidation Loss: 2.447590\n",
      "Epoch: 1602 \tTraining Loss: 2.424841 \tValidation Loss: 2.447492\n",
      "Validation loss decreased (2.447587 --> 2.447492).  Saving model ...\n",
      "Epoch: 1603 \tTraining Loss: 2.421983 \tValidation Loss: 2.447486\n",
      "Validation loss decreased (2.447492 --> 2.447486).  Saving model ...\n",
      "Epoch: 1604 \tTraining Loss: 2.414734 \tValidation Loss: 2.447476\n",
      "Validation loss decreased (2.447486 --> 2.447476).  Saving model ...\n",
      "Epoch: 1605 \tTraining Loss: 2.416996 \tValidation Loss: 2.447491\n",
      "Epoch: 1606 \tTraining Loss: 2.423805 \tValidation Loss: 2.447513\n",
      "Epoch: 1607 \tTraining Loss: 2.421111 \tValidation Loss: 2.447503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1608 \tTraining Loss: 2.424757 \tValidation Loss: 2.447438\n",
      "Validation loss decreased (2.447476 --> 2.447438).  Saving model ...\n",
      "Epoch: 1609 \tTraining Loss: 2.421999 \tValidation Loss: 2.447361\n",
      "Validation loss decreased (2.447438 --> 2.447361).  Saving model ...\n",
      "Epoch: 1610 \tTraining Loss: 2.415578 \tValidation Loss: 2.447285\n",
      "Validation loss decreased (2.447361 --> 2.447285).  Saving model ...\n",
      "Epoch: 1611 \tTraining Loss: 2.429133 \tValidation Loss: 2.447266\n",
      "Validation loss decreased (2.447285 --> 2.447266).  Saving model ...\n",
      "Epoch: 1612 \tTraining Loss: 2.420544 \tValidation Loss: 2.447203\n",
      "Validation loss decreased (2.447266 --> 2.447203).  Saving model ...\n",
      "Epoch: 1613 \tTraining Loss: 2.416596 \tValidation Loss: 2.447211\n",
      "Epoch: 1614 \tTraining Loss: 2.426599 \tValidation Loss: 2.447193\n",
      "Validation loss decreased (2.447203 --> 2.447193).  Saving model ...\n",
      "Epoch: 1615 \tTraining Loss: 2.427362 \tValidation Loss: 2.447189\n",
      "Validation loss decreased (2.447193 --> 2.447189).  Saving model ...\n",
      "Epoch: 1616 \tTraining Loss: 2.432848 \tValidation Loss: 2.447187\n",
      "Validation loss decreased (2.447189 --> 2.447187).  Saving model ...\n",
      "Epoch: 1617 \tTraining Loss: 2.418133 \tValidation Loss: 2.447214\n",
      "Epoch: 1618 \tTraining Loss: 2.413991 \tValidation Loss: 2.447162\n",
      "Validation loss decreased (2.447187 --> 2.447162).  Saving model ...\n",
      "Epoch: 1619 \tTraining Loss: 2.420750 \tValidation Loss: 2.447180\n",
      "Epoch: 1620 \tTraining Loss: 2.418420 \tValidation Loss: 2.447177\n",
      "Epoch: 1621 \tTraining Loss: 2.414730 \tValidation Loss: 2.447184\n",
      "Epoch: 1622 \tTraining Loss: 2.415234 \tValidation Loss: 2.447145\n",
      "Validation loss decreased (2.447162 --> 2.447145).  Saving model ...\n",
      "Epoch: 1623 \tTraining Loss: 2.422658 \tValidation Loss: 2.447150\n",
      "Epoch: 1624 \tTraining Loss: 2.413204 \tValidation Loss: 2.447114\n",
      "Validation loss decreased (2.447145 --> 2.447114).  Saving model ...\n",
      "Epoch: 1625 \tTraining Loss: 2.427169 \tValidation Loss: 2.447086\n",
      "Validation loss decreased (2.447114 --> 2.447086).  Saving model ...\n",
      "Epoch: 1626 \tTraining Loss: 2.425608 \tValidation Loss: 2.447095\n",
      "Epoch: 1627 \tTraining Loss: 2.414195 \tValidation Loss: 2.446929\n",
      "Validation loss decreased (2.447086 --> 2.446929).  Saving model ...\n",
      "Epoch: 1628 \tTraining Loss: 2.420810 \tValidation Loss: 2.446940\n",
      "Epoch: 1629 \tTraining Loss: 2.417830 \tValidation Loss: 2.446937\n",
      "Epoch: 1630 \tTraining Loss: 2.425227 \tValidation Loss: 2.446886\n",
      "Validation loss decreased (2.446929 --> 2.446886).  Saving model ...\n",
      "Epoch: 1631 \tTraining Loss: 2.425919 \tValidation Loss: 2.446910\n",
      "Epoch: 1632 \tTraining Loss: 2.415879 \tValidation Loss: 2.446860\n",
      "Validation loss decreased (2.446886 --> 2.446860).  Saving model ...\n",
      "Epoch: 1633 \tTraining Loss: 2.424706 \tValidation Loss: 2.446882\n",
      "Epoch: 1634 \tTraining Loss: 2.424792 \tValidation Loss: 2.446801\n",
      "Validation loss decreased (2.446860 --> 2.446801).  Saving model ...\n",
      "Epoch: 1635 \tTraining Loss: 2.414988 \tValidation Loss: 2.446717\n",
      "Validation loss decreased (2.446801 --> 2.446717).  Saving model ...\n",
      "Epoch: 1636 \tTraining Loss: 2.421407 \tValidation Loss: 2.446681\n",
      "Validation loss decreased (2.446717 --> 2.446681).  Saving model ...\n",
      "Epoch: 1637 \tTraining Loss: 2.425696 \tValidation Loss: 2.446624\n",
      "Validation loss decreased (2.446681 --> 2.446624).  Saving model ...\n",
      "Epoch: 1638 \tTraining Loss: 2.418956 \tValidation Loss: 2.446555\n",
      "Validation loss decreased (2.446624 --> 2.446555).  Saving model ...\n",
      "Epoch: 1639 \tTraining Loss: 2.415951 \tValidation Loss: 2.446528\n",
      "Validation loss decreased (2.446555 --> 2.446528).  Saving model ...\n",
      "Epoch: 1640 \tTraining Loss: 2.429727 \tValidation Loss: 2.446522\n",
      "Validation loss decreased (2.446528 --> 2.446522).  Saving model ...\n",
      "Epoch: 1641 \tTraining Loss: 2.417516 \tValidation Loss: 2.446512\n",
      "Validation loss decreased (2.446522 --> 2.446512).  Saving model ...\n",
      "Epoch: 1642 \tTraining Loss: 2.420807 \tValidation Loss: 2.446527\n",
      "Epoch: 1643 \tTraining Loss: 2.415828 \tValidation Loss: 2.446424\n",
      "Validation loss decreased (2.446512 --> 2.446424).  Saving model ...\n",
      "Epoch: 1644 \tTraining Loss: 2.422825 \tValidation Loss: 2.446371\n",
      "Validation loss decreased (2.446424 --> 2.446371).  Saving model ...\n",
      "Epoch: 1645 \tTraining Loss: 2.412333 \tValidation Loss: 2.446303\n",
      "Validation loss decreased (2.446371 --> 2.446303).  Saving model ...\n",
      "Epoch: 1646 \tTraining Loss: 2.409333 \tValidation Loss: 2.446243\n",
      "Validation loss decreased (2.446303 --> 2.446243).  Saving model ...\n",
      "Epoch: 1647 \tTraining Loss: 2.409192 \tValidation Loss: 2.446162\n",
      "Validation loss decreased (2.446243 --> 2.446162).  Saving model ...\n",
      "Epoch: 1648 \tTraining Loss: 2.425428 \tValidation Loss: 2.446131\n",
      "Validation loss decreased (2.446162 --> 2.446131).  Saving model ...\n",
      "Epoch: 1649 \tTraining Loss: 2.421503 \tValidation Loss: 2.446051\n",
      "Validation loss decreased (2.446131 --> 2.446051).  Saving model ...\n",
      "Epoch: 1650 \tTraining Loss: 2.428391 \tValidation Loss: 2.446018\n",
      "Validation loss decreased (2.446051 --> 2.446018).  Saving model ...\n",
      "Epoch: 1651 \tTraining Loss: 2.411862 \tValidation Loss: 2.445973\n",
      "Validation loss decreased (2.446018 --> 2.445973).  Saving model ...\n",
      "Epoch: 1652 \tTraining Loss: 2.417405 \tValidation Loss: 2.445892\n",
      "Validation loss decreased (2.445973 --> 2.445892).  Saving model ...\n",
      "Epoch: 1653 \tTraining Loss: 2.420780 \tValidation Loss: 2.445822\n",
      "Validation loss decreased (2.445892 --> 2.445822).  Saving model ...\n",
      "Epoch: 1654 \tTraining Loss: 2.425133 \tValidation Loss: 2.445772\n",
      "Validation loss decreased (2.445822 --> 2.445772).  Saving model ...\n",
      "Epoch: 1655 \tTraining Loss: 2.415640 \tValidation Loss: 2.445775\n",
      "Epoch: 1656 \tTraining Loss: 2.409721 \tValidation Loss: 2.445746\n",
      "Validation loss decreased (2.445772 --> 2.445746).  Saving model ...\n",
      "Epoch: 1657 \tTraining Loss: 2.416061 \tValidation Loss: 2.445763\n",
      "Epoch: 1658 \tTraining Loss: 2.418426 \tValidation Loss: 2.445759\n",
      "Epoch: 1659 \tTraining Loss: 2.412582 \tValidation Loss: 2.445746\n",
      "Epoch: 1660 \tTraining Loss: 2.425489 \tValidation Loss: 2.445660\n",
      "Validation loss decreased (2.445746 --> 2.445660).  Saving model ...\n",
      "Epoch: 1661 \tTraining Loss: 2.423376 \tValidation Loss: 2.445667\n",
      "Epoch: 1662 \tTraining Loss: 2.411561 \tValidation Loss: 2.445615\n",
      "Validation loss decreased (2.445660 --> 2.445615).  Saving model ...\n",
      "Epoch: 1663 \tTraining Loss: 2.423529 \tValidation Loss: 2.445601\n",
      "Validation loss decreased (2.445615 --> 2.445601).  Saving model ...\n",
      "Epoch: 1664 \tTraining Loss: 2.429498 \tValidation Loss: 2.445493\n",
      "Validation loss decreased (2.445601 --> 2.445493).  Saving model ...\n",
      "Epoch: 1665 \tTraining Loss: 2.412033 \tValidation Loss: 2.445447\n",
      "Validation loss decreased (2.445493 --> 2.445447).  Saving model ...\n",
      "Epoch: 1666 \tTraining Loss: 2.420375 \tValidation Loss: 2.445450\n",
      "Epoch: 1667 \tTraining Loss: 2.423458 \tValidation Loss: 2.445434\n",
      "Validation loss decreased (2.445447 --> 2.445434).  Saving model ...\n",
      "Epoch: 1668 \tTraining Loss: 2.427111 \tValidation Loss: 2.445433\n",
      "Validation loss decreased (2.445434 --> 2.445433).  Saving model ...\n",
      "Epoch: 1669 \tTraining Loss: 2.408865 \tValidation Loss: 2.445364\n",
      "Validation loss decreased (2.445433 --> 2.445364).  Saving model ...\n",
      "Epoch: 1670 \tTraining Loss: 2.420655 \tValidation Loss: 2.445357\n",
      "Validation loss decreased (2.445364 --> 2.445357).  Saving model ...\n",
      "Epoch: 1671 \tTraining Loss: 2.424340 \tValidation Loss: 2.445338\n",
      "Validation loss decreased (2.445357 --> 2.445338).  Saving model ...\n",
      "Epoch: 1672 \tTraining Loss: 2.426498 \tValidation Loss: 2.445287\n",
      "Validation loss decreased (2.445338 --> 2.445287).  Saving model ...\n",
      "Epoch: 1673 \tTraining Loss: 2.429976 \tValidation Loss: 2.445222\n",
      "Validation loss decreased (2.445287 --> 2.445222).  Saving model ...\n",
      "Epoch: 1674 \tTraining Loss: 2.416909 \tValidation Loss: 2.445180\n",
      "Validation loss decreased (2.445222 --> 2.445180).  Saving model ...\n",
      "Epoch: 1675 \tTraining Loss: 2.416310 \tValidation Loss: 2.445123\n",
      "Validation loss decreased (2.445180 --> 2.445123).  Saving model ...\n",
      "Epoch: 1676 \tTraining Loss: 2.422101 \tValidation Loss: 2.445094\n",
      "Validation loss decreased (2.445123 --> 2.445094).  Saving model ...\n",
      "Epoch: 1677 \tTraining Loss: 2.426327 \tValidation Loss: 2.445037\n",
      "Validation loss decreased (2.445094 --> 2.445037).  Saving model ...\n",
      "Epoch: 1678 \tTraining Loss: 2.413558 \tValidation Loss: 2.444981\n",
      "Validation loss decreased (2.445037 --> 2.444981).  Saving model ...\n",
      "Epoch: 1679 \tTraining Loss: 2.425034 \tValidation Loss: 2.444941\n",
      "Validation loss decreased (2.444981 --> 2.444941).  Saving model ...\n",
      "Epoch: 1680 \tTraining Loss: 2.423974 \tValidation Loss: 2.444909\n",
      "Validation loss decreased (2.444941 --> 2.444909).  Saving model ...\n",
      "Epoch: 1681 \tTraining Loss: 2.416588 \tValidation Loss: 2.444892\n",
      "Validation loss decreased (2.444909 --> 2.444892).  Saving model ...\n",
      "Epoch: 1682 \tTraining Loss: 2.415638 \tValidation Loss: 2.444843\n",
      "Validation loss decreased (2.444892 --> 2.444843).  Saving model ...\n",
      "Epoch: 1683 \tTraining Loss: 2.418371 \tValidation Loss: 2.444784\n",
      "Validation loss decreased (2.444843 --> 2.444784).  Saving model ...\n",
      "Epoch: 1684 \tTraining Loss: 2.416539 \tValidation Loss: 2.444815\n",
      "Epoch: 1685 \tTraining Loss: 2.425402 \tValidation Loss: 2.444712\n",
      "Validation loss decreased (2.444784 --> 2.444712).  Saving model ...\n",
      "Epoch: 1686 \tTraining Loss: 2.413155 \tValidation Loss: 2.444663\n",
      "Validation loss decreased (2.444712 --> 2.444663).  Saving model ...\n",
      "Epoch: 1687 \tTraining Loss: 2.416208 \tValidation Loss: 2.444661\n",
      "Validation loss decreased (2.444663 --> 2.444661).  Saving model ...\n",
      "Epoch: 1688 \tTraining Loss: 2.415687 \tValidation Loss: 2.444651\n",
      "Validation loss decreased (2.444661 --> 2.444651).  Saving model ...\n",
      "Epoch: 1689 \tTraining Loss: 2.418202 \tValidation Loss: 2.444567\n",
      "Validation loss decreased (2.444651 --> 2.444567).  Saving model ...\n",
      "Epoch: 1690 \tTraining Loss: 2.421831 \tValidation Loss: 2.444483\n",
      "Validation loss decreased (2.444567 --> 2.444483).  Saving model ...\n",
      "Epoch: 1691 \tTraining Loss: 2.419626 \tValidation Loss: 2.444512\n",
      "Epoch: 1692 \tTraining Loss: 2.417497 \tValidation Loss: 2.444522\n",
      "Epoch: 1693 \tTraining Loss: 2.416051 \tValidation Loss: 2.444508\n",
      "Epoch: 1694 \tTraining Loss: 2.425956 \tValidation Loss: 2.444483\n",
      "Epoch: 1695 \tTraining Loss: 2.421475 \tValidation Loss: 2.444458\n",
      "Validation loss decreased (2.444483 --> 2.444458).  Saving model ...\n",
      "Epoch: 1696 \tTraining Loss: 2.422249 \tValidation Loss: 2.444445\n",
      "Validation loss decreased (2.444458 --> 2.444445).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1697 \tTraining Loss: 2.408544 \tValidation Loss: 2.444405\n",
      "Validation loss decreased (2.444445 --> 2.444405).  Saving model ...\n",
      "Epoch: 1698 \tTraining Loss: 2.407696 \tValidation Loss: 2.444353\n",
      "Validation loss decreased (2.444405 --> 2.444353).  Saving model ...\n",
      "Epoch: 1699 \tTraining Loss: 2.422006 \tValidation Loss: 2.444300\n",
      "Validation loss decreased (2.444353 --> 2.444300).  Saving model ...\n",
      "Epoch: 1700 \tTraining Loss: 2.422362 \tValidation Loss: 2.444260\n",
      "Validation loss decreased (2.444300 --> 2.444260).  Saving model ...\n",
      "Epoch: 1701 \tTraining Loss: 2.414696 \tValidation Loss: 2.444184\n",
      "Validation loss decreased (2.444260 --> 2.444184).  Saving model ...\n",
      "Epoch: 1702 \tTraining Loss: 2.425502 \tValidation Loss: 2.444183\n",
      "Validation loss decreased (2.444184 --> 2.444183).  Saving model ...\n",
      "Epoch: 1703 \tTraining Loss: 2.417043 \tValidation Loss: 2.444113\n",
      "Validation loss decreased (2.444183 --> 2.444113).  Saving model ...\n",
      "Epoch: 1704 \tTraining Loss: 2.415703 \tValidation Loss: 2.444058\n",
      "Validation loss decreased (2.444113 --> 2.444058).  Saving model ...\n",
      "Epoch: 1705 \tTraining Loss: 2.411920 \tValidation Loss: 2.444065\n",
      "Epoch: 1706 \tTraining Loss: 2.426775 \tValidation Loss: 2.444025\n",
      "Validation loss decreased (2.444058 --> 2.444025).  Saving model ...\n",
      "Epoch: 1707 \tTraining Loss: 2.410707 \tValidation Loss: 2.444043\n",
      "Epoch: 1708 \tTraining Loss: 2.405669 \tValidation Loss: 2.444012\n",
      "Validation loss decreased (2.444025 --> 2.444012).  Saving model ...\n",
      "Epoch: 1709 \tTraining Loss: 2.415353 \tValidation Loss: 2.443928\n",
      "Validation loss decreased (2.444012 --> 2.443928).  Saving model ...\n",
      "Epoch: 1710 \tTraining Loss: 2.416678 \tValidation Loss: 2.443901\n",
      "Validation loss decreased (2.443928 --> 2.443901).  Saving model ...\n",
      "Epoch: 1711 \tTraining Loss: 2.413822 \tValidation Loss: 2.443873\n",
      "Validation loss decreased (2.443901 --> 2.443873).  Saving model ...\n",
      "Epoch: 1712 \tTraining Loss: 2.415028 \tValidation Loss: 2.443881\n",
      "Epoch: 1713 \tTraining Loss: 2.423649 \tValidation Loss: 2.443882\n",
      "Epoch: 1714 \tTraining Loss: 2.414647 \tValidation Loss: 2.443802\n",
      "Validation loss decreased (2.443873 --> 2.443802).  Saving model ...\n",
      "Epoch: 1715 \tTraining Loss: 2.410028 \tValidation Loss: 2.443725\n",
      "Validation loss decreased (2.443802 --> 2.443725).  Saving model ...\n",
      "Epoch: 1716 \tTraining Loss: 2.421264 \tValidation Loss: 2.443739\n",
      "Epoch: 1717 \tTraining Loss: 2.407763 \tValidation Loss: 2.443701\n",
      "Validation loss decreased (2.443725 --> 2.443701).  Saving model ...\n",
      "Epoch: 1718 \tTraining Loss: 2.411985 \tValidation Loss: 2.443612\n",
      "Validation loss decreased (2.443701 --> 2.443612).  Saving model ...\n",
      "Epoch: 1719 \tTraining Loss: 2.414493 \tValidation Loss: 2.443659\n",
      "Epoch: 1720 \tTraining Loss: 2.419810 \tValidation Loss: 2.443581\n",
      "Validation loss decreased (2.443612 --> 2.443581).  Saving model ...\n",
      "Epoch: 1721 \tTraining Loss: 2.415953 \tValidation Loss: 2.443487\n",
      "Validation loss decreased (2.443581 --> 2.443487).  Saving model ...\n",
      "Epoch: 1722 \tTraining Loss: 2.406445 \tValidation Loss: 2.443407\n",
      "Validation loss decreased (2.443487 --> 2.443407).  Saving model ...\n",
      "Epoch: 1723 \tTraining Loss: 2.414444 \tValidation Loss: 2.443334\n",
      "Validation loss decreased (2.443407 --> 2.443334).  Saving model ...\n",
      "Epoch: 1724 \tTraining Loss: 2.412158 \tValidation Loss: 2.443329\n",
      "Validation loss decreased (2.443334 --> 2.443329).  Saving model ...\n",
      "Epoch: 1725 \tTraining Loss: 2.409733 \tValidation Loss: 2.443250\n",
      "Validation loss decreased (2.443329 --> 2.443250).  Saving model ...\n",
      "Epoch: 1726 \tTraining Loss: 2.408094 \tValidation Loss: 2.443213\n",
      "Validation loss decreased (2.443250 --> 2.443213).  Saving model ...\n",
      "Epoch: 1727 \tTraining Loss: 2.409574 \tValidation Loss: 2.443154\n",
      "Validation loss decreased (2.443213 --> 2.443154).  Saving model ...\n",
      "Epoch: 1728 \tTraining Loss: 2.406494 \tValidation Loss: 2.443090\n",
      "Validation loss decreased (2.443154 --> 2.443090).  Saving model ...\n",
      "Epoch: 1729 \tTraining Loss: 2.429082 \tValidation Loss: 2.443106\n",
      "Epoch: 1730 \tTraining Loss: 2.424844 \tValidation Loss: 2.443110\n",
      "Epoch: 1731 \tTraining Loss: 2.417355 \tValidation Loss: 2.443173\n",
      "Epoch: 1732 \tTraining Loss: 2.416892 \tValidation Loss: 2.443110\n",
      "Epoch: 1733 \tTraining Loss: 2.410204 \tValidation Loss: 2.443027\n",
      "Validation loss decreased (2.443090 --> 2.443027).  Saving model ...\n",
      "Epoch: 1734 \tTraining Loss: 2.414865 \tValidation Loss: 2.442987\n",
      "Validation loss decreased (2.443027 --> 2.442987).  Saving model ...\n",
      "Epoch: 1735 \tTraining Loss: 2.416888 \tValidation Loss: 2.442940\n",
      "Validation loss decreased (2.442987 --> 2.442940).  Saving model ...\n",
      "Epoch: 1736 \tTraining Loss: 2.405281 \tValidation Loss: 2.442923\n",
      "Validation loss decreased (2.442940 --> 2.442923).  Saving model ...\n",
      "Epoch: 1737 \tTraining Loss: 2.413516 \tValidation Loss: 2.442961\n",
      "Epoch: 1738 \tTraining Loss: 2.420681 \tValidation Loss: 2.442909\n",
      "Validation loss decreased (2.442923 --> 2.442909).  Saving model ...\n",
      "Epoch: 1739 \tTraining Loss: 2.411251 \tValidation Loss: 2.442938\n",
      "Epoch: 1740 \tTraining Loss: 2.415109 \tValidation Loss: 2.442862\n",
      "Validation loss decreased (2.442909 --> 2.442862).  Saving model ...\n",
      "Epoch: 1741 \tTraining Loss: 2.423521 \tValidation Loss: 2.442895\n",
      "Epoch: 1742 \tTraining Loss: 2.407705 \tValidation Loss: 2.442827\n",
      "Validation loss decreased (2.442862 --> 2.442827).  Saving model ...\n",
      "Epoch: 1743 \tTraining Loss: 2.404713 \tValidation Loss: 2.442775\n",
      "Validation loss decreased (2.442827 --> 2.442775).  Saving model ...\n",
      "Epoch: 1744 \tTraining Loss: 2.419271 \tValidation Loss: 2.442765\n",
      "Validation loss decreased (2.442775 --> 2.442765).  Saving model ...\n",
      "Epoch: 1745 \tTraining Loss: 2.422467 \tValidation Loss: 2.442788\n",
      "Epoch: 1746 \tTraining Loss: 2.412328 \tValidation Loss: 2.442737\n",
      "Validation loss decreased (2.442765 --> 2.442737).  Saving model ...\n",
      "Epoch: 1747 \tTraining Loss: 2.417840 \tValidation Loss: 2.442729\n",
      "Validation loss decreased (2.442737 --> 2.442729).  Saving model ...\n",
      "Epoch: 1748 \tTraining Loss: 2.408255 \tValidation Loss: 2.442718\n",
      "Validation loss decreased (2.442729 --> 2.442718).  Saving model ...\n",
      "Epoch: 1749 \tTraining Loss: 2.415488 \tValidation Loss: 2.442691\n",
      "Validation loss decreased (2.442718 --> 2.442691).  Saving model ...\n",
      "Epoch: 1750 \tTraining Loss: 2.422484 \tValidation Loss: 2.442757\n",
      "Epoch: 1751 \tTraining Loss: 2.411540 \tValidation Loss: 2.442715\n",
      "Epoch: 1752 \tTraining Loss: 2.416652 \tValidation Loss: 2.442620\n",
      "Validation loss decreased (2.442691 --> 2.442620).  Saving model ...\n",
      "Epoch: 1753 \tTraining Loss: 2.411708 \tValidation Loss: 2.442527\n",
      "Validation loss decreased (2.442620 --> 2.442527).  Saving model ...\n",
      "Epoch: 1754 \tTraining Loss: 2.405448 \tValidation Loss: 2.442503\n",
      "Validation loss decreased (2.442527 --> 2.442503).  Saving model ...\n",
      "Epoch: 1755 \tTraining Loss: 2.412381 \tValidation Loss: 2.442447\n",
      "Validation loss decreased (2.442503 --> 2.442447).  Saving model ...\n",
      "Epoch: 1756 \tTraining Loss: 2.421152 \tValidation Loss: 2.442466\n",
      "Epoch: 1757 \tTraining Loss: 2.419393 \tValidation Loss: 2.442468\n",
      "Epoch: 1758 \tTraining Loss: 2.407691 \tValidation Loss: 2.442404\n",
      "Validation loss decreased (2.442447 --> 2.442404).  Saving model ...\n",
      "Epoch: 1759 \tTraining Loss: 2.406830 \tValidation Loss: 2.442398\n",
      "Validation loss decreased (2.442404 --> 2.442398).  Saving model ...\n",
      "Epoch: 1760 \tTraining Loss: 2.416126 \tValidation Loss: 2.442395\n",
      "Validation loss decreased (2.442398 --> 2.442395).  Saving model ...\n",
      "Epoch: 1761 \tTraining Loss: 2.401570 \tValidation Loss: 2.442363\n",
      "Validation loss decreased (2.442395 --> 2.442363).  Saving model ...\n",
      "Epoch: 1762 \tTraining Loss: 2.425265 \tValidation Loss: 2.442367\n",
      "Epoch: 1763 \tTraining Loss: 2.410767 \tValidation Loss: 2.442320\n",
      "Validation loss decreased (2.442363 --> 2.442320).  Saving model ...\n",
      "Epoch: 1764 \tTraining Loss: 2.411699 \tValidation Loss: 2.442231\n",
      "Validation loss decreased (2.442320 --> 2.442231).  Saving model ...\n",
      "Epoch: 1765 \tTraining Loss: 2.407377 \tValidation Loss: 2.442149\n",
      "Validation loss decreased (2.442231 --> 2.442149).  Saving model ...\n",
      "Epoch: 1766 \tTraining Loss: 2.417893 \tValidation Loss: 2.442145\n",
      "Validation loss decreased (2.442149 --> 2.442145).  Saving model ...\n",
      "Epoch: 1767 \tTraining Loss: 2.415376 \tValidation Loss: 2.442162\n",
      "Epoch: 1768 \tTraining Loss: 2.406221 \tValidation Loss: 2.442058\n",
      "Validation loss decreased (2.442145 --> 2.442058).  Saving model ...\n",
      "Epoch: 1769 \tTraining Loss: 2.413694 \tValidation Loss: 2.441996\n",
      "Validation loss decreased (2.442058 --> 2.441996).  Saving model ...\n",
      "Epoch: 1770 \tTraining Loss: 2.419179 \tValidation Loss: 2.441979\n",
      "Validation loss decreased (2.441996 --> 2.441979).  Saving model ...\n",
      "Epoch: 1771 \tTraining Loss: 2.411783 \tValidation Loss: 2.441924\n",
      "Validation loss decreased (2.441979 --> 2.441924).  Saving model ...\n",
      "Epoch: 1772 \tTraining Loss: 2.411931 \tValidation Loss: 2.441899\n",
      "Validation loss decreased (2.441924 --> 2.441899).  Saving model ...\n",
      "Epoch: 1773 \tTraining Loss: 2.396971 \tValidation Loss: 2.441849\n",
      "Validation loss decreased (2.441899 --> 2.441849).  Saving model ...\n",
      "Epoch: 1774 \tTraining Loss: 2.413795 \tValidation Loss: 2.441811\n",
      "Validation loss decreased (2.441849 --> 2.441811).  Saving model ...\n",
      "Epoch: 1775 \tTraining Loss: 2.407264 \tValidation Loss: 2.441824\n",
      "Epoch: 1776 \tTraining Loss: 2.419340 \tValidation Loss: 2.441891\n",
      "Epoch: 1777 \tTraining Loss: 2.415519 \tValidation Loss: 2.441870\n",
      "Epoch: 1778 \tTraining Loss: 2.414444 \tValidation Loss: 2.441859\n",
      "Epoch: 1779 \tTraining Loss: 2.413474 \tValidation Loss: 2.441814\n",
      "Epoch: 1780 \tTraining Loss: 2.408740 \tValidation Loss: 2.441796\n",
      "Validation loss decreased (2.441811 --> 2.441796).  Saving model ...\n",
      "Epoch: 1781 \tTraining Loss: 2.426954 \tValidation Loss: 2.441814\n",
      "Epoch: 1782 \tTraining Loss: 2.404500 \tValidation Loss: 2.441711\n",
      "Validation loss decreased (2.441796 --> 2.441711).  Saving model ...\n",
      "Epoch: 1783 \tTraining Loss: 2.400731 \tValidation Loss: 2.441687\n",
      "Validation loss decreased (2.441711 --> 2.441687).  Saving model ...\n",
      "Epoch: 1784 \tTraining Loss: 2.404298 \tValidation Loss: 2.441683\n",
      "Validation loss decreased (2.441687 --> 2.441683).  Saving model ...\n",
      "Epoch: 1785 \tTraining Loss: 2.402476 \tValidation Loss: 2.441594\n",
      "Validation loss decreased (2.441683 --> 2.441594).  Saving model ...\n",
      "Epoch: 1786 \tTraining Loss: 2.400190 \tValidation Loss: 2.441520\n",
      "Validation loss decreased (2.441594 --> 2.441520).  Saving model ...\n",
      "Epoch: 1787 \tTraining Loss: 2.409208 \tValidation Loss: 2.441477\n",
      "Validation loss decreased (2.441520 --> 2.441477).  Saving model ...\n",
      "Epoch: 1788 \tTraining Loss: 2.411159 \tValidation Loss: 2.441396\n",
      "Validation loss decreased (2.441477 --> 2.441396).  Saving model ...\n",
      "Epoch: 1789 \tTraining Loss: 2.415031 \tValidation Loss: 2.441400\n",
      "Epoch: 1790 \tTraining Loss: 2.409021 \tValidation Loss: 2.441418\n",
      "Epoch: 1791 \tTraining Loss: 2.399943 \tValidation Loss: 2.441337\n",
      "Validation loss decreased (2.441396 --> 2.441337).  Saving model ...\n",
      "Epoch: 1792 \tTraining Loss: 2.407941 \tValidation Loss: 2.441290\n",
      "Validation loss decreased (2.441337 --> 2.441290).  Saving model ...\n",
      "Epoch: 1793 \tTraining Loss: 2.398536 \tValidation Loss: 2.441224\n",
      "Validation loss decreased (2.441290 --> 2.441224).  Saving model ...\n",
      "Epoch: 1794 \tTraining Loss: 2.412367 \tValidation Loss: 2.441130\n",
      "Validation loss decreased (2.441224 --> 2.441130).  Saving model ...\n",
      "Epoch: 1795 \tTraining Loss: 2.404440 \tValidation Loss: 2.441104\n",
      "Validation loss decreased (2.441130 --> 2.441104).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1796 \tTraining Loss: 2.415133 \tValidation Loss: 2.441142\n",
      "Epoch: 1797 \tTraining Loss: 2.408480 \tValidation Loss: 2.441128\n",
      "Epoch: 1798 \tTraining Loss: 2.417874 \tValidation Loss: 2.441109\n",
      "Epoch: 1799 \tTraining Loss: 2.419791 \tValidation Loss: 2.441198\n",
      "Epoch: 1800 \tTraining Loss: 2.401310 \tValidation Loss: 2.441131\n",
      "Epoch: 1801 \tTraining Loss: 2.414818 \tValidation Loss: 2.441093\n",
      "Validation loss decreased (2.441104 --> 2.441093).  Saving model ...\n",
      "Epoch: 1802 \tTraining Loss: 2.409220 \tValidation Loss: 2.441030\n",
      "Validation loss decreased (2.441093 --> 2.441030).  Saving model ...\n",
      "Epoch: 1803 \tTraining Loss: 2.400893 \tValidation Loss: 2.440889\n",
      "Validation loss decreased (2.441030 --> 2.440889).  Saving model ...\n",
      "Epoch: 1804 \tTraining Loss: 2.409992 \tValidation Loss: 2.440764\n",
      "Validation loss decreased (2.440889 --> 2.440764).  Saving model ...\n",
      "Epoch: 1805 \tTraining Loss: 2.410560 \tValidation Loss: 2.440762\n",
      "Validation loss decreased (2.440764 --> 2.440762).  Saving model ...\n",
      "Epoch: 1806 \tTraining Loss: 2.400552 \tValidation Loss: 2.440709\n",
      "Validation loss decreased (2.440762 --> 2.440709).  Saving model ...\n",
      "Epoch: 1807 \tTraining Loss: 2.404344 \tValidation Loss: 2.440659\n",
      "Validation loss decreased (2.440709 --> 2.440659).  Saving model ...\n",
      "Epoch: 1808 \tTraining Loss: 2.407102 \tValidation Loss: 2.440586\n",
      "Validation loss decreased (2.440659 --> 2.440586).  Saving model ...\n",
      "Epoch: 1809 \tTraining Loss: 2.406160 \tValidation Loss: 2.440526\n",
      "Validation loss decreased (2.440586 --> 2.440526).  Saving model ...\n",
      "Epoch: 1810 \tTraining Loss: 2.400349 \tValidation Loss: 2.440463\n",
      "Validation loss decreased (2.440526 --> 2.440463).  Saving model ...\n",
      "Epoch: 1811 \tTraining Loss: 2.415947 \tValidation Loss: 2.440434\n",
      "Validation loss decreased (2.440463 --> 2.440434).  Saving model ...\n",
      "Epoch: 1812 \tTraining Loss: 2.413859 \tValidation Loss: 2.440490\n",
      "Epoch: 1813 \tTraining Loss: 2.405568 \tValidation Loss: 2.440457\n",
      "Epoch: 1814 \tTraining Loss: 2.415634 \tValidation Loss: 2.440416\n",
      "Validation loss decreased (2.440434 --> 2.440416).  Saving model ...\n",
      "Epoch: 1815 \tTraining Loss: 2.409082 \tValidation Loss: 2.440387\n",
      "Validation loss decreased (2.440416 --> 2.440387).  Saving model ...\n",
      "Epoch: 1816 \tTraining Loss: 2.421841 \tValidation Loss: 2.440393\n",
      "Epoch: 1817 \tTraining Loss: 2.410779 \tValidation Loss: 2.440417\n",
      "Epoch: 1818 \tTraining Loss: 2.410297 \tValidation Loss: 2.440411\n",
      "Epoch: 1819 \tTraining Loss: 2.403423 \tValidation Loss: 2.440446\n",
      "Epoch: 1820 \tTraining Loss: 2.416739 \tValidation Loss: 2.440433\n",
      "Epoch: 1821 \tTraining Loss: 2.400466 \tValidation Loss: 2.440358\n",
      "Validation loss decreased (2.440387 --> 2.440358).  Saving model ...\n",
      "Epoch: 1822 \tTraining Loss: 2.409615 \tValidation Loss: 2.440335\n",
      "Validation loss decreased (2.440358 --> 2.440335).  Saving model ...\n",
      "Epoch: 1823 \tTraining Loss: 2.417290 \tValidation Loss: 2.440258\n",
      "Validation loss decreased (2.440335 --> 2.440258).  Saving model ...\n",
      "Epoch: 1824 \tTraining Loss: 2.406987 \tValidation Loss: 2.440232\n",
      "Validation loss decreased (2.440258 --> 2.440232).  Saving model ...\n",
      "Epoch: 1825 \tTraining Loss: 2.407048 \tValidation Loss: 2.440183\n",
      "Validation loss decreased (2.440232 --> 2.440183).  Saving model ...\n",
      "Epoch: 1826 \tTraining Loss: 2.404555 \tValidation Loss: 2.440080\n",
      "Validation loss decreased (2.440183 --> 2.440080).  Saving model ...\n",
      "Epoch: 1827 \tTraining Loss: 2.416595 \tValidation Loss: 2.440071\n",
      "Validation loss decreased (2.440080 --> 2.440071).  Saving model ...\n",
      "Epoch: 1828 \tTraining Loss: 2.407573 \tValidation Loss: 2.440064\n",
      "Validation loss decreased (2.440071 --> 2.440064).  Saving model ...\n",
      "Epoch: 1829 \tTraining Loss: 2.403987 \tValidation Loss: 2.440082\n",
      "Epoch: 1830 \tTraining Loss: 2.410000 \tValidation Loss: 2.440113\n",
      "Epoch: 1831 \tTraining Loss: 2.409090 \tValidation Loss: 2.440013\n",
      "Validation loss decreased (2.440064 --> 2.440013).  Saving model ...\n",
      "Epoch: 1832 \tTraining Loss: 2.405569 \tValidation Loss: 2.439922\n",
      "Validation loss decreased (2.440013 --> 2.439922).  Saving model ...\n",
      "Epoch: 1833 \tTraining Loss: 2.410815 \tValidation Loss: 2.439879\n",
      "Validation loss decreased (2.439922 --> 2.439879).  Saving model ...\n",
      "Epoch: 1834 \tTraining Loss: 2.405747 \tValidation Loss: 2.439826\n",
      "Validation loss decreased (2.439879 --> 2.439826).  Saving model ...\n",
      "Epoch: 1835 \tTraining Loss: 2.407883 \tValidation Loss: 2.439831\n",
      "Epoch: 1836 \tTraining Loss: 2.404047 \tValidation Loss: 2.439814\n",
      "Validation loss decreased (2.439826 --> 2.439814).  Saving model ...\n",
      "Epoch: 1837 \tTraining Loss: 2.408667 \tValidation Loss: 2.439893\n",
      "Epoch: 1838 \tTraining Loss: 2.415094 \tValidation Loss: 2.439885\n",
      "Epoch: 1839 \tTraining Loss: 2.404778 \tValidation Loss: 2.439821\n",
      "Epoch: 1840 \tTraining Loss: 2.400199 \tValidation Loss: 2.439690\n",
      "Validation loss decreased (2.439814 --> 2.439690).  Saving model ...\n",
      "Epoch: 1841 \tTraining Loss: 2.411391 \tValidation Loss: 2.439705\n",
      "Epoch: 1842 \tTraining Loss: 2.415101 \tValidation Loss: 2.439690\n",
      "Epoch: 1843 \tTraining Loss: 2.413355 \tValidation Loss: 2.439600\n",
      "Validation loss decreased (2.439690 --> 2.439600).  Saving model ...\n",
      "Epoch: 1844 \tTraining Loss: 2.412492 \tValidation Loss: 2.439599\n",
      "Validation loss decreased (2.439600 --> 2.439599).  Saving model ...\n",
      "Epoch: 1845 \tTraining Loss: 2.400174 \tValidation Loss: 2.439563\n",
      "Validation loss decreased (2.439599 --> 2.439563).  Saving model ...\n",
      "Epoch: 1846 \tTraining Loss: 2.401592 \tValidation Loss: 2.439533\n",
      "Validation loss decreased (2.439563 --> 2.439533).  Saving model ...\n",
      "Epoch: 1847 \tTraining Loss: 2.395739 \tValidation Loss: 2.439443\n",
      "Validation loss decreased (2.439533 --> 2.439443).  Saving model ...\n",
      "Epoch: 1848 \tTraining Loss: 2.401817 \tValidation Loss: 2.439411\n",
      "Validation loss decreased (2.439443 --> 2.439411).  Saving model ...\n",
      "Epoch: 1849 \tTraining Loss: 2.408325 \tValidation Loss: 2.439426\n",
      "Epoch: 1850 \tTraining Loss: 2.410888 \tValidation Loss: 2.439461\n",
      "Epoch: 1851 \tTraining Loss: 2.407813 \tValidation Loss: 2.439473\n",
      "Epoch: 1852 \tTraining Loss: 2.420583 \tValidation Loss: 2.439476\n",
      "Epoch: 1853 \tTraining Loss: 2.424847 \tValidation Loss: 2.439525\n",
      "Epoch: 1854 \tTraining Loss: 2.413642 \tValidation Loss: 2.439509\n",
      "Epoch: 1855 \tTraining Loss: 2.415399 \tValidation Loss: 2.439487\n",
      "Epoch: 1856 \tTraining Loss: 2.421649 \tValidation Loss: 2.439488\n",
      "Epoch: 1857 \tTraining Loss: 2.418585 \tValidation Loss: 2.439495\n",
      "Epoch: 1858 \tTraining Loss: 2.401936 \tValidation Loss: 2.439478\n",
      "Epoch: 1859 \tTraining Loss: 2.408853 \tValidation Loss: 2.439442\n",
      "Epoch: 1860 \tTraining Loss: 2.429856 \tValidation Loss: 2.439450\n",
      "Epoch: 1861 \tTraining Loss: 2.406569 \tValidation Loss: 2.439402\n",
      "Validation loss decreased (2.439411 --> 2.439402).  Saving model ...\n",
      "Epoch: 1862 \tTraining Loss: 2.400169 \tValidation Loss: 2.439363\n",
      "Validation loss decreased (2.439402 --> 2.439363).  Saving model ...\n",
      "Epoch: 1863 \tTraining Loss: 2.401172 \tValidation Loss: 2.439313\n",
      "Validation loss decreased (2.439363 --> 2.439313).  Saving model ...\n",
      "Epoch: 1864 \tTraining Loss: 2.419432 \tValidation Loss: 2.439236\n",
      "Validation loss decreased (2.439313 --> 2.439236).  Saving model ...\n",
      "Epoch: 1865 \tTraining Loss: 2.401536 \tValidation Loss: 2.439120\n",
      "Validation loss decreased (2.439236 --> 2.439120).  Saving model ...\n",
      "Epoch: 1866 \tTraining Loss: 2.407278 \tValidation Loss: 2.439094\n",
      "Validation loss decreased (2.439120 --> 2.439094).  Saving model ...\n",
      "Epoch: 1867 \tTraining Loss: 2.412604 \tValidation Loss: 2.439063\n",
      "Validation loss decreased (2.439094 --> 2.439063).  Saving model ...\n",
      "Epoch: 1868 \tTraining Loss: 2.399185 \tValidation Loss: 2.438896\n",
      "Validation loss decreased (2.439063 --> 2.438896).  Saving model ...\n",
      "Epoch: 1869 \tTraining Loss: 2.399335 \tValidation Loss: 2.438831\n",
      "Validation loss decreased (2.438896 --> 2.438831).  Saving model ...\n",
      "Epoch: 1870 \tTraining Loss: 2.405276 \tValidation Loss: 2.438788\n",
      "Validation loss decreased (2.438831 --> 2.438788).  Saving model ...\n",
      "Epoch: 1871 \tTraining Loss: 2.407870 \tValidation Loss: 2.438789\n",
      "Epoch: 1872 \tTraining Loss: 2.409083 \tValidation Loss: 2.438761\n",
      "Validation loss decreased (2.438788 --> 2.438761).  Saving model ...\n",
      "Epoch: 1873 \tTraining Loss: 2.407375 \tValidation Loss: 2.438756\n",
      "Validation loss decreased (2.438761 --> 2.438756).  Saving model ...\n",
      "Epoch: 1874 \tTraining Loss: 2.409840 \tValidation Loss: 2.438752\n",
      "Validation loss decreased (2.438756 --> 2.438752).  Saving model ...\n",
      "Epoch: 1875 \tTraining Loss: 2.391721 \tValidation Loss: 2.438695\n",
      "Validation loss decreased (2.438752 --> 2.438695).  Saving model ...\n",
      "Epoch: 1876 \tTraining Loss: 2.396060 \tValidation Loss: 2.438628\n",
      "Validation loss decreased (2.438695 --> 2.438628).  Saving model ...\n",
      "Epoch: 1877 \tTraining Loss: 2.414034 \tValidation Loss: 2.438633\n",
      "Epoch: 1878 \tTraining Loss: 2.412187 \tValidation Loss: 2.438593\n",
      "Validation loss decreased (2.438628 --> 2.438593).  Saving model ...\n",
      "Epoch: 1879 \tTraining Loss: 2.414148 \tValidation Loss: 2.438602\n",
      "Epoch: 1880 \tTraining Loss: 2.403165 \tValidation Loss: 2.438523\n",
      "Validation loss decreased (2.438593 --> 2.438523).  Saving model ...\n",
      "Epoch: 1881 \tTraining Loss: 2.404969 \tValidation Loss: 2.438448\n",
      "Validation loss decreased (2.438523 --> 2.438448).  Saving model ...\n",
      "Epoch: 1882 \tTraining Loss: 2.410792 \tValidation Loss: 2.438463\n",
      "Epoch: 1883 \tTraining Loss: 2.417125 \tValidation Loss: 2.438520\n",
      "Epoch: 1884 \tTraining Loss: 2.405248 \tValidation Loss: 2.438496\n",
      "Epoch: 1885 \tTraining Loss: 2.392401 \tValidation Loss: 2.438418\n",
      "Validation loss decreased (2.438448 --> 2.438418).  Saving model ...\n",
      "Epoch: 1886 \tTraining Loss: 2.402795 \tValidation Loss: 2.438375\n",
      "Validation loss decreased (2.438418 --> 2.438375).  Saving model ...\n",
      "Epoch: 1887 \tTraining Loss: 2.402278 \tValidation Loss: 2.438339\n",
      "Validation loss decreased (2.438375 --> 2.438339).  Saving model ...\n",
      "Epoch: 1888 \tTraining Loss: 2.409088 \tValidation Loss: 2.438301\n",
      "Validation loss decreased (2.438339 --> 2.438301).  Saving model ...\n",
      "Epoch: 1889 \tTraining Loss: 2.403901 \tValidation Loss: 2.438265\n",
      "Validation loss decreased (2.438301 --> 2.438265).  Saving model ...\n",
      "Epoch: 1890 \tTraining Loss: 2.414988 \tValidation Loss: 2.438277\n",
      "Epoch: 1891 \tTraining Loss: 2.406121 \tValidation Loss: 2.438256\n",
      "Validation loss decreased (2.438265 --> 2.438256).  Saving model ...\n",
      "Epoch: 1892 \tTraining Loss: 2.413071 \tValidation Loss: 2.438238\n",
      "Validation loss decreased (2.438256 --> 2.438238).  Saving model ...\n",
      "Epoch: 1893 \tTraining Loss: 2.410927 \tValidation Loss: 2.438198\n",
      "Validation loss decreased (2.438238 --> 2.438198).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1894 \tTraining Loss: 2.406294 \tValidation Loss: 2.438188\n",
      "Validation loss decreased (2.438198 --> 2.438188).  Saving model ...\n",
      "Epoch: 1895 \tTraining Loss: 2.401475 \tValidation Loss: 2.438204\n",
      "Epoch: 1896 \tTraining Loss: 2.408459 \tValidation Loss: 2.438148\n",
      "Validation loss decreased (2.438188 --> 2.438148).  Saving model ...\n",
      "Epoch: 1897 \tTraining Loss: 2.401429 \tValidation Loss: 2.438131\n",
      "Validation loss decreased (2.438148 --> 2.438131).  Saving model ...\n",
      "Epoch: 1898 \tTraining Loss: 2.417275 \tValidation Loss: 2.438171\n",
      "Epoch: 1899 \tTraining Loss: 2.410226 \tValidation Loss: 2.438127\n",
      "Validation loss decreased (2.438131 --> 2.438127).  Saving model ...\n",
      "Epoch: 1900 \tTraining Loss: 2.419221 \tValidation Loss: 2.438118\n",
      "Validation loss decreased (2.438127 --> 2.438118).  Saving model ...\n",
      "Epoch: 1901 \tTraining Loss: 2.411979 \tValidation Loss: 2.438141\n",
      "Epoch: 1902 \tTraining Loss: 2.410076 \tValidation Loss: 2.438078\n",
      "Validation loss decreased (2.438118 --> 2.438078).  Saving model ...\n",
      "Epoch: 1903 \tTraining Loss: 2.416388 \tValidation Loss: 2.438109\n",
      "Epoch: 1904 \tTraining Loss: 2.410275 \tValidation Loss: 2.438112\n",
      "Epoch: 1905 \tTraining Loss: 2.423095 \tValidation Loss: 2.438141\n",
      "Epoch: 1906 \tTraining Loss: 2.413198 \tValidation Loss: 2.438133\n",
      "Epoch: 1907 \tTraining Loss: 2.414305 \tValidation Loss: 2.438071\n",
      "Validation loss decreased (2.438078 --> 2.438071).  Saving model ...\n",
      "Epoch: 1908 \tTraining Loss: 2.407748 \tValidation Loss: 2.437989\n",
      "Validation loss decreased (2.438071 --> 2.437989).  Saving model ...\n",
      "Epoch: 1909 \tTraining Loss: 2.400124 \tValidation Loss: 2.437889\n",
      "Validation loss decreased (2.437989 --> 2.437889).  Saving model ...\n",
      "Epoch: 1910 \tTraining Loss: 2.410610 \tValidation Loss: 2.437879\n",
      "Validation loss decreased (2.437889 --> 2.437879).  Saving model ...\n",
      "Epoch: 1911 \tTraining Loss: 2.401329 \tValidation Loss: 2.437832\n",
      "Validation loss decreased (2.437879 --> 2.437832).  Saving model ...\n",
      "Epoch: 1912 \tTraining Loss: 2.402865 \tValidation Loss: 2.437805\n",
      "Validation loss decreased (2.437832 --> 2.437805).  Saving model ...\n",
      "Epoch: 1913 \tTraining Loss: 2.408715 \tValidation Loss: 2.437682\n",
      "Validation loss decreased (2.437805 --> 2.437682).  Saving model ...\n",
      "Epoch: 1914 \tTraining Loss: 2.404725 \tValidation Loss: 2.437743\n",
      "Epoch: 1915 \tTraining Loss: 2.405617 \tValidation Loss: 2.437730\n",
      "Epoch: 1916 \tTraining Loss: 2.403655 \tValidation Loss: 2.437685\n",
      "Epoch: 1917 \tTraining Loss: 2.397192 \tValidation Loss: 2.437632\n",
      "Validation loss decreased (2.437682 --> 2.437632).  Saving model ...\n",
      "Epoch: 1918 \tTraining Loss: 2.416380 \tValidation Loss: 2.437596\n",
      "Validation loss decreased (2.437632 --> 2.437596).  Saving model ...\n",
      "Epoch: 1919 \tTraining Loss: 2.410131 \tValidation Loss: 2.437571\n",
      "Validation loss decreased (2.437596 --> 2.437571).  Saving model ...\n",
      "Epoch: 1920 \tTraining Loss: 2.405717 \tValidation Loss: 2.437602\n",
      "Epoch: 1921 \tTraining Loss: 2.404170 \tValidation Loss: 2.437590\n",
      "Epoch: 1922 \tTraining Loss: 2.415740 \tValidation Loss: 2.437553\n",
      "Validation loss decreased (2.437571 --> 2.437553).  Saving model ...\n",
      "Epoch: 1923 \tTraining Loss: 2.405305 \tValidation Loss: 2.437521\n",
      "Validation loss decreased (2.437553 --> 2.437521).  Saving model ...\n",
      "Epoch: 1924 \tTraining Loss: 2.392838 \tValidation Loss: 2.437494\n",
      "Validation loss decreased (2.437521 --> 2.437494).  Saving model ...\n",
      "Epoch: 1925 \tTraining Loss: 2.409940 \tValidation Loss: 2.437448\n",
      "Validation loss decreased (2.437494 --> 2.437448).  Saving model ...\n",
      "Epoch: 1926 \tTraining Loss: 2.409817 \tValidation Loss: 2.437401\n",
      "Validation loss decreased (2.437448 --> 2.437401).  Saving model ...\n",
      "Epoch: 1927 \tTraining Loss: 2.402370 \tValidation Loss: 2.437405\n",
      "Epoch: 1928 \tTraining Loss: 2.409281 \tValidation Loss: 2.437345\n",
      "Validation loss decreased (2.437401 --> 2.437345).  Saving model ...\n",
      "Epoch: 1929 \tTraining Loss: 2.408884 \tValidation Loss: 2.437368\n",
      "Epoch: 1930 \tTraining Loss: 2.409150 \tValidation Loss: 2.437352\n",
      "Epoch: 1931 \tTraining Loss: 2.404598 \tValidation Loss: 2.437329\n",
      "Validation loss decreased (2.437345 --> 2.437329).  Saving model ...\n",
      "Epoch: 1932 \tTraining Loss: 2.403741 \tValidation Loss: 2.437238\n",
      "Validation loss decreased (2.437329 --> 2.437238).  Saving model ...\n",
      "Epoch: 1933 \tTraining Loss: 2.405085 \tValidation Loss: 2.437165\n",
      "Validation loss decreased (2.437238 --> 2.437165).  Saving model ...\n",
      "Epoch: 1934 \tTraining Loss: 2.413314 \tValidation Loss: 2.437108\n",
      "Validation loss decreased (2.437165 --> 2.437108).  Saving model ...\n",
      "Epoch: 1935 \tTraining Loss: 2.414981 \tValidation Loss: 2.437113\n",
      "Epoch: 1936 \tTraining Loss: 2.405397 \tValidation Loss: 2.437021\n",
      "Validation loss decreased (2.437108 --> 2.437021).  Saving model ...\n",
      "Epoch: 1937 \tTraining Loss: 2.398745 \tValidation Loss: 2.436952\n",
      "Validation loss decreased (2.437021 --> 2.436952).  Saving model ...\n",
      "Epoch: 1938 \tTraining Loss: 2.422334 \tValidation Loss: 2.437021\n",
      "Epoch: 1939 \tTraining Loss: 2.412951 \tValidation Loss: 2.437057\n",
      "Epoch: 1940 \tTraining Loss: 2.414075 \tValidation Loss: 2.437049\n",
      "Epoch: 1941 \tTraining Loss: 2.416235 \tValidation Loss: 2.437069\n",
      "Epoch: 1942 \tTraining Loss: 2.408560 \tValidation Loss: 2.437082\n",
      "Epoch: 1943 \tTraining Loss: 2.411565 \tValidation Loss: 2.436991\n",
      "Epoch: 1944 \tTraining Loss: 2.397073 \tValidation Loss: 2.436915\n",
      "Validation loss decreased (2.436952 --> 2.436915).  Saving model ...\n",
      "Epoch: 1945 \tTraining Loss: 2.403720 \tValidation Loss: 2.436903\n",
      "Validation loss decreased (2.436915 --> 2.436903).  Saving model ...\n",
      "Epoch: 1946 \tTraining Loss: 2.407016 \tValidation Loss: 2.436858\n",
      "Validation loss decreased (2.436903 --> 2.436858).  Saving model ...\n",
      "Epoch: 1947 \tTraining Loss: 2.410120 \tValidation Loss: 2.436801\n",
      "Validation loss decreased (2.436858 --> 2.436801).  Saving model ...\n",
      "Epoch: 1948 \tTraining Loss: 2.412113 \tValidation Loss: 2.436739\n",
      "Validation loss decreased (2.436801 --> 2.436739).  Saving model ...\n",
      "Epoch: 1949 \tTraining Loss: 2.401971 \tValidation Loss: 2.436713\n",
      "Validation loss decreased (2.436739 --> 2.436713).  Saving model ...\n",
      "Epoch: 1950 \tTraining Loss: 2.411801 \tValidation Loss: 2.436724\n",
      "Epoch: 1951 \tTraining Loss: 2.398723 \tValidation Loss: 2.436686\n",
      "Validation loss decreased (2.436713 --> 2.436686).  Saving model ...\n",
      "Epoch: 1952 \tTraining Loss: 2.411205 \tValidation Loss: 2.436657\n",
      "Validation loss decreased (2.436686 --> 2.436657).  Saving model ...\n",
      "Epoch: 1953 \tTraining Loss: 2.406910 \tValidation Loss: 2.436659\n",
      "Epoch: 1954 \tTraining Loss: 2.412091 \tValidation Loss: 2.436573\n",
      "Validation loss decreased (2.436657 --> 2.436573).  Saving model ...\n",
      "Epoch: 1955 \tTraining Loss: 2.405399 \tValidation Loss: 2.436588\n",
      "Epoch: 1956 \tTraining Loss: 2.380465 \tValidation Loss: 2.436520\n",
      "Validation loss decreased (2.436573 --> 2.436520).  Saving model ...\n",
      "Epoch: 1957 \tTraining Loss: 2.403041 \tValidation Loss: 2.436538\n",
      "Epoch: 1958 \tTraining Loss: 2.411452 \tValidation Loss: 2.436529\n",
      "Epoch: 1959 \tTraining Loss: 2.399791 \tValidation Loss: 2.436507\n",
      "Validation loss decreased (2.436520 --> 2.436507).  Saving model ...\n",
      "Epoch: 1960 \tTraining Loss: 2.393078 \tValidation Loss: 2.436454\n",
      "Validation loss decreased (2.436507 --> 2.436454).  Saving model ...\n",
      "Epoch: 1961 \tTraining Loss: 2.411646 \tValidation Loss: 2.436405\n",
      "Validation loss decreased (2.436454 --> 2.436405).  Saving model ...\n",
      "Epoch: 1962 \tTraining Loss: 2.389806 \tValidation Loss: 2.436243\n",
      "Validation loss decreased (2.436405 --> 2.436243).  Saving model ...\n",
      "Epoch: 1963 \tTraining Loss: 2.400907 \tValidation Loss: 2.436176\n",
      "Validation loss decreased (2.436243 --> 2.436176).  Saving model ...\n",
      "Epoch: 1964 \tTraining Loss: 2.405009 \tValidation Loss: 2.436142\n",
      "Validation loss decreased (2.436176 --> 2.436142).  Saving model ...\n",
      "Epoch: 1965 \tTraining Loss: 2.398481 \tValidation Loss: 2.436101\n",
      "Validation loss decreased (2.436142 --> 2.436101).  Saving model ...\n",
      "Epoch: 1966 \tTraining Loss: 2.397723 \tValidation Loss: 2.436004\n",
      "Validation loss decreased (2.436101 --> 2.436004).  Saving model ...\n",
      "Epoch: 1967 \tTraining Loss: 2.408774 \tValidation Loss: 2.435983\n",
      "Validation loss decreased (2.436004 --> 2.435983).  Saving model ...\n",
      "Epoch: 1968 \tTraining Loss: 2.402002 \tValidation Loss: 2.435932\n",
      "Validation loss decreased (2.435983 --> 2.435932).  Saving model ...\n",
      "Epoch: 1969 \tTraining Loss: 2.405700 \tValidation Loss: 2.435872\n",
      "Validation loss decreased (2.435932 --> 2.435872).  Saving model ...\n",
      "Epoch: 1970 \tTraining Loss: 2.401271 \tValidation Loss: 2.435833\n",
      "Validation loss decreased (2.435872 --> 2.435833).  Saving model ...\n",
      "Epoch: 1971 \tTraining Loss: 2.402898 \tValidation Loss: 2.435730\n",
      "Validation loss decreased (2.435833 --> 2.435730).  Saving model ...\n",
      "Epoch: 1972 \tTraining Loss: 2.409761 \tValidation Loss: 2.435659\n",
      "Validation loss decreased (2.435730 --> 2.435659).  Saving model ...\n",
      "Epoch: 1973 \tTraining Loss: 2.394808 \tValidation Loss: 2.435572\n",
      "Validation loss decreased (2.435659 --> 2.435572).  Saving model ...\n",
      "Epoch: 1974 \tTraining Loss: 2.406776 \tValidation Loss: 2.435577\n",
      "Epoch: 1975 \tTraining Loss: 2.407779 \tValidation Loss: 2.435511\n",
      "Validation loss decreased (2.435572 --> 2.435511).  Saving model ...\n",
      "Epoch: 1976 \tTraining Loss: 2.399831 \tValidation Loss: 2.435523\n",
      "Epoch: 1977 \tTraining Loss: 2.416588 \tValidation Loss: 2.435541\n",
      "Epoch: 1978 \tTraining Loss: 2.391230 \tValidation Loss: 2.435523\n",
      "Epoch: 1979 \tTraining Loss: 2.414035 \tValidation Loss: 2.435538\n",
      "Epoch: 1980 \tTraining Loss: 2.389734 \tValidation Loss: 2.435478\n",
      "Validation loss decreased (2.435511 --> 2.435478).  Saving model ...\n",
      "Epoch: 1981 \tTraining Loss: 2.386879 \tValidation Loss: 2.435382\n",
      "Validation loss decreased (2.435478 --> 2.435382).  Saving model ...\n",
      "Epoch: 1982 \tTraining Loss: 2.402553 \tValidation Loss: 2.435341\n",
      "Validation loss decreased (2.435382 --> 2.435341).  Saving model ...\n",
      "Epoch: 1983 \tTraining Loss: 2.400596 \tValidation Loss: 2.435179\n",
      "Validation loss decreased (2.435341 --> 2.435179).  Saving model ...\n",
      "Epoch: 1984 \tTraining Loss: 2.397051 \tValidation Loss: 2.435158\n",
      "Validation loss decreased (2.435179 --> 2.435158).  Saving model ...\n",
      "Epoch: 1985 \tTraining Loss: 2.398649 \tValidation Loss: 2.435113\n",
      "Validation loss decreased (2.435158 --> 2.435113).  Saving model ...\n",
      "Epoch: 1986 \tTraining Loss: 2.400899 \tValidation Loss: 2.435023\n",
      "Validation loss decreased (2.435113 --> 2.435023).  Saving model ...\n",
      "Epoch: 1987 \tTraining Loss: 2.402857 \tValidation Loss: 2.434991\n",
      "Validation loss decreased (2.435023 --> 2.434991).  Saving model ...\n",
      "Epoch: 1988 \tTraining Loss: 2.403406 \tValidation Loss: 2.434976\n",
      "Validation loss decreased (2.434991 --> 2.434976).  Saving model ...\n",
      "Epoch: 1989 \tTraining Loss: 2.399001 \tValidation Loss: 2.434941\n",
      "Validation loss decreased (2.434976 --> 2.434941).  Saving model ...\n",
      "Epoch: 1990 \tTraining Loss: 2.405691 \tValidation Loss: 2.434919\n",
      "Validation loss decreased (2.434941 --> 2.434919).  Saving model ...\n",
      "Epoch: 1991 \tTraining Loss: 2.396922 \tValidation Loss: 2.434860\n",
      "Validation loss decreased (2.434919 --> 2.434860).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1992 \tTraining Loss: 2.403718 \tValidation Loss: 2.434777\n",
      "Validation loss decreased (2.434860 --> 2.434777).  Saving model ...\n",
      "Epoch: 1993 \tTraining Loss: 2.400548 \tValidation Loss: 2.434746\n",
      "Validation loss decreased (2.434777 --> 2.434746).  Saving model ...\n",
      "Epoch: 1994 \tTraining Loss: 2.387638 \tValidation Loss: 2.434655\n",
      "Validation loss decreased (2.434746 --> 2.434655).  Saving model ...\n",
      "Epoch: 1995 \tTraining Loss: 2.407532 \tValidation Loss: 2.434686\n",
      "Epoch: 1996 \tTraining Loss: 2.396305 \tValidation Loss: 2.434566\n",
      "Validation loss decreased (2.434655 --> 2.434566).  Saving model ...\n",
      "Epoch: 1997 \tTraining Loss: 2.409297 \tValidation Loss: 2.434555\n",
      "Validation loss decreased (2.434566 --> 2.434555).  Saving model ...\n",
      "Epoch: 1998 \tTraining Loss: 2.387345 \tValidation Loss: 2.434531\n",
      "Validation loss decreased (2.434555 --> 2.434531).  Saving model ...\n",
      "Epoch: 1999 \tTraining Loss: 2.405025 \tValidation Loss: 2.434553\n",
      "Epoch: 2000 \tTraining Loss: 2.396980 \tValidation Loss: 2.434466\n",
      "Validation loss decreased (2.434531 --> 2.434466).  Saving model ...\n",
      "Epoch: 2001 \tTraining Loss: 2.407328 \tValidation Loss: 2.434453\n",
      "Validation loss decreased (2.434466 --> 2.434453).  Saving model ...\n",
      "Epoch: 2002 \tTraining Loss: 2.397481 \tValidation Loss: 2.434332\n",
      "Validation loss decreased (2.434453 --> 2.434332).  Saving model ...\n",
      "Epoch: 2003 \tTraining Loss: 2.392994 \tValidation Loss: 2.434254\n",
      "Validation loss decreased (2.434332 --> 2.434254).  Saving model ...\n",
      "Epoch: 2004 \tTraining Loss: 2.394169 \tValidation Loss: 2.434253\n",
      "Validation loss decreased (2.434254 --> 2.434253).  Saving model ...\n",
      "Epoch: 2005 \tTraining Loss: 2.409134 \tValidation Loss: 2.434191\n",
      "Validation loss decreased (2.434253 --> 2.434191).  Saving model ...\n",
      "Epoch: 2006 \tTraining Loss: 2.393055 \tValidation Loss: 2.434167\n",
      "Validation loss decreased (2.434191 --> 2.434167).  Saving model ...\n",
      "Epoch: 2007 \tTraining Loss: 2.395355 \tValidation Loss: 2.434073\n",
      "Validation loss decreased (2.434167 --> 2.434073).  Saving model ...\n",
      "Epoch: 2008 \tTraining Loss: 2.397805 \tValidation Loss: 2.434067\n",
      "Validation loss decreased (2.434073 --> 2.434067).  Saving model ...\n",
      "Epoch: 2009 \tTraining Loss: 2.404567 \tValidation Loss: 2.434031\n",
      "Validation loss decreased (2.434067 --> 2.434031).  Saving model ...\n",
      "Epoch: 2010 \tTraining Loss: 2.402576 \tValidation Loss: 2.433994\n",
      "Validation loss decreased (2.434031 --> 2.433994).  Saving model ...\n",
      "Epoch: 2011 \tTraining Loss: 2.397170 \tValidation Loss: 2.433959\n",
      "Validation loss decreased (2.433994 --> 2.433959).  Saving model ...\n",
      "Epoch: 2012 \tTraining Loss: 2.402668 \tValidation Loss: 2.433902\n",
      "Validation loss decreased (2.433959 --> 2.433902).  Saving model ...\n",
      "Epoch: 2013 \tTraining Loss: 2.391544 \tValidation Loss: 2.433814\n",
      "Validation loss decreased (2.433902 --> 2.433814).  Saving model ...\n",
      "Epoch: 2014 \tTraining Loss: 2.388746 \tValidation Loss: 2.433733\n",
      "Validation loss decreased (2.433814 --> 2.433733).  Saving model ...\n",
      "Epoch: 2015 \tTraining Loss: 2.403701 \tValidation Loss: 2.433736\n",
      "Epoch: 2016 \tTraining Loss: 2.411843 \tValidation Loss: 2.433771\n",
      "Epoch: 2017 \tTraining Loss: 2.409261 \tValidation Loss: 2.433792\n",
      "Epoch: 2018 \tTraining Loss: 2.399373 \tValidation Loss: 2.433786\n",
      "Epoch: 2019 \tTraining Loss: 2.403755 \tValidation Loss: 2.433790\n",
      "Epoch: 2020 \tTraining Loss: 2.406296 \tValidation Loss: 2.433815\n",
      "Epoch: 2021 \tTraining Loss: 2.387017 \tValidation Loss: 2.433825\n",
      "Epoch: 2022 \tTraining Loss: 2.410642 \tValidation Loss: 2.433899\n",
      "Epoch: 2023 \tTraining Loss: 2.397238 \tValidation Loss: 2.433921\n",
      "Epoch: 2024 \tTraining Loss: 2.405560 \tValidation Loss: 2.433904\n",
      "Epoch: 2025 \tTraining Loss: 2.404979 \tValidation Loss: 2.433875\n",
      "Epoch: 2026 \tTraining Loss: 2.403114 \tValidation Loss: 2.433859\n",
      "Epoch: 2027 \tTraining Loss: 2.393027 \tValidation Loss: 2.433781\n",
      "Epoch: 2028 \tTraining Loss: 2.402292 \tValidation Loss: 2.433686\n",
      "Validation loss decreased (2.433733 --> 2.433686).  Saving model ...\n",
      "Epoch: 2029 \tTraining Loss: 2.396632 \tValidation Loss: 2.433697\n",
      "Epoch: 2030 \tTraining Loss: 2.395654 \tValidation Loss: 2.433672\n",
      "Validation loss decreased (2.433686 --> 2.433672).  Saving model ...\n",
      "Epoch: 2031 \tTraining Loss: 2.399363 \tValidation Loss: 2.433594\n",
      "Validation loss decreased (2.433672 --> 2.433594).  Saving model ...\n",
      "Epoch: 2032 \tTraining Loss: 2.391760 \tValidation Loss: 2.433558\n",
      "Validation loss decreased (2.433594 --> 2.433558).  Saving model ...\n",
      "Epoch: 2033 \tTraining Loss: 2.410370 \tValidation Loss: 2.433501\n",
      "Validation loss decreased (2.433558 --> 2.433501).  Saving model ...\n",
      "Epoch: 2034 \tTraining Loss: 2.399262 \tValidation Loss: 2.433528\n",
      "Epoch: 2035 \tTraining Loss: 2.391757 \tValidation Loss: 2.433498\n",
      "Validation loss decreased (2.433501 --> 2.433498).  Saving model ...\n",
      "Epoch: 2036 \tTraining Loss: 2.388885 \tValidation Loss: 2.433497\n",
      "Validation loss decreased (2.433498 --> 2.433497).  Saving model ...\n",
      "Epoch: 2037 \tTraining Loss: 2.408366 \tValidation Loss: 2.433459\n",
      "Validation loss decreased (2.433497 --> 2.433459).  Saving model ...\n",
      "Epoch: 2038 \tTraining Loss: 2.407430 \tValidation Loss: 2.433420\n",
      "Validation loss decreased (2.433459 --> 2.433420).  Saving model ...\n",
      "Epoch: 2039 \tTraining Loss: 2.395853 \tValidation Loss: 2.433323\n",
      "Validation loss decreased (2.433420 --> 2.433323).  Saving model ...\n",
      "Epoch: 2040 \tTraining Loss: 2.406715 \tValidation Loss: 2.433296\n",
      "Validation loss decreased (2.433323 --> 2.433296).  Saving model ...\n",
      "Epoch: 2041 \tTraining Loss: 2.405321 \tValidation Loss: 2.433254\n",
      "Validation loss decreased (2.433296 --> 2.433254).  Saving model ...\n",
      "Epoch: 2042 \tTraining Loss: 2.399365 \tValidation Loss: 2.433228\n",
      "Validation loss decreased (2.433254 --> 2.433228).  Saving model ...\n",
      "Epoch: 2043 \tTraining Loss: 2.407585 \tValidation Loss: 2.433187\n",
      "Validation loss decreased (2.433228 --> 2.433187).  Saving model ...\n",
      "Epoch: 2044 \tTraining Loss: 2.407063 \tValidation Loss: 2.433197\n",
      "Epoch: 2045 \tTraining Loss: 2.389810 \tValidation Loss: 2.433178\n",
      "Validation loss decreased (2.433187 --> 2.433178).  Saving model ...\n",
      "Epoch: 2046 \tTraining Loss: 2.398148 \tValidation Loss: 2.433122\n",
      "Validation loss decreased (2.433178 --> 2.433122).  Saving model ...\n",
      "Epoch: 2047 \tTraining Loss: 2.400870 \tValidation Loss: 2.433104\n",
      "Validation loss decreased (2.433122 --> 2.433104).  Saving model ...\n",
      "Epoch: 2048 \tTraining Loss: 2.403581 \tValidation Loss: 2.433056\n",
      "Validation loss decreased (2.433104 --> 2.433056).  Saving model ...\n",
      "Epoch: 2049 \tTraining Loss: 2.397989 \tValidation Loss: 2.433058\n",
      "Epoch: 2050 \tTraining Loss: 2.397657 \tValidation Loss: 2.432947\n",
      "Validation loss decreased (2.433056 --> 2.432947).  Saving model ...\n",
      "Epoch: 2051 \tTraining Loss: 2.389822 \tValidation Loss: 2.432855\n",
      "Validation loss decreased (2.432947 --> 2.432855).  Saving model ...\n",
      "Epoch: 2052 \tTraining Loss: 2.400174 \tValidation Loss: 2.432881\n",
      "Epoch: 2053 \tTraining Loss: 2.401431 \tValidation Loss: 2.432862\n",
      "Epoch: 2054 \tTraining Loss: 2.400324 \tValidation Loss: 2.432801\n",
      "Validation loss decreased (2.432855 --> 2.432801).  Saving model ...\n",
      "Epoch: 2055 \tTraining Loss: 2.385582 \tValidation Loss: 2.432758\n",
      "Validation loss decreased (2.432801 --> 2.432758).  Saving model ...\n",
      "Epoch: 2056 \tTraining Loss: 2.392291 \tValidation Loss: 2.432722\n",
      "Validation loss decreased (2.432758 --> 2.432722).  Saving model ...\n",
      "Epoch: 2057 \tTraining Loss: 2.407197 \tValidation Loss: 2.432749\n",
      "Epoch: 2058 \tTraining Loss: 2.401405 \tValidation Loss: 2.432693\n",
      "Validation loss decreased (2.432722 --> 2.432693).  Saving model ...\n",
      "Epoch: 2059 \tTraining Loss: 2.404629 \tValidation Loss: 2.432665\n",
      "Validation loss decreased (2.432693 --> 2.432665).  Saving model ...\n",
      "Epoch: 2060 \tTraining Loss: 2.404433 \tValidation Loss: 2.432621\n",
      "Validation loss decreased (2.432665 --> 2.432621).  Saving model ...\n",
      "Epoch: 2061 \tTraining Loss: 2.396387 \tValidation Loss: 2.432621\n",
      "Epoch: 2062 \tTraining Loss: 2.383604 \tValidation Loss: 2.432539\n",
      "Validation loss decreased (2.432621 --> 2.432539).  Saving model ...\n",
      "Epoch: 2063 \tTraining Loss: 2.394268 \tValidation Loss: 2.432464\n",
      "Validation loss decreased (2.432539 --> 2.432464).  Saving model ...\n",
      "Epoch: 2064 \tTraining Loss: 2.391491 \tValidation Loss: 2.432384\n",
      "Validation loss decreased (2.432464 --> 2.432384).  Saving model ...\n",
      "Epoch: 2065 \tTraining Loss: 2.399761 \tValidation Loss: 2.432344\n",
      "Validation loss decreased (2.432384 --> 2.432344).  Saving model ...\n",
      "Epoch: 2066 \tTraining Loss: 2.398800 \tValidation Loss: 2.432291\n",
      "Validation loss decreased (2.432344 --> 2.432291).  Saving model ...\n",
      "Epoch: 2067 \tTraining Loss: 2.405787 \tValidation Loss: 2.432270\n",
      "Validation loss decreased (2.432291 --> 2.432270).  Saving model ...\n",
      "Epoch: 2068 \tTraining Loss: 2.400372 \tValidation Loss: 2.432224\n",
      "Validation loss decreased (2.432270 --> 2.432224).  Saving model ...\n",
      "Epoch: 2069 \tTraining Loss: 2.402754 \tValidation Loss: 2.432248\n",
      "Epoch: 2070 \tTraining Loss: 2.386976 \tValidation Loss: 2.432267\n",
      "Epoch: 2071 \tTraining Loss: 2.403110 \tValidation Loss: 2.432226\n",
      "Epoch: 2072 \tTraining Loss: 2.387538 \tValidation Loss: 2.432075\n",
      "Validation loss decreased (2.432224 --> 2.432075).  Saving model ...\n",
      "Epoch: 2073 \tTraining Loss: 2.403361 \tValidation Loss: 2.432086\n",
      "Epoch: 2074 \tTraining Loss: 2.399517 \tValidation Loss: 2.432097\n",
      "Epoch: 2075 \tTraining Loss: 2.408422 \tValidation Loss: 2.432204\n",
      "Epoch: 2076 \tTraining Loss: 2.410821 \tValidation Loss: 2.432271\n",
      "Epoch: 2077 \tTraining Loss: 2.390546 \tValidation Loss: 2.432204\n",
      "Epoch: 2078 \tTraining Loss: 2.387520 \tValidation Loss: 2.432130\n",
      "Epoch: 2079 \tTraining Loss: 2.395114 \tValidation Loss: 2.432095\n",
      "Epoch: 2080 \tTraining Loss: 2.394473 \tValidation Loss: 2.432052\n",
      "Validation loss decreased (2.432075 --> 2.432052).  Saving model ...\n",
      "Epoch: 2081 \tTraining Loss: 2.398959 \tValidation Loss: 2.432024\n",
      "Validation loss decreased (2.432052 --> 2.432024).  Saving model ...\n",
      "Epoch: 2082 \tTraining Loss: 2.402937 \tValidation Loss: 2.432017\n",
      "Validation loss decreased (2.432024 --> 2.432017).  Saving model ...\n",
      "Epoch: 2083 \tTraining Loss: 2.404363 \tValidation Loss: 2.432057\n",
      "Epoch: 2084 \tTraining Loss: 2.395541 \tValidation Loss: 2.432030\n",
      "Epoch: 2085 \tTraining Loss: 2.405185 \tValidation Loss: 2.432069\n",
      "Epoch: 2086 \tTraining Loss: 2.406220 \tValidation Loss: 2.432092\n",
      "Epoch: 2087 \tTraining Loss: 2.395981 \tValidation Loss: 2.432114\n",
      "Epoch: 2088 \tTraining Loss: 2.402531 \tValidation Loss: 2.432048\n",
      "Epoch: 2089 \tTraining Loss: 2.397198 \tValidation Loss: 2.432035\n",
      "Epoch: 2090 \tTraining Loss: 2.387000 \tValidation Loss: 2.431945\n",
      "Validation loss decreased (2.432017 --> 2.431945).  Saving model ...\n",
      "Epoch: 2091 \tTraining Loss: 2.394759 \tValidation Loss: 2.431959\n",
      "Epoch: 2092 \tTraining Loss: 2.384088 \tValidation Loss: 2.431834\n",
      "Validation loss decreased (2.431945 --> 2.431834).  Saving model ...\n",
      "Epoch: 2093 \tTraining Loss: 2.397482 \tValidation Loss: 2.431814\n",
      "Validation loss decreased (2.431834 --> 2.431814).  Saving model ...\n",
      "Epoch: 2094 \tTraining Loss: 2.397392 \tValidation Loss: 2.431812\n",
      "Validation loss decreased (2.431814 --> 2.431812).  Saving model ...\n",
      "Epoch: 2095 \tTraining Loss: 2.391466 \tValidation Loss: 2.431736\n",
      "Validation loss decreased (2.431812 --> 2.431736).  Saving model ...\n",
      "Epoch: 2096 \tTraining Loss: 2.407586 \tValidation Loss: 2.431727\n",
      "Validation loss decreased (2.431736 --> 2.431727).  Saving model ...\n",
      "Epoch: 2097 \tTraining Loss: 2.398089 \tValidation Loss: 2.431722\n",
      "Validation loss decreased (2.431727 --> 2.431722).  Saving model ...\n",
      "Epoch: 2098 \tTraining Loss: 2.391190 \tValidation Loss: 2.431627\n",
      "Validation loss decreased (2.431722 --> 2.431627).  Saving model ...\n",
      "Epoch: 2099 \tTraining Loss: 2.402884 \tValidation Loss: 2.431614\n",
      "Validation loss decreased (2.431627 --> 2.431614).  Saving model ...\n",
      "Epoch: 2100 \tTraining Loss: 2.404387 \tValidation Loss: 2.431550\n",
      "Validation loss decreased (2.431614 --> 2.431550).  Saving model ...\n",
      "Epoch: 2101 \tTraining Loss: 2.393019 \tValidation Loss: 2.431453\n",
      "Validation loss decreased (2.431550 --> 2.431453).  Saving model ...\n",
      "Epoch: 2102 \tTraining Loss: 2.399520 \tValidation Loss: 2.431479\n",
      "Epoch: 2103 \tTraining Loss: 2.394935 \tValidation Loss: 2.431427\n",
      "Validation loss decreased (2.431453 --> 2.431427).  Saving model ...\n",
      "Epoch: 2104 \tTraining Loss: 2.394694 \tValidation Loss: 2.431448\n",
      "Epoch: 2105 \tTraining Loss: 2.398219 \tValidation Loss: 2.431445\n",
      "Epoch: 2106 \tTraining Loss: 2.398638 \tValidation Loss: 2.431440\n",
      "Epoch: 2107 \tTraining Loss: 2.402092 \tValidation Loss: 2.431318\n",
      "Validation loss decreased (2.431427 --> 2.431318).  Saving model ...\n",
      "Epoch: 2108 \tTraining Loss: 2.399918 \tValidation Loss: 2.431278\n",
      "Validation loss decreased (2.431318 --> 2.431278).  Saving model ...\n",
      "Epoch: 2109 \tTraining Loss: 2.398031 \tValidation Loss: 2.431270\n",
      "Validation loss decreased (2.431278 --> 2.431270).  Saving model ...\n",
      "Epoch: 2110 \tTraining Loss: 2.399433 \tValidation Loss: 2.431217\n",
      "Validation loss decreased (2.431270 --> 2.431217).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2111 \tTraining Loss: 2.387886 \tValidation Loss: 2.431151\n",
      "Validation loss decreased (2.431217 --> 2.431151).  Saving model ...\n",
      "Epoch: 2112 \tTraining Loss: 2.407115 \tValidation Loss: 2.431166\n",
      "Epoch: 2113 \tTraining Loss: 2.402954 \tValidation Loss: 2.431096\n",
      "Validation loss decreased (2.431151 --> 2.431096).  Saving model ...\n",
      "Epoch: 2114 \tTraining Loss: 2.397868 \tValidation Loss: 2.431089\n",
      "Validation loss decreased (2.431096 --> 2.431089).  Saving model ...\n",
      "Epoch: 2115 \tTraining Loss: 2.391845 \tValidation Loss: 2.431050\n",
      "Validation loss decreased (2.431089 --> 2.431050).  Saving model ...\n",
      "Epoch: 2116 \tTraining Loss: 2.384086 \tValidation Loss: 2.430998\n",
      "Validation loss decreased (2.431050 --> 2.430998).  Saving model ...\n",
      "Epoch: 2117 \tTraining Loss: 2.392456 \tValidation Loss: 2.431044\n",
      "Epoch: 2118 \tTraining Loss: 2.397926 \tValidation Loss: 2.430994\n",
      "Validation loss decreased (2.430998 --> 2.430994).  Saving model ...\n",
      "Epoch: 2119 \tTraining Loss: 2.400994 \tValidation Loss: 2.431019\n",
      "Epoch: 2120 \tTraining Loss: 2.389675 \tValidation Loss: 2.431021\n",
      "Epoch: 2121 \tTraining Loss: 2.390479 \tValidation Loss: 2.430979\n",
      "Validation loss decreased (2.430994 --> 2.430979).  Saving model ...\n",
      "Epoch: 2122 \tTraining Loss: 2.404354 \tValidation Loss: 2.430997\n",
      "Epoch: 2123 \tTraining Loss: 2.396600 \tValidation Loss: 2.431009\n",
      "Epoch: 2124 \tTraining Loss: 2.399622 \tValidation Loss: 2.430992\n",
      "Epoch: 2125 \tTraining Loss: 2.401051 \tValidation Loss: 2.430974\n",
      "Validation loss decreased (2.430979 --> 2.430974).  Saving model ...\n",
      "Epoch: 2126 \tTraining Loss: 2.393848 \tValidation Loss: 2.430886\n",
      "Validation loss decreased (2.430974 --> 2.430886).  Saving model ...\n",
      "Epoch: 2127 \tTraining Loss: 2.387919 \tValidation Loss: 2.430772\n",
      "Validation loss decreased (2.430886 --> 2.430772).  Saving model ...\n",
      "Epoch: 2128 \tTraining Loss: 2.407592 \tValidation Loss: 2.430857\n",
      "Epoch: 2129 \tTraining Loss: 2.385556 \tValidation Loss: 2.430799\n",
      "Epoch: 2130 \tTraining Loss: 2.396183 \tValidation Loss: 2.430738\n",
      "Validation loss decreased (2.430772 --> 2.430738).  Saving model ...\n",
      "Epoch: 2131 \tTraining Loss: 2.400045 \tValidation Loss: 2.430722\n",
      "Validation loss decreased (2.430738 --> 2.430722).  Saving model ...\n",
      "Epoch: 2132 \tTraining Loss: 2.380636 \tValidation Loss: 2.430650\n",
      "Validation loss decreased (2.430722 --> 2.430650).  Saving model ...\n",
      "Epoch: 2133 \tTraining Loss: 2.403513 \tValidation Loss: 2.430665\n",
      "Epoch: 2134 \tTraining Loss: 2.388571 \tValidation Loss: 2.430593\n",
      "Validation loss decreased (2.430650 --> 2.430593).  Saving model ...\n",
      "Epoch: 2135 \tTraining Loss: 2.400520 \tValidation Loss: 2.430593\n",
      "Epoch: 2136 \tTraining Loss: 2.388015 \tValidation Loss: 2.430569\n",
      "Validation loss decreased (2.430593 --> 2.430569).  Saving model ...\n",
      "Epoch: 2137 \tTraining Loss: 2.389543 \tValidation Loss: 2.430474\n",
      "Validation loss decreased (2.430569 --> 2.430474).  Saving model ...\n",
      "Epoch: 2138 \tTraining Loss: 2.392886 \tValidation Loss: 2.430443\n",
      "Validation loss decreased (2.430474 --> 2.430443).  Saving model ...\n",
      "Epoch: 2139 \tTraining Loss: 2.405823 \tValidation Loss: 2.430420\n",
      "Validation loss decreased (2.430443 --> 2.430420).  Saving model ...\n",
      "Epoch: 2140 \tTraining Loss: 2.392622 \tValidation Loss: 2.430371\n",
      "Validation loss decreased (2.430420 --> 2.430371).  Saving model ...\n",
      "Epoch: 2141 \tTraining Loss: 2.398391 \tValidation Loss: 2.430345\n",
      "Validation loss decreased (2.430371 --> 2.430345).  Saving model ...\n",
      "Epoch: 2142 \tTraining Loss: 2.403259 \tValidation Loss: 2.430417\n",
      "Epoch: 2143 \tTraining Loss: 2.399628 \tValidation Loss: 2.430379\n",
      "Epoch: 2144 \tTraining Loss: 2.409733 \tValidation Loss: 2.430331\n",
      "Validation loss decreased (2.430345 --> 2.430331).  Saving model ...\n",
      "Epoch: 2145 \tTraining Loss: 2.405071 \tValidation Loss: 2.430370\n",
      "Epoch: 2146 \tTraining Loss: 2.385322 \tValidation Loss: 2.430363\n",
      "Epoch: 2147 \tTraining Loss: 2.396279 \tValidation Loss: 2.430377\n",
      "Epoch: 2148 \tTraining Loss: 2.399016 \tValidation Loss: 2.430326\n",
      "Validation loss decreased (2.430331 --> 2.430326).  Saving model ...\n",
      "Epoch: 2149 \tTraining Loss: 2.400565 \tValidation Loss: 2.430215\n",
      "Validation loss decreased (2.430326 --> 2.430215).  Saving model ...\n",
      "Epoch: 2150 \tTraining Loss: 2.396813 \tValidation Loss: 2.430264\n",
      "Epoch: 2151 \tTraining Loss: 2.393842 \tValidation Loss: 2.430211\n",
      "Validation loss decreased (2.430215 --> 2.430211).  Saving model ...\n",
      "Epoch: 2152 \tTraining Loss: 2.393811 \tValidation Loss: 2.430210\n",
      "Validation loss decreased (2.430211 --> 2.430210).  Saving model ...\n",
      "Epoch: 2153 \tTraining Loss: 2.385554 \tValidation Loss: 2.430191\n",
      "Validation loss decreased (2.430210 --> 2.430191).  Saving model ...\n",
      "Epoch: 2154 \tTraining Loss: 2.402018 \tValidation Loss: 2.430173\n",
      "Validation loss decreased (2.430191 --> 2.430173).  Saving model ...\n",
      "Epoch: 2155 \tTraining Loss: 2.405957 \tValidation Loss: 2.430148\n",
      "Validation loss decreased (2.430173 --> 2.430148).  Saving model ...\n",
      "Epoch: 2156 \tTraining Loss: 2.390859 \tValidation Loss: 2.430133\n",
      "Validation loss decreased (2.430148 --> 2.430133).  Saving model ...\n",
      "Epoch: 2157 \tTraining Loss: 2.408195 \tValidation Loss: 2.430129\n",
      "Validation loss decreased (2.430133 --> 2.430129).  Saving model ...\n",
      "Epoch: 2158 \tTraining Loss: 2.391082 \tValidation Loss: 2.430067\n",
      "Validation loss decreased (2.430129 --> 2.430067).  Saving model ...\n",
      "Epoch: 2159 \tTraining Loss: 2.404431 \tValidation Loss: 2.430065\n",
      "Validation loss decreased (2.430067 --> 2.430065).  Saving model ...\n",
      "Epoch: 2160 \tTraining Loss: 2.397206 \tValidation Loss: 2.430028\n",
      "Validation loss decreased (2.430065 --> 2.430028).  Saving model ...\n",
      "Epoch: 2161 \tTraining Loss: 2.405486 \tValidation Loss: 2.430063\n",
      "Epoch: 2162 \tTraining Loss: 2.388504 \tValidation Loss: 2.429979\n",
      "Validation loss decreased (2.430028 --> 2.429979).  Saving model ...\n",
      "Epoch: 2163 \tTraining Loss: 2.403487 \tValidation Loss: 2.429983\n",
      "Epoch: 2164 \tTraining Loss: 2.390929 \tValidation Loss: 2.429965\n",
      "Validation loss decreased (2.429979 --> 2.429965).  Saving model ...\n",
      "Epoch: 2165 \tTraining Loss: 2.402481 \tValidation Loss: 2.429907\n",
      "Validation loss decreased (2.429965 --> 2.429907).  Saving model ...\n",
      "Epoch: 2166 \tTraining Loss: 2.398072 \tValidation Loss: 2.429872\n",
      "Validation loss decreased (2.429907 --> 2.429872).  Saving model ...\n",
      "Epoch: 2167 \tTraining Loss: 2.401128 \tValidation Loss: 2.429885\n",
      "Epoch: 2168 \tTraining Loss: 2.408256 \tValidation Loss: 2.429954\n",
      "Epoch: 2169 \tTraining Loss: 2.396504 \tValidation Loss: 2.429920\n",
      "Epoch: 2170 \tTraining Loss: 2.399273 \tValidation Loss: 2.429898\n",
      "Epoch: 2171 \tTraining Loss: 2.388646 \tValidation Loss: 2.429851\n",
      "Validation loss decreased (2.429872 --> 2.429851).  Saving model ...\n",
      "Epoch: 2172 \tTraining Loss: 2.395376 \tValidation Loss: 2.429856\n",
      "Epoch: 2173 \tTraining Loss: 2.395037 \tValidation Loss: 2.429810\n",
      "Validation loss decreased (2.429851 --> 2.429810).  Saving model ...\n",
      "Epoch: 2174 \tTraining Loss: 2.389247 \tValidation Loss: 2.429761\n",
      "Validation loss decreased (2.429810 --> 2.429761).  Saving model ...\n",
      "Epoch: 2175 \tTraining Loss: 2.400130 \tValidation Loss: 2.429718\n",
      "Validation loss decreased (2.429761 --> 2.429718).  Saving model ...\n",
      "Epoch: 2176 \tTraining Loss: 2.394012 \tValidation Loss: 2.429695\n",
      "Validation loss decreased (2.429718 --> 2.429695).  Saving model ...\n",
      "Epoch: 2177 \tTraining Loss: 2.394348 \tValidation Loss: 2.429641\n",
      "Validation loss decreased (2.429695 --> 2.429641).  Saving model ...\n",
      "Epoch: 2178 \tTraining Loss: 2.389882 \tValidation Loss: 2.429670\n",
      "Epoch: 2179 \tTraining Loss: 2.397173 \tValidation Loss: 2.429676\n",
      "Epoch: 2180 \tTraining Loss: 2.389605 \tValidation Loss: 2.429591\n",
      "Validation loss decreased (2.429641 --> 2.429591).  Saving model ...\n",
      "Epoch: 2181 \tTraining Loss: 2.393153 \tValidation Loss: 2.429588\n",
      "Validation loss decreased (2.429591 --> 2.429588).  Saving model ...\n",
      "Epoch: 2182 \tTraining Loss: 2.398068 \tValidation Loss: 2.429506\n",
      "Validation loss decreased (2.429588 --> 2.429506).  Saving model ...\n",
      "Epoch: 2183 \tTraining Loss: 2.393013 \tValidation Loss: 2.429415\n",
      "Validation loss decreased (2.429506 --> 2.429415).  Saving model ...\n",
      "Epoch: 2184 \tTraining Loss: 2.393448 \tValidation Loss: 2.429443\n",
      "Epoch: 2185 \tTraining Loss: 2.385057 \tValidation Loss: 2.429360\n",
      "Validation loss decreased (2.429415 --> 2.429360).  Saving model ...\n",
      "Epoch: 2186 \tTraining Loss: 2.394907 \tValidation Loss: 2.429413\n",
      "Epoch: 2187 \tTraining Loss: 2.385076 \tValidation Loss: 2.429440\n",
      "Epoch: 2188 \tTraining Loss: 2.402336 \tValidation Loss: 2.429533\n",
      "Epoch: 2189 \tTraining Loss: 2.399481 \tValidation Loss: 2.429427\n",
      "Epoch: 2190 \tTraining Loss: 2.387179 \tValidation Loss: 2.429404\n",
      "Epoch: 2191 \tTraining Loss: 2.397212 \tValidation Loss: 2.429333\n",
      "Validation loss decreased (2.429360 --> 2.429333).  Saving model ...\n",
      "Epoch: 2192 \tTraining Loss: 2.387010 \tValidation Loss: 2.429291\n",
      "Validation loss decreased (2.429333 --> 2.429291).  Saving model ...\n",
      "Epoch: 2193 \tTraining Loss: 2.387726 \tValidation Loss: 2.429255\n",
      "Validation loss decreased (2.429291 --> 2.429255).  Saving model ...\n",
      "Epoch: 2194 \tTraining Loss: 2.406487 \tValidation Loss: 2.429199\n",
      "Validation loss decreased (2.429255 --> 2.429199).  Saving model ...\n",
      "Epoch: 2195 \tTraining Loss: 2.394011 \tValidation Loss: 2.429195\n",
      "Validation loss decreased (2.429199 --> 2.429195).  Saving model ...\n",
      "Epoch: 2196 \tTraining Loss: 2.377082 \tValidation Loss: 2.429120\n",
      "Validation loss decreased (2.429195 --> 2.429120).  Saving model ...\n",
      "Epoch: 2197 \tTraining Loss: 2.383573 \tValidation Loss: 2.429053\n",
      "Validation loss decreased (2.429120 --> 2.429053).  Saving model ...\n",
      "Epoch: 2198 \tTraining Loss: 2.399491 \tValidation Loss: 2.429003\n",
      "Validation loss decreased (2.429053 --> 2.429003).  Saving model ...\n",
      "Epoch: 2199 \tTraining Loss: 2.395548 \tValidation Loss: 2.428947\n",
      "Validation loss decreased (2.429003 --> 2.428947).  Saving model ...\n",
      "Epoch: 2200 \tTraining Loss: 2.389869 \tValidation Loss: 2.428875\n",
      "Validation loss decreased (2.428947 --> 2.428875).  Saving model ...\n",
      "Epoch: 2201 \tTraining Loss: 2.393555 \tValidation Loss: 2.428879\n",
      "Epoch: 2202 \tTraining Loss: 2.390412 \tValidation Loss: 2.428902\n",
      "Epoch: 2203 \tTraining Loss: 2.400088 \tValidation Loss: 2.428880\n",
      "Epoch: 2204 \tTraining Loss: 2.416011 \tValidation Loss: 2.428874\n",
      "Validation loss decreased (2.428875 --> 2.428874).  Saving model ...\n",
      "Epoch: 2205 \tTraining Loss: 2.387994 \tValidation Loss: 2.428865\n",
      "Validation loss decreased (2.428874 --> 2.428865).  Saving model ...\n",
      "Epoch: 2206 \tTraining Loss: 2.403926 \tValidation Loss: 2.428886\n",
      "Epoch: 2207 \tTraining Loss: 2.383785 \tValidation Loss: 2.428820\n",
      "Validation loss decreased (2.428865 --> 2.428820).  Saving model ...\n",
      "Epoch: 2208 \tTraining Loss: 2.386530 \tValidation Loss: 2.428806\n",
      "Validation loss decreased (2.428820 --> 2.428806).  Saving model ...\n",
      "Epoch: 2209 \tTraining Loss: 2.391892 \tValidation Loss: 2.428782\n",
      "Validation loss decreased (2.428806 --> 2.428782).  Saving model ...\n",
      "Epoch: 2210 \tTraining Loss: 2.387617 \tValidation Loss: 2.428744\n",
      "Validation loss decreased (2.428782 --> 2.428744).  Saving model ...\n",
      "Epoch: 2211 \tTraining Loss: 2.392993 \tValidation Loss: 2.428738\n",
      "Validation loss decreased (2.428744 --> 2.428738).  Saving model ...\n",
      "Epoch: 2212 \tTraining Loss: 2.390929 \tValidation Loss: 2.428632\n",
      "Validation loss decreased (2.428738 --> 2.428632).  Saving model ...\n",
      "Epoch: 2213 \tTraining Loss: 2.392713 \tValidation Loss: 2.428636\n",
      "Epoch: 2214 \tTraining Loss: 2.382680 \tValidation Loss: 2.428568\n",
      "Validation loss decreased (2.428632 --> 2.428568).  Saving model ...\n",
      "Epoch: 2215 \tTraining Loss: 2.397167 \tValidation Loss: 2.428581\n",
      "Epoch: 2216 \tTraining Loss: 2.398782 \tValidation Loss: 2.428577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2217 \tTraining Loss: 2.398546 \tValidation Loss: 2.428577\n",
      "Epoch: 2218 \tTraining Loss: 2.397646 \tValidation Loss: 2.428546\n",
      "Validation loss decreased (2.428568 --> 2.428546).  Saving model ...\n",
      "Epoch: 2219 \tTraining Loss: 2.390102 \tValidation Loss: 2.428503\n",
      "Validation loss decreased (2.428546 --> 2.428503).  Saving model ...\n",
      "Epoch: 2220 \tTraining Loss: 2.393304 \tValidation Loss: 2.428470\n",
      "Validation loss decreased (2.428503 --> 2.428470).  Saving model ...\n",
      "Epoch: 2221 \tTraining Loss: 2.397597 \tValidation Loss: 2.428458\n",
      "Validation loss decreased (2.428470 --> 2.428458).  Saving model ...\n",
      "Epoch: 2222 \tTraining Loss: 2.396160 \tValidation Loss: 2.428406\n",
      "Validation loss decreased (2.428458 --> 2.428406).  Saving model ...\n",
      "Epoch: 2223 \tTraining Loss: 2.393072 \tValidation Loss: 2.428359\n",
      "Validation loss decreased (2.428406 --> 2.428359).  Saving model ...\n",
      "Epoch: 2224 \tTraining Loss: 2.402079 \tValidation Loss: 2.428356\n",
      "Validation loss decreased (2.428359 --> 2.428356).  Saving model ...\n",
      "Epoch: 2225 \tTraining Loss: 2.395480 \tValidation Loss: 2.428391\n",
      "Epoch: 2226 \tTraining Loss: 2.404236 \tValidation Loss: 2.428447\n",
      "Epoch: 2227 \tTraining Loss: 2.397431 \tValidation Loss: 2.428402\n",
      "Epoch: 2228 \tTraining Loss: 2.397610 \tValidation Loss: 2.428387\n",
      "Epoch: 2229 \tTraining Loss: 2.390314 \tValidation Loss: 2.428345\n",
      "Validation loss decreased (2.428356 --> 2.428345).  Saving model ...\n",
      "Epoch: 2230 \tTraining Loss: 2.395673 \tValidation Loss: 2.428329\n",
      "Validation loss decreased (2.428345 --> 2.428329).  Saving model ...\n",
      "Epoch: 2231 \tTraining Loss: 2.388866 \tValidation Loss: 2.428223\n",
      "Validation loss decreased (2.428329 --> 2.428223).  Saving model ...\n",
      "Epoch: 2232 \tTraining Loss: 2.378438 \tValidation Loss: 2.428174\n",
      "Validation loss decreased (2.428223 --> 2.428174).  Saving model ...\n",
      "Epoch: 2233 \tTraining Loss: 2.392187 \tValidation Loss: 2.428154\n",
      "Validation loss decreased (2.428174 --> 2.428154).  Saving model ...\n",
      "Epoch: 2234 \tTraining Loss: 2.376797 \tValidation Loss: 2.428073\n",
      "Validation loss decreased (2.428154 --> 2.428073).  Saving model ...\n",
      "Epoch: 2235 \tTraining Loss: 2.398835 \tValidation Loss: 2.428077\n",
      "Epoch: 2236 \tTraining Loss: 2.389627 \tValidation Loss: 2.428015\n",
      "Validation loss decreased (2.428073 --> 2.428015).  Saving model ...\n",
      "Epoch: 2237 \tTraining Loss: 2.401237 \tValidation Loss: 2.428009\n",
      "Validation loss decreased (2.428015 --> 2.428009).  Saving model ...\n",
      "Epoch: 2238 \tTraining Loss: 2.375065 \tValidation Loss: 2.427971\n",
      "Validation loss decreased (2.428009 --> 2.427971).  Saving model ...\n",
      "Epoch: 2239 \tTraining Loss: 2.394255 \tValidation Loss: 2.427981\n",
      "Epoch: 2240 \tTraining Loss: 2.393817 \tValidation Loss: 2.428012\n",
      "Epoch: 2241 \tTraining Loss: 2.388413 \tValidation Loss: 2.427962\n",
      "Validation loss decreased (2.427971 --> 2.427962).  Saving model ...\n",
      "Epoch: 2242 \tTraining Loss: 2.394276 \tValidation Loss: 2.428027\n",
      "Epoch: 2243 \tTraining Loss: 2.393201 \tValidation Loss: 2.427997\n",
      "Epoch: 2244 \tTraining Loss: 2.387959 \tValidation Loss: 2.427979\n",
      "Epoch: 2245 \tTraining Loss: 2.384385 \tValidation Loss: 2.427943\n",
      "Validation loss decreased (2.427962 --> 2.427943).  Saving model ...\n",
      "Epoch: 2246 \tTraining Loss: 2.387650 \tValidation Loss: 2.427842\n",
      "Validation loss decreased (2.427943 --> 2.427842).  Saving model ...\n",
      "Epoch: 2247 \tTraining Loss: 2.404597 \tValidation Loss: 2.427793\n",
      "Validation loss decreased (2.427842 --> 2.427793).  Saving model ...\n",
      "Epoch: 2248 \tTraining Loss: 2.388708 \tValidation Loss: 2.427774\n",
      "Validation loss decreased (2.427793 --> 2.427774).  Saving model ...\n",
      "Epoch: 2249 \tTraining Loss: 2.398943 \tValidation Loss: 2.427771\n",
      "Validation loss decreased (2.427774 --> 2.427771).  Saving model ...\n",
      "Epoch: 2250 \tTraining Loss: 2.389449 \tValidation Loss: 2.427722\n",
      "Validation loss decreased (2.427771 --> 2.427722).  Saving model ...\n",
      "Epoch: 2251 \tTraining Loss: 2.392901 \tValidation Loss: 2.427627\n",
      "Validation loss decreased (2.427722 --> 2.427627).  Saving model ...\n",
      "Epoch: 2252 \tTraining Loss: 2.390236 \tValidation Loss: 2.427598\n",
      "Validation loss decreased (2.427627 --> 2.427598).  Saving model ...\n",
      "Epoch: 2253 \tTraining Loss: 2.398894 \tValidation Loss: 2.427588\n",
      "Validation loss decreased (2.427598 --> 2.427588).  Saving model ...\n",
      "Epoch: 2254 \tTraining Loss: 2.388224 \tValidation Loss: 2.427491\n",
      "Validation loss decreased (2.427588 --> 2.427491).  Saving model ...\n",
      "Epoch: 2255 \tTraining Loss: 2.382366 \tValidation Loss: 2.427459\n",
      "Validation loss decreased (2.427491 --> 2.427459).  Saving model ...\n",
      "Epoch: 2256 \tTraining Loss: 2.398539 \tValidation Loss: 2.427495\n",
      "Epoch: 2257 \tTraining Loss: 2.394097 \tValidation Loss: 2.427436\n",
      "Validation loss decreased (2.427459 --> 2.427436).  Saving model ...\n",
      "Epoch: 2258 \tTraining Loss: 2.396192 \tValidation Loss: 2.427359\n",
      "Validation loss decreased (2.427436 --> 2.427359).  Saving model ...\n",
      "Epoch: 2259 \tTraining Loss: 2.391841 \tValidation Loss: 2.427342\n",
      "Validation loss decreased (2.427359 --> 2.427342).  Saving model ...\n",
      "Epoch: 2260 \tTraining Loss: 2.393483 \tValidation Loss: 2.427326\n",
      "Validation loss decreased (2.427342 --> 2.427326).  Saving model ...\n",
      "Epoch: 2261 \tTraining Loss: 2.387521 \tValidation Loss: 2.427263\n",
      "Validation loss decreased (2.427326 --> 2.427263).  Saving model ...\n",
      "Epoch: 2262 \tTraining Loss: 2.389368 \tValidation Loss: 2.427175\n",
      "Validation loss decreased (2.427263 --> 2.427175).  Saving model ...\n",
      "Epoch: 2263 \tTraining Loss: 2.391600 \tValidation Loss: 2.427123\n",
      "Validation loss decreased (2.427175 --> 2.427123).  Saving model ...\n",
      "Epoch: 2264 \tTraining Loss: 2.390828 \tValidation Loss: 2.427096\n",
      "Validation loss decreased (2.427123 --> 2.427096).  Saving model ...\n",
      "Epoch: 2265 \tTraining Loss: 2.384068 \tValidation Loss: 2.427073\n",
      "Validation loss decreased (2.427096 --> 2.427073).  Saving model ...\n",
      "Epoch: 2266 \tTraining Loss: 2.386301 \tValidation Loss: 2.427133\n",
      "Epoch: 2267 \tTraining Loss: 2.385210 \tValidation Loss: 2.427071\n",
      "Validation loss decreased (2.427073 --> 2.427071).  Saving model ...\n",
      "Epoch: 2268 \tTraining Loss: 2.399827 \tValidation Loss: 2.427089\n",
      "Epoch: 2269 \tTraining Loss: 2.396413 \tValidation Loss: 2.427071\n",
      "Epoch: 2270 \tTraining Loss: 2.387001 \tValidation Loss: 2.427022\n",
      "Validation loss decreased (2.427071 --> 2.427022).  Saving model ...\n",
      "Epoch: 2271 \tTraining Loss: 2.388919 \tValidation Loss: 2.426986\n",
      "Validation loss decreased (2.427022 --> 2.426986).  Saving model ...\n",
      "Epoch: 2272 \tTraining Loss: 2.393245 \tValidation Loss: 2.427015\n",
      "Epoch: 2273 \tTraining Loss: 2.396003 \tValidation Loss: 2.426970\n",
      "Validation loss decreased (2.426986 --> 2.426970).  Saving model ...\n",
      "Epoch: 2274 \tTraining Loss: 2.393139 \tValidation Loss: 2.426896\n",
      "Validation loss decreased (2.426970 --> 2.426896).  Saving model ...\n",
      "Epoch: 2275 \tTraining Loss: 2.393326 \tValidation Loss: 2.426965\n",
      "Epoch: 2276 \tTraining Loss: 2.390165 \tValidation Loss: 2.426953\n",
      "Epoch: 2277 \tTraining Loss: 2.393038 \tValidation Loss: 2.426915\n",
      "Epoch: 2278 \tTraining Loss: 2.397322 \tValidation Loss: 2.426811\n",
      "Validation loss decreased (2.426896 --> 2.426811).  Saving model ...\n",
      "Epoch: 2279 \tTraining Loss: 2.389362 \tValidation Loss: 2.426782\n",
      "Validation loss decreased (2.426811 --> 2.426782).  Saving model ...\n",
      "Epoch: 2280 \tTraining Loss: 2.386455 \tValidation Loss: 2.426802\n",
      "Epoch: 2281 \tTraining Loss: 2.373491 \tValidation Loss: 2.426712\n",
      "Validation loss decreased (2.426782 --> 2.426712).  Saving model ...\n",
      "Epoch: 2282 \tTraining Loss: 2.387717 \tValidation Loss: 2.426673\n",
      "Validation loss decreased (2.426712 --> 2.426673).  Saving model ...\n",
      "Epoch: 2283 \tTraining Loss: 2.382726 \tValidation Loss: 2.426568\n",
      "Validation loss decreased (2.426673 --> 2.426568).  Saving model ...\n",
      "Epoch: 2284 \tTraining Loss: 2.395525 \tValidation Loss: 2.426567\n",
      "Validation loss decreased (2.426568 --> 2.426567).  Saving model ...\n",
      "Epoch: 2285 \tTraining Loss: 2.404147 \tValidation Loss: 2.426615\n",
      "Epoch: 2286 \tTraining Loss: 2.387443 \tValidation Loss: 2.426598\n",
      "Epoch: 2287 \tTraining Loss: 2.394078 \tValidation Loss: 2.426599\n",
      "Epoch: 2288 \tTraining Loss: 2.387401 \tValidation Loss: 2.426526\n",
      "Validation loss decreased (2.426567 --> 2.426526).  Saving model ...\n",
      "Epoch: 2289 \tTraining Loss: 2.388736 \tValidation Loss: 2.426448\n",
      "Validation loss decreased (2.426526 --> 2.426448).  Saving model ...\n",
      "Epoch: 2290 \tTraining Loss: 2.392438 \tValidation Loss: 2.426395\n",
      "Validation loss decreased (2.426448 --> 2.426395).  Saving model ...\n",
      "Epoch: 2291 \tTraining Loss: 2.389618 \tValidation Loss: 2.426343\n",
      "Validation loss decreased (2.426395 --> 2.426343).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2292 \tTraining Loss: 2.383492 \tValidation Loss: 2.426340\n",
      "Validation loss decreased (2.426343 --> 2.426340).  Saving model ...\n",
      "Epoch: 2293 \tTraining Loss: 2.379946 \tValidation Loss: 2.426301\n",
      "Validation loss decreased (2.426340 --> 2.426301).  Saving model ...\n",
      "Epoch: 2294 \tTraining Loss: 2.388370 \tValidation Loss: 2.426202\n",
      "Validation loss decreased (2.426301 --> 2.426202).  Saving model ...\n",
      "Epoch: 2295 \tTraining Loss: 2.385951 \tValidation Loss: 2.426114\n",
      "Validation loss decreased (2.426202 --> 2.426114).  Saving model ...\n",
      "Epoch: 2296 \tTraining Loss: 2.387930 \tValidation Loss: 2.425985\n",
      "Validation loss decreased (2.426114 --> 2.425985).  Saving model ...\n",
      "Epoch: 2297 \tTraining Loss: 2.395307 \tValidation Loss: 2.425942\n",
      "Validation loss decreased (2.425985 --> 2.425942).  Saving model ...\n",
      "Epoch: 2298 \tTraining Loss: 2.381319 \tValidation Loss: 2.425833\n",
      "Validation loss decreased (2.425942 --> 2.425833).  Saving model ...\n",
      "Epoch: 2299 \tTraining Loss: 2.394507 \tValidation Loss: 2.425862\n",
      "Epoch: 2300 \tTraining Loss: 2.385784 \tValidation Loss: 2.425803\n",
      "Validation loss decreased (2.425833 --> 2.425803).  Saving model ...\n",
      "Epoch: 2301 \tTraining Loss: 2.378933 \tValidation Loss: 2.425781\n",
      "Validation loss decreased (2.425803 --> 2.425781).  Saving model ...\n",
      "Epoch: 2302 \tTraining Loss: 2.383218 \tValidation Loss: 2.425830\n",
      "Epoch: 2303 \tTraining Loss: 2.383833 \tValidation Loss: 2.425818\n",
      "Epoch: 2304 \tTraining Loss: 2.391693 \tValidation Loss: 2.425836\n",
      "Epoch: 2305 \tTraining Loss: 2.383295 \tValidation Loss: 2.425766\n",
      "Validation loss decreased (2.425781 --> 2.425766).  Saving model ...\n",
      "Epoch: 2306 \tTraining Loss: 2.394148 \tValidation Loss: 2.425775\n",
      "Epoch: 2307 \tTraining Loss: 2.392797 \tValidation Loss: 2.425731\n",
      "Validation loss decreased (2.425766 --> 2.425731).  Saving model ...\n",
      "Epoch: 2308 \tTraining Loss: 2.392462 \tValidation Loss: 2.425771\n",
      "Epoch: 2309 \tTraining Loss: 2.380521 \tValidation Loss: 2.425722\n",
      "Validation loss decreased (2.425731 --> 2.425722).  Saving model ...\n",
      "Epoch: 2310 \tTraining Loss: 2.388896 \tValidation Loss: 2.425831\n",
      "Epoch: 2311 \tTraining Loss: 2.380754 \tValidation Loss: 2.425814\n",
      "Epoch: 2312 \tTraining Loss: 2.387505 \tValidation Loss: 2.425773\n",
      "Epoch: 2313 \tTraining Loss: 2.387069 \tValidation Loss: 2.425727\n",
      "Epoch: 2314 \tTraining Loss: 2.382485 \tValidation Loss: 2.425715\n",
      "Validation loss decreased (2.425722 --> 2.425715).  Saving model ...\n",
      "Epoch: 2315 \tTraining Loss: 2.379328 \tValidation Loss: 2.425696\n",
      "Validation loss decreased (2.425715 --> 2.425696).  Saving model ...\n",
      "Epoch: 2316 \tTraining Loss: 2.383327 \tValidation Loss: 2.425686\n",
      "Validation loss decreased (2.425696 --> 2.425686).  Saving model ...\n",
      "Epoch: 2317 \tTraining Loss: 2.392298 \tValidation Loss: 2.425716\n",
      "Epoch: 2318 \tTraining Loss: 2.386813 \tValidation Loss: 2.425681\n",
      "Validation loss decreased (2.425686 --> 2.425681).  Saving model ...\n",
      "Epoch: 2319 \tTraining Loss: 2.393864 \tValidation Loss: 2.425685\n",
      "Epoch: 2320 \tTraining Loss: 2.385236 \tValidation Loss: 2.425631\n",
      "Validation loss decreased (2.425681 --> 2.425631).  Saving model ...\n",
      "Epoch: 2321 \tTraining Loss: 2.378165 \tValidation Loss: 2.425563\n",
      "Validation loss decreased (2.425631 --> 2.425563).  Saving model ...\n",
      "Epoch: 2322 \tTraining Loss: 2.385710 \tValidation Loss: 2.425566\n",
      "Epoch: 2323 \tTraining Loss: 2.394373 \tValidation Loss: 2.425642\n",
      "Epoch: 2324 \tTraining Loss: 2.398238 \tValidation Loss: 2.425645\n",
      "Epoch: 2325 \tTraining Loss: 2.381476 \tValidation Loss: 2.425666\n",
      "Epoch: 2326 \tTraining Loss: 2.386722 \tValidation Loss: 2.425686\n",
      "Epoch: 2327 \tTraining Loss: 2.395229 \tValidation Loss: 2.425705\n",
      "Epoch: 2328 \tTraining Loss: 2.398283 \tValidation Loss: 2.425684\n",
      "Epoch: 2329 \tTraining Loss: 2.390129 \tValidation Loss: 2.425686\n",
      "Epoch: 2330 \tTraining Loss: 2.397565 \tValidation Loss: 2.425668\n",
      "Epoch: 2331 \tTraining Loss: 2.396526 \tValidation Loss: 2.425605\n",
      "Epoch: 2332 \tTraining Loss: 2.382378 \tValidation Loss: 2.425580\n",
      "Epoch: 2333 \tTraining Loss: 2.390631 \tValidation Loss: 2.425524\n",
      "Validation loss decreased (2.425563 --> 2.425524).  Saving model ...\n",
      "Epoch: 2334 \tTraining Loss: 2.377470 \tValidation Loss: 2.425427\n",
      "Validation loss decreased (2.425524 --> 2.425427).  Saving model ...\n",
      "Epoch: 2335 \tTraining Loss: 2.377368 \tValidation Loss: 2.425364\n",
      "Validation loss decreased (2.425427 --> 2.425364).  Saving model ...\n",
      "Epoch: 2336 \tTraining Loss: 2.395671 \tValidation Loss: 2.425346\n",
      "Validation loss decreased (2.425364 --> 2.425346).  Saving model ...\n",
      "Epoch: 2337 \tTraining Loss: 2.380999 \tValidation Loss: 2.425232\n",
      "Validation loss decreased (2.425346 --> 2.425232).  Saving model ...\n",
      "Epoch: 2338 \tTraining Loss: 2.399439 \tValidation Loss: 2.425195\n",
      "Validation loss decreased (2.425232 --> 2.425195).  Saving model ...\n",
      "Epoch: 2339 \tTraining Loss: 2.390664 \tValidation Loss: 2.425197\n",
      "Epoch: 2340 \tTraining Loss: 2.377522 \tValidation Loss: 2.425208\n",
      "Epoch: 2341 \tTraining Loss: 2.399845 \tValidation Loss: 2.425267\n",
      "Epoch: 2342 \tTraining Loss: 2.393725 \tValidation Loss: 2.425361\n",
      "Epoch: 2343 \tTraining Loss: 2.377312 \tValidation Loss: 2.425343\n",
      "Epoch: 2344 \tTraining Loss: 2.380428 \tValidation Loss: 2.425281\n",
      "Epoch: 2345 \tTraining Loss: 2.376286 \tValidation Loss: 2.425276\n",
      "Epoch: 2346 \tTraining Loss: 2.384211 \tValidation Loss: 2.425281\n",
      "Epoch: 2347 \tTraining Loss: 2.393382 \tValidation Loss: 2.425231\n",
      "Epoch: 2348 \tTraining Loss: 2.400441 \tValidation Loss: 2.425328\n",
      "Epoch: 2349 \tTraining Loss: 2.390791 \tValidation Loss: 2.425293\n",
      "Epoch: 2350 \tTraining Loss: 2.399438 \tValidation Loss: 2.425201\n",
      "Epoch: 2351 \tTraining Loss: 2.382157 \tValidation Loss: 2.425218\n",
      "Epoch: 2352 \tTraining Loss: 2.388875 \tValidation Loss: 2.425134\n",
      "Validation loss decreased (2.425195 --> 2.425134).  Saving model ...\n",
      "Epoch: 2353 \tTraining Loss: 2.392103 \tValidation Loss: 2.425104\n",
      "Validation loss decreased (2.425134 --> 2.425104).  Saving model ...\n",
      "Epoch: 2354 \tTraining Loss: 2.392480 \tValidation Loss: 2.425089\n",
      "Validation loss decreased (2.425104 --> 2.425089).  Saving model ...\n",
      "Epoch: 2355 \tTraining Loss: 2.382694 \tValidation Loss: 2.425044\n",
      "Validation loss decreased (2.425089 --> 2.425044).  Saving model ...\n",
      "Epoch: 2356 \tTraining Loss: 2.386888 \tValidation Loss: 2.425083\n",
      "Epoch: 2357 \tTraining Loss: 2.385255 \tValidation Loss: 2.425107\n",
      "Epoch: 2358 \tTraining Loss: 2.382964 \tValidation Loss: 2.425084\n",
      "Epoch: 2359 \tTraining Loss: 2.372523 \tValidation Loss: 2.425054\n",
      "Epoch: 2360 \tTraining Loss: 2.389037 \tValidation Loss: 2.425016\n",
      "Validation loss decreased (2.425044 --> 2.425016).  Saving model ...\n",
      "Epoch: 2361 \tTraining Loss: 2.380665 \tValidation Loss: 2.425016\n",
      "Validation loss decreased (2.425016 --> 2.425016).  Saving model ...\n",
      "Epoch: 2362 \tTraining Loss: 2.383068 \tValidation Loss: 2.424959\n",
      "Validation loss decreased (2.425016 --> 2.424959).  Saving model ...\n",
      "Epoch: 2363 \tTraining Loss: 2.389288 \tValidation Loss: 2.424908\n",
      "Validation loss decreased (2.424959 --> 2.424908).  Saving model ...\n",
      "Epoch: 2364 \tTraining Loss: 2.383757 \tValidation Loss: 2.424818\n",
      "Validation loss decreased (2.424908 --> 2.424818).  Saving model ...\n",
      "Epoch: 2365 \tTraining Loss: 2.388360 \tValidation Loss: 2.424803\n",
      "Validation loss decreased (2.424818 --> 2.424803).  Saving model ...\n",
      "Epoch: 2366 \tTraining Loss: 2.388678 \tValidation Loss: 2.424683\n",
      "Validation loss decreased (2.424803 --> 2.424683).  Saving model ...\n",
      "Epoch: 2367 \tTraining Loss: 2.387253 \tValidation Loss: 2.424674\n",
      "Validation loss decreased (2.424683 --> 2.424674).  Saving model ...\n",
      "Epoch: 2368 \tTraining Loss: 2.392831 \tValidation Loss: 2.424692\n",
      "Epoch: 2369 \tTraining Loss: 2.380252 \tValidation Loss: 2.424637\n",
      "Validation loss decreased (2.424674 --> 2.424637).  Saving model ...\n",
      "Epoch: 2370 \tTraining Loss: 2.386559 \tValidation Loss: 2.424737\n",
      "Epoch: 2371 \tTraining Loss: 2.388408 \tValidation Loss: 2.424723\n",
      "Epoch: 2372 \tTraining Loss: 2.372553 \tValidation Loss: 2.424680\n",
      "Epoch: 2373 \tTraining Loss: 2.388094 \tValidation Loss: 2.424633\n",
      "Validation loss decreased (2.424637 --> 2.424633).  Saving model ...\n",
      "Epoch: 2374 \tTraining Loss: 2.385954 \tValidation Loss: 2.424605\n",
      "Validation loss decreased (2.424633 --> 2.424605).  Saving model ...\n",
      "Epoch: 2375 \tTraining Loss: 2.393285 \tValidation Loss: 2.424591\n",
      "Validation loss decreased (2.424605 --> 2.424591).  Saving model ...\n",
      "Epoch: 2376 \tTraining Loss: 2.375350 \tValidation Loss: 2.424433\n",
      "Validation loss decreased (2.424591 --> 2.424433).  Saving model ...\n",
      "Epoch: 2377 \tTraining Loss: 2.381795 \tValidation Loss: 2.424361\n",
      "Validation loss decreased (2.424433 --> 2.424361).  Saving model ...\n",
      "Epoch: 2378 \tTraining Loss: 2.372854 \tValidation Loss: 2.424346\n",
      "Validation loss decreased (2.424361 --> 2.424346).  Saving model ...\n",
      "Epoch: 2379 \tTraining Loss: 2.386490 \tValidation Loss: 2.424308\n",
      "Validation loss decreased (2.424346 --> 2.424308).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2380 \tTraining Loss: 2.384989 \tValidation Loss: 2.424295\n",
      "Validation loss decreased (2.424308 --> 2.424295).  Saving model ...\n",
      "Epoch: 2381 \tTraining Loss: 2.392448 \tValidation Loss: 2.424276\n",
      "Validation loss decreased (2.424295 --> 2.424276).  Saving model ...\n",
      "Epoch: 2382 \tTraining Loss: 2.386548 \tValidation Loss: 2.424202\n",
      "Validation loss decreased (2.424276 --> 2.424202).  Saving model ...\n",
      "Epoch: 2383 \tTraining Loss: 2.382600 \tValidation Loss: 2.424131\n",
      "Validation loss decreased (2.424202 --> 2.424131).  Saving model ...\n",
      "Epoch: 2384 \tTraining Loss: 2.382530 \tValidation Loss: 2.424130\n",
      "Validation loss decreased (2.424131 --> 2.424130).  Saving model ...\n",
      "Epoch: 2385 \tTraining Loss: 2.388391 \tValidation Loss: 2.424094\n",
      "Validation loss decreased (2.424130 --> 2.424094).  Saving model ...\n",
      "Epoch: 2386 \tTraining Loss: 2.394786 \tValidation Loss: 2.424100\n",
      "Epoch: 2387 \tTraining Loss: 2.374378 \tValidation Loss: 2.424083\n",
      "Validation loss decreased (2.424094 --> 2.424083).  Saving model ...\n",
      "Epoch: 2388 \tTraining Loss: 2.383840 \tValidation Loss: 2.423955\n",
      "Validation loss decreased (2.424083 --> 2.423955).  Saving model ...\n",
      "Epoch: 2389 \tTraining Loss: 2.386228 \tValidation Loss: 2.423932\n",
      "Validation loss decreased (2.423955 --> 2.423932).  Saving model ...\n",
      "Epoch: 2390 \tTraining Loss: 2.387302 \tValidation Loss: 2.423923\n",
      "Validation loss decreased (2.423932 --> 2.423923).  Saving model ...\n",
      "Epoch: 2391 \tTraining Loss: 2.375034 \tValidation Loss: 2.423820\n",
      "Validation loss decreased (2.423923 --> 2.423820).  Saving model ...\n",
      "Epoch: 2392 \tTraining Loss: 2.390801 \tValidation Loss: 2.423826\n",
      "Epoch: 2393 \tTraining Loss: 2.389985 \tValidation Loss: 2.423846\n",
      "Epoch: 2394 \tTraining Loss: 2.386214 \tValidation Loss: 2.423793\n",
      "Validation loss decreased (2.423820 --> 2.423793).  Saving model ...\n",
      "Epoch: 2395 \tTraining Loss: 2.380708 \tValidation Loss: 2.423764\n",
      "Validation loss decreased (2.423793 --> 2.423764).  Saving model ...\n",
      "Epoch: 2396 \tTraining Loss: 2.389972 \tValidation Loss: 2.423712\n",
      "Validation loss decreased (2.423764 --> 2.423712).  Saving model ...\n",
      "Epoch: 2397 \tTraining Loss: 2.382910 \tValidation Loss: 2.423674\n",
      "Validation loss decreased (2.423712 --> 2.423674).  Saving model ...\n",
      "Epoch: 2398 \tTraining Loss: 2.381152 \tValidation Loss: 2.423715\n",
      "Epoch: 2399 \tTraining Loss: 2.380490 \tValidation Loss: 2.423661\n",
      "Validation loss decreased (2.423674 --> 2.423661).  Saving model ...\n",
      "Epoch: 2400 \tTraining Loss: 2.397989 \tValidation Loss: 2.423670\n",
      "Epoch: 2401 \tTraining Loss: 2.378940 \tValidation Loss: 2.423656\n",
      "Validation loss decreased (2.423661 --> 2.423656).  Saving model ...\n",
      "Epoch: 2402 \tTraining Loss: 2.383197 \tValidation Loss: 2.423635\n",
      "Validation loss decreased (2.423656 --> 2.423635).  Saving model ...\n",
      "Epoch: 2403 \tTraining Loss: 2.384674 \tValidation Loss: 2.423579\n",
      "Validation loss decreased (2.423635 --> 2.423579).  Saving model ...\n",
      "Epoch: 2404 \tTraining Loss: 2.381832 \tValidation Loss: 2.423501\n",
      "Validation loss decreased (2.423579 --> 2.423501).  Saving model ...\n",
      "Epoch: 2405 \tTraining Loss: 2.392247 \tValidation Loss: 2.423497\n",
      "Validation loss decreased (2.423501 --> 2.423497).  Saving model ...\n",
      "Epoch: 2406 \tTraining Loss: 2.375603 \tValidation Loss: 2.423423\n",
      "Validation loss decreased (2.423497 --> 2.423423).  Saving model ...\n",
      "Epoch: 2407 \tTraining Loss: 2.380681 \tValidation Loss: 2.423396\n",
      "Validation loss decreased (2.423423 --> 2.423396).  Saving model ...\n",
      "Epoch: 2408 \tTraining Loss: 2.382807 \tValidation Loss: 2.423309\n",
      "Validation loss decreased (2.423396 --> 2.423309).  Saving model ...\n",
      "Epoch: 2409 \tTraining Loss: 2.375427 \tValidation Loss: 2.423203\n",
      "Validation loss decreased (2.423309 --> 2.423203).  Saving model ...\n",
      "Epoch: 2410 \tTraining Loss: 2.389078 \tValidation Loss: 2.423220\n",
      "Epoch: 2411 \tTraining Loss: 2.380295 \tValidation Loss: 2.423147\n",
      "Validation loss decreased (2.423203 --> 2.423147).  Saving model ...\n",
      "Epoch: 2412 \tTraining Loss: 2.373101 \tValidation Loss: 2.423097\n",
      "Validation loss decreased (2.423147 --> 2.423097).  Saving model ...\n",
      "Epoch: 2413 \tTraining Loss: 2.378016 \tValidation Loss: 2.423050\n",
      "Validation loss decreased (2.423097 --> 2.423050).  Saving model ...\n",
      "Epoch: 2414 \tTraining Loss: 2.384141 \tValidation Loss: 2.422940\n",
      "Validation loss decreased (2.423050 --> 2.422940).  Saving model ...\n",
      "Epoch: 2415 \tTraining Loss: 2.375557 \tValidation Loss: 2.422885\n",
      "Validation loss decreased (2.422940 --> 2.422885).  Saving model ...\n",
      "Epoch: 2416 \tTraining Loss: 2.370225 \tValidation Loss: 2.422850\n",
      "Validation loss decreased (2.422885 --> 2.422850).  Saving model ...\n",
      "Epoch: 2417 \tTraining Loss: 2.383387 \tValidation Loss: 2.422844\n",
      "Validation loss decreased (2.422850 --> 2.422844).  Saving model ...\n",
      "Epoch: 2418 \tTraining Loss: 2.386715 \tValidation Loss: 2.422883\n",
      "Epoch: 2419 \tTraining Loss: 2.372461 \tValidation Loss: 2.422828\n",
      "Validation loss decreased (2.422844 --> 2.422828).  Saving model ...\n",
      "Epoch: 2420 \tTraining Loss: 2.399621 \tValidation Loss: 2.422861\n",
      "Epoch: 2421 \tTraining Loss: 2.384057 \tValidation Loss: 2.422872\n",
      "Epoch: 2422 \tTraining Loss: 2.383969 \tValidation Loss: 2.422887\n",
      "Epoch: 2423 \tTraining Loss: 2.387290 \tValidation Loss: 2.422834\n",
      "Epoch: 2424 \tTraining Loss: 2.383643 \tValidation Loss: 2.422864\n",
      "Epoch: 2425 \tTraining Loss: 2.384367 \tValidation Loss: 2.422750\n",
      "Validation loss decreased (2.422828 --> 2.422750).  Saving model ...\n",
      "Epoch: 2426 \tTraining Loss: 2.384151 \tValidation Loss: 2.422671\n",
      "Validation loss decreased (2.422750 --> 2.422671).  Saving model ...\n",
      "Epoch: 2427 \tTraining Loss: 2.387196 \tValidation Loss: 2.422646\n",
      "Validation loss decreased (2.422671 --> 2.422646).  Saving model ...\n",
      "Epoch: 2428 \tTraining Loss: 2.386216 \tValidation Loss: 2.422587\n",
      "Validation loss decreased (2.422646 --> 2.422587).  Saving model ...\n",
      "Epoch: 2429 \tTraining Loss: 2.385641 \tValidation Loss: 2.422529\n",
      "Validation loss decreased (2.422587 --> 2.422529).  Saving model ...\n",
      "Epoch: 2430 \tTraining Loss: 2.392463 \tValidation Loss: 2.422640\n",
      "Epoch: 2431 \tTraining Loss: 2.398065 \tValidation Loss: 2.422696\n",
      "Epoch: 2432 \tTraining Loss: 2.396048 \tValidation Loss: 2.422725\n",
      "Epoch: 2433 \tTraining Loss: 2.385282 \tValidation Loss: 2.422676\n",
      "Epoch: 2434 \tTraining Loss: 2.384644 \tValidation Loss: 2.422710\n",
      "Epoch: 2435 \tTraining Loss: 2.384505 \tValidation Loss: 2.422794\n",
      "Epoch: 2436 \tTraining Loss: 2.378261 \tValidation Loss: 2.422734\n",
      "Epoch: 2437 \tTraining Loss: 2.380844 \tValidation Loss: 2.422649\n",
      "Epoch: 2438 \tTraining Loss: 2.366396 \tValidation Loss: 2.422555\n",
      "Epoch: 2439 \tTraining Loss: 2.376806 \tValidation Loss: 2.422464\n",
      "Validation loss decreased (2.422529 --> 2.422464).  Saving model ...\n",
      "Epoch: 2440 \tTraining Loss: 2.381587 \tValidation Loss: 2.422414\n",
      "Validation loss decreased (2.422464 --> 2.422414).  Saving model ...\n",
      "Epoch: 2441 \tTraining Loss: 2.373633 \tValidation Loss: 2.422345\n",
      "Validation loss decreased (2.422414 --> 2.422345).  Saving model ...\n",
      "Epoch: 2442 \tTraining Loss: 2.376412 \tValidation Loss: 2.422344\n",
      "Validation loss decreased (2.422345 --> 2.422344).  Saving model ...\n",
      "Epoch: 2443 \tTraining Loss: 2.370916 \tValidation Loss: 2.422209\n",
      "Validation loss decreased (2.422344 --> 2.422209).  Saving model ...\n",
      "Epoch: 2444 \tTraining Loss: 2.374395 \tValidation Loss: 2.422170\n",
      "Validation loss decreased (2.422209 --> 2.422170).  Saving model ...\n",
      "Epoch: 2445 \tTraining Loss: 2.385185 \tValidation Loss: 2.422129\n",
      "Validation loss decreased (2.422170 --> 2.422129).  Saving model ...\n",
      "Epoch: 2446 \tTraining Loss: 2.378014 \tValidation Loss: 2.422116\n",
      "Validation loss decreased (2.422129 --> 2.422116).  Saving model ...\n",
      "Epoch: 2447 \tTraining Loss: 2.392056 \tValidation Loss: 2.422197\n",
      "Epoch: 2448 \tTraining Loss: 2.378304 \tValidation Loss: 2.422152\n",
      "Epoch: 2449 \tTraining Loss: 2.376469 \tValidation Loss: 2.422133\n",
      "Epoch: 2450 \tTraining Loss: 2.380824 \tValidation Loss: 2.422116\n",
      "Validation loss decreased (2.422116 --> 2.422116).  Saving model ...\n",
      "Epoch: 2451 \tTraining Loss: 2.382529 \tValidation Loss: 2.422114\n",
      "Validation loss decreased (2.422116 --> 2.422114).  Saving model ...\n",
      "Epoch: 2452 \tTraining Loss: 2.377868 \tValidation Loss: 2.422080\n",
      "Validation loss decreased (2.422114 --> 2.422080).  Saving model ...\n",
      "Epoch: 2453 \tTraining Loss: 2.371408 \tValidation Loss: 2.422017\n",
      "Validation loss decreased (2.422080 --> 2.422017).  Saving model ...\n",
      "Epoch: 2454 \tTraining Loss: 2.376647 \tValidation Loss: 2.421978\n",
      "Validation loss decreased (2.422017 --> 2.421978).  Saving model ...\n",
      "Epoch: 2455 \tTraining Loss: 2.379460 \tValidation Loss: 2.421979\n",
      "Epoch: 2456 \tTraining Loss: 2.377458 \tValidation Loss: 2.421961\n",
      "Validation loss decreased (2.421978 --> 2.421961).  Saving model ...\n",
      "Epoch: 2457 \tTraining Loss: 2.393516 \tValidation Loss: 2.422047\n",
      "Epoch: 2458 \tTraining Loss: 2.377523 \tValidation Loss: 2.422012\n",
      "Epoch: 2459 \tTraining Loss: 2.382165 \tValidation Loss: 2.421967\n",
      "Epoch: 2460 \tTraining Loss: 2.372568 \tValidation Loss: 2.421926\n",
      "Validation loss decreased (2.421961 --> 2.421926).  Saving model ...\n",
      "Epoch: 2461 \tTraining Loss: 2.381483 \tValidation Loss: 2.421950\n",
      "Epoch: 2462 \tTraining Loss: 2.381998 \tValidation Loss: 2.421956\n",
      "Epoch: 2463 \tTraining Loss: 2.382046 \tValidation Loss: 2.421982\n",
      "Epoch: 2464 \tTraining Loss: 2.372300 \tValidation Loss: 2.421942\n",
      "Epoch: 2465 \tTraining Loss: 2.375228 \tValidation Loss: 2.421815\n",
      "Validation loss decreased (2.421926 --> 2.421815).  Saving model ...\n",
      "Epoch: 2466 \tTraining Loss: 2.380883 \tValidation Loss: 2.421743\n",
      "Validation loss decreased (2.421815 --> 2.421743).  Saving model ...\n",
      "Epoch: 2467 \tTraining Loss: 2.371787 \tValidation Loss: 2.421692\n",
      "Validation loss decreased (2.421743 --> 2.421692).  Saving model ...\n",
      "Epoch: 2468 \tTraining Loss: 2.369832 \tValidation Loss: 2.421603\n",
      "Validation loss decreased (2.421692 --> 2.421603).  Saving model ...\n",
      "Epoch: 2469 \tTraining Loss: 2.390767 \tValidation Loss: 2.421592\n",
      "Validation loss decreased (2.421603 --> 2.421592).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2470 \tTraining Loss: 2.390922 \tValidation Loss: 2.421599\n",
      "Epoch: 2471 \tTraining Loss: 2.382353 \tValidation Loss: 2.421634\n",
      "Epoch: 2472 \tTraining Loss: 2.383011 \tValidation Loss: 2.421623\n",
      "Epoch: 2473 \tTraining Loss: 2.374009 \tValidation Loss: 2.421572\n",
      "Validation loss decreased (2.421592 --> 2.421572).  Saving model ...\n",
      "Epoch: 2474 \tTraining Loss: 2.370466 \tValidation Loss: 2.421503\n",
      "Validation loss decreased (2.421572 --> 2.421503).  Saving model ...\n",
      "Epoch: 2475 \tTraining Loss: 2.367765 \tValidation Loss: 2.421498\n",
      "Validation loss decreased (2.421503 --> 2.421498).  Saving model ...\n",
      "Epoch: 2476 \tTraining Loss: 2.389551 \tValidation Loss: 2.421458\n",
      "Validation loss decreased (2.421498 --> 2.421458).  Saving model ...\n",
      "Epoch: 2477 \tTraining Loss: 2.373113 \tValidation Loss: 2.421410\n",
      "Validation loss decreased (2.421458 --> 2.421410).  Saving model ...\n",
      "Epoch: 2478 \tTraining Loss: 2.379876 \tValidation Loss: 2.421422\n",
      "Epoch: 2479 \tTraining Loss: 2.390485 \tValidation Loss: 2.421405\n",
      "Validation loss decreased (2.421410 --> 2.421405).  Saving model ...\n",
      "Epoch: 2480 \tTraining Loss: 2.399876 \tValidation Loss: 2.421457\n",
      "Epoch: 2481 \tTraining Loss: 2.385331 \tValidation Loss: 2.421433\n",
      "Epoch: 2482 \tTraining Loss: 2.382197 \tValidation Loss: 2.421351\n",
      "Validation loss decreased (2.421405 --> 2.421351).  Saving model ...\n",
      "Epoch: 2483 \tTraining Loss: 2.364190 \tValidation Loss: 2.421301\n",
      "Validation loss decreased (2.421351 --> 2.421301).  Saving model ...\n",
      "Epoch: 2484 \tTraining Loss: 2.379161 \tValidation Loss: 2.421268\n",
      "Validation loss decreased (2.421301 --> 2.421268).  Saving model ...\n",
      "Epoch: 2485 \tTraining Loss: 2.370597 \tValidation Loss: 2.421218\n",
      "Validation loss decreased (2.421268 --> 2.421218).  Saving model ...\n",
      "Epoch: 2486 \tTraining Loss: 2.378456 \tValidation Loss: 2.421181\n",
      "Validation loss decreased (2.421218 --> 2.421181).  Saving model ...\n",
      "Epoch: 2487 \tTraining Loss: 2.390308 \tValidation Loss: 2.421216\n",
      "Epoch: 2488 \tTraining Loss: 2.384779 \tValidation Loss: 2.421194\n",
      "Epoch: 2489 \tTraining Loss: 2.386750 \tValidation Loss: 2.421163\n",
      "Validation loss decreased (2.421181 --> 2.421163).  Saving model ...\n",
      "Epoch: 2490 \tTraining Loss: 2.377255 \tValidation Loss: 2.421134\n",
      "Validation loss decreased (2.421163 --> 2.421134).  Saving model ...\n",
      "Epoch: 2491 \tTraining Loss: 2.378260 \tValidation Loss: 2.421096\n",
      "Validation loss decreased (2.421134 --> 2.421096).  Saving model ...\n",
      "Epoch: 2492 \tTraining Loss: 2.374977 \tValidation Loss: 2.421046\n",
      "Validation loss decreased (2.421096 --> 2.421046).  Saving model ...\n",
      "Epoch: 2493 \tTraining Loss: 2.380566 \tValidation Loss: 2.421087\n",
      "Epoch: 2494 \tTraining Loss: 2.380368 \tValidation Loss: 2.421086\n",
      "Epoch: 2495 \tTraining Loss: 2.374046 \tValidation Loss: 2.421058\n",
      "Epoch: 2496 \tTraining Loss: 2.376356 \tValidation Loss: 2.420980\n",
      "Validation loss decreased (2.421046 --> 2.420980).  Saving model ...\n",
      "Epoch: 2497 \tTraining Loss: 2.385758 \tValidation Loss: 2.420972\n",
      "Validation loss decreased (2.420980 --> 2.420972).  Saving model ...\n",
      "Epoch: 2498 \tTraining Loss: 2.374638 \tValidation Loss: 2.420887\n",
      "Validation loss decreased (2.420972 --> 2.420887).  Saving model ...\n",
      "Epoch: 2499 \tTraining Loss: 2.381045 \tValidation Loss: 2.420892\n",
      "Epoch: 2500 \tTraining Loss: 2.379884 \tValidation Loss: 2.420788\n",
      "Validation loss decreased (2.420887 --> 2.420788).  Saving model ...\n",
      "Epoch: 2501 \tTraining Loss: 2.384056 \tValidation Loss: 2.420796\n",
      "Epoch: 2502 \tTraining Loss: 2.384580 \tValidation Loss: 2.420835\n",
      "Epoch: 2503 \tTraining Loss: 2.370482 \tValidation Loss: 2.420843\n",
      "Epoch: 2504 \tTraining Loss: 2.375964 \tValidation Loss: 2.420823\n",
      "Epoch: 2505 \tTraining Loss: 2.382598 \tValidation Loss: 2.420832\n",
      "Epoch: 2506 \tTraining Loss: 2.389155 \tValidation Loss: 2.420777\n",
      "Validation loss decreased (2.420788 --> 2.420777).  Saving model ...\n",
      "Epoch: 2507 \tTraining Loss: 2.367763 \tValidation Loss: 2.420781\n",
      "Epoch: 2508 \tTraining Loss: 2.382175 \tValidation Loss: 2.420841\n",
      "Epoch: 2509 \tTraining Loss: 2.380070 \tValidation Loss: 2.420824\n",
      "Epoch: 2510 \tTraining Loss: 2.373257 \tValidation Loss: 2.420778\n",
      "Epoch: 2511 \tTraining Loss: 2.387132 \tValidation Loss: 2.420746\n",
      "Validation loss decreased (2.420777 --> 2.420746).  Saving model ...\n",
      "Epoch: 2512 \tTraining Loss: 2.386548 \tValidation Loss: 2.420729\n",
      "Validation loss decreased (2.420746 --> 2.420729).  Saving model ...\n",
      "Epoch: 2513 \tTraining Loss: 2.372607 \tValidation Loss: 2.420696\n",
      "Validation loss decreased (2.420729 --> 2.420696).  Saving model ...\n",
      "Epoch: 2514 \tTraining Loss: 2.394206 \tValidation Loss: 2.420690\n",
      "Validation loss decreased (2.420696 --> 2.420690).  Saving model ...\n",
      "Epoch: 2515 \tTraining Loss: 2.383849 \tValidation Loss: 2.420677\n",
      "Validation loss decreased (2.420690 --> 2.420677).  Saving model ...\n",
      "Epoch: 2516 \tTraining Loss: 2.388175 \tValidation Loss: 2.420627\n",
      "Validation loss decreased (2.420677 --> 2.420627).  Saving model ...\n",
      "Epoch: 2517 \tTraining Loss: 2.384050 \tValidation Loss: 2.420667\n",
      "Epoch: 2518 \tTraining Loss: 2.361741 \tValidation Loss: 2.420561\n",
      "Validation loss decreased (2.420627 --> 2.420561).  Saving model ...\n",
      "Epoch: 2519 \tTraining Loss: 2.365110 \tValidation Loss: 2.420533\n",
      "Validation loss decreased (2.420561 --> 2.420533).  Saving model ...\n",
      "Epoch: 2520 \tTraining Loss: 2.375001 \tValidation Loss: 2.420494\n",
      "Validation loss decreased (2.420533 --> 2.420494).  Saving model ...\n",
      "Epoch: 2521 \tTraining Loss: 2.373971 \tValidation Loss: 2.420480\n",
      "Validation loss decreased (2.420494 --> 2.420480).  Saving model ...\n",
      "Epoch: 2522 \tTraining Loss: 2.366347 \tValidation Loss: 2.420432\n",
      "Validation loss decreased (2.420480 --> 2.420432).  Saving model ...\n",
      "Epoch: 2523 \tTraining Loss: 2.381499 \tValidation Loss: 2.420444\n",
      "Epoch: 2524 \tTraining Loss: 2.381821 \tValidation Loss: 2.420365\n",
      "Validation loss decreased (2.420432 --> 2.420365).  Saving model ...\n",
      "Epoch: 2525 \tTraining Loss: 2.383494 \tValidation Loss: 2.420348\n",
      "Validation loss decreased (2.420365 --> 2.420348).  Saving model ...\n",
      "Epoch: 2526 \tTraining Loss: 2.371518 \tValidation Loss: 2.420332\n",
      "Validation loss decreased (2.420348 --> 2.420332).  Saving model ...\n",
      "Epoch: 2527 \tTraining Loss: 2.390918 \tValidation Loss: 2.420284\n",
      "Validation loss decreased (2.420332 --> 2.420284).  Saving model ...\n",
      "Epoch: 2528 \tTraining Loss: 2.382313 \tValidation Loss: 2.420328\n",
      "Epoch: 2529 \tTraining Loss: 2.375125 \tValidation Loss: 2.420285\n",
      "Epoch: 2530 \tTraining Loss: 2.369777 \tValidation Loss: 2.420219\n",
      "Validation loss decreased (2.420284 --> 2.420219).  Saving model ...\n",
      "Epoch: 2531 \tTraining Loss: 2.369648 \tValidation Loss: 2.420203\n",
      "Validation loss decreased (2.420219 --> 2.420203).  Saving model ...\n",
      "Epoch: 2532 \tTraining Loss: 2.362507 \tValidation Loss: 2.420147\n",
      "Validation loss decreased (2.420203 --> 2.420147).  Saving model ...\n",
      "Epoch: 2533 \tTraining Loss: 2.385928 \tValidation Loss: 2.420100\n",
      "Validation loss decreased (2.420147 --> 2.420100).  Saving model ...\n",
      "Epoch: 2534 \tTraining Loss: 2.373868 \tValidation Loss: 2.420123\n",
      "Epoch: 2535 \tTraining Loss: 2.370677 \tValidation Loss: 2.419981\n",
      "Validation loss decreased (2.420100 --> 2.419981).  Saving model ...\n",
      "Epoch: 2536 \tTraining Loss: 2.380913 \tValidation Loss: 2.419941\n",
      "Validation loss decreased (2.419981 --> 2.419941).  Saving model ...\n",
      "Epoch: 2537 \tTraining Loss: 2.373398 \tValidation Loss: 2.419913\n",
      "Validation loss decreased (2.419941 --> 2.419913).  Saving model ...\n",
      "Epoch: 2538 \tTraining Loss: 2.374382 \tValidation Loss: 2.419872\n",
      "Validation loss decreased (2.419913 --> 2.419872).  Saving model ...\n",
      "Epoch: 2539 \tTraining Loss: 2.381361 \tValidation Loss: 2.419890\n",
      "Epoch: 2540 \tTraining Loss: 2.382113 \tValidation Loss: 2.419933\n",
      "Epoch: 2541 \tTraining Loss: 2.373016 \tValidation Loss: 2.419873\n",
      "Epoch: 2542 \tTraining Loss: 2.376169 \tValidation Loss: 2.419901\n",
      "Epoch: 2543 \tTraining Loss: 2.372413 \tValidation Loss: 2.419864\n",
      "Validation loss decreased (2.419872 --> 2.419864).  Saving model ...\n",
      "Epoch: 2544 \tTraining Loss: 2.375769 \tValidation Loss: 2.419815\n",
      "Validation loss decreased (2.419864 --> 2.419815).  Saving model ...\n",
      "Epoch: 2545 \tTraining Loss: 2.365989 \tValidation Loss: 2.419853\n",
      "Epoch: 2546 \tTraining Loss: 2.384578 \tValidation Loss: 2.419888\n",
      "Epoch: 2547 \tTraining Loss: 2.356401 \tValidation Loss: 2.419818\n",
      "Epoch: 2548 \tTraining Loss: 2.372668 \tValidation Loss: 2.419802\n",
      "Validation loss decreased (2.419815 --> 2.419802).  Saving model ...\n",
      "Epoch: 2549 \tTraining Loss: 2.384227 \tValidation Loss: 2.419786\n",
      "Validation loss decreased (2.419802 --> 2.419786).  Saving model ...\n",
      "Epoch: 2550 \tTraining Loss: 2.372711 \tValidation Loss: 2.419761\n",
      "Validation loss decreased (2.419786 --> 2.419761).  Saving model ...\n",
      "Epoch: 2551 \tTraining Loss: 2.385908 \tValidation Loss: 2.419827\n",
      "Epoch: 2552 \tTraining Loss: 2.384075 \tValidation Loss: 2.419856\n",
      "Epoch: 2553 \tTraining Loss: 2.370629 \tValidation Loss: 2.419884\n",
      "Epoch: 2554 \tTraining Loss: 2.384168 \tValidation Loss: 2.419887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2555 \tTraining Loss: 2.379056 \tValidation Loss: 2.419879\n",
      "Epoch: 2556 \tTraining Loss: 2.386654 \tValidation Loss: 2.419849\n",
      "Epoch: 2557 \tTraining Loss: 2.375040 \tValidation Loss: 2.419755\n",
      "Validation loss decreased (2.419761 --> 2.419755).  Saving model ...\n",
      "Epoch: 2558 \tTraining Loss: 2.374057 \tValidation Loss: 2.419667\n",
      "Validation loss decreased (2.419755 --> 2.419667).  Saving model ...\n",
      "Epoch: 2559 \tTraining Loss: 2.388976 \tValidation Loss: 2.419584\n",
      "Validation loss decreased (2.419667 --> 2.419584).  Saving model ...\n",
      "Epoch: 2560 \tTraining Loss: 2.379892 \tValidation Loss: 2.419497\n",
      "Validation loss decreased (2.419584 --> 2.419497).  Saving model ...\n",
      "Epoch: 2561 \tTraining Loss: 2.384756 \tValidation Loss: 2.419510\n",
      "Epoch: 2562 \tTraining Loss: 2.383993 \tValidation Loss: 2.419577\n",
      "Epoch: 2563 \tTraining Loss: 2.379915 \tValidation Loss: 2.419574\n",
      "Epoch: 2564 \tTraining Loss: 2.374204 \tValidation Loss: 2.419598\n",
      "Epoch: 2565 \tTraining Loss: 2.391599 \tValidation Loss: 2.419601\n",
      "Epoch: 2566 \tTraining Loss: 2.366756 \tValidation Loss: 2.419530\n",
      "Epoch: 2567 \tTraining Loss: 2.375500 \tValidation Loss: 2.419368\n",
      "Validation loss decreased (2.419497 --> 2.419368).  Saving model ...\n",
      "Epoch: 2568 \tTraining Loss: 2.381191 \tValidation Loss: 2.419413\n",
      "Epoch: 2569 \tTraining Loss: 2.381611 \tValidation Loss: 2.419347\n",
      "Validation loss decreased (2.419368 --> 2.419347).  Saving model ...\n",
      "Epoch: 2570 \tTraining Loss: 2.392709 \tValidation Loss: 2.419423\n",
      "Epoch: 2571 \tTraining Loss: 2.369992 \tValidation Loss: 2.419393\n",
      "Epoch: 2572 \tTraining Loss: 2.372050 \tValidation Loss: 2.419394\n",
      "Epoch: 2573 \tTraining Loss: 2.374949 \tValidation Loss: 2.419456\n",
      "Epoch: 2574 \tTraining Loss: 2.375319 \tValidation Loss: 2.419442\n",
      "Epoch: 2575 \tTraining Loss: 2.380279 \tValidation Loss: 2.419491\n",
      "Epoch: 2576 \tTraining Loss: 2.377562 \tValidation Loss: 2.419379\n",
      "Epoch: 2577 \tTraining Loss: 2.374284 \tValidation Loss: 2.419296\n",
      "Validation loss decreased (2.419347 --> 2.419296).  Saving model ...\n",
      "Epoch: 2578 \tTraining Loss: 2.382640 \tValidation Loss: 2.419272\n",
      "Validation loss decreased (2.419296 --> 2.419272).  Saving model ...\n",
      "Epoch: 2579 \tTraining Loss: 2.389494 \tValidation Loss: 2.419287\n",
      "Epoch: 2580 \tTraining Loss: 2.380901 \tValidation Loss: 2.419333\n",
      "Epoch: 2581 \tTraining Loss: 2.372233 \tValidation Loss: 2.419255\n",
      "Validation loss decreased (2.419272 --> 2.419255).  Saving model ...\n",
      "Epoch: 2582 \tTraining Loss: 2.376043 \tValidation Loss: 2.419173\n",
      "Validation loss decreased (2.419255 --> 2.419173).  Saving model ...\n",
      "Epoch: 2583 \tTraining Loss: 2.371269 \tValidation Loss: 2.419114\n",
      "Validation loss decreased (2.419173 --> 2.419114).  Saving model ...\n",
      "Epoch: 2584 \tTraining Loss: 2.376161 \tValidation Loss: 2.419072\n",
      "Validation loss decreased (2.419114 --> 2.419072).  Saving model ...\n",
      "Epoch: 2585 \tTraining Loss: 2.363296 \tValidation Loss: 2.418943\n",
      "Validation loss decreased (2.419072 --> 2.418943).  Saving model ...\n",
      "Epoch: 2586 \tTraining Loss: 2.390558 \tValidation Loss: 2.418987\n",
      "Epoch: 2587 \tTraining Loss: 2.372546 \tValidation Loss: 2.419010\n",
      "Epoch: 2588 \tTraining Loss: 2.377266 \tValidation Loss: 2.419120\n",
      "Epoch: 2589 \tTraining Loss: 2.378266 \tValidation Loss: 2.419150\n",
      "Epoch: 2590 \tTraining Loss: 2.373053 \tValidation Loss: 2.419088\n",
      "Epoch: 2591 \tTraining Loss: 2.385769 \tValidation Loss: 2.419031\n",
      "Epoch: 2592 \tTraining Loss: 2.376649 \tValidation Loss: 2.418972\n",
      "Epoch: 2593 \tTraining Loss: 2.387225 \tValidation Loss: 2.418901\n",
      "Validation loss decreased (2.418943 --> 2.418901).  Saving model ...\n",
      "Epoch: 2594 \tTraining Loss: 2.387614 \tValidation Loss: 2.418924\n",
      "Epoch: 2595 \tTraining Loss: 2.370646 \tValidation Loss: 2.418823\n",
      "Validation loss decreased (2.418901 --> 2.418823).  Saving model ...\n",
      "Epoch: 2596 \tTraining Loss: 2.364157 \tValidation Loss: 2.418741\n",
      "Validation loss decreased (2.418823 --> 2.418741).  Saving model ...\n",
      "Epoch: 2597 \tTraining Loss: 2.382957 \tValidation Loss: 2.418716\n",
      "Validation loss decreased (2.418741 --> 2.418716).  Saving model ...\n",
      "Epoch: 2598 \tTraining Loss: 2.373163 \tValidation Loss: 2.418581\n",
      "Validation loss decreased (2.418716 --> 2.418581).  Saving model ...\n",
      "Epoch: 2599 \tTraining Loss: 2.377445 \tValidation Loss: 2.418597\n",
      "Epoch: 2600 \tTraining Loss: 2.375858 \tValidation Loss: 2.418528\n",
      "Validation loss decreased (2.418581 --> 2.418528).  Saving model ...\n",
      "Epoch: 2601 \tTraining Loss: 2.370251 \tValidation Loss: 2.418465\n",
      "Validation loss decreased (2.418528 --> 2.418465).  Saving model ...\n",
      "Epoch: 2602 \tTraining Loss: 2.384497 \tValidation Loss: 2.418398\n",
      "Validation loss decreased (2.418465 --> 2.418398).  Saving model ...\n",
      "Epoch: 2603 \tTraining Loss: 2.386667 \tValidation Loss: 2.418396\n",
      "Validation loss decreased (2.418398 --> 2.418396).  Saving model ...\n",
      "Epoch: 2604 \tTraining Loss: 2.366617 \tValidation Loss: 2.418324\n",
      "Validation loss decreased (2.418396 --> 2.418324).  Saving model ...\n",
      "Epoch: 2605 \tTraining Loss: 2.367182 \tValidation Loss: 2.418175\n",
      "Validation loss decreased (2.418324 --> 2.418175).  Saving model ...\n",
      "Epoch: 2606 \tTraining Loss: 2.377777 \tValidation Loss: 2.418110\n",
      "Validation loss decreased (2.418175 --> 2.418110).  Saving model ...\n",
      "Epoch: 2607 \tTraining Loss: 2.388339 \tValidation Loss: 2.418092\n",
      "Validation loss decreased (2.418110 --> 2.418092).  Saving model ...\n",
      "Epoch: 2608 \tTraining Loss: 2.368101 \tValidation Loss: 2.418006\n",
      "Validation loss decreased (2.418092 --> 2.418006).  Saving model ...\n",
      "Epoch: 2609 \tTraining Loss: 2.376859 \tValidation Loss: 2.417987\n",
      "Validation loss decreased (2.418006 --> 2.417987).  Saving model ...\n",
      "Epoch: 2610 \tTraining Loss: 2.369331 \tValidation Loss: 2.417954\n",
      "Validation loss decreased (2.417987 --> 2.417954).  Saving model ...\n",
      "Epoch: 2611 \tTraining Loss: 2.363436 \tValidation Loss: 2.417971\n",
      "Epoch: 2612 \tTraining Loss: 2.368348 \tValidation Loss: 2.417882\n",
      "Validation loss decreased (2.417954 --> 2.417882).  Saving model ...\n",
      "Epoch: 2613 \tTraining Loss: 2.366617 \tValidation Loss: 2.417877\n",
      "Validation loss decreased (2.417882 --> 2.417877).  Saving model ...\n",
      "Epoch: 2614 \tTraining Loss: 2.378611 \tValidation Loss: 2.417877\n",
      "Validation loss decreased (2.417877 --> 2.417877).  Saving model ...\n",
      "Epoch: 2615 \tTraining Loss: 2.376895 \tValidation Loss: 2.417835\n",
      "Validation loss decreased (2.417877 --> 2.417835).  Saving model ...\n",
      "Epoch: 2616 \tTraining Loss: 2.374660 \tValidation Loss: 2.417807\n",
      "Validation loss decreased (2.417835 --> 2.417807).  Saving model ...\n",
      "Epoch: 2617 \tTraining Loss: 2.377434 \tValidation Loss: 2.417895\n",
      "Epoch: 2618 \tTraining Loss: 2.377563 \tValidation Loss: 2.417890\n",
      "Epoch: 2619 \tTraining Loss: 2.375282 \tValidation Loss: 2.417841\n",
      "Epoch: 2620 \tTraining Loss: 2.386880 \tValidation Loss: 2.417886\n",
      "Epoch: 2621 \tTraining Loss: 2.367673 \tValidation Loss: 2.417911\n",
      "Epoch: 2622 \tTraining Loss: 2.381139 \tValidation Loss: 2.417972\n",
      "Epoch: 2623 \tTraining Loss: 2.380036 \tValidation Loss: 2.417938\n",
      "Epoch: 2624 \tTraining Loss: 2.373780 \tValidation Loss: 2.417888\n",
      "Epoch: 2625 \tTraining Loss: 2.373592 \tValidation Loss: 2.417863\n",
      "Epoch: 2626 \tTraining Loss: 2.376328 \tValidation Loss: 2.417892\n",
      "Epoch: 2627 \tTraining Loss: 2.371772 \tValidation Loss: 2.417838\n",
      "Epoch: 2628 \tTraining Loss: 2.387030 \tValidation Loss: 2.417845\n",
      "Epoch: 2629 \tTraining Loss: 2.380669 \tValidation Loss: 2.417914\n",
      "Epoch: 2630 \tTraining Loss: 2.368679 \tValidation Loss: 2.417883\n",
      "Epoch: 2631 \tTraining Loss: 2.379899 \tValidation Loss: 2.417942\n",
      "Epoch: 2632 \tTraining Loss: 2.377145 \tValidation Loss: 2.417887\n",
      "Epoch: 2633 \tTraining Loss: 2.379676 \tValidation Loss: 2.417780\n",
      "Validation loss decreased (2.417807 --> 2.417780).  Saving model ...\n",
      "Epoch: 2634 \tTraining Loss: 2.357784 \tValidation Loss: 2.417665\n",
      "Validation loss decreased (2.417780 --> 2.417665).  Saving model ...\n",
      "Epoch: 2635 \tTraining Loss: 2.359220 \tValidation Loss: 2.417561\n",
      "Validation loss decreased (2.417665 --> 2.417561).  Saving model ...\n",
      "Epoch: 2636 \tTraining Loss: 2.380540 \tValidation Loss: 2.417552\n",
      "Validation loss decreased (2.417561 --> 2.417552).  Saving model ...\n",
      "Epoch: 2637 \tTraining Loss: 2.373640 \tValidation Loss: 2.417430\n",
      "Validation loss decreased (2.417552 --> 2.417430).  Saving model ...\n",
      "Epoch: 2638 \tTraining Loss: 2.360370 \tValidation Loss: 2.417289\n",
      "Validation loss decreased (2.417430 --> 2.417289).  Saving model ...\n",
      "Epoch: 2639 \tTraining Loss: 2.377153 \tValidation Loss: 2.417227\n",
      "Validation loss decreased (2.417289 --> 2.417227).  Saving model ...\n",
      "Epoch: 2640 \tTraining Loss: 2.370757 \tValidation Loss: 2.417295\n",
      "Epoch: 2641 \tTraining Loss: 2.372847 \tValidation Loss: 2.417218\n",
      "Validation loss decreased (2.417227 --> 2.417218).  Saving model ...\n",
      "Epoch: 2642 \tTraining Loss: 2.369936 \tValidation Loss: 2.417235\n",
      "Epoch: 2643 \tTraining Loss: 2.381098 \tValidation Loss: 2.417322\n",
      "Epoch: 2644 \tTraining Loss: 2.377127 \tValidation Loss: 2.417318\n",
      "Epoch: 2645 \tTraining Loss: 2.371831 \tValidation Loss: 2.417262\n",
      "Epoch: 2646 \tTraining Loss: 2.369139 \tValidation Loss: 2.417232\n",
      "Epoch: 2647 \tTraining Loss: 2.371089 \tValidation Loss: 2.417267\n",
      "Epoch: 2648 \tTraining Loss: 2.372909 \tValidation Loss: 2.417259\n",
      "Epoch: 2649 \tTraining Loss: 2.371150 \tValidation Loss: 2.417241\n",
      "Epoch: 2650 \tTraining Loss: 2.369314 \tValidation Loss: 2.417166\n",
      "Validation loss decreased (2.417218 --> 2.417166).  Saving model ...\n",
      "Epoch: 2651 \tTraining Loss: 2.384608 \tValidation Loss: 2.417167\n",
      "Epoch: 2652 \tTraining Loss: 2.373254 \tValidation Loss: 2.417110\n",
      "Validation loss decreased (2.417166 --> 2.417110).  Saving model ...\n",
      "Epoch: 2653 \tTraining Loss: 2.376723 \tValidation Loss: 2.417065\n",
      "Validation loss decreased (2.417110 --> 2.417065).  Saving model ...\n",
      "Epoch: 2654 \tTraining Loss: 2.370670 \tValidation Loss: 2.417069\n",
      "Epoch: 2655 \tTraining Loss: 2.371098 \tValidation Loss: 2.416964\n",
      "Validation loss decreased (2.417065 --> 2.416964).  Saving model ...\n",
      "Epoch: 2656 \tTraining Loss: 2.376150 \tValidation Loss: 2.416981\n",
      "Epoch: 2657 \tTraining Loss: 2.382058 \tValidation Loss: 2.416984\n",
      "Epoch: 2658 \tTraining Loss: 2.398837 \tValidation Loss: 2.417060\n",
      "Epoch: 2659 \tTraining Loss: 2.362164 \tValidation Loss: 2.417037\n",
      "Epoch: 2660 \tTraining Loss: 2.373144 \tValidation Loss: 2.416929\n",
      "Validation loss decreased (2.416964 --> 2.416929).  Saving model ...\n",
      "Epoch: 2661 \tTraining Loss: 2.365488 \tValidation Loss: 2.416797\n",
      "Validation loss decreased (2.416929 --> 2.416797).  Saving model ...\n",
      "Epoch: 2662 \tTraining Loss: 2.369202 \tValidation Loss: 2.416855\n",
      "Epoch: 2663 \tTraining Loss: 2.377239 \tValidation Loss: 2.416872\n",
      "Epoch: 2664 \tTraining Loss: 2.378123 \tValidation Loss: 2.416737\n",
      "Validation loss decreased (2.416797 --> 2.416737).  Saving model ...\n",
      "Epoch: 2665 \tTraining Loss: 2.382613 \tValidation Loss: 2.416716\n",
      "Validation loss decreased (2.416737 --> 2.416716).  Saving model ...\n",
      "Epoch: 2666 \tTraining Loss: 2.375650 \tValidation Loss: 2.416690\n",
      "Validation loss decreased (2.416716 --> 2.416690).  Saving model ...\n",
      "Epoch: 2667 \tTraining Loss: 2.365686 \tValidation Loss: 2.416623\n",
      "Validation loss decreased (2.416690 --> 2.416623).  Saving model ...\n",
      "Epoch: 2668 \tTraining Loss: 2.371499 \tValidation Loss: 2.416550\n",
      "Validation loss decreased (2.416623 --> 2.416550).  Saving model ...\n",
      "Epoch: 2669 \tTraining Loss: 2.382391 \tValidation Loss: 2.416564\n",
      "Epoch: 2670 \tTraining Loss: 2.374986 \tValidation Loss: 2.416653\n",
      "Epoch: 2671 \tTraining Loss: 2.376956 \tValidation Loss: 2.416671\n",
      "Epoch: 2672 \tTraining Loss: 2.363959 \tValidation Loss: 2.416698\n",
      "Epoch: 2673 \tTraining Loss: 2.367394 \tValidation Loss: 2.416564\n",
      "Epoch: 2674 \tTraining Loss: 2.373036 \tValidation Loss: 2.416591\n",
      "Epoch: 2675 \tTraining Loss: 2.379601 \tValidation Loss: 2.416562\n",
      "Epoch: 2676 \tTraining Loss: 2.375699 \tValidation Loss: 2.416595\n",
      "Epoch: 2677 \tTraining Loss: 2.371960 \tValidation Loss: 2.416619\n",
      "Epoch: 2678 \tTraining Loss: 2.369030 \tValidation Loss: 2.416573\n",
      "Epoch: 2679 \tTraining Loss: 2.369221 \tValidation Loss: 2.416459\n",
      "Validation loss decreased (2.416550 --> 2.416459).  Saving model ...\n",
      "Epoch: 2680 \tTraining Loss: 2.367884 \tValidation Loss: 2.416464\n",
      "Epoch: 2681 \tTraining Loss: 2.364294 \tValidation Loss: 2.416509\n",
      "Epoch: 2682 \tTraining Loss: 2.383691 \tValidation Loss: 2.416562\n",
      "Epoch: 2683 \tTraining Loss: 2.372211 \tValidation Loss: 2.416480\n",
      "Epoch: 2684 \tTraining Loss: 2.374295 \tValidation Loss: 2.416534\n",
      "Epoch: 2685 \tTraining Loss: 2.364111 \tValidation Loss: 2.416455\n",
      "Validation loss decreased (2.416459 --> 2.416455).  Saving model ...\n",
      "Epoch: 2686 \tTraining Loss: 2.371601 \tValidation Loss: 2.416384\n",
      "Validation loss decreased (2.416455 --> 2.416384).  Saving model ...\n",
      "Epoch: 2687 \tTraining Loss: 2.371647 \tValidation Loss: 2.416405\n",
      "Epoch: 2688 \tTraining Loss: 2.374353 \tValidation Loss: 2.416289\n",
      "Validation loss decreased (2.416384 --> 2.416289).  Saving model ...\n",
      "Epoch: 2689 \tTraining Loss: 2.367882 \tValidation Loss: 2.416315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2690 \tTraining Loss: 2.356116 \tValidation Loss: 2.416270\n",
      "Validation loss decreased (2.416289 --> 2.416270).  Saving model ...\n",
      "Epoch: 2691 \tTraining Loss: 2.379354 \tValidation Loss: 2.416398\n",
      "Epoch: 2692 \tTraining Loss: 2.384081 \tValidation Loss: 2.416363\n",
      "Epoch: 2693 \tTraining Loss: 2.365572 \tValidation Loss: 2.416312\n",
      "Epoch: 2694 \tTraining Loss: 2.374802 \tValidation Loss: 2.416337\n",
      "Epoch: 2695 \tTraining Loss: 2.367122 \tValidation Loss: 2.416298\n",
      "Epoch: 2696 \tTraining Loss: 2.376379 \tValidation Loss: 2.416181\n",
      "Validation loss decreased (2.416270 --> 2.416181).  Saving model ...\n",
      "Epoch: 2697 \tTraining Loss: 2.364964 \tValidation Loss: 2.416171\n",
      "Validation loss decreased (2.416181 --> 2.416171).  Saving model ...\n",
      "Epoch: 2698 \tTraining Loss: 2.371159 \tValidation Loss: 2.416150\n",
      "Validation loss decreased (2.416171 --> 2.416150).  Saving model ...\n",
      "Epoch: 2699 \tTraining Loss: 2.375505 \tValidation Loss: 2.416079\n",
      "Validation loss decreased (2.416150 --> 2.416079).  Saving model ...\n",
      "Epoch: 2700 \tTraining Loss: 2.366597 \tValidation Loss: 2.416057\n",
      "Validation loss decreased (2.416079 --> 2.416057).  Saving model ...\n",
      "Epoch: 2701 \tTraining Loss: 2.378785 \tValidation Loss: 2.416071\n",
      "Epoch: 2702 \tTraining Loss: 2.363167 \tValidation Loss: 2.415968\n",
      "Validation loss decreased (2.416057 --> 2.415968).  Saving model ...\n",
      "Epoch: 2703 \tTraining Loss: 2.370475 \tValidation Loss: 2.415889\n",
      "Validation loss decreased (2.415968 --> 2.415889).  Saving model ...\n",
      "Epoch: 2704 \tTraining Loss: 2.373991 \tValidation Loss: 2.415881\n",
      "Validation loss decreased (2.415889 --> 2.415881).  Saving model ...\n",
      "Epoch: 2705 \tTraining Loss: 2.358738 \tValidation Loss: 2.415839\n",
      "Validation loss decreased (2.415881 --> 2.415839).  Saving model ...\n",
      "Epoch: 2706 \tTraining Loss: 2.365857 \tValidation Loss: 2.415849\n",
      "Epoch: 2707 \tTraining Loss: 2.379255 \tValidation Loss: 2.415887\n",
      "Epoch: 2708 \tTraining Loss: 2.375655 \tValidation Loss: 2.416010\n",
      "Epoch: 2709 \tTraining Loss: 2.378626 \tValidation Loss: 2.416026\n",
      "Epoch: 2710 \tTraining Loss: 2.365853 \tValidation Loss: 2.415949\n",
      "Epoch: 2711 \tTraining Loss: 2.376713 \tValidation Loss: 2.415905\n",
      "Epoch: 2712 \tTraining Loss: 2.369302 \tValidation Loss: 2.415846\n",
      "Epoch: 2713 \tTraining Loss: 2.368329 \tValidation Loss: 2.415741\n",
      "Validation loss decreased (2.415839 --> 2.415741).  Saving model ...\n",
      "Epoch: 2714 \tTraining Loss: 2.374134 \tValidation Loss: 2.415795\n",
      "Epoch: 2715 \tTraining Loss: 2.364846 \tValidation Loss: 2.415701\n",
      "Validation loss decreased (2.415741 --> 2.415701).  Saving model ...\n",
      "Epoch: 2716 \tTraining Loss: 2.372875 \tValidation Loss: 2.415639\n",
      "Validation loss decreased (2.415701 --> 2.415639).  Saving model ...\n",
      "Epoch: 2717 \tTraining Loss: 2.373219 \tValidation Loss: 2.415603\n",
      "Validation loss decreased (2.415639 --> 2.415603).  Saving model ...\n",
      "Epoch: 2718 \tTraining Loss: 2.387045 \tValidation Loss: 2.415673\n",
      "Epoch: 2719 \tTraining Loss: 2.377685 \tValidation Loss: 2.415693\n",
      "Epoch: 2720 \tTraining Loss: 2.358040 \tValidation Loss: 2.415562\n",
      "Validation loss decreased (2.415603 --> 2.415562).  Saving model ...\n",
      "Epoch: 2721 \tTraining Loss: 2.374003 \tValidation Loss: 2.415546\n",
      "Validation loss decreased (2.415562 --> 2.415546).  Saving model ...\n",
      "Epoch: 2722 \tTraining Loss: 2.374536 \tValidation Loss: 2.415426\n",
      "Validation loss decreased (2.415546 --> 2.415426).  Saving model ...\n",
      "Epoch: 2723 \tTraining Loss: 2.380140 \tValidation Loss: 2.415492\n",
      "Epoch: 2724 \tTraining Loss: 2.380925 \tValidation Loss: 2.415549\n",
      "Epoch: 2725 \tTraining Loss: 2.361899 \tValidation Loss: 2.415500\n",
      "Epoch: 2726 \tTraining Loss: 2.371791 \tValidation Loss: 2.415487\n",
      "Epoch: 2727 \tTraining Loss: 2.376542 \tValidation Loss: 2.415464\n",
      "Epoch: 2728 \tTraining Loss: 2.368325 \tValidation Loss: 2.415388\n",
      "Validation loss decreased (2.415426 --> 2.415388).  Saving model ...\n",
      "Epoch: 2729 \tTraining Loss: 2.368975 \tValidation Loss: 2.415362\n",
      "Validation loss decreased (2.415388 --> 2.415362).  Saving model ...\n",
      "Epoch: 2730 \tTraining Loss: 2.381903 \tValidation Loss: 2.415387\n",
      "Epoch: 2731 \tTraining Loss: 2.369649 \tValidation Loss: 2.415314\n",
      "Validation loss decreased (2.415362 --> 2.415314).  Saving model ...\n",
      "Epoch: 2732 \tTraining Loss: 2.370843 \tValidation Loss: 2.415268\n",
      "Validation loss decreased (2.415314 --> 2.415268).  Saving model ...\n",
      "Epoch: 2733 \tTraining Loss: 2.370580 \tValidation Loss: 2.415205\n",
      "Validation loss decreased (2.415268 --> 2.415205).  Saving model ...\n",
      "Epoch: 2734 \tTraining Loss: 2.370369 \tValidation Loss: 2.415223\n",
      "Epoch: 2735 \tTraining Loss: 2.366135 \tValidation Loss: 2.415314\n",
      "Epoch: 2736 \tTraining Loss: 2.363997 \tValidation Loss: 2.415254\n",
      "Epoch: 2737 \tTraining Loss: 2.363066 \tValidation Loss: 2.415150\n",
      "Validation loss decreased (2.415205 --> 2.415150).  Saving model ...\n",
      "Epoch: 2738 \tTraining Loss: 2.367079 \tValidation Loss: 2.415136\n",
      "Validation loss decreased (2.415150 --> 2.415136).  Saving model ...\n",
      "Epoch: 2739 \tTraining Loss: 2.370179 \tValidation Loss: 2.415083\n",
      "Validation loss decreased (2.415136 --> 2.415083).  Saving model ...\n",
      "Epoch: 2740 \tTraining Loss: 2.380066 \tValidation Loss: 2.415063\n",
      "Validation loss decreased (2.415083 --> 2.415063).  Saving model ...\n",
      "Epoch: 2741 \tTraining Loss: 2.375609 \tValidation Loss: 2.415116\n",
      "Epoch: 2742 \tTraining Loss: 2.365509 \tValidation Loss: 2.415058\n",
      "Validation loss decreased (2.415063 --> 2.415058).  Saving model ...\n",
      "Epoch: 2743 \tTraining Loss: 2.369330 \tValidation Loss: 2.415051\n",
      "Validation loss decreased (2.415058 --> 2.415051).  Saving model ...\n",
      "Epoch: 2744 \tTraining Loss: 2.365322 \tValidation Loss: 2.415043\n",
      "Validation loss decreased (2.415051 --> 2.415043).  Saving model ...\n",
      "Epoch: 2745 \tTraining Loss: 2.360100 \tValidation Loss: 2.415054\n",
      "Epoch: 2746 \tTraining Loss: 2.350389 \tValidation Loss: 2.414986\n",
      "Validation loss decreased (2.415043 --> 2.414986).  Saving model ...\n",
      "Epoch: 2747 \tTraining Loss: 2.370753 \tValidation Loss: 2.414987\n",
      "Epoch: 2748 \tTraining Loss: 2.371827 \tValidation Loss: 2.414893\n",
      "Validation loss decreased (2.414986 --> 2.414893).  Saving model ...\n",
      "Epoch: 2749 \tTraining Loss: 2.370980 \tValidation Loss: 2.414869\n",
      "Validation loss decreased (2.414893 --> 2.414869).  Saving model ...\n",
      "Epoch: 2750 \tTraining Loss: 2.375077 \tValidation Loss: 2.414889\n",
      "Epoch: 2751 \tTraining Loss: 2.371192 \tValidation Loss: 2.414855\n",
      "Validation loss decreased (2.414869 --> 2.414855).  Saving model ...\n",
      "Epoch: 2752 \tTraining Loss: 2.366626 \tValidation Loss: 2.414785\n",
      "Validation loss decreased (2.414855 --> 2.414785).  Saving model ...\n",
      "Epoch: 2753 \tTraining Loss: 2.374440 \tValidation Loss: 2.414780\n",
      "Validation loss decreased (2.414785 --> 2.414780).  Saving model ...\n",
      "Epoch: 2754 \tTraining Loss: 2.369535 \tValidation Loss: 2.414661\n",
      "Validation loss decreased (2.414780 --> 2.414661).  Saving model ...\n",
      "Epoch: 2755 \tTraining Loss: 2.351191 \tValidation Loss: 2.414575\n",
      "Validation loss decreased (2.414661 --> 2.414575).  Saving model ...\n",
      "Epoch: 2756 \tTraining Loss: 2.360478 \tValidation Loss: 2.414513\n",
      "Validation loss decreased (2.414575 --> 2.414513).  Saving model ...\n",
      "Epoch: 2757 \tTraining Loss: 2.379719 \tValidation Loss: 2.414559\n",
      "Epoch: 2758 \tTraining Loss: 2.369182 \tValidation Loss: 2.414447\n",
      "Validation loss decreased (2.414513 --> 2.414447).  Saving model ...\n",
      "Epoch: 2759 \tTraining Loss: 2.374626 \tValidation Loss: 2.414524\n",
      "Epoch: 2760 \tTraining Loss: 2.366144 \tValidation Loss: 2.414362\n",
      "Validation loss decreased (2.414447 --> 2.414362).  Saving model ...\n",
      "Epoch: 2761 \tTraining Loss: 2.367892 \tValidation Loss: 2.414367\n",
      "Epoch: 2762 \tTraining Loss: 2.358799 \tValidation Loss: 2.414436\n",
      "Epoch: 2763 \tTraining Loss: 2.362409 \tValidation Loss: 2.414520\n",
      "Epoch: 2764 \tTraining Loss: 2.378809 \tValidation Loss: 2.414606\n",
      "Epoch: 2765 \tTraining Loss: 2.371974 \tValidation Loss: 2.414540\n",
      "Epoch: 2766 \tTraining Loss: 2.361285 \tValidation Loss: 2.414574\n",
      "Epoch: 2767 \tTraining Loss: 2.355362 \tValidation Loss: 2.414579\n",
      "Epoch: 2768 \tTraining Loss: 2.363872 \tValidation Loss: 2.414530\n",
      "Epoch: 2769 \tTraining Loss: 2.366540 \tValidation Loss: 2.414543\n",
      "Epoch: 2770 \tTraining Loss: 2.364211 \tValidation Loss: 2.414577\n",
      "Epoch: 2771 \tTraining Loss: 2.375620 \tValidation Loss: 2.414669\n",
      "Epoch: 2772 \tTraining Loss: 2.360524 \tValidation Loss: 2.414487\n",
      "Epoch: 2773 \tTraining Loss: 2.375635 \tValidation Loss: 2.414417\n",
      "Epoch: 2774 \tTraining Loss: 2.356370 \tValidation Loss: 2.414314\n",
      "Validation loss decreased (2.414362 --> 2.414314).  Saving model ...\n",
      "Epoch: 2775 \tTraining Loss: 2.362018 \tValidation Loss: 2.414228\n",
      "Validation loss decreased (2.414314 --> 2.414228).  Saving model ...\n",
      "Epoch: 2776 \tTraining Loss: 2.364171 \tValidation Loss: 2.414234\n",
      "Epoch: 2777 \tTraining Loss: 2.363965 \tValidation Loss: 2.414184\n",
      "Validation loss decreased (2.414228 --> 2.414184).  Saving model ...\n",
      "Epoch: 2778 \tTraining Loss: 2.363674 \tValidation Loss: 2.414181\n",
      "Validation loss decreased (2.414184 --> 2.414181).  Saving model ...\n",
      "Epoch: 2779 \tTraining Loss: 2.355017 \tValidation Loss: 2.414108\n",
      "Validation loss decreased (2.414181 --> 2.414108).  Saving model ...\n",
      "Epoch: 2780 \tTraining Loss: 2.378293 \tValidation Loss: 2.414150\n",
      "Epoch: 2781 \tTraining Loss: 2.371832 \tValidation Loss: 2.414082\n",
      "Validation loss decreased (2.414108 --> 2.414082).  Saving model ...\n",
      "Epoch: 2782 \tTraining Loss: 2.372512 \tValidation Loss: 2.414072\n",
      "Validation loss decreased (2.414082 --> 2.414072).  Saving model ...\n",
      "Epoch: 2783 \tTraining Loss: 2.354055 \tValidation Loss: 2.414038\n",
      "Validation loss decreased (2.414072 --> 2.414038).  Saving model ...\n",
      "Epoch: 2784 \tTraining Loss: 2.392705 \tValidation Loss: 2.414033\n",
      "Validation loss decreased (2.414038 --> 2.414033).  Saving model ...\n",
      "Epoch: 2785 \tTraining Loss: 2.360796 \tValidation Loss: 2.414039\n",
      "Epoch: 2786 \tTraining Loss: 2.382016 \tValidation Loss: 2.414125\n",
      "Epoch: 2787 \tTraining Loss: 2.382389 \tValidation Loss: 2.414068\n",
      "Epoch: 2788 \tTraining Loss: 2.371038 \tValidation Loss: 2.414067\n",
      "Epoch: 2789 \tTraining Loss: 2.369240 \tValidation Loss: 2.414041\n",
      "Epoch: 2790 \tTraining Loss: 2.370779 \tValidation Loss: 2.414102\n",
      "Epoch: 2791 \tTraining Loss: 2.371871 \tValidation Loss: 2.414078\n",
      "Epoch: 2792 \tTraining Loss: 2.371561 \tValidation Loss: 2.414042\n",
      "Epoch: 2793 \tTraining Loss: 2.357972 \tValidation Loss: 2.413883\n",
      "Validation loss decreased (2.414033 --> 2.413883).  Saving model ...\n",
      "Epoch: 2794 \tTraining Loss: 2.381610 \tValidation Loss: 2.413934\n",
      "Epoch: 2795 \tTraining Loss: 2.366085 \tValidation Loss: 2.413938\n",
      "Epoch: 2796 \tTraining Loss: 2.358800 \tValidation Loss: 2.413877\n",
      "Validation loss decreased (2.413883 --> 2.413877).  Saving model ...\n",
      "Epoch: 2797 \tTraining Loss: 2.352041 \tValidation Loss: 2.413808\n",
      "Validation loss decreased (2.413877 --> 2.413808).  Saving model ...\n",
      "Epoch: 2798 \tTraining Loss: 2.353214 \tValidation Loss: 2.413676\n",
      "Validation loss decreased (2.413808 --> 2.413676).  Saving model ...\n",
      "Epoch: 2799 \tTraining Loss: 2.367338 \tValidation Loss: 2.413756\n",
      "Epoch: 2800 \tTraining Loss: 2.349922 \tValidation Loss: 2.413702\n",
      "Epoch: 2801 \tTraining Loss: 2.366175 \tValidation Loss: 2.413695\n",
      "Epoch: 2802 \tTraining Loss: 2.351170 \tValidation Loss: 2.413531\n",
      "Validation loss decreased (2.413676 --> 2.413531).  Saving model ...\n",
      "Epoch: 2803 \tTraining Loss: 2.347678 \tValidation Loss: 2.413441\n",
      "Validation loss decreased (2.413531 --> 2.413441).  Saving model ...\n",
      "Epoch: 2804 \tTraining Loss: 2.379642 \tValidation Loss: 2.413408\n",
      "Validation loss decreased (2.413441 --> 2.413408).  Saving model ...\n",
      "Epoch: 2805 \tTraining Loss: 2.359719 \tValidation Loss: 2.413324\n",
      "Validation loss decreased (2.413408 --> 2.413324).  Saving model ...\n",
      "Epoch: 2806 \tTraining Loss: 2.364941 \tValidation Loss: 2.413288\n",
      "Validation loss decreased (2.413324 --> 2.413288).  Saving model ...\n",
      "Epoch: 2807 \tTraining Loss: 2.370990 \tValidation Loss: 2.413272\n",
      "Validation loss decreased (2.413288 --> 2.413272).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2808 \tTraining Loss: 2.361511 \tValidation Loss: 2.413108\n",
      "Validation loss decreased (2.413272 --> 2.413108).  Saving model ...\n",
      "Epoch: 2809 \tTraining Loss: 2.358140 \tValidation Loss: 2.413114\n",
      "Epoch: 2810 \tTraining Loss: 2.355824 \tValidation Loss: 2.413059\n",
      "Validation loss decreased (2.413108 --> 2.413059).  Saving model ...\n",
      "Epoch: 2811 \tTraining Loss: 2.370895 \tValidation Loss: 2.413112\n",
      "Epoch: 2812 \tTraining Loss: 2.358156 \tValidation Loss: 2.413064\n",
      "Epoch: 2813 \tTraining Loss: 2.348629 \tValidation Loss: 2.412923\n",
      "Validation loss decreased (2.413059 --> 2.412923).  Saving model ...\n",
      "Epoch: 2814 \tTraining Loss: 2.370779 \tValidation Loss: 2.412858\n",
      "Validation loss decreased (2.412923 --> 2.412858).  Saving model ...\n",
      "Epoch: 2815 \tTraining Loss: 2.367017 \tValidation Loss: 2.412912\n",
      "Epoch: 2816 \tTraining Loss: 2.369131 \tValidation Loss: 2.412980\n",
      "Epoch: 2817 \tTraining Loss: 2.370055 \tValidation Loss: 2.412879\n",
      "Epoch: 2818 \tTraining Loss: 2.370135 \tValidation Loss: 2.412889\n",
      "Epoch: 2819 \tTraining Loss: 2.357641 \tValidation Loss: 2.412880\n",
      "Epoch: 2820 \tTraining Loss: 2.357976 \tValidation Loss: 2.412837\n",
      "Validation loss decreased (2.412858 --> 2.412837).  Saving model ...\n",
      "Epoch: 2821 \tTraining Loss: 2.360774 \tValidation Loss: 2.412825\n",
      "Validation loss decreased (2.412837 --> 2.412825).  Saving model ...\n",
      "Epoch: 2822 \tTraining Loss: 2.363554 \tValidation Loss: 2.412829\n",
      "Epoch: 2823 \tTraining Loss: 2.363417 \tValidation Loss: 2.412787\n",
      "Validation loss decreased (2.412825 --> 2.412787).  Saving model ...\n",
      "Epoch: 2824 \tTraining Loss: 2.374779 \tValidation Loss: 2.412750\n",
      "Validation loss decreased (2.412787 --> 2.412750).  Saving model ...\n",
      "Epoch: 2825 \tTraining Loss: 2.359370 \tValidation Loss: 2.412706\n",
      "Validation loss decreased (2.412750 --> 2.412706).  Saving model ...\n",
      "Epoch: 2826 \tTraining Loss: 2.369853 \tValidation Loss: 2.412714\n",
      "Epoch: 2827 \tTraining Loss: 2.370478 \tValidation Loss: 2.412711\n",
      "Epoch: 2828 \tTraining Loss: 2.357297 \tValidation Loss: 2.412717\n",
      "Epoch: 2829 \tTraining Loss: 2.365649 \tValidation Loss: 2.412793\n",
      "Epoch: 2830 \tTraining Loss: 2.377836 \tValidation Loss: 2.412748\n",
      "Epoch: 2831 \tTraining Loss: 2.356234 \tValidation Loss: 2.412641\n",
      "Validation loss decreased (2.412706 --> 2.412641).  Saving model ...\n",
      "Epoch: 2832 \tTraining Loss: 2.364331 \tValidation Loss: 2.412621\n",
      "Validation loss decreased (2.412641 --> 2.412621).  Saving model ...\n",
      "Epoch: 2833 \tTraining Loss: 2.364092 \tValidation Loss: 2.412569\n",
      "Validation loss decreased (2.412621 --> 2.412569).  Saving model ...\n",
      "Epoch: 2834 \tTraining Loss: 2.359064 \tValidation Loss: 2.412636\n",
      "Epoch: 2835 \tTraining Loss: 2.355147 \tValidation Loss: 2.412673\n",
      "Epoch: 2836 \tTraining Loss: 2.369425 \tValidation Loss: 2.412772\n",
      "Epoch: 2837 \tTraining Loss: 2.362425 \tValidation Loss: 2.412748\n",
      "Epoch: 2838 \tTraining Loss: 2.372202 \tValidation Loss: 2.412726\n",
      "Epoch: 2839 \tTraining Loss: 2.362199 \tValidation Loss: 2.412671\n",
      "Epoch: 2840 \tTraining Loss: 2.357629 \tValidation Loss: 2.412647\n",
      "Epoch: 2841 \tTraining Loss: 2.367109 \tValidation Loss: 2.412692\n",
      "Epoch: 2842 \tTraining Loss: 2.359733 \tValidation Loss: 2.412675\n",
      "Epoch: 2843 \tTraining Loss: 2.370271 \tValidation Loss: 2.412646\n",
      "Epoch: 2844 \tTraining Loss: 2.362092 \tValidation Loss: 2.412648\n",
      "Epoch: 2845 \tTraining Loss: 2.365143 \tValidation Loss: 2.412567\n",
      "Validation loss decreased (2.412569 --> 2.412567).  Saving model ...\n",
      "Epoch: 2846 \tTraining Loss: 2.381713 \tValidation Loss: 2.412576\n",
      "Epoch: 2847 \tTraining Loss: 2.365816 \tValidation Loss: 2.412543\n",
      "Validation loss decreased (2.412567 --> 2.412543).  Saving model ...\n",
      "Epoch: 2848 \tTraining Loss: 2.367023 \tValidation Loss: 2.412543\n",
      "Validation loss decreased (2.412543 --> 2.412543).  Saving model ...\n",
      "Epoch: 2849 \tTraining Loss: 2.375018 \tValidation Loss: 2.412501\n",
      "Validation loss decreased (2.412543 --> 2.412501).  Saving model ...\n",
      "Epoch: 2850 \tTraining Loss: 2.364157 \tValidation Loss: 2.412470\n",
      "Validation loss decreased (2.412501 --> 2.412470).  Saving model ...\n",
      "Epoch: 2851 \tTraining Loss: 2.353627 \tValidation Loss: 2.412493\n",
      "Epoch: 2852 \tTraining Loss: 2.369869 \tValidation Loss: 2.412421\n",
      "Validation loss decreased (2.412470 --> 2.412421).  Saving model ...\n",
      "Epoch: 2853 \tTraining Loss: 2.353112 \tValidation Loss: 2.412400\n",
      "Validation loss decreased (2.412421 --> 2.412400).  Saving model ...\n",
      "Epoch: 2854 \tTraining Loss: 2.363991 \tValidation Loss: 2.412281\n",
      "Validation loss decreased (2.412400 --> 2.412281).  Saving model ...\n",
      "Epoch: 2855 \tTraining Loss: 2.355756 \tValidation Loss: 2.412305\n",
      "Epoch: 2856 \tTraining Loss: 2.363434 \tValidation Loss: 2.412252\n",
      "Validation loss decreased (2.412281 --> 2.412252).  Saving model ...\n",
      "Epoch: 2857 \tTraining Loss: 2.347998 \tValidation Loss: 2.412104\n",
      "Validation loss decreased (2.412252 --> 2.412104).  Saving model ...\n",
      "Epoch: 2858 \tTraining Loss: 2.356126 \tValidation Loss: 2.412070\n",
      "Validation loss decreased (2.412104 --> 2.412070).  Saving model ...\n",
      "Epoch: 2859 \tTraining Loss: 2.373955 \tValidation Loss: 2.412083\n",
      "Epoch: 2860 \tTraining Loss: 2.363621 \tValidation Loss: 2.412130\n",
      "Epoch: 2861 \tTraining Loss: 2.358037 \tValidation Loss: 2.412039\n",
      "Validation loss decreased (2.412070 --> 2.412039).  Saving model ...\n",
      "Epoch: 2862 \tTraining Loss: 2.354025 \tValidation Loss: 2.411950\n",
      "Validation loss decreased (2.412039 --> 2.411950).  Saving model ...\n",
      "Epoch: 2863 \tTraining Loss: 2.368064 \tValidation Loss: 2.411964\n",
      "Epoch: 2864 \tTraining Loss: 2.378003 \tValidation Loss: 2.411875\n",
      "Validation loss decreased (2.411950 --> 2.411875).  Saving model ...\n",
      "Epoch: 2865 \tTraining Loss: 2.356695 \tValidation Loss: 2.411757\n",
      "Validation loss decreased (2.411875 --> 2.411757).  Saving model ...\n",
      "Epoch: 2866 \tTraining Loss: 2.370022 \tValidation Loss: 2.411686\n",
      "Validation loss decreased (2.411757 --> 2.411686).  Saving model ...\n",
      "Epoch: 2867 \tTraining Loss: 2.351724 \tValidation Loss: 2.411610\n",
      "Validation loss decreased (2.411686 --> 2.411610).  Saving model ...\n",
      "Epoch: 2868 \tTraining Loss: 2.350882 \tValidation Loss: 2.411606\n",
      "Validation loss decreased (2.411610 --> 2.411606).  Saving model ...\n",
      "Epoch: 2869 \tTraining Loss: 2.358870 \tValidation Loss: 2.411549\n",
      "Validation loss decreased (2.411606 --> 2.411549).  Saving model ...\n",
      "Epoch: 2870 \tTraining Loss: 2.351016 \tValidation Loss: 2.411508\n",
      "Validation loss decreased (2.411549 --> 2.411508).  Saving model ...\n",
      "Epoch: 2871 \tTraining Loss: 2.365005 \tValidation Loss: 2.411492\n",
      "Validation loss decreased (2.411508 --> 2.411492).  Saving model ...\n",
      "Epoch: 2872 \tTraining Loss: 2.355878 \tValidation Loss: 2.411447\n",
      "Validation loss decreased (2.411492 --> 2.411447).  Saving model ...\n",
      "Epoch: 2873 \tTraining Loss: 2.372781 \tValidation Loss: 2.411401\n",
      "Validation loss decreased (2.411447 --> 2.411401).  Saving model ...\n",
      "Epoch: 2874 \tTraining Loss: 2.353535 \tValidation Loss: 2.411437\n",
      "Epoch: 2875 \tTraining Loss: 2.368385 \tValidation Loss: 2.411567\n",
      "Epoch: 2876 \tTraining Loss: 2.361718 \tValidation Loss: 2.411583\n",
      "Epoch: 2877 \tTraining Loss: 2.360715 \tValidation Loss: 2.411658\n",
      "Epoch: 2878 \tTraining Loss: 2.344817 \tValidation Loss: 2.411559\n",
      "Epoch: 2879 \tTraining Loss: 2.365931 \tValidation Loss: 2.411496\n",
      "Epoch: 2880 \tTraining Loss: 2.352075 \tValidation Loss: 2.411515\n",
      "Epoch: 2881 \tTraining Loss: 2.361596 \tValidation Loss: 2.411496\n",
      "Epoch: 2882 \tTraining Loss: 2.377726 \tValidation Loss: 2.411518\n",
      "Epoch: 2883 \tTraining Loss: 2.364304 \tValidation Loss: 2.411514\n",
      "Epoch: 2884 \tTraining Loss: 2.358364 \tValidation Loss: 2.411444\n",
      "Epoch: 2885 \tTraining Loss: 2.363687 \tValidation Loss: 2.411386\n",
      "Validation loss decreased (2.411401 --> 2.411386).  Saving model ...\n",
      "Epoch: 2886 \tTraining Loss: 2.357975 \tValidation Loss: 2.411401\n",
      "Epoch: 2887 \tTraining Loss: 2.377425 \tValidation Loss: 2.411364\n",
      "Validation loss decreased (2.411386 --> 2.411364).  Saving model ...\n",
      "Epoch: 2888 \tTraining Loss: 2.358526 \tValidation Loss: 2.411310\n",
      "Validation loss decreased (2.411364 --> 2.411310).  Saving model ...\n",
      "Epoch: 2889 \tTraining Loss: 2.366159 \tValidation Loss: 2.411283\n",
      "Validation loss decreased (2.411310 --> 2.411283).  Saving model ...\n",
      "Epoch: 2890 \tTraining Loss: 2.373038 \tValidation Loss: 2.411286\n",
      "Epoch: 2891 \tTraining Loss: 2.358850 \tValidation Loss: 2.411184\n",
      "Validation loss decreased (2.411283 --> 2.411184).  Saving model ...\n",
      "Epoch: 2892 \tTraining Loss: 2.359020 \tValidation Loss: 2.411127\n",
      "Validation loss decreased (2.411184 --> 2.411127).  Saving model ...\n",
      "Epoch: 2893 \tTraining Loss: 2.365576 \tValidation Loss: 2.411069\n",
      "Validation loss decreased (2.411127 --> 2.411069).  Saving model ...\n",
      "Epoch: 2894 \tTraining Loss: 2.368948 \tValidation Loss: 2.411119\n",
      "Epoch: 2895 \tTraining Loss: 2.358015 \tValidation Loss: 2.411235\n",
      "Epoch: 2896 \tTraining Loss: 2.371101 \tValidation Loss: 2.411260\n",
      "Epoch: 2897 \tTraining Loss: 2.354162 \tValidation Loss: 2.411225\n",
      "Epoch: 2898 \tTraining Loss: 2.354423 \tValidation Loss: 2.411175\n",
      "Epoch: 2899 \tTraining Loss: 2.356718 \tValidation Loss: 2.411223\n",
      "Epoch: 2900 \tTraining Loss: 2.357991 \tValidation Loss: 2.411205\n",
      "Epoch: 2901 \tTraining Loss: 2.367937 \tValidation Loss: 2.411164\n",
      "Epoch: 2902 \tTraining Loss: 2.374784 \tValidation Loss: 2.411223\n",
      "Epoch: 2903 \tTraining Loss: 2.359487 \tValidation Loss: 2.411169\n",
      "Epoch: 2904 \tTraining Loss: 2.359655 \tValidation Loss: 2.411136\n",
      "Epoch: 2905 \tTraining Loss: 2.357482 \tValidation Loss: 2.411040\n",
      "Validation loss decreased (2.411069 --> 2.411040).  Saving model ...\n",
      "Epoch: 2906 \tTraining Loss: 2.354020 \tValidation Loss: 2.410985\n",
      "Validation loss decreased (2.411040 --> 2.410985).  Saving model ...\n",
      "Epoch: 2907 \tTraining Loss: 2.356633 \tValidation Loss: 2.410942\n",
      "Validation loss decreased (2.410985 --> 2.410942).  Saving model ...\n",
      "Epoch: 2908 \tTraining Loss: 2.343405 \tValidation Loss: 2.410822\n",
      "Validation loss decreased (2.410942 --> 2.410822).  Saving model ...\n",
      "Epoch: 2909 \tTraining Loss: 2.355969 \tValidation Loss: 2.410713\n",
      "Validation loss decreased (2.410822 --> 2.410713).  Saving model ...\n",
      "Epoch: 2910 \tTraining Loss: 2.365849 \tValidation Loss: 2.410870\n",
      "Epoch: 2911 \tTraining Loss: 2.360849 \tValidation Loss: 2.410852\n",
      "Epoch: 2912 \tTraining Loss: 2.348748 \tValidation Loss: 2.410836\n",
      "Epoch: 2913 \tTraining Loss: 2.374900 \tValidation Loss: 2.410836\n",
      "Epoch: 2914 \tTraining Loss: 2.346842 \tValidation Loss: 2.410750\n",
      "Epoch: 2915 \tTraining Loss: 2.371093 \tValidation Loss: 2.410742\n",
      "Epoch: 2916 \tTraining Loss: 2.353500 \tValidation Loss: 2.410630\n",
      "Validation loss decreased (2.410713 --> 2.410630).  Saving model ...\n",
      "Epoch: 2917 \tTraining Loss: 2.362476 \tValidation Loss: 2.410705\n",
      "Epoch: 2918 \tTraining Loss: 2.346831 \tValidation Loss: 2.410662\n",
      "Epoch: 2919 \tTraining Loss: 2.362497 \tValidation Loss: 2.410707\n",
      "Epoch: 2920 \tTraining Loss: 2.367837 \tValidation Loss: 2.410700\n",
      "Epoch: 2921 \tTraining Loss: 2.355047 \tValidation Loss: 2.410569\n",
      "Validation loss decreased (2.410630 --> 2.410569).  Saving model ...\n",
      "Epoch: 2922 \tTraining Loss: 2.361907 \tValidation Loss: 2.410553\n",
      "Validation loss decreased (2.410569 --> 2.410553).  Saving model ...\n",
      "Epoch: 2923 \tTraining Loss: 2.362305 \tValidation Loss: 2.410624\n",
      "Epoch: 2924 \tTraining Loss: 2.361028 \tValidation Loss: 2.410496\n",
      "Validation loss decreased (2.410553 --> 2.410496).  Saving model ...\n",
      "Epoch: 2925 \tTraining Loss: 2.344412 \tValidation Loss: 2.410373\n",
      "Validation loss decreased (2.410496 --> 2.410373).  Saving model ...\n",
      "Epoch: 2926 \tTraining Loss: 2.363224 \tValidation Loss: 2.410358\n",
      "Validation loss decreased (2.410373 --> 2.410358).  Saving model ...\n",
      "Epoch: 2927 \tTraining Loss: 2.369508 \tValidation Loss: 2.410425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2928 \tTraining Loss: 2.365909 \tValidation Loss: 2.410425\n",
      "Epoch: 2929 \tTraining Loss: 2.360297 \tValidation Loss: 2.410393\n",
      "Epoch: 2930 \tTraining Loss: 2.348342 \tValidation Loss: 2.410280\n",
      "Validation loss decreased (2.410358 --> 2.410280).  Saving model ...\n",
      "Epoch: 2931 \tTraining Loss: 2.353920 \tValidation Loss: 2.410167\n",
      "Validation loss decreased (2.410280 --> 2.410167).  Saving model ...\n",
      "Epoch: 2932 \tTraining Loss: 2.351137 \tValidation Loss: 2.410124\n",
      "Validation loss decreased (2.410167 --> 2.410124).  Saving model ...\n",
      "Epoch: 2933 \tTraining Loss: 2.339792 \tValidation Loss: 2.410019\n",
      "Validation loss decreased (2.410124 --> 2.410019).  Saving model ...\n",
      "Epoch: 2934 \tTraining Loss: 2.360465 \tValidation Loss: 2.410016\n",
      "Validation loss decreased (2.410019 --> 2.410016).  Saving model ...\n",
      "Epoch: 2935 \tTraining Loss: 2.364527 \tValidation Loss: 2.409987\n",
      "Validation loss decreased (2.410016 --> 2.409987).  Saving model ...\n",
      "Epoch: 2936 \tTraining Loss: 2.364577 \tValidation Loss: 2.409927\n",
      "Validation loss decreased (2.409987 --> 2.409927).  Saving model ...\n",
      "Epoch: 2937 \tTraining Loss: 2.360409 \tValidation Loss: 2.409815\n",
      "Validation loss decreased (2.409927 --> 2.409815).  Saving model ...\n",
      "Epoch: 2938 \tTraining Loss: 2.353237 \tValidation Loss: 2.409764\n",
      "Validation loss decreased (2.409815 --> 2.409764).  Saving model ...\n",
      "Epoch: 2939 \tTraining Loss: 2.380894 \tValidation Loss: 2.409856\n",
      "Epoch: 2940 \tTraining Loss: 2.358674 \tValidation Loss: 2.409906\n",
      "Epoch: 2941 \tTraining Loss: 2.362185 \tValidation Loss: 2.409864\n",
      "Epoch: 2942 \tTraining Loss: 2.362813 \tValidation Loss: 2.409834\n",
      "Epoch: 2943 \tTraining Loss: 2.357089 \tValidation Loss: 2.409807\n",
      "Epoch: 2944 \tTraining Loss: 2.366138 \tValidation Loss: 2.409821\n",
      "Epoch: 2945 \tTraining Loss: 2.356838 \tValidation Loss: 2.409905\n",
      "Epoch: 2946 \tTraining Loss: 2.360427 \tValidation Loss: 2.409908\n",
      "Epoch: 2947 \tTraining Loss: 2.369051 \tValidation Loss: 2.409901\n",
      "Epoch: 2948 \tTraining Loss: 2.352083 \tValidation Loss: 2.409921\n",
      "Epoch: 2949 \tTraining Loss: 2.362822 \tValidation Loss: 2.409924\n",
      "Epoch: 2950 \tTraining Loss: 2.351720 \tValidation Loss: 2.409915\n",
      "Epoch: 2951 \tTraining Loss: 2.354064 \tValidation Loss: 2.409856\n",
      "Epoch: 2952 \tTraining Loss: 2.368654 \tValidation Loss: 2.409899\n",
      "Epoch: 2953 \tTraining Loss: 2.349874 \tValidation Loss: 2.409915\n",
      "Epoch: 2954 \tTraining Loss: 2.355089 \tValidation Loss: 2.409846\n",
      "Epoch: 2955 \tTraining Loss: 2.362236 \tValidation Loss: 2.409812\n",
      "Epoch: 2956 \tTraining Loss: 2.354954 \tValidation Loss: 2.409843\n",
      "Epoch: 2957 \tTraining Loss: 2.367357 \tValidation Loss: 2.409867\n",
      "Epoch: 2958 \tTraining Loss: 2.361509 \tValidation Loss: 2.409804\n",
      "Epoch: 2959 \tTraining Loss: 2.367277 \tValidation Loss: 2.409814\n",
      "Epoch: 2960 \tTraining Loss: 2.359667 \tValidation Loss: 2.409839\n",
      "Epoch: 2961 \tTraining Loss: 2.342003 \tValidation Loss: 2.409755\n",
      "Validation loss decreased (2.409764 --> 2.409755).  Saving model ...\n",
      "Epoch: 2962 \tTraining Loss: 2.366114 \tValidation Loss: 2.409755\n",
      "Validation loss decreased (2.409755 --> 2.409755).  Saving model ...\n",
      "Epoch: 2963 \tTraining Loss: 2.356488 \tValidation Loss: 2.409732\n",
      "Validation loss decreased (2.409755 --> 2.409732).  Saving model ...\n",
      "Epoch: 2964 \tTraining Loss: 2.363095 \tValidation Loss: 2.409751\n",
      "Epoch: 2965 \tTraining Loss: 2.367781 \tValidation Loss: 2.409840\n",
      "Epoch: 2966 \tTraining Loss: 2.358934 \tValidation Loss: 2.409846\n",
      "Epoch: 2967 \tTraining Loss: 2.374392 \tValidation Loss: 2.409895\n",
      "Epoch: 2968 \tTraining Loss: 2.357580 \tValidation Loss: 2.409911\n",
      "Epoch: 2969 \tTraining Loss: 2.361664 \tValidation Loss: 2.409898\n",
      "Epoch: 2970 \tTraining Loss: 2.360331 \tValidation Loss: 2.409770\n",
      "Epoch: 2971 \tTraining Loss: 2.351697 \tValidation Loss: 2.409699\n",
      "Validation loss decreased (2.409732 --> 2.409699).  Saving model ...\n",
      "Epoch: 2972 \tTraining Loss: 2.365883 \tValidation Loss: 2.409549\n",
      "Validation loss decreased (2.409699 --> 2.409549).  Saving model ...\n",
      "Epoch: 2973 \tTraining Loss: 2.363571 \tValidation Loss: 2.409388\n",
      "Validation loss decreased (2.409549 --> 2.409388).  Saving model ...\n",
      "Epoch: 2974 \tTraining Loss: 2.368458 \tValidation Loss: 2.409408\n",
      "Epoch: 2975 \tTraining Loss: 2.356510 \tValidation Loss: 2.409322\n",
      "Validation loss decreased (2.409388 --> 2.409322).  Saving model ...\n",
      "Epoch: 2976 \tTraining Loss: 2.363022 \tValidation Loss: 2.409287\n",
      "Validation loss decreased (2.409322 --> 2.409287).  Saving model ...\n",
      "Epoch: 2977 \tTraining Loss: 2.364427 \tValidation Loss: 2.409398\n",
      "Epoch: 2978 \tTraining Loss: 2.343610 \tValidation Loss: 2.409342\n",
      "Epoch: 2979 \tTraining Loss: 2.357356 \tValidation Loss: 2.409320\n",
      "Epoch: 2980 \tTraining Loss: 2.346050 \tValidation Loss: 2.409217\n",
      "Validation loss decreased (2.409287 --> 2.409217).  Saving model ...\n",
      "Epoch: 2981 \tTraining Loss: 2.360245 \tValidation Loss: 2.409172\n",
      "Validation loss decreased (2.409217 --> 2.409172).  Saving model ...\n",
      "Epoch: 2982 \tTraining Loss: 2.357887 \tValidation Loss: 2.409027\n",
      "Validation loss decreased (2.409172 --> 2.409027).  Saving model ...\n",
      "Epoch: 2983 \tTraining Loss: 2.365616 \tValidation Loss: 2.409147\n",
      "Epoch: 2984 \tTraining Loss: 2.356248 \tValidation Loss: 2.409151\n",
      "Epoch: 2985 \tTraining Loss: 2.354766 \tValidation Loss: 2.409097\n",
      "Epoch: 2986 \tTraining Loss: 2.357853 \tValidation Loss: 2.409042\n",
      "Epoch: 2987 \tTraining Loss: 2.352024 \tValidation Loss: 2.408956\n",
      "Validation loss decreased (2.409027 --> 2.408956).  Saving model ...\n",
      "Epoch: 2988 \tTraining Loss: 2.361710 \tValidation Loss: 2.408916\n",
      "Validation loss decreased (2.408956 --> 2.408916).  Saving model ...\n",
      "Epoch: 2989 \tTraining Loss: 2.354231 \tValidation Loss: 2.408850\n",
      "Validation loss decreased (2.408916 --> 2.408850).  Saving model ...\n",
      "Epoch: 2990 \tTraining Loss: 2.370231 \tValidation Loss: 2.408865\n",
      "Epoch: 2991 \tTraining Loss: 2.360094 \tValidation Loss: 2.408995\n",
      "Epoch: 2992 \tTraining Loss: 2.362411 \tValidation Loss: 2.408996\n",
      "Epoch: 2993 \tTraining Loss: 2.348270 \tValidation Loss: 2.408929\n",
      "Epoch: 2994 \tTraining Loss: 2.350545 \tValidation Loss: 2.408822\n",
      "Validation loss decreased (2.408850 --> 2.408822).  Saving model ...\n",
      "Epoch: 2995 \tTraining Loss: 2.355465 \tValidation Loss: 2.408910\n",
      "Epoch: 2996 \tTraining Loss: 2.356528 \tValidation Loss: 2.408916\n",
      "Epoch: 2997 \tTraining Loss: 2.354745 \tValidation Loss: 2.408866\n",
      "Epoch: 2998 \tTraining Loss: 2.354578 \tValidation Loss: 2.408848\n",
      "Epoch: 2999 \tTraining Loss: 2.355231 \tValidation Loss: 2.408803\n",
      "Validation loss decreased (2.408822 --> 2.408803).  Saving model ...\n",
      "Epoch: 3000 \tTraining Loss: 2.353929 \tValidation Loss: 2.408844\n",
      "Epoch: 3001 \tTraining Loss: 2.361681 \tValidation Loss: 2.408799\n",
      "Validation loss decreased (2.408803 --> 2.408799).  Saving model ...\n",
      "Epoch: 3002 \tTraining Loss: 2.344363 \tValidation Loss: 2.408649\n",
      "Validation loss decreased (2.408799 --> 2.408649).  Saving model ...\n",
      "Epoch: 3003 \tTraining Loss: 2.352447 \tValidation Loss: 2.408520\n",
      "Validation loss decreased (2.408649 --> 2.408520).  Saving model ...\n",
      "Epoch: 3004 \tTraining Loss: 2.355596 \tValidation Loss: 2.408604\n",
      "Epoch: 3005 \tTraining Loss: 2.361255 \tValidation Loss: 2.408618\n",
      "Epoch: 3006 \tTraining Loss: 2.370177 \tValidation Loss: 2.408662\n",
      "Epoch: 3007 \tTraining Loss: 2.359605 \tValidation Loss: 2.408687\n",
      "Epoch: 3008 \tTraining Loss: 2.366814 \tValidation Loss: 2.408760\n",
      "Epoch: 3009 \tTraining Loss: 2.362669 \tValidation Loss: 2.408774\n",
      "Epoch: 3010 \tTraining Loss: 2.353652 \tValidation Loss: 2.408716\n",
      "Epoch: 3011 \tTraining Loss: 2.368642 \tValidation Loss: 2.408891\n",
      "Epoch: 3012 \tTraining Loss: 2.351742 \tValidation Loss: 2.408856\n",
      "Epoch: 3013 \tTraining Loss: 2.360089 \tValidation Loss: 2.408873\n",
      "Epoch: 3014 \tTraining Loss: 2.349935 \tValidation Loss: 2.408798\n",
      "Epoch: 3015 \tTraining Loss: 2.368325 \tValidation Loss: 2.408814\n",
      "Epoch: 3016 \tTraining Loss: 2.361837 \tValidation Loss: 2.408720\n",
      "Epoch: 3017 \tTraining Loss: 2.360740 \tValidation Loss: 2.408687\n",
      "Epoch: 3018 \tTraining Loss: 2.354591 \tValidation Loss: 2.408745\n",
      "Epoch: 3019 \tTraining Loss: 2.358105 \tValidation Loss: 2.408626\n",
      "Epoch: 3020 \tTraining Loss: 2.353042 \tValidation Loss: 2.408591\n",
      "Epoch: 3021 \tTraining Loss: 2.347606 \tValidation Loss: 2.408419\n",
      "Validation loss decreased (2.408520 --> 2.408419).  Saving model ...\n",
      "Epoch: 3022 \tTraining Loss: 2.368726 \tValidation Loss: 2.408438\n",
      "Epoch: 3023 \tTraining Loss: 2.355199 \tValidation Loss: 2.408330\n",
      "Validation loss decreased (2.408419 --> 2.408330).  Saving model ...\n",
      "Epoch: 3024 \tTraining Loss: 2.351230 \tValidation Loss: 2.408282\n",
      "Validation loss decreased (2.408330 --> 2.408282).  Saving model ...\n",
      "Epoch: 3025 \tTraining Loss: 2.363827 \tValidation Loss: 2.408296\n",
      "Epoch: 3026 \tTraining Loss: 2.360726 \tValidation Loss: 2.408289\n",
      "Epoch: 3027 \tTraining Loss: 2.343663 \tValidation Loss: 2.408162\n",
      "Validation loss decreased (2.408282 --> 2.408162).  Saving model ...\n",
      "Epoch: 3028 \tTraining Loss: 2.357258 \tValidation Loss: 2.408097\n",
      "Validation loss decreased (2.408162 --> 2.408097).  Saving model ...\n",
      "Epoch: 3029 \tTraining Loss: 2.354945 \tValidation Loss: 2.408117\n",
      "Epoch: 3030 \tTraining Loss: 2.349035 \tValidation Loss: 2.408093\n",
      "Validation loss decreased (2.408097 --> 2.408093).  Saving model ...\n",
      "Epoch: 3031 \tTraining Loss: 2.337222 \tValidation Loss: 2.408099\n",
      "Epoch: 3032 \tTraining Loss: 2.359837 \tValidation Loss: 2.408206\n",
      "Epoch: 3033 \tTraining Loss: 2.363046 \tValidation Loss: 2.408135\n",
      "Epoch: 3034 \tTraining Loss: 2.363799 \tValidation Loss: 2.408091\n",
      "Validation loss decreased (2.408093 --> 2.408091).  Saving model ...\n",
      "Epoch: 3035 \tTraining Loss: 2.361412 \tValidation Loss: 2.407971\n",
      "Validation loss decreased (2.408091 --> 2.407971).  Saving model ...\n",
      "Epoch: 3036 \tTraining Loss: 2.350609 \tValidation Loss: 2.407928\n",
      "Validation loss decreased (2.407971 --> 2.407928).  Saving model ...\n",
      "Epoch: 3037 \tTraining Loss: 2.361068 \tValidation Loss: 2.407977\n",
      "Epoch: 3038 \tTraining Loss: 2.357536 \tValidation Loss: 2.407930\n",
      "Epoch: 3039 \tTraining Loss: 2.348895 \tValidation Loss: 2.407864\n",
      "Validation loss decreased (2.407928 --> 2.407864).  Saving model ...\n",
      "Epoch: 3040 \tTraining Loss: 2.355499 \tValidation Loss: 2.407866\n",
      "Epoch: 3041 \tTraining Loss: 2.346192 \tValidation Loss: 2.407839\n",
      "Validation loss decreased (2.407864 --> 2.407839).  Saving model ...\n",
      "Epoch: 3042 \tTraining Loss: 2.348364 \tValidation Loss: 2.407793\n",
      "Validation loss decreased (2.407839 --> 2.407793).  Saving model ...\n",
      "Epoch: 3043 \tTraining Loss: 2.357868 \tValidation Loss: 2.407729\n",
      "Validation loss decreased (2.407793 --> 2.407729).  Saving model ...\n",
      "Epoch: 3044 \tTraining Loss: 2.339211 \tValidation Loss: 2.407630\n",
      "Validation loss decreased (2.407729 --> 2.407630).  Saving model ...\n",
      "Epoch: 3045 \tTraining Loss: 2.355293 \tValidation Loss: 2.407701\n",
      "Epoch: 3046 \tTraining Loss: 2.358474 \tValidation Loss: 2.407687\n",
      "Epoch: 3047 \tTraining Loss: 2.355042 \tValidation Loss: 2.407616\n",
      "Validation loss decreased (2.407630 --> 2.407616).  Saving model ...\n",
      "Epoch: 3048 \tTraining Loss: 2.340554 \tValidation Loss: 2.407575\n",
      "Validation loss decreased (2.407616 --> 2.407575).  Saving model ...\n",
      "Epoch: 3049 \tTraining Loss: 2.364455 \tValidation Loss: 2.407620\n",
      "Epoch: 3050 \tTraining Loss: 2.357785 \tValidation Loss: 2.407591\n",
      "Epoch: 3051 \tTraining Loss: 2.353918 \tValidation Loss: 2.407444\n",
      "Validation loss decreased (2.407575 --> 2.407444).  Saving model ...\n",
      "Epoch: 3052 \tTraining Loss: 2.351984 \tValidation Loss: 2.407361\n",
      "Validation loss decreased (2.407444 --> 2.407361).  Saving model ...\n",
      "Epoch: 3053 \tTraining Loss: 2.359552 \tValidation Loss: 2.407373\n",
      "Epoch: 3054 \tTraining Loss: 2.362886 \tValidation Loss: 2.407401\n",
      "Epoch: 3055 \tTraining Loss: 2.351545 \tValidation Loss: 2.407486\n",
      "Epoch: 3056 \tTraining Loss: 2.367372 \tValidation Loss: 2.407462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3057 \tTraining Loss: 2.351577 \tValidation Loss: 2.407480\n",
      "Epoch: 3058 \tTraining Loss: 2.350355 \tValidation Loss: 2.407426\n",
      "Epoch: 3059 \tTraining Loss: 2.346394 \tValidation Loss: 2.407358\n",
      "Validation loss decreased (2.407361 --> 2.407358).  Saving model ...\n",
      "Epoch: 3060 \tTraining Loss: 2.342808 \tValidation Loss: 2.407355\n",
      "Validation loss decreased (2.407358 --> 2.407355).  Saving model ...\n",
      "Epoch: 3061 \tTraining Loss: 2.340563 \tValidation Loss: 2.407242\n",
      "Validation loss decreased (2.407355 --> 2.407242).  Saving model ...\n",
      "Epoch: 3062 \tTraining Loss: 2.364505 \tValidation Loss: 2.407335\n",
      "Epoch: 3063 \tTraining Loss: 2.350241 \tValidation Loss: 2.407292\n",
      "Epoch: 3064 \tTraining Loss: 2.346354 \tValidation Loss: 2.407291\n",
      "Epoch: 3065 \tTraining Loss: 2.357691 \tValidation Loss: 2.407313\n",
      "Epoch: 3066 \tTraining Loss: 2.351166 \tValidation Loss: 2.407337\n",
      "Epoch: 3067 \tTraining Loss: 2.360513 \tValidation Loss: 2.407381\n",
      "Epoch: 3068 \tTraining Loss: 2.353234 \tValidation Loss: 2.407405\n",
      "Epoch: 3069 \tTraining Loss: 2.353047 \tValidation Loss: 2.407388\n",
      "Epoch: 3070 \tTraining Loss: 2.343494 \tValidation Loss: 2.407327\n",
      "Epoch: 3071 \tTraining Loss: 2.365786 \tValidation Loss: 2.407288\n",
      "Epoch: 3072 \tTraining Loss: 2.344696 \tValidation Loss: 2.407183\n",
      "Validation loss decreased (2.407242 --> 2.407183).  Saving model ...\n",
      "Epoch: 3073 \tTraining Loss: 2.342401 \tValidation Loss: 2.407202\n",
      "Epoch: 3074 \tTraining Loss: 2.357554 \tValidation Loss: 2.407199\n",
      "Epoch: 3075 \tTraining Loss: 2.336509 \tValidation Loss: 2.407181\n",
      "Validation loss decreased (2.407183 --> 2.407181).  Saving model ...\n",
      "Epoch: 3076 \tTraining Loss: 2.359651 \tValidation Loss: 2.407187\n",
      "Epoch: 3077 \tTraining Loss: 2.363095 \tValidation Loss: 2.407159\n",
      "Validation loss decreased (2.407181 --> 2.407159).  Saving model ...\n",
      "Epoch: 3078 \tTraining Loss: 2.355713 \tValidation Loss: 2.407118\n",
      "Validation loss decreased (2.407159 --> 2.407118).  Saving model ...\n",
      "Epoch: 3079 \tTraining Loss: 2.350067 \tValidation Loss: 2.407142\n",
      "Epoch: 3080 \tTraining Loss: 2.345483 \tValidation Loss: 2.407058\n",
      "Validation loss decreased (2.407118 --> 2.407058).  Saving model ...\n",
      "Epoch: 3081 \tTraining Loss: 2.339880 \tValidation Loss: 2.407041\n",
      "Validation loss decreased (2.407058 --> 2.407041).  Saving model ...\n",
      "Epoch: 3082 \tTraining Loss: 2.347497 \tValidation Loss: 2.406971\n",
      "Validation loss decreased (2.407041 --> 2.406971).  Saving model ...\n",
      "Epoch: 3083 \tTraining Loss: 2.346201 \tValidation Loss: 2.407009\n",
      "Epoch: 3084 \tTraining Loss: 2.350571 \tValidation Loss: 2.407073\n",
      "Epoch: 3085 \tTraining Loss: 2.341823 \tValidation Loss: 2.406997\n",
      "Epoch: 3086 \tTraining Loss: 2.337234 \tValidation Loss: 2.406908\n",
      "Validation loss decreased (2.406971 --> 2.406908).  Saving model ...\n",
      "Epoch: 3087 \tTraining Loss: 2.348592 \tValidation Loss: 2.406856\n",
      "Validation loss decreased (2.406908 --> 2.406856).  Saving model ...\n",
      "Epoch: 3088 \tTraining Loss: 2.349796 \tValidation Loss: 2.406864\n",
      "Epoch: 3089 \tTraining Loss: 2.347260 \tValidation Loss: 2.406887\n",
      "Epoch: 3090 \tTraining Loss: 2.348288 \tValidation Loss: 2.406840\n",
      "Validation loss decreased (2.406856 --> 2.406840).  Saving model ...\n",
      "Epoch: 3091 \tTraining Loss: 2.345531 \tValidation Loss: 2.406756\n",
      "Validation loss decreased (2.406840 --> 2.406756).  Saving model ...\n",
      "Epoch: 3092 \tTraining Loss: 2.352544 \tValidation Loss: 2.406852\n",
      "Epoch: 3093 \tTraining Loss: 2.356188 \tValidation Loss: 2.406864\n",
      "Epoch: 3094 \tTraining Loss: 2.353566 \tValidation Loss: 2.406776\n",
      "Epoch: 3095 \tTraining Loss: 2.340638 \tValidation Loss: 2.406772\n",
      "Epoch: 3096 \tTraining Loss: 2.338665 \tValidation Loss: 2.406659\n",
      "Validation loss decreased (2.406756 --> 2.406659).  Saving model ...\n",
      "Epoch: 3097 \tTraining Loss: 2.351598 \tValidation Loss: 2.406745\n",
      "Epoch: 3098 \tTraining Loss: 2.346747 \tValidation Loss: 2.406806\n",
      "Epoch: 3099 \tTraining Loss: 2.348309 \tValidation Loss: 2.406903\n",
      "Epoch: 3100 \tTraining Loss: 2.371391 \tValidation Loss: 2.406969\n",
      "Epoch: 3101 \tTraining Loss: 2.346515 \tValidation Loss: 2.406862\n",
      "Epoch: 3102 \tTraining Loss: 2.344836 \tValidation Loss: 2.406848\n",
      "Epoch: 3103 \tTraining Loss: 2.345091 \tValidation Loss: 2.406730\n",
      "Epoch: 3104 \tTraining Loss: 2.351663 \tValidation Loss: 2.406685\n",
      "Epoch: 3105 \tTraining Loss: 2.365799 \tValidation Loss: 2.406693\n",
      "Epoch: 3106 \tTraining Loss: 2.343564 \tValidation Loss: 2.406654\n",
      "Validation loss decreased (2.406659 --> 2.406654).  Saving model ...\n",
      "Epoch: 3107 \tTraining Loss: 2.334196 \tValidation Loss: 2.406559\n",
      "Validation loss decreased (2.406654 --> 2.406559).  Saving model ...\n",
      "Epoch: 3108 \tTraining Loss: 2.354388 \tValidation Loss: 2.406454\n",
      "Validation loss decreased (2.406559 --> 2.406454).  Saving model ...\n",
      "Epoch: 3109 \tTraining Loss: 2.348228 \tValidation Loss: 2.406547\n",
      "Epoch: 3110 \tTraining Loss: 2.340367 \tValidation Loss: 2.406405\n",
      "Validation loss decreased (2.406454 --> 2.406405).  Saving model ...\n",
      "Epoch: 3111 \tTraining Loss: 2.343104 \tValidation Loss: 2.406365\n",
      "Validation loss decreased (2.406405 --> 2.406365).  Saving model ...\n",
      "Epoch: 3112 \tTraining Loss: 2.353783 \tValidation Loss: 2.406331\n",
      "Validation loss decreased (2.406365 --> 2.406331).  Saving model ...\n",
      "Epoch: 3113 \tTraining Loss: 2.359182 \tValidation Loss: 2.406255\n",
      "Validation loss decreased (2.406331 --> 2.406255).  Saving model ...\n",
      "Epoch: 3114 \tTraining Loss: 2.357807 \tValidation Loss: 2.406239\n",
      "Validation loss decreased (2.406255 --> 2.406239).  Saving model ...\n",
      "Epoch: 3115 \tTraining Loss: 2.342195 \tValidation Loss: 2.406201\n",
      "Validation loss decreased (2.406239 --> 2.406201).  Saving model ...\n",
      "Epoch: 3116 \tTraining Loss: 2.357327 \tValidation Loss: 2.406188\n",
      "Validation loss decreased (2.406201 --> 2.406188).  Saving model ...\n",
      "Epoch: 3117 \tTraining Loss: 2.348259 \tValidation Loss: 2.406101\n",
      "Validation loss decreased (2.406188 --> 2.406101).  Saving model ...\n",
      "Epoch: 3118 \tTraining Loss: 2.352178 \tValidation Loss: 2.406057\n",
      "Validation loss decreased (2.406101 --> 2.406057).  Saving model ...\n",
      "Epoch: 3119 \tTraining Loss: 2.367173 \tValidation Loss: 2.406116\n",
      "Epoch: 3120 \tTraining Loss: 2.364484 \tValidation Loss: 2.406216\n",
      "Epoch: 3121 \tTraining Loss: 2.348196 \tValidation Loss: 2.406184\n",
      "Epoch: 3122 \tTraining Loss: 2.357474 \tValidation Loss: 2.406155\n",
      "Epoch: 3123 \tTraining Loss: 2.355246 \tValidation Loss: 2.406027\n",
      "Validation loss decreased (2.406057 --> 2.406027).  Saving model ...\n",
      "Epoch: 3124 \tTraining Loss: 2.353219 \tValidation Loss: 2.406067\n",
      "Epoch: 3125 \tTraining Loss: 2.347847 \tValidation Loss: 2.405980\n",
      "Validation loss decreased (2.406027 --> 2.405980).  Saving model ...\n",
      "Epoch: 3126 \tTraining Loss: 2.340456 \tValidation Loss: 2.405860\n",
      "Validation loss decreased (2.405980 --> 2.405860).  Saving model ...\n",
      "Epoch: 3127 \tTraining Loss: 2.356767 \tValidation Loss: 2.405916\n",
      "Epoch: 3128 \tTraining Loss: 2.355137 \tValidation Loss: 2.405885\n",
      "Epoch: 3129 \tTraining Loss: 2.346795 \tValidation Loss: 2.405769\n",
      "Validation loss decreased (2.405860 --> 2.405769).  Saving model ...\n",
      "Epoch: 3130 \tTraining Loss: 2.342236 \tValidation Loss: 2.405695\n",
      "Validation loss decreased (2.405769 --> 2.405695).  Saving model ...\n",
      "Epoch: 3131 \tTraining Loss: 2.354998 \tValidation Loss: 2.405734\n",
      "Epoch: 3132 \tTraining Loss: 2.355631 \tValidation Loss: 2.405648\n",
      "Validation loss decreased (2.405695 --> 2.405648).  Saving model ...\n",
      "Epoch: 3133 \tTraining Loss: 2.353858 \tValidation Loss: 2.405682\n",
      "Epoch: 3134 \tTraining Loss: 2.346626 \tValidation Loss: 2.405580\n",
      "Validation loss decreased (2.405648 --> 2.405580).  Saving model ...\n",
      "Epoch: 3135 \tTraining Loss: 2.335333 \tValidation Loss: 2.405539\n",
      "Validation loss decreased (2.405580 --> 2.405539).  Saving model ...\n",
      "Epoch: 3136 \tTraining Loss: 2.348845 \tValidation Loss: 2.405463\n",
      "Validation loss decreased (2.405539 --> 2.405463).  Saving model ...\n",
      "Epoch: 3137 \tTraining Loss: 2.347538 \tValidation Loss: 2.405487\n",
      "Epoch: 3138 \tTraining Loss: 2.352788 \tValidation Loss: 2.405479\n",
      "Epoch: 3139 \tTraining Loss: 2.350105 \tValidation Loss: 2.405460\n",
      "Validation loss decreased (2.405463 --> 2.405460).  Saving model ...\n",
      "Epoch: 3140 \tTraining Loss: 2.354913 \tValidation Loss: 2.405387\n",
      "Validation loss decreased (2.405460 --> 2.405387).  Saving model ...\n",
      "Epoch: 3141 \tTraining Loss: 2.346368 \tValidation Loss: 2.405356\n",
      "Validation loss decreased (2.405387 --> 2.405356).  Saving model ...\n",
      "Epoch: 3142 \tTraining Loss: 2.344705 \tValidation Loss: 2.405346\n",
      "Validation loss decreased (2.405356 --> 2.405346).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3143 \tTraining Loss: 2.348986 \tValidation Loss: 2.405497\n",
      "Epoch: 3144 \tTraining Loss: 2.356597 \tValidation Loss: 2.405509\n",
      "Epoch: 3145 \tTraining Loss: 2.350090 \tValidation Loss: 2.405556\n",
      "Epoch: 3146 \tTraining Loss: 2.355793 \tValidation Loss: 2.405462\n",
      "Epoch: 3147 \tTraining Loss: 2.341809 \tValidation Loss: 2.405438\n",
      "Epoch: 3148 \tTraining Loss: 2.343397 \tValidation Loss: 2.405449\n",
      "Epoch: 3149 \tTraining Loss: 2.359597 \tValidation Loss: 2.405303\n",
      "Validation loss decreased (2.405346 --> 2.405303).  Saving model ...\n",
      "Epoch: 3150 \tTraining Loss: 2.356647 \tValidation Loss: 2.405328\n",
      "Epoch: 3151 \tTraining Loss: 2.358966 \tValidation Loss: 2.405303\n",
      "Validation loss decreased (2.405303 --> 2.405303).  Saving model ...\n",
      "Epoch: 3152 \tTraining Loss: 2.358061 \tValidation Loss: 2.405268\n",
      "Validation loss decreased (2.405303 --> 2.405268).  Saving model ...\n",
      "Epoch: 3153 \tTraining Loss: 2.349128 \tValidation Loss: 2.405145\n",
      "Validation loss decreased (2.405268 --> 2.405145).  Saving model ...\n",
      "Epoch: 3154 \tTraining Loss: 2.349902 \tValidation Loss: 2.405157\n",
      "Epoch: 3155 \tTraining Loss: 2.355713 \tValidation Loss: 2.405319\n",
      "Epoch: 3156 \tTraining Loss: 2.347706 \tValidation Loss: 2.405347\n",
      "Epoch: 3157 \tTraining Loss: 2.341876 \tValidation Loss: 2.405398\n",
      "Epoch: 3158 \tTraining Loss: 2.351107 \tValidation Loss: 2.405394\n",
      "Epoch: 3159 \tTraining Loss: 2.341845 \tValidation Loss: 2.405349\n",
      "Epoch: 3160 \tTraining Loss: 2.352572 \tValidation Loss: 2.405355\n",
      "Epoch: 3161 \tTraining Loss: 2.343205 \tValidation Loss: 2.405269\n",
      "Epoch: 3162 \tTraining Loss: 2.361011 \tValidation Loss: 2.405174\n",
      "Epoch: 3163 \tTraining Loss: 2.341286 \tValidation Loss: 2.404955\n",
      "Validation loss decreased (2.405145 --> 2.404955).  Saving model ...\n",
      "Epoch: 3164 \tTraining Loss: 2.345059 \tValidation Loss: 2.404994\n",
      "Epoch: 3165 \tTraining Loss: 2.361788 \tValidation Loss: 2.405047\n",
      "Epoch: 3166 \tTraining Loss: 2.347194 \tValidation Loss: 2.405017\n",
      "Epoch: 3167 \tTraining Loss: 2.350704 \tValidation Loss: 2.405021\n",
      "Epoch: 3168 \tTraining Loss: 2.330970 \tValidation Loss: 2.404930\n",
      "Validation loss decreased (2.404955 --> 2.404930).  Saving model ...\n",
      "Epoch: 3169 \tTraining Loss: 2.346360 \tValidation Loss: 2.404931\n",
      "Epoch: 3170 \tTraining Loss: 2.352022 \tValidation Loss: 2.404942\n",
      "Epoch: 3171 \tTraining Loss: 2.336915 \tValidation Loss: 2.404836\n",
      "Validation loss decreased (2.404930 --> 2.404836).  Saving model ...\n",
      "Epoch: 3172 \tTraining Loss: 2.343910 \tValidation Loss: 2.404893\n",
      "Epoch: 3173 \tTraining Loss: 2.351841 \tValidation Loss: 2.404891\n",
      "Epoch: 3174 \tTraining Loss: 2.361376 \tValidation Loss: 2.404864\n",
      "Epoch: 3175 \tTraining Loss: 2.352800 \tValidation Loss: 2.404729\n",
      "Validation loss decreased (2.404836 --> 2.404729).  Saving model ...\n",
      "Epoch: 3176 \tTraining Loss: 2.347711 \tValidation Loss: 2.404752\n",
      "Epoch: 3177 \tTraining Loss: 2.342387 \tValidation Loss: 2.404696\n",
      "Validation loss decreased (2.404729 --> 2.404696).  Saving model ...\n",
      "Epoch: 3178 \tTraining Loss: 2.351778 \tValidation Loss: 2.404610\n",
      "Validation loss decreased (2.404696 --> 2.404610).  Saving model ...\n",
      "Epoch: 3179 \tTraining Loss: 2.354922 \tValidation Loss: 2.404600\n",
      "Validation loss decreased (2.404610 --> 2.404600).  Saving model ...\n",
      "Epoch: 3180 \tTraining Loss: 2.352094 \tValidation Loss: 2.404580\n",
      "Validation loss decreased (2.404600 --> 2.404580).  Saving model ...\n",
      "Epoch: 3181 \tTraining Loss: 2.347213 \tValidation Loss: 2.404578\n",
      "Validation loss decreased (2.404580 --> 2.404578).  Saving model ...\n",
      "Epoch: 3182 \tTraining Loss: 2.345800 \tValidation Loss: 2.404507\n",
      "Validation loss decreased (2.404578 --> 2.404507).  Saving model ...\n",
      "Epoch: 3183 \tTraining Loss: 2.346661 \tValidation Loss: 2.404545\n",
      "Epoch: 3184 \tTraining Loss: 2.350768 \tValidation Loss: 2.404549\n",
      "Epoch: 3185 \tTraining Loss: 2.358305 \tValidation Loss: 2.404558\n",
      "Epoch: 3186 \tTraining Loss: 2.361023 \tValidation Loss: 2.404534\n",
      "Epoch: 3187 \tTraining Loss: 2.348248 \tValidation Loss: 2.404460\n",
      "Validation loss decreased (2.404507 --> 2.404460).  Saving model ...\n",
      "Epoch: 3188 \tTraining Loss: 2.346800 \tValidation Loss: 2.404540\n",
      "Epoch: 3189 \tTraining Loss: 2.339948 \tValidation Loss: 2.404505\n",
      "Epoch: 3190 \tTraining Loss: 2.355357 \tValidation Loss: 2.404469\n",
      "Epoch: 3191 \tTraining Loss: 2.348457 \tValidation Loss: 2.404481\n",
      "Epoch: 3192 \tTraining Loss: 2.348803 \tValidation Loss: 2.404416\n",
      "Validation loss decreased (2.404460 --> 2.404416).  Saving model ...\n",
      "Epoch: 3193 \tTraining Loss: 2.333163 \tValidation Loss: 2.404239\n",
      "Validation loss decreased (2.404416 --> 2.404239).  Saving model ...\n",
      "Epoch: 3194 \tTraining Loss: 2.351541 \tValidation Loss: 2.404300\n",
      "Epoch: 3195 \tTraining Loss: 2.353076 \tValidation Loss: 2.404294\n",
      "Epoch: 3196 \tTraining Loss: 2.343411 \tValidation Loss: 2.404196\n",
      "Validation loss decreased (2.404239 --> 2.404196).  Saving model ...\n",
      "Epoch: 3197 \tTraining Loss: 2.326967 \tValidation Loss: 2.404022\n",
      "Validation loss decreased (2.404196 --> 2.404022).  Saving model ...\n",
      "Epoch: 3198 \tTraining Loss: 2.332096 \tValidation Loss: 2.403992\n",
      "Validation loss decreased (2.404022 --> 2.403992).  Saving model ...\n",
      "Epoch: 3199 \tTraining Loss: 2.349835 \tValidation Loss: 2.403967\n",
      "Validation loss decreased (2.403992 --> 2.403967).  Saving model ...\n",
      "Epoch: 3200 \tTraining Loss: 2.340952 \tValidation Loss: 2.404045\n",
      "Epoch: 3201 \tTraining Loss: 2.346114 \tValidation Loss: 2.404095\n",
      "Epoch: 3202 \tTraining Loss: 2.337633 \tValidation Loss: 2.404035\n",
      "Epoch: 3203 \tTraining Loss: 2.353339 \tValidation Loss: 2.404102\n",
      "Epoch: 3204 \tTraining Loss: 2.344643 \tValidation Loss: 2.404064\n",
      "Epoch: 3205 \tTraining Loss: 2.341377 \tValidation Loss: 2.403976\n",
      "Epoch: 3206 \tTraining Loss: 2.342498 \tValidation Loss: 2.403978\n",
      "Epoch: 3207 \tTraining Loss: 2.350234 \tValidation Loss: 2.403959\n",
      "Validation loss decreased (2.403967 --> 2.403959).  Saving model ...\n",
      "Epoch: 3208 \tTraining Loss: 2.337529 \tValidation Loss: 2.403788\n",
      "Validation loss decreased (2.403959 --> 2.403788).  Saving model ...\n",
      "Epoch: 3209 \tTraining Loss: 2.334166 \tValidation Loss: 2.403748\n",
      "Validation loss decreased (2.403788 --> 2.403748).  Saving model ...\n",
      "Epoch: 3210 \tTraining Loss: 2.359490 \tValidation Loss: 2.403639\n",
      "Validation loss decreased (2.403748 --> 2.403639).  Saving model ...\n",
      "Epoch: 3211 \tTraining Loss: 2.344243 \tValidation Loss: 2.403663\n",
      "Epoch: 3212 \tTraining Loss: 2.336318 \tValidation Loss: 2.403563\n",
      "Validation loss decreased (2.403639 --> 2.403563).  Saving model ...\n",
      "Epoch: 3213 \tTraining Loss: 2.345552 \tValidation Loss: 2.403498\n",
      "Validation loss decreased (2.403563 --> 2.403498).  Saving model ...\n",
      "Epoch: 3214 \tTraining Loss: 2.355963 \tValidation Loss: 2.403453\n",
      "Validation loss decreased (2.403498 --> 2.403453).  Saving model ...\n",
      "Epoch: 3215 \tTraining Loss: 2.342461 \tValidation Loss: 2.403444\n",
      "Validation loss decreased (2.403453 --> 2.403444).  Saving model ...\n",
      "Epoch: 3216 \tTraining Loss: 2.353921 \tValidation Loss: 2.403530\n",
      "Epoch: 3217 \tTraining Loss: 2.344329 \tValidation Loss: 2.403501\n",
      "Epoch: 3218 \tTraining Loss: 2.348059 \tValidation Loss: 2.403503\n",
      "Epoch: 3219 \tTraining Loss: 2.337006 \tValidation Loss: 2.403316\n",
      "Validation loss decreased (2.403444 --> 2.403316).  Saving model ...\n",
      "Epoch: 3220 \tTraining Loss: 2.347851 \tValidation Loss: 2.403404\n",
      "Epoch: 3221 \tTraining Loss: 2.347369 \tValidation Loss: 2.403500\n",
      "Epoch: 3222 \tTraining Loss: 2.346271 \tValidation Loss: 2.403484\n",
      "Epoch: 3223 \tTraining Loss: 2.344092 \tValidation Loss: 2.403480\n",
      "Epoch: 3224 \tTraining Loss: 2.341395 \tValidation Loss: 2.403411\n",
      "Epoch: 3225 \tTraining Loss: 2.353373 \tValidation Loss: 2.403412\n",
      "Epoch: 3226 \tTraining Loss: 2.357651 \tValidation Loss: 2.403426\n",
      "Epoch: 3227 \tTraining Loss: 2.354436 \tValidation Loss: 2.403430\n",
      "Epoch: 3228 \tTraining Loss: 2.340347 \tValidation Loss: 2.403445\n",
      "Epoch: 3229 \tTraining Loss: 2.358950 \tValidation Loss: 2.403459\n",
      "Epoch: 3230 \tTraining Loss: 2.337488 \tValidation Loss: 2.403341\n",
      "Epoch: 3231 \tTraining Loss: 2.342097 \tValidation Loss: 2.403325\n",
      "Epoch: 3232 \tTraining Loss: 2.338800 \tValidation Loss: 2.403181\n",
      "Validation loss decreased (2.403316 --> 2.403181).  Saving model ...\n",
      "Epoch: 3233 \tTraining Loss: 2.340531 \tValidation Loss: 2.403177\n",
      "Validation loss decreased (2.403181 --> 2.403177).  Saving model ...\n",
      "Epoch: 3234 \tTraining Loss: 2.345105 \tValidation Loss: 2.403143\n",
      "Validation loss decreased (2.403177 --> 2.403143).  Saving model ...\n",
      "Epoch: 3235 \tTraining Loss: 2.331686 \tValidation Loss: 2.403070\n",
      "Validation loss decreased (2.403143 --> 2.403070).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3236 \tTraining Loss: 2.344206 \tValidation Loss: 2.403101\n",
      "Epoch: 3237 \tTraining Loss: 2.341916 \tValidation Loss: 2.402987\n",
      "Validation loss decreased (2.403070 --> 2.402987).  Saving model ...\n",
      "Epoch: 3238 \tTraining Loss: 2.354351 \tValidation Loss: 2.403032\n",
      "Epoch: 3239 \tTraining Loss: 2.342241 \tValidation Loss: 2.402922\n",
      "Validation loss decreased (2.402987 --> 2.402922).  Saving model ...\n",
      "Epoch: 3240 \tTraining Loss: 2.356995 \tValidation Loss: 2.402840\n",
      "Validation loss decreased (2.402922 --> 2.402840).  Saving model ...\n",
      "Epoch: 3241 \tTraining Loss: 2.356684 \tValidation Loss: 2.402878\n",
      "Epoch: 3242 \tTraining Loss: 2.324401 \tValidation Loss: 2.402775\n",
      "Validation loss decreased (2.402840 --> 2.402775).  Saving model ...\n",
      "Epoch: 3243 \tTraining Loss: 2.343815 \tValidation Loss: 2.402815\n",
      "Epoch: 3244 \tTraining Loss: 2.343041 \tValidation Loss: 2.402819\n",
      "Epoch: 3245 \tTraining Loss: 2.344664 \tValidation Loss: 2.402797\n",
      "Epoch: 3246 \tTraining Loss: 2.334483 \tValidation Loss: 2.402781\n",
      "Epoch: 3247 \tTraining Loss: 2.341529 \tValidation Loss: 2.402683\n",
      "Validation loss decreased (2.402775 --> 2.402683).  Saving model ...\n",
      "Epoch: 3248 \tTraining Loss: 2.339677 \tValidation Loss: 2.402702\n",
      "Epoch: 3249 \tTraining Loss: 2.342401 \tValidation Loss: 2.402691\n",
      "Epoch: 3250 \tTraining Loss: 2.339165 \tValidation Loss: 2.402612\n",
      "Validation loss decreased (2.402683 --> 2.402612).  Saving model ...\n",
      "Epoch: 3251 \tTraining Loss: 2.331733 \tValidation Loss: 2.402593\n",
      "Validation loss decreased (2.402612 --> 2.402593).  Saving model ...\n",
      "Epoch: 3252 \tTraining Loss: 2.347471 \tValidation Loss: 2.402630\n",
      "Epoch: 3253 \tTraining Loss: 2.349539 \tValidation Loss: 2.402661\n",
      "Epoch: 3254 \tTraining Loss: 2.345350 \tValidation Loss: 2.402542\n",
      "Validation loss decreased (2.402593 --> 2.402542).  Saving model ...\n",
      "Epoch: 3255 \tTraining Loss: 2.328843 \tValidation Loss: 2.402417\n",
      "Validation loss decreased (2.402542 --> 2.402417).  Saving model ...\n",
      "Epoch: 3256 \tTraining Loss: 2.348278 \tValidation Loss: 2.402378\n",
      "Validation loss decreased (2.402417 --> 2.402378).  Saving model ...\n",
      "Epoch: 3257 \tTraining Loss: 2.352127 \tValidation Loss: 2.402319\n",
      "Validation loss decreased (2.402378 --> 2.402319).  Saving model ...\n",
      "Epoch: 3258 \tTraining Loss: 2.338571 \tValidation Loss: 2.402207\n",
      "Validation loss decreased (2.402319 --> 2.402207).  Saving model ...\n",
      "Epoch: 3259 \tTraining Loss: 2.343380 \tValidation Loss: 2.402191\n",
      "Validation loss decreased (2.402207 --> 2.402191).  Saving model ...\n",
      "Epoch: 3260 \tTraining Loss: 2.336651 \tValidation Loss: 2.402142\n",
      "Validation loss decreased (2.402191 --> 2.402142).  Saving model ...\n",
      "Epoch: 3261 \tTraining Loss: 2.348429 \tValidation Loss: 2.402110\n",
      "Validation loss decreased (2.402142 --> 2.402110).  Saving model ...\n",
      "Epoch: 3262 \tTraining Loss: 2.341758 \tValidation Loss: 2.401942\n",
      "Validation loss decreased (2.402110 --> 2.401942).  Saving model ...\n",
      "Epoch: 3263 \tTraining Loss: 2.351516 \tValidation Loss: 2.401897\n",
      "Validation loss decreased (2.401942 --> 2.401897).  Saving model ...\n",
      "Epoch: 3264 \tTraining Loss: 2.344369 \tValidation Loss: 2.401987\n",
      "Epoch: 3265 \tTraining Loss: 2.337968 \tValidation Loss: 2.401913\n",
      "Epoch: 3266 \tTraining Loss: 2.343127 \tValidation Loss: 2.401859\n",
      "Validation loss decreased (2.401897 --> 2.401859).  Saving model ...\n",
      "Epoch: 3267 \tTraining Loss: 2.346816 \tValidation Loss: 2.401849\n",
      "Validation loss decreased (2.401859 --> 2.401849).  Saving model ...\n",
      "Epoch: 3268 \tTraining Loss: 2.329268 \tValidation Loss: 2.401828\n",
      "Validation loss decreased (2.401849 --> 2.401828).  Saving model ...\n",
      "Epoch: 3269 \tTraining Loss: 2.355187 \tValidation Loss: 2.401883\n",
      "Epoch: 3270 \tTraining Loss: 2.341510 \tValidation Loss: 2.401763\n",
      "Validation loss decreased (2.401828 --> 2.401763).  Saving model ...\n",
      "Epoch: 3271 \tTraining Loss: 2.349213 \tValidation Loss: 2.401778\n",
      "Epoch: 3272 \tTraining Loss: 2.349833 \tValidation Loss: 2.401825\n",
      "Epoch: 3273 \tTraining Loss: 2.345766 \tValidation Loss: 2.401740\n",
      "Validation loss decreased (2.401763 --> 2.401740).  Saving model ...\n",
      "Epoch: 3274 \tTraining Loss: 2.336755 \tValidation Loss: 2.401724\n",
      "Validation loss decreased (2.401740 --> 2.401724).  Saving model ...\n",
      "Epoch: 3275 \tTraining Loss: 2.343740 \tValidation Loss: 2.401575\n",
      "Validation loss decreased (2.401724 --> 2.401575).  Saving model ...\n",
      "Epoch: 3276 \tTraining Loss: 2.339176 \tValidation Loss: 2.401492\n",
      "Validation loss decreased (2.401575 --> 2.401492).  Saving model ...\n",
      "Epoch: 3277 \tTraining Loss: 2.338963 \tValidation Loss: 2.401441\n",
      "Validation loss decreased (2.401492 --> 2.401441).  Saving model ...\n",
      "Epoch: 3278 \tTraining Loss: 2.348934 \tValidation Loss: 2.401487\n",
      "Epoch: 3279 \tTraining Loss: 2.348329 \tValidation Loss: 2.401363\n",
      "Validation loss decreased (2.401441 --> 2.401363).  Saving model ...\n",
      "Epoch: 3280 \tTraining Loss: 2.353140 \tValidation Loss: 2.401403\n",
      "Epoch: 3281 \tTraining Loss: 2.347047 \tValidation Loss: 2.401444\n",
      "Epoch: 3282 \tTraining Loss: 2.339546 \tValidation Loss: 2.401593\n",
      "Epoch: 3283 \tTraining Loss: 2.335623 \tValidation Loss: 2.401634\n",
      "Epoch: 3284 \tTraining Loss: 2.350530 \tValidation Loss: 2.401684\n",
      "Epoch: 3285 \tTraining Loss: 2.346092 \tValidation Loss: 2.401777\n",
      "Epoch: 3286 \tTraining Loss: 2.357049 \tValidation Loss: 2.401770\n",
      "Epoch: 3287 \tTraining Loss: 2.342073 \tValidation Loss: 2.401771\n",
      "Epoch: 3288 \tTraining Loss: 2.336621 \tValidation Loss: 2.401763\n",
      "Epoch: 3289 \tTraining Loss: 2.338299 \tValidation Loss: 2.401717\n",
      "Epoch: 3290 \tTraining Loss: 2.343887 \tValidation Loss: 2.401738\n",
      "Epoch: 3291 \tTraining Loss: 2.340577 \tValidation Loss: 2.401650\n",
      "Epoch: 3292 \tTraining Loss: 2.340332 \tValidation Loss: 2.401586\n",
      "Epoch: 3293 \tTraining Loss: 2.335989 \tValidation Loss: 2.401593\n",
      "Epoch: 3294 \tTraining Loss: 2.338244 \tValidation Loss: 2.401521\n",
      "Epoch: 3295 \tTraining Loss: 2.330077 \tValidation Loss: 2.401450\n",
      "Epoch: 3296 \tTraining Loss: 2.340079 \tValidation Loss: 2.401379\n",
      "Epoch: 3297 \tTraining Loss: 2.332905 \tValidation Loss: 2.401535\n",
      "Epoch: 3298 \tTraining Loss: 2.339706 \tValidation Loss: 2.401449\n",
      "Epoch: 3299 \tTraining Loss: 2.346541 \tValidation Loss: 2.401513\n",
      "Epoch: 3300 \tTraining Loss: 2.337811 \tValidation Loss: 2.401507\n",
      "Epoch: 3301 \tTraining Loss: 2.332028 \tValidation Loss: 2.401408\n",
      "Epoch: 3302 \tTraining Loss: 2.360168 \tValidation Loss: 2.401422\n",
      "Epoch: 3303 \tTraining Loss: 2.346910 \tValidation Loss: 2.401397\n",
      "Epoch: 3304 \tTraining Loss: 2.345469 \tValidation Loss: 2.401411\n",
      "Epoch: 3305 \tTraining Loss: 2.337907 \tValidation Loss: 2.401386\n",
      "Epoch: 3306 \tTraining Loss: 2.350951 \tValidation Loss: 2.401442\n",
      "Epoch: 3307 \tTraining Loss: 2.333502 \tValidation Loss: 2.401370\n",
      "Epoch: 3308 \tTraining Loss: 2.343571 \tValidation Loss: 2.401338\n",
      "Validation loss decreased (2.401363 --> 2.401338).  Saving model ...\n",
      "Epoch: 3309 \tTraining Loss: 2.343308 \tValidation Loss: 2.401264\n",
      "Validation loss decreased (2.401338 --> 2.401264).  Saving model ...\n",
      "Epoch: 3310 \tTraining Loss: 2.327413 \tValidation Loss: 2.401177\n",
      "Validation loss decreased (2.401264 --> 2.401177).  Saving model ...\n",
      "Epoch: 3311 \tTraining Loss: 2.352612 \tValidation Loss: 2.401129\n",
      "Validation loss decreased (2.401177 --> 2.401129).  Saving model ...\n",
      "Epoch: 3312 \tTraining Loss: 2.333963 \tValidation Loss: 2.401067\n",
      "Validation loss decreased (2.401129 --> 2.401067).  Saving model ...\n",
      "Epoch: 3313 \tTraining Loss: 2.333133 \tValidation Loss: 2.400974\n",
      "Validation loss decreased (2.401067 --> 2.400974).  Saving model ...\n",
      "Epoch: 3314 \tTraining Loss: 2.344429 \tValidation Loss: 2.401042\n",
      "Epoch: 3315 \tTraining Loss: 2.349277 \tValidation Loss: 2.401021\n",
      "Epoch: 3316 \tTraining Loss: 2.350349 \tValidation Loss: 2.401067\n",
      "Epoch: 3317 \tTraining Loss: 2.325269 \tValidation Loss: 2.401002\n",
      "Epoch: 3318 \tTraining Loss: 2.339497 \tValidation Loss: 2.400977\n",
      "Epoch: 3319 \tTraining Loss: 2.331143 \tValidation Loss: 2.400956\n",
      "Validation loss decreased (2.400974 --> 2.400956).  Saving model ...\n",
      "Epoch: 3320 \tTraining Loss: 2.337966 \tValidation Loss: 2.400903\n",
      "Validation loss decreased (2.400956 --> 2.400903).  Saving model ...\n",
      "Epoch: 3321 \tTraining Loss: 2.363236 \tValidation Loss: 2.401097\n",
      "Epoch: 3322 \tTraining Loss: 2.325968 \tValidation Loss: 2.400975\n",
      "Epoch: 3323 \tTraining Loss: 2.346655 \tValidation Loss: 2.400910\n",
      "Epoch: 3324 \tTraining Loss: 2.338997 \tValidation Loss: 2.400810\n",
      "Validation loss decreased (2.400903 --> 2.400810).  Saving model ...\n",
      "Epoch: 3325 \tTraining Loss: 2.339886 \tValidation Loss: 2.400883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3326 \tTraining Loss: 2.342049 \tValidation Loss: 2.400785\n",
      "Validation loss decreased (2.400810 --> 2.400785).  Saving model ...\n",
      "Epoch: 3327 \tTraining Loss: 2.344912 \tValidation Loss: 2.400766\n",
      "Validation loss decreased (2.400785 --> 2.400766).  Saving model ...\n",
      "Epoch: 3328 \tTraining Loss: 2.355733 \tValidation Loss: 2.400897\n",
      "Epoch: 3329 \tTraining Loss: 2.328593 \tValidation Loss: 2.400826\n",
      "Epoch: 3330 \tTraining Loss: 2.321986 \tValidation Loss: 2.400772\n",
      "Epoch: 3331 \tTraining Loss: 2.333045 \tValidation Loss: 2.400753\n",
      "Validation loss decreased (2.400766 --> 2.400753).  Saving model ...\n",
      "Epoch: 3332 \tTraining Loss: 2.353455 \tValidation Loss: 2.400711\n",
      "Validation loss decreased (2.400753 --> 2.400711).  Saving model ...\n",
      "Epoch: 3333 \tTraining Loss: 2.344319 \tValidation Loss: 2.400589\n",
      "Validation loss decreased (2.400711 --> 2.400589).  Saving model ...\n",
      "Epoch: 3334 \tTraining Loss: 2.328748 \tValidation Loss: 2.400608\n",
      "Epoch: 3335 \tTraining Loss: 2.339434 \tValidation Loss: 2.400384\n",
      "Validation loss decreased (2.400589 --> 2.400384).  Saving model ...\n",
      "Epoch: 3336 \tTraining Loss: 2.344948 \tValidation Loss: 2.400363\n",
      "Validation loss decreased (2.400384 --> 2.400363).  Saving model ...\n",
      "Epoch: 3337 \tTraining Loss: 2.336780 \tValidation Loss: 2.400231\n",
      "Validation loss decreased (2.400363 --> 2.400231).  Saving model ...\n",
      "Epoch: 3338 \tTraining Loss: 2.351201 \tValidation Loss: 2.400248\n",
      "Epoch: 3339 \tTraining Loss: 2.354055 \tValidation Loss: 2.400303\n",
      "Epoch: 3340 \tTraining Loss: 2.341785 \tValidation Loss: 2.400260\n",
      "Epoch: 3341 \tTraining Loss: 2.343542 \tValidation Loss: 2.400146\n",
      "Validation loss decreased (2.400231 --> 2.400146).  Saving model ...\n",
      "Epoch: 3342 \tTraining Loss: 2.333729 \tValidation Loss: 2.400082\n",
      "Validation loss decreased (2.400146 --> 2.400082).  Saving model ...\n",
      "Epoch: 3343 \tTraining Loss: 2.351287 \tValidation Loss: 2.400080\n",
      "Validation loss decreased (2.400082 --> 2.400080).  Saving model ...\n",
      "Epoch: 3344 \tTraining Loss: 2.324410 \tValidation Loss: 2.400005\n",
      "Validation loss decreased (2.400080 --> 2.400005).  Saving model ...\n",
      "Epoch: 3345 \tTraining Loss: 2.334020 \tValidation Loss: 2.400058\n",
      "Epoch: 3346 \tTraining Loss: 2.350309 \tValidation Loss: 2.400124\n",
      "Epoch: 3347 \tTraining Loss: 2.357088 \tValidation Loss: 2.400241\n",
      "Epoch: 3348 \tTraining Loss: 2.326499 \tValidation Loss: 2.400178\n",
      "Epoch: 3349 \tTraining Loss: 2.346689 \tValidation Loss: 2.400183\n",
      "Epoch: 3350 \tTraining Loss: 2.335638 \tValidation Loss: 2.400147\n",
      "Epoch: 3351 \tTraining Loss: 2.349087 \tValidation Loss: 2.400080\n",
      "Epoch: 3352 \tTraining Loss: 2.336162 \tValidation Loss: 2.400129\n",
      "Epoch: 3353 \tTraining Loss: 2.341695 \tValidation Loss: 2.400243\n",
      "Epoch: 3354 \tTraining Loss: 2.343949 \tValidation Loss: 2.400138\n",
      "Epoch: 3355 \tTraining Loss: 2.355133 \tValidation Loss: 2.400216\n",
      "Epoch: 3356 \tTraining Loss: 2.333062 \tValidation Loss: 2.400120\n",
      "Epoch: 3357 \tTraining Loss: 2.351018 \tValidation Loss: 2.400065\n",
      "Epoch: 3358 \tTraining Loss: 2.327707 \tValidation Loss: 2.400015\n",
      "Epoch: 3359 \tTraining Loss: 2.326357 \tValidation Loss: 2.400040\n",
      "Epoch: 3360 \tTraining Loss: 2.335146 \tValidation Loss: 2.400040\n",
      "Epoch: 3361 \tTraining Loss: 2.321288 \tValidation Loss: 2.399935\n",
      "Validation loss decreased (2.400005 --> 2.399935).  Saving model ...\n",
      "Epoch: 3362 \tTraining Loss: 2.336015 \tValidation Loss: 2.399958\n",
      "Epoch: 3363 \tTraining Loss: 2.324093 \tValidation Loss: 2.399883\n",
      "Validation loss decreased (2.399935 --> 2.399883).  Saving model ...\n",
      "Epoch: 3364 \tTraining Loss: 2.323559 \tValidation Loss: 2.399880\n",
      "Validation loss decreased (2.399883 --> 2.399880).  Saving model ...\n",
      "Epoch: 3365 \tTraining Loss: 2.342653 \tValidation Loss: 2.399908\n",
      "Epoch: 3366 \tTraining Loss: 2.342551 \tValidation Loss: 2.399912\n",
      "Epoch: 3367 \tTraining Loss: 2.336941 \tValidation Loss: 2.399815\n",
      "Validation loss decreased (2.399880 --> 2.399815).  Saving model ...\n",
      "Epoch: 3368 \tTraining Loss: 2.348933 \tValidation Loss: 2.399825\n",
      "Epoch: 3369 \tTraining Loss: 2.336353 \tValidation Loss: 2.399902\n",
      "Epoch: 3370 \tTraining Loss: 2.333875 \tValidation Loss: 2.399906\n",
      "Epoch: 3371 \tTraining Loss: 2.347033 \tValidation Loss: 2.399902\n",
      "Epoch: 3372 \tTraining Loss: 2.343819 \tValidation Loss: 2.399965\n",
      "Epoch: 3373 \tTraining Loss: 2.341230 \tValidation Loss: 2.399949\n",
      "Epoch: 3374 \tTraining Loss: 2.345124 \tValidation Loss: 2.399940\n",
      "Epoch: 3375 \tTraining Loss: 2.343654 \tValidation Loss: 2.399887\n",
      "Epoch: 3376 \tTraining Loss: 2.337608 \tValidation Loss: 2.399783\n",
      "Validation loss decreased (2.399815 --> 2.399783).  Saving model ...\n",
      "Epoch: 3377 \tTraining Loss: 2.336689 \tValidation Loss: 2.399758\n",
      "Validation loss decreased (2.399783 --> 2.399758).  Saving model ...\n",
      "Epoch: 3378 \tTraining Loss: 2.330949 \tValidation Loss: 2.399785\n",
      "Epoch: 3379 \tTraining Loss: 2.330009 \tValidation Loss: 2.399790\n",
      "Epoch: 3380 \tTraining Loss: 2.325618 \tValidation Loss: 2.399881\n",
      "Epoch: 3381 \tTraining Loss: 2.337913 \tValidation Loss: 2.399813\n",
      "Epoch: 3382 \tTraining Loss: 2.353004 \tValidation Loss: 2.399860\n",
      "Epoch: 3383 \tTraining Loss: 2.355270 \tValidation Loss: 2.399784\n",
      "Epoch: 3384 \tTraining Loss: 2.344795 \tValidation Loss: 2.399808\n",
      "Epoch: 3385 \tTraining Loss: 2.329086 \tValidation Loss: 2.399707\n",
      "Validation loss decreased (2.399758 --> 2.399707).  Saving model ...\n",
      "Epoch: 3386 \tTraining Loss: 2.337754 \tValidation Loss: 2.399737\n",
      "Epoch: 3387 \tTraining Loss: 2.341624 \tValidation Loss: 2.399678\n",
      "Validation loss decreased (2.399707 --> 2.399678).  Saving model ...\n",
      "Epoch: 3388 \tTraining Loss: 2.341162 \tValidation Loss: 2.399703\n",
      "Epoch: 3389 \tTraining Loss: 2.338338 \tValidation Loss: 2.399627\n",
      "Validation loss decreased (2.399678 --> 2.399627).  Saving model ...\n",
      "Epoch: 3390 \tTraining Loss: 2.343760 \tValidation Loss: 2.399529\n",
      "Validation loss decreased (2.399627 --> 2.399529).  Saving model ...\n",
      "Epoch: 3391 \tTraining Loss: 2.331084 \tValidation Loss: 2.399539\n",
      "Epoch: 3392 \tTraining Loss: 2.333986 \tValidation Loss: 2.399594\n",
      "Epoch: 3393 \tTraining Loss: 2.345188 \tValidation Loss: 2.399562\n",
      "Epoch: 3394 \tTraining Loss: 2.336348 \tValidation Loss: 2.399540\n",
      "Epoch: 3395 \tTraining Loss: 2.330633 \tValidation Loss: 2.399535\n",
      "Epoch: 3396 \tTraining Loss: 2.341571 \tValidation Loss: 2.399486\n",
      "Validation loss decreased (2.399529 --> 2.399486).  Saving model ...\n",
      "Epoch: 3397 \tTraining Loss: 2.329843 \tValidation Loss: 2.399406\n",
      "Validation loss decreased (2.399486 --> 2.399406).  Saving model ...\n",
      "Epoch: 3398 \tTraining Loss: 2.337277 \tValidation Loss: 2.399276\n",
      "Validation loss decreased (2.399406 --> 2.399276).  Saving model ...\n",
      "Epoch: 3399 \tTraining Loss: 2.333257 \tValidation Loss: 2.399269\n",
      "Validation loss decreased (2.399276 --> 2.399269).  Saving model ...\n",
      "Epoch: 3400 \tTraining Loss: 2.317799 \tValidation Loss: 2.399191\n",
      "Validation loss decreased (2.399269 --> 2.399191).  Saving model ...\n",
      "Epoch: 3401 \tTraining Loss: 2.330774 \tValidation Loss: 2.399255\n",
      "Epoch: 3402 \tTraining Loss: 2.337416 \tValidation Loss: 2.399256\n",
      "Epoch: 3403 \tTraining Loss: 2.340300 \tValidation Loss: 2.399344\n",
      "Epoch: 3404 \tTraining Loss: 2.354429 \tValidation Loss: 2.399404\n",
      "Epoch: 3405 \tTraining Loss: 2.338020 \tValidation Loss: 2.399362\n",
      "Epoch: 3406 \tTraining Loss: 2.330513 \tValidation Loss: 2.399310\n",
      "Epoch: 3407 \tTraining Loss: 2.331897 \tValidation Loss: 2.399207\n",
      "Epoch: 3408 \tTraining Loss: 2.336042 \tValidation Loss: 2.399154\n",
      "Validation loss decreased (2.399191 --> 2.399154).  Saving model ...\n",
      "Epoch: 3409 \tTraining Loss: 2.353945 \tValidation Loss: 2.399156\n",
      "Epoch: 3410 \tTraining Loss: 2.333601 \tValidation Loss: 2.399136\n",
      "Validation loss decreased (2.399154 --> 2.399136).  Saving model ...\n",
      "Epoch: 3411 \tTraining Loss: 2.333035 \tValidation Loss: 2.399079\n",
      "Validation loss decreased (2.399136 --> 2.399079).  Saving model ...\n",
      "Epoch: 3412 \tTraining Loss: 2.333359 \tValidation Loss: 2.399125\n",
      "Epoch: 3413 \tTraining Loss: 2.331694 \tValidation Loss: 2.399024\n",
      "Validation loss decreased (2.399079 --> 2.399024).  Saving model ...\n",
      "Epoch: 3414 \tTraining Loss: 2.339884 \tValidation Loss: 2.398929\n",
      "Validation loss decreased (2.399024 --> 2.398929).  Saving model ...\n",
      "Epoch: 3415 \tTraining Loss: 2.343591 \tValidation Loss: 2.398991\n",
      "Epoch: 3416 \tTraining Loss: 2.334210 \tValidation Loss: 2.398986\n",
      "Epoch: 3417 \tTraining Loss: 2.323988 \tValidation Loss: 2.398939\n",
      "Epoch: 3418 \tTraining Loss: 2.332402 \tValidation Loss: 2.398859\n",
      "Validation loss decreased (2.398929 --> 2.398859).  Saving model ...\n",
      "Epoch: 3419 \tTraining Loss: 2.340847 \tValidation Loss: 2.398780\n",
      "Validation loss decreased (2.398859 --> 2.398780).  Saving model ...\n",
      "Epoch: 3420 \tTraining Loss: 2.331151 \tValidation Loss: 2.398647\n",
      "Validation loss decreased (2.398780 --> 2.398647).  Saving model ...\n",
      "Epoch: 3421 \tTraining Loss: 2.342037 \tValidation Loss: 2.398696\n",
      "Epoch: 3422 \tTraining Loss: 2.339876 \tValidation Loss: 2.398626\n",
      "Validation loss decreased (2.398647 --> 2.398626).  Saving model ...\n",
      "Epoch: 3423 \tTraining Loss: 2.332810 \tValidation Loss: 2.398668\n",
      "Epoch: 3424 \tTraining Loss: 2.331057 \tValidation Loss: 2.398614\n",
      "Validation loss decreased (2.398626 --> 2.398614).  Saving model ...\n",
      "Epoch: 3425 \tTraining Loss: 2.332895 \tValidation Loss: 2.398633\n",
      "Epoch: 3426 \tTraining Loss: 2.331237 \tValidation Loss: 2.398616\n",
      "Epoch: 3427 \tTraining Loss: 2.328248 \tValidation Loss: 2.398649\n",
      "Epoch: 3428 \tTraining Loss: 2.338250 \tValidation Loss: 2.398531\n",
      "Validation loss decreased (2.398614 --> 2.398531).  Saving model ...\n",
      "Epoch: 3429 \tTraining Loss: 2.319257 \tValidation Loss: 2.398566\n",
      "Epoch: 3430 \tTraining Loss: 2.332950 \tValidation Loss: 2.398542\n",
      "Epoch: 3431 \tTraining Loss: 2.329990 \tValidation Loss: 2.398601\n",
      "Epoch: 3432 \tTraining Loss: 2.336628 \tValidation Loss: 2.398563\n",
      "Epoch: 3433 \tTraining Loss: 2.337064 \tValidation Loss: 2.398421\n",
      "Validation loss decreased (2.398531 --> 2.398421).  Saving model ...\n",
      "Epoch: 3434 \tTraining Loss: 2.332804 \tValidation Loss: 2.398356\n",
      "Validation loss decreased (2.398421 --> 2.398356).  Saving model ...\n",
      "Epoch: 3435 \tTraining Loss: 2.331371 \tValidation Loss: 2.398341\n",
      "Validation loss decreased (2.398356 --> 2.398341).  Saving model ...\n",
      "Epoch: 3436 \tTraining Loss: 2.331637 \tValidation Loss: 2.398327\n",
      "Validation loss decreased (2.398341 --> 2.398327).  Saving model ...\n",
      "Epoch: 3437 \tTraining Loss: 2.337775 \tValidation Loss: 2.398359\n",
      "Epoch: 3438 \tTraining Loss: 2.339566 \tValidation Loss: 2.398474\n",
      "Epoch: 3439 \tTraining Loss: 2.333960 \tValidation Loss: 2.398434\n",
      "Epoch: 3440 \tTraining Loss: 2.343985 \tValidation Loss: 2.398475\n",
      "Epoch: 3441 \tTraining Loss: 2.341590 \tValidation Loss: 2.398396\n",
      "Epoch: 3442 \tTraining Loss: 2.330650 \tValidation Loss: 2.398364\n",
      "Epoch: 3443 \tTraining Loss: 2.320397 \tValidation Loss: 2.398316\n",
      "Validation loss decreased (2.398327 --> 2.398316).  Saving model ...\n",
      "Epoch: 3444 \tTraining Loss: 2.337252 \tValidation Loss: 2.398347\n",
      "Epoch: 3445 \tTraining Loss: 2.325470 \tValidation Loss: 2.398228\n",
      "Validation loss decreased (2.398316 --> 2.398228).  Saving model ...\n",
      "Epoch: 3446 \tTraining Loss: 2.343001 \tValidation Loss: 2.398197\n",
      "Validation loss decreased (2.398228 --> 2.398197).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3447 \tTraining Loss: 2.341422 \tValidation Loss: 2.398349\n",
      "Epoch: 3448 \tTraining Loss: 2.332243 \tValidation Loss: 2.398420\n",
      "Epoch: 3449 \tTraining Loss: 2.334090 \tValidation Loss: 2.398329\n",
      "Epoch: 3450 \tTraining Loss: 2.315981 \tValidation Loss: 2.398230\n",
      "Epoch: 3451 \tTraining Loss: 2.319659 \tValidation Loss: 2.398000\n",
      "Validation loss decreased (2.398197 --> 2.398000).  Saving model ...\n",
      "Epoch: 3452 \tTraining Loss: 2.335525 \tValidation Loss: 2.397841\n",
      "Validation loss decreased (2.398000 --> 2.397841).  Saving model ...\n",
      "Epoch: 3453 \tTraining Loss: 2.352565 \tValidation Loss: 2.397942\n",
      "Epoch: 3454 \tTraining Loss: 2.336321 \tValidation Loss: 2.397933\n",
      "Epoch: 3455 \tTraining Loss: 2.342423 \tValidation Loss: 2.397875\n",
      "Epoch: 3456 \tTraining Loss: 2.325628 \tValidation Loss: 2.397921\n",
      "Epoch: 3457 \tTraining Loss: 2.325499 \tValidation Loss: 2.397849\n",
      "Epoch: 3458 \tTraining Loss: 2.328392 \tValidation Loss: 2.397829\n",
      "Validation loss decreased (2.397841 --> 2.397829).  Saving model ...\n",
      "Epoch: 3459 \tTraining Loss: 2.328850 \tValidation Loss: 2.397687\n",
      "Validation loss decreased (2.397829 --> 2.397687).  Saving model ...\n",
      "Epoch: 3460 \tTraining Loss: 2.339078 \tValidation Loss: 2.397663\n",
      "Validation loss decreased (2.397687 --> 2.397663).  Saving model ...\n",
      "Epoch: 3461 \tTraining Loss: 2.319483 \tValidation Loss: 2.397716\n",
      "Epoch: 3462 \tTraining Loss: 2.334182 \tValidation Loss: 2.397579\n",
      "Validation loss decreased (2.397663 --> 2.397579).  Saving model ...\n",
      "Epoch: 3463 \tTraining Loss: 2.327500 \tValidation Loss: 2.397552\n",
      "Validation loss decreased (2.397579 --> 2.397552).  Saving model ...\n",
      "Epoch: 3464 \tTraining Loss: 2.343080 \tValidation Loss: 2.397516\n",
      "Validation loss decreased (2.397552 --> 2.397516).  Saving model ...\n",
      "Epoch: 3465 \tTraining Loss: 2.328453 \tValidation Loss: 2.397526\n",
      "Epoch: 3466 \tTraining Loss: 2.336302 \tValidation Loss: 2.397567\n",
      "Epoch: 3467 \tTraining Loss: 2.339837 \tValidation Loss: 2.397510\n",
      "Validation loss decreased (2.397516 --> 2.397510).  Saving model ...\n",
      "Epoch: 3468 \tTraining Loss: 2.335693 \tValidation Loss: 2.397586\n",
      "Epoch: 3469 \tTraining Loss: 2.340911 \tValidation Loss: 2.397545\n",
      "Epoch: 3470 \tTraining Loss: 2.333253 \tValidation Loss: 2.397505\n",
      "Validation loss decreased (2.397510 --> 2.397505).  Saving model ...\n",
      "Epoch: 3471 \tTraining Loss: 2.339929 \tValidation Loss: 2.397503\n",
      "Validation loss decreased (2.397505 --> 2.397503).  Saving model ...\n",
      "Epoch: 3472 \tTraining Loss: 2.337801 \tValidation Loss: 2.397434\n",
      "Validation loss decreased (2.397503 --> 2.397434).  Saving model ...\n",
      "Epoch: 3473 \tTraining Loss: 2.323985 \tValidation Loss: 2.397390\n",
      "Validation loss decreased (2.397434 --> 2.397390).  Saving model ...\n",
      "Epoch: 3474 \tTraining Loss: 2.322019 \tValidation Loss: 2.397290\n",
      "Validation loss decreased (2.397390 --> 2.397290).  Saving model ...\n",
      "Epoch: 3475 \tTraining Loss: 2.326940 \tValidation Loss: 2.397393\n",
      "Epoch: 3476 \tTraining Loss: 2.341691 \tValidation Loss: 2.397450\n",
      "Epoch: 3477 \tTraining Loss: 2.332535 \tValidation Loss: 2.397395\n",
      "Epoch: 3478 \tTraining Loss: 2.341011 \tValidation Loss: 2.397472\n",
      "Epoch: 3479 \tTraining Loss: 2.328825 \tValidation Loss: 2.397430\n",
      "Epoch: 3480 \tTraining Loss: 2.326621 \tValidation Loss: 2.397357\n",
      "Epoch: 3481 \tTraining Loss: 2.333870 \tValidation Loss: 2.397316\n",
      "Epoch: 3482 \tTraining Loss: 2.337546 \tValidation Loss: 2.397342\n",
      "Epoch: 3483 \tTraining Loss: 2.343654 \tValidation Loss: 2.397508\n",
      "Epoch: 3484 \tTraining Loss: 2.339563 \tValidation Loss: 2.397437\n",
      "Epoch: 3485 \tTraining Loss: 2.326707 \tValidation Loss: 2.397333\n",
      "Epoch: 3486 \tTraining Loss: 2.337659 \tValidation Loss: 2.397284\n",
      "Validation loss decreased (2.397290 --> 2.397284).  Saving model ...\n",
      "Epoch: 3487 \tTraining Loss: 2.332519 \tValidation Loss: 2.397105\n",
      "Validation loss decreased (2.397284 --> 2.397105).  Saving model ...\n",
      "Epoch: 3488 \tTraining Loss: 2.346912 \tValidation Loss: 2.397030\n",
      "Validation loss decreased (2.397105 --> 2.397030).  Saving model ...\n",
      "Epoch: 3489 \tTraining Loss: 2.329726 \tValidation Loss: 2.397041\n",
      "Epoch: 3490 \tTraining Loss: 2.342315 \tValidation Loss: 2.397167\n",
      "Epoch: 3491 \tTraining Loss: 2.332355 \tValidation Loss: 2.397213\n",
      "Epoch: 3492 \tTraining Loss: 2.322749 \tValidation Loss: 2.397169\n",
      "Epoch: 3493 \tTraining Loss: 2.336049 \tValidation Loss: 2.397176\n",
      "Epoch: 3494 \tTraining Loss: 2.337026 \tValidation Loss: 2.397044\n",
      "Epoch: 3495 \tTraining Loss: 2.336806 \tValidation Loss: 2.397009\n",
      "Validation loss decreased (2.397030 --> 2.397009).  Saving model ...\n",
      "Epoch: 3496 \tTraining Loss: 2.315732 \tValidation Loss: 2.396928\n",
      "Validation loss decreased (2.397009 --> 2.396928).  Saving model ...\n",
      "Epoch: 3497 \tTraining Loss: 2.332463 \tValidation Loss: 2.396996\n",
      "Epoch: 3498 \tTraining Loss: 2.323764 \tValidation Loss: 2.396916\n",
      "Validation loss decreased (2.396928 --> 2.396916).  Saving model ...\n",
      "Epoch: 3499 \tTraining Loss: 2.339729 \tValidation Loss: 2.397026\n",
      "Epoch: 3500 \tTraining Loss: 2.327671 \tValidation Loss: 2.397067\n",
      "Epoch: 3501 \tTraining Loss: 2.347109 \tValidation Loss: 2.397007\n",
      "Epoch: 3502 \tTraining Loss: 2.325963 \tValidation Loss: 2.396908\n",
      "Validation loss decreased (2.396916 --> 2.396908).  Saving model ...\n",
      "Epoch: 3503 \tTraining Loss: 2.326313 \tValidation Loss: 2.396853\n",
      "Validation loss decreased (2.396908 --> 2.396853).  Saving model ...\n",
      "Epoch: 3504 \tTraining Loss: 2.321420 \tValidation Loss: 2.396695\n",
      "Validation loss decreased (2.396853 --> 2.396695).  Saving model ...\n",
      "Epoch: 3505 \tTraining Loss: 2.337504 \tValidation Loss: 2.396709\n",
      "Epoch: 3506 \tTraining Loss: 2.332215 \tValidation Loss: 2.396746\n",
      "Epoch: 3507 \tTraining Loss: 2.332505 \tValidation Loss: 2.396629\n",
      "Validation loss decreased (2.396695 --> 2.396629).  Saving model ...\n",
      "Epoch: 3508 \tTraining Loss: 2.327612 \tValidation Loss: 2.396597\n",
      "Validation loss decreased (2.396629 --> 2.396597).  Saving model ...\n",
      "Epoch: 3509 \tTraining Loss: 2.313871 \tValidation Loss: 2.396549\n",
      "Validation loss decreased (2.396597 --> 2.396549).  Saving model ...\n",
      "Epoch: 3510 \tTraining Loss: 2.327893 \tValidation Loss: 2.396545\n",
      "Validation loss decreased (2.396549 --> 2.396545).  Saving model ...\n",
      "Epoch: 3511 \tTraining Loss: 2.324746 \tValidation Loss: 2.396520\n",
      "Validation loss decreased (2.396545 --> 2.396520).  Saving model ...\n",
      "Epoch: 3512 \tTraining Loss: 2.330626 \tValidation Loss: 2.396457\n",
      "Validation loss decreased (2.396520 --> 2.396457).  Saving model ...\n",
      "Epoch: 3513 \tTraining Loss: 2.337988 \tValidation Loss: 2.396444\n",
      "Validation loss decreased (2.396457 --> 2.396444).  Saving model ...\n",
      "Epoch: 3514 \tTraining Loss: 2.333589 \tValidation Loss: 2.396466\n",
      "Epoch: 3515 \tTraining Loss: 2.321371 \tValidation Loss: 2.396491\n",
      "Epoch: 3516 \tTraining Loss: 2.322024 \tValidation Loss: 2.396462\n",
      "Epoch: 3517 \tTraining Loss: 2.324877 \tValidation Loss: 2.396428\n",
      "Validation loss decreased (2.396444 --> 2.396428).  Saving model ...\n",
      "Epoch: 3518 \tTraining Loss: 2.323663 \tValidation Loss: 2.396486\n",
      "Epoch: 3519 \tTraining Loss: 2.339770 \tValidation Loss: 2.396576\n",
      "Epoch: 3520 \tTraining Loss: 2.322313 \tValidation Loss: 2.396541\n",
      "Epoch: 3521 \tTraining Loss: 2.321522 \tValidation Loss: 2.396562\n",
      "Epoch: 3522 \tTraining Loss: 2.321241 \tValidation Loss: 2.396645\n",
      "Epoch: 3523 \tTraining Loss: 2.332182 \tValidation Loss: 2.396600\n",
      "Epoch: 3524 \tTraining Loss: 2.337170 \tValidation Loss: 2.396608\n",
      "Epoch: 3525 \tTraining Loss: 2.317354 \tValidation Loss: 2.396474\n",
      "Epoch: 3526 \tTraining Loss: 2.317771 \tValidation Loss: 2.396364\n",
      "Validation loss decreased (2.396428 --> 2.396364).  Saving model ...\n",
      "Epoch: 3527 \tTraining Loss: 2.342926 \tValidation Loss: 2.396485\n",
      "Epoch: 3528 \tTraining Loss: 2.319370 \tValidation Loss: 2.396270\n",
      "Validation loss decreased (2.396364 --> 2.396270).  Saving model ...\n",
      "Epoch: 3529 \tTraining Loss: 2.331471 \tValidation Loss: 2.396292\n",
      "Epoch: 3530 \tTraining Loss: 2.332240 \tValidation Loss: 2.396185\n",
      "Validation loss decreased (2.396270 --> 2.396185).  Saving model ...\n",
      "Epoch: 3531 \tTraining Loss: 2.322767 \tValidation Loss: 2.396055\n",
      "Validation loss decreased (2.396185 --> 2.396055).  Saving model ...\n",
      "Epoch: 3532 \tTraining Loss: 2.317144 \tValidation Loss: 2.395928\n",
      "Validation loss decreased (2.396055 --> 2.395928).  Saving model ...\n",
      "Epoch: 3533 \tTraining Loss: 2.338107 \tValidation Loss: 2.395973\n",
      "Epoch: 3534 \tTraining Loss: 2.331005 \tValidation Loss: 2.396028\n",
      "Epoch: 3535 \tTraining Loss: 2.340297 \tValidation Loss: 2.396136\n",
      "Epoch: 3536 \tTraining Loss: 2.331432 \tValidation Loss: 2.396091\n",
      "Epoch: 3537 \tTraining Loss: 2.332545 \tValidation Loss: 2.396076\n",
      "Epoch: 3538 \tTraining Loss: 2.320866 \tValidation Loss: 2.396163\n",
      "Epoch: 3539 \tTraining Loss: 2.327495 \tValidation Loss: 2.396081\n",
      "Epoch: 3540 \tTraining Loss: 2.329857 \tValidation Loss: 2.396098\n",
      "Epoch: 3541 \tTraining Loss: 2.333869 \tValidation Loss: 2.396163\n",
      "Epoch: 3542 \tTraining Loss: 2.329296 \tValidation Loss: 2.396159\n",
      "Epoch: 3543 \tTraining Loss: 2.329639 \tValidation Loss: 2.396040\n",
      "Epoch: 3544 \tTraining Loss: 2.336494 \tValidation Loss: 2.396182\n",
      "Epoch: 3545 \tTraining Loss: 2.320887 \tValidation Loss: 2.396221\n",
      "Epoch: 3546 \tTraining Loss: 2.331335 \tValidation Loss: 2.396259\n",
      "Epoch: 3547 \tTraining Loss: 2.337624 \tValidation Loss: 2.396230\n",
      "Epoch: 3548 \tTraining Loss: 2.317344 \tValidation Loss: 2.396091\n",
      "Epoch: 3549 \tTraining Loss: 2.324139 \tValidation Loss: 2.395933\n",
      "Epoch: 3550 \tTraining Loss: 2.328255 \tValidation Loss: 2.395916\n",
      "Validation loss decreased (2.395928 --> 2.395916).  Saving model ...\n",
      "Epoch: 3551 \tTraining Loss: 2.327959 \tValidation Loss: 2.395839\n",
      "Validation loss decreased (2.395916 --> 2.395839).  Saving model ...\n",
      "Epoch: 3552 \tTraining Loss: 2.321195 \tValidation Loss: 2.395833\n",
      "Validation loss decreased (2.395839 --> 2.395833).  Saving model ...\n",
      "Epoch: 3553 \tTraining Loss: 2.318639 \tValidation Loss: 2.395684\n",
      "Validation loss decreased (2.395833 --> 2.395684).  Saving model ...\n",
      "Epoch: 3554 \tTraining Loss: 2.325375 \tValidation Loss: 2.395627\n",
      "Validation loss decreased (2.395684 --> 2.395627).  Saving model ...\n",
      "Epoch: 3555 \tTraining Loss: 2.330510 \tValidation Loss: 2.395576\n",
      "Validation loss decreased (2.395627 --> 2.395576).  Saving model ...\n",
      "Epoch: 3556 \tTraining Loss: 2.325249 \tValidation Loss: 2.395625\n",
      "Epoch: 3557 \tTraining Loss: 2.329044 \tValidation Loss: 2.395499\n",
      "Validation loss decreased (2.395576 --> 2.395499).  Saving model ...\n",
      "Epoch: 3558 \tTraining Loss: 2.338704 \tValidation Loss: 2.395588\n",
      "Epoch: 3559 \tTraining Loss: 2.335756 \tValidation Loss: 2.395577\n",
      "Epoch: 3560 \tTraining Loss: 2.331383 \tValidation Loss: 2.395529\n",
      "Epoch: 3561 \tTraining Loss: 2.326493 \tValidation Loss: 2.395573\n",
      "Epoch: 3562 \tTraining Loss: 2.320972 \tValidation Loss: 2.395555\n",
      "Epoch: 3563 \tTraining Loss: 2.320498 \tValidation Loss: 2.395566\n",
      "Epoch: 3564 \tTraining Loss: 2.323727 \tValidation Loss: 2.395394\n",
      "Validation loss decreased (2.395499 --> 2.395394).  Saving model ...\n",
      "Epoch: 3565 \tTraining Loss: 2.317308 \tValidation Loss: 2.395286\n",
      "Validation loss decreased (2.395394 --> 2.395286).  Saving model ...\n",
      "Epoch: 3566 \tTraining Loss: 2.325210 \tValidation Loss: 2.395224\n",
      "Validation loss decreased (2.395286 --> 2.395224).  Saving model ...\n",
      "Epoch: 3567 \tTraining Loss: 2.338045 \tValidation Loss: 2.395082\n",
      "Validation loss decreased (2.395224 --> 2.395082).  Saving model ...\n",
      "Epoch: 3568 \tTraining Loss: 2.313593 \tValidation Loss: 2.395120\n",
      "Epoch: 3569 \tTraining Loss: 2.335647 \tValidation Loss: 2.395304\n",
      "Epoch: 3570 \tTraining Loss: 2.317309 \tValidation Loss: 2.395207\n",
      "Epoch: 3571 \tTraining Loss: 2.317334 \tValidation Loss: 2.395243\n",
      "Epoch: 3572 \tTraining Loss: 2.344030 \tValidation Loss: 2.395249\n",
      "Epoch: 3573 \tTraining Loss: 2.323090 \tValidation Loss: 2.395253\n",
      "Epoch: 3574 \tTraining Loss: 2.334875 \tValidation Loss: 2.395292\n",
      "Epoch: 3575 \tTraining Loss: 2.330791 \tValidation Loss: 2.395246\n",
      "Epoch: 3576 \tTraining Loss: 2.318284 \tValidation Loss: 2.395263\n",
      "Epoch: 3577 \tTraining Loss: 2.330868 \tValidation Loss: 2.395208\n",
      "Epoch: 3578 \tTraining Loss: 2.333066 \tValidation Loss: 2.395291\n",
      "Epoch: 3579 \tTraining Loss: 2.319146 \tValidation Loss: 2.395298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3580 \tTraining Loss: 2.315928 \tValidation Loss: 2.395267\n",
      "Epoch: 3581 \tTraining Loss: 2.324329 \tValidation Loss: 2.395203\n",
      "Epoch: 3582 \tTraining Loss: 2.321116 \tValidation Loss: 2.395220\n",
      "Epoch: 3583 \tTraining Loss: 2.327096 \tValidation Loss: 2.395147\n",
      "Epoch: 3584 \tTraining Loss: 2.333120 \tValidation Loss: 2.395074\n",
      "Validation loss decreased (2.395082 --> 2.395074).  Saving model ...\n",
      "Epoch: 3585 \tTraining Loss: 2.326236 \tValidation Loss: 2.395001\n",
      "Validation loss decreased (2.395074 --> 2.395001).  Saving model ...\n",
      "Epoch: 3586 \tTraining Loss: 2.313778 \tValidation Loss: 2.394999\n",
      "Validation loss decreased (2.395001 --> 2.394999).  Saving model ...\n",
      "Epoch: 3587 \tTraining Loss: 2.323640 \tValidation Loss: 2.394991\n",
      "Validation loss decreased (2.394999 --> 2.394991).  Saving model ...\n",
      "Epoch: 3588 \tTraining Loss: 2.319109 \tValidation Loss: 2.394981\n",
      "Validation loss decreased (2.394991 --> 2.394981).  Saving model ...\n",
      "Epoch: 3589 \tTraining Loss: 2.326002 \tValidation Loss: 2.394978\n",
      "Validation loss decreased (2.394981 --> 2.394978).  Saving model ...\n",
      "Epoch: 3590 \tTraining Loss: 2.314270 \tValidation Loss: 2.395036\n",
      "Epoch: 3591 \tTraining Loss: 2.318557 \tValidation Loss: 2.394823\n",
      "Validation loss decreased (2.394978 --> 2.394823).  Saving model ...\n",
      "Epoch: 3592 \tTraining Loss: 2.318915 \tValidation Loss: 2.394679\n",
      "Validation loss decreased (2.394823 --> 2.394679).  Saving model ...\n",
      "Epoch: 3593 \tTraining Loss: 2.331056 \tValidation Loss: 2.394729\n",
      "Epoch: 3594 \tTraining Loss: 2.333736 \tValidation Loss: 2.394767\n",
      "Epoch: 3595 \tTraining Loss: 2.323332 \tValidation Loss: 2.394732\n",
      "Epoch: 3596 \tTraining Loss: 2.350646 \tValidation Loss: 2.394877\n",
      "Epoch: 3597 \tTraining Loss: 2.319693 \tValidation Loss: 2.394753\n",
      "Epoch: 3598 \tTraining Loss: 2.337610 \tValidation Loss: 2.394691\n",
      "Epoch: 3599 \tTraining Loss: 2.332576 \tValidation Loss: 2.394572\n",
      "Validation loss decreased (2.394679 --> 2.394572).  Saving model ...\n",
      "Epoch: 3600 \tTraining Loss: 2.311300 \tValidation Loss: 2.394454\n",
      "Validation loss decreased (2.394572 --> 2.394454).  Saving model ...\n",
      "Epoch: 3601 \tTraining Loss: 2.324387 \tValidation Loss: 2.394420\n",
      "Validation loss decreased (2.394454 --> 2.394420).  Saving model ...\n",
      "Epoch: 3602 \tTraining Loss: 2.311999 \tValidation Loss: 2.394476\n",
      "Epoch: 3603 \tTraining Loss: 2.310951 \tValidation Loss: 2.394351\n",
      "Validation loss decreased (2.394420 --> 2.394351).  Saving model ...\n",
      "Epoch: 3604 \tTraining Loss: 2.323157 \tValidation Loss: 2.394392\n",
      "Epoch: 3605 \tTraining Loss: 2.319245 \tValidation Loss: 2.394347\n",
      "Validation loss decreased (2.394351 --> 2.394347).  Saving model ...\n",
      "Epoch: 3606 \tTraining Loss: 2.331714 \tValidation Loss: 2.394402\n",
      "Epoch: 3607 \tTraining Loss: 2.321035 \tValidation Loss: 2.394381\n",
      "Epoch: 3608 \tTraining Loss: 2.322283 \tValidation Loss: 2.394280\n",
      "Validation loss decreased (2.394347 --> 2.394280).  Saving model ...\n",
      "Epoch: 3609 \tTraining Loss: 2.327804 \tValidation Loss: 2.394296\n",
      "Epoch: 3610 \tTraining Loss: 2.327770 \tValidation Loss: 2.394336\n",
      "Epoch: 3611 \tTraining Loss: 2.335736 \tValidation Loss: 2.394445\n",
      "Epoch: 3612 \tTraining Loss: 2.329786 \tValidation Loss: 2.394402\n",
      "Epoch: 3613 \tTraining Loss: 2.304530 \tValidation Loss: 2.394321\n",
      "Epoch: 3614 \tTraining Loss: 2.323196 \tValidation Loss: 2.394392\n",
      "Epoch: 3615 \tTraining Loss: 2.318344 \tValidation Loss: 2.394265\n",
      "Validation loss decreased (2.394280 --> 2.394265).  Saving model ...\n",
      "Epoch: 3616 \tTraining Loss: 2.318503 \tValidation Loss: 2.394311\n",
      "Epoch: 3617 \tTraining Loss: 2.320070 \tValidation Loss: 2.394230\n",
      "Validation loss decreased (2.394265 --> 2.394230).  Saving model ...\n",
      "Epoch: 3618 \tTraining Loss: 2.321828 \tValidation Loss: 2.394205\n",
      "Validation loss decreased (2.394230 --> 2.394205).  Saving model ...\n",
      "Epoch: 3619 \tTraining Loss: 2.321460 \tValidation Loss: 2.394196\n",
      "Validation loss decreased (2.394205 --> 2.394196).  Saving model ...\n",
      "Epoch: 3620 \tTraining Loss: 2.325132 \tValidation Loss: 2.394233\n",
      "Epoch: 3621 \tTraining Loss: 2.337969 \tValidation Loss: 2.394215\n",
      "Epoch: 3622 \tTraining Loss: 2.317188 \tValidation Loss: 2.394197\n",
      "Epoch: 3623 \tTraining Loss: 2.311077 \tValidation Loss: 2.394100\n",
      "Validation loss decreased (2.394196 --> 2.394100).  Saving model ...\n",
      "Epoch: 3624 \tTraining Loss: 2.332307 \tValidation Loss: 2.394014\n",
      "Validation loss decreased (2.394100 --> 2.394014).  Saving model ...\n",
      "Epoch: 3625 \tTraining Loss: 2.311854 \tValidation Loss: 2.393936\n",
      "Validation loss decreased (2.394014 --> 2.393936).  Saving model ...\n",
      "Epoch: 3626 \tTraining Loss: 2.320739 \tValidation Loss: 2.393831\n",
      "Validation loss decreased (2.393936 --> 2.393831).  Saving model ...\n",
      "Epoch: 3627 \tTraining Loss: 2.324771 \tValidation Loss: 2.393957\n",
      "Epoch: 3628 \tTraining Loss: 2.321522 \tValidation Loss: 2.394006\n",
      "Epoch: 3629 \tTraining Loss: 2.307928 \tValidation Loss: 2.393880\n",
      "Epoch: 3630 \tTraining Loss: 2.315557 \tValidation Loss: 2.393962\n",
      "Epoch: 3631 \tTraining Loss: 2.327437 \tValidation Loss: 2.393895\n",
      "Epoch: 3632 \tTraining Loss: 2.324155 \tValidation Loss: 2.393899\n",
      "Epoch: 3633 \tTraining Loss: 2.315498 \tValidation Loss: 2.393856\n",
      "Epoch: 3634 \tTraining Loss: 2.314785 \tValidation Loss: 2.393778\n",
      "Validation loss decreased (2.393831 --> 2.393778).  Saving model ...\n",
      "Epoch: 3635 \tTraining Loss: 2.341639 \tValidation Loss: 2.393780\n",
      "Epoch: 3636 \tTraining Loss: 2.329105 \tValidation Loss: 2.393847\n",
      "Epoch: 3637 \tTraining Loss: 2.346295 \tValidation Loss: 2.393849\n",
      "Epoch: 3638 \tTraining Loss: 2.328313 \tValidation Loss: 2.393893\n",
      "Epoch: 3639 \tTraining Loss: 2.314898 \tValidation Loss: 2.393824\n",
      "Epoch: 3640 \tTraining Loss: 2.316062 \tValidation Loss: 2.393715\n",
      "Validation loss decreased (2.393778 --> 2.393715).  Saving model ...\n",
      "Epoch: 3641 \tTraining Loss: 2.336198 \tValidation Loss: 2.393750\n",
      "Epoch: 3642 \tTraining Loss: 2.316362 \tValidation Loss: 2.393640\n",
      "Validation loss decreased (2.393715 --> 2.393640).  Saving model ...\n",
      "Epoch: 3643 \tTraining Loss: 2.325530 \tValidation Loss: 2.393638\n",
      "Validation loss decreased (2.393640 --> 2.393638).  Saving model ...\n",
      "Epoch: 3644 \tTraining Loss: 2.335871 \tValidation Loss: 2.393708\n",
      "Epoch: 3645 \tTraining Loss: 2.321664 \tValidation Loss: 2.393755\n",
      "Epoch: 3646 \tTraining Loss: 2.324672 \tValidation Loss: 2.393647\n",
      "Epoch: 3647 \tTraining Loss: 2.319211 \tValidation Loss: 2.393644\n",
      "Epoch: 3648 \tTraining Loss: 2.311283 \tValidation Loss: 2.393534\n",
      "Validation loss decreased (2.393638 --> 2.393534).  Saving model ...\n",
      "Epoch: 3649 \tTraining Loss: 2.333507 \tValidation Loss: 2.393532\n",
      "Validation loss decreased (2.393534 --> 2.393532).  Saving model ...\n",
      "Epoch: 3650 \tTraining Loss: 2.315303 \tValidation Loss: 2.393496\n",
      "Validation loss decreased (2.393532 --> 2.393496).  Saving model ...\n",
      "Epoch: 3651 \tTraining Loss: 2.331993 \tValidation Loss: 2.393477\n",
      "Validation loss decreased (2.393496 --> 2.393477).  Saving model ...\n",
      "Epoch: 3652 \tTraining Loss: 2.321479 \tValidation Loss: 2.393518\n",
      "Epoch: 3653 \tTraining Loss: 2.326669 \tValidation Loss: 2.393475\n",
      "Validation loss decreased (2.393477 --> 2.393475).  Saving model ...\n",
      "Epoch: 3654 \tTraining Loss: 2.321322 \tValidation Loss: 2.393540\n",
      "Epoch: 3655 \tTraining Loss: 2.317467 \tValidation Loss: 2.393497\n",
      "Epoch: 3656 \tTraining Loss: 2.315497 \tValidation Loss: 2.393453\n",
      "Validation loss decreased (2.393475 --> 2.393453).  Saving model ...\n",
      "Epoch: 3657 \tTraining Loss: 2.326835 \tValidation Loss: 2.393577\n",
      "Epoch: 3658 \tTraining Loss: 2.327115 \tValidation Loss: 2.393505\n",
      "Epoch: 3659 \tTraining Loss: 2.326194 \tValidation Loss: 2.393408\n",
      "Validation loss decreased (2.393453 --> 2.393408).  Saving model ...\n",
      "Epoch: 3660 \tTraining Loss: 2.320004 \tValidation Loss: 2.393427\n",
      "Epoch: 3661 \tTraining Loss: 2.316831 \tValidation Loss: 2.393345\n",
      "Validation loss decreased (2.393408 --> 2.393345).  Saving model ...\n",
      "Epoch: 3662 \tTraining Loss: 2.328727 \tValidation Loss: 2.393358\n",
      "Epoch: 3663 \tTraining Loss: 2.322306 \tValidation Loss: 2.393286\n",
      "Validation loss decreased (2.393345 --> 2.393286).  Saving model ...\n",
      "Epoch: 3664 \tTraining Loss: 2.311442 \tValidation Loss: 2.393223\n",
      "Validation loss decreased (2.393286 --> 2.393223).  Saving model ...\n",
      "Epoch: 3665 \tTraining Loss: 2.328953 \tValidation Loss: 2.393265\n",
      "Epoch: 3666 \tTraining Loss: 2.328235 \tValidation Loss: 2.393252\n",
      "Epoch: 3667 \tTraining Loss: 2.324559 \tValidation Loss: 2.393255\n",
      "Epoch: 3668 \tTraining Loss: 2.332331 \tValidation Loss: 2.393301\n",
      "Epoch: 3669 \tTraining Loss: 2.304540 \tValidation Loss: 2.393242\n",
      "Epoch: 3670 \tTraining Loss: 2.311842 \tValidation Loss: 2.393212\n",
      "Validation loss decreased (2.393223 --> 2.393212).  Saving model ...\n",
      "Epoch: 3671 \tTraining Loss: 2.321919 \tValidation Loss: 2.393275\n",
      "Epoch: 3672 \tTraining Loss: 2.329834 \tValidation Loss: 2.393351\n",
      "Epoch: 3673 \tTraining Loss: 2.328753 \tValidation Loss: 2.393438\n",
      "Epoch: 3674 \tTraining Loss: 2.315390 \tValidation Loss: 2.393349\n",
      "Epoch: 3675 \tTraining Loss: 2.313792 \tValidation Loss: 2.393288\n",
      "Epoch: 3676 \tTraining Loss: 2.311000 \tValidation Loss: 2.393253\n",
      "Epoch: 3677 \tTraining Loss: 2.315936 \tValidation Loss: 2.393150\n",
      "Validation loss decreased (2.393212 --> 2.393150).  Saving model ...\n",
      "Epoch: 3678 \tTraining Loss: 2.319210 \tValidation Loss: 2.393040\n",
      "Validation loss decreased (2.393150 --> 2.393040).  Saving model ...\n",
      "Epoch: 3679 \tTraining Loss: 2.318197 \tValidation Loss: 2.392975\n",
      "Validation loss decreased (2.393040 --> 2.392975).  Saving model ...\n",
      "Epoch: 3680 \tTraining Loss: 2.325770 \tValidation Loss: 2.392961\n",
      "Validation loss decreased (2.392975 --> 2.392961).  Saving model ...\n",
      "Epoch: 3681 \tTraining Loss: 2.315010 \tValidation Loss: 2.392926\n",
      "Validation loss decreased (2.392961 --> 2.392926).  Saving model ...\n",
      "Epoch: 3682 \tTraining Loss: 2.317298 \tValidation Loss: 2.392855\n",
      "Validation loss decreased (2.392926 --> 2.392855).  Saving model ...\n",
      "Epoch: 3683 \tTraining Loss: 2.315573 \tValidation Loss: 2.392891\n",
      "Epoch: 3684 \tTraining Loss: 2.330124 \tValidation Loss: 2.392774\n",
      "Validation loss decreased (2.392855 --> 2.392774).  Saving model ...\n",
      "Epoch: 3685 \tTraining Loss: 2.305717 \tValidation Loss: 2.392746\n",
      "Validation loss decreased (2.392774 --> 2.392746).  Saving model ...\n",
      "Epoch: 3686 \tTraining Loss: 2.332348 \tValidation Loss: 2.392802\n",
      "Epoch: 3687 \tTraining Loss: 2.331418 \tValidation Loss: 2.392835\n",
      "Epoch: 3688 \tTraining Loss: 2.330906 \tValidation Loss: 2.392837\n",
      "Epoch: 3689 \tTraining Loss: 2.318828 \tValidation Loss: 2.392813\n",
      "Epoch: 3690 \tTraining Loss: 2.318122 \tValidation Loss: 2.392813\n",
      "Epoch: 3691 \tTraining Loss: 2.317955 \tValidation Loss: 2.392888\n",
      "Epoch: 3692 \tTraining Loss: 2.315887 \tValidation Loss: 2.392927\n",
      "Epoch: 3693 \tTraining Loss: 2.323114 \tValidation Loss: 2.392774\n",
      "Epoch: 3694 \tTraining Loss: 2.328524 \tValidation Loss: 2.392654\n",
      "Validation loss decreased (2.392746 --> 2.392654).  Saving model ...\n",
      "Epoch: 3695 \tTraining Loss: 2.322345 \tValidation Loss: 2.392629\n",
      "Validation loss decreased (2.392654 --> 2.392629).  Saving model ...\n",
      "Epoch: 3696 \tTraining Loss: 2.321391 \tValidation Loss: 2.392703\n",
      "Epoch: 3697 \tTraining Loss: 2.337725 \tValidation Loss: 2.392766\n",
      "Epoch: 3698 \tTraining Loss: 2.327947 \tValidation Loss: 2.392708\n",
      "Epoch: 3699 \tTraining Loss: 2.329910 \tValidation Loss: 2.392626\n",
      "Validation loss decreased (2.392629 --> 2.392626).  Saving model ...\n",
      "Epoch: 3700 \tTraining Loss: 2.325468 \tValidation Loss: 2.392694\n",
      "Epoch: 3701 \tTraining Loss: 2.338860 \tValidation Loss: 2.392663\n",
      "Epoch: 3702 \tTraining Loss: 2.309093 \tValidation Loss: 2.392622\n",
      "Validation loss decreased (2.392626 --> 2.392622).  Saving model ...\n",
      "Epoch: 3703 \tTraining Loss: 2.315939 \tValidation Loss: 2.392484\n",
      "Validation loss decreased (2.392622 --> 2.392484).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3704 \tTraining Loss: 2.326482 \tValidation Loss: 2.392396\n",
      "Validation loss decreased (2.392484 --> 2.392396).  Saving model ...\n",
      "Epoch: 3705 \tTraining Loss: 2.323695 \tValidation Loss: 2.392300\n",
      "Validation loss decreased (2.392396 --> 2.392300).  Saving model ...\n",
      "Epoch: 3706 \tTraining Loss: 2.306823 \tValidation Loss: 2.392218\n",
      "Validation loss decreased (2.392300 --> 2.392218).  Saving model ...\n",
      "Epoch: 3707 \tTraining Loss: 2.317016 \tValidation Loss: 2.392209\n",
      "Validation loss decreased (2.392218 --> 2.392209).  Saving model ...\n",
      "Epoch: 3708 \tTraining Loss: 2.316586 \tValidation Loss: 2.392231\n",
      "Epoch: 3709 \tTraining Loss: 2.318030 \tValidation Loss: 2.392370\n",
      "Epoch: 3710 \tTraining Loss: 2.318025 \tValidation Loss: 2.392265\n",
      "Epoch: 3711 \tTraining Loss: 2.322528 \tValidation Loss: 2.392094\n",
      "Validation loss decreased (2.392209 --> 2.392094).  Saving model ...\n",
      "Epoch: 3712 \tTraining Loss: 2.319741 \tValidation Loss: 2.392069\n",
      "Validation loss decreased (2.392094 --> 2.392069).  Saving model ...\n",
      "Epoch: 3713 \tTraining Loss: 2.313878 \tValidation Loss: 2.392090\n",
      "Epoch: 3714 \tTraining Loss: 2.316939 \tValidation Loss: 2.392027\n",
      "Validation loss decreased (2.392069 --> 2.392027).  Saving model ...\n",
      "Epoch: 3715 \tTraining Loss: 2.304090 \tValidation Loss: 2.392068\n",
      "Epoch: 3716 \tTraining Loss: 2.330858 \tValidation Loss: 2.392087\n",
      "Epoch: 3717 \tTraining Loss: 2.314649 \tValidation Loss: 2.392045\n",
      "Epoch: 3718 \tTraining Loss: 2.312153 \tValidation Loss: 2.392011\n",
      "Validation loss decreased (2.392027 --> 2.392011).  Saving model ...\n",
      "Epoch: 3719 \tTraining Loss: 2.314785 \tValidation Loss: 2.391912\n",
      "Validation loss decreased (2.392011 --> 2.391912).  Saving model ...\n",
      "Epoch: 3720 \tTraining Loss: 2.320708 \tValidation Loss: 2.391909\n",
      "Validation loss decreased (2.391912 --> 2.391909).  Saving model ...\n",
      "Epoch: 3721 \tTraining Loss: 2.319913 \tValidation Loss: 2.391966\n",
      "Epoch: 3722 \tTraining Loss: 2.330411 \tValidation Loss: 2.392018\n",
      "Epoch: 3723 \tTraining Loss: 2.318433 \tValidation Loss: 2.391974\n",
      "Epoch: 3724 \tTraining Loss: 2.312537 \tValidation Loss: 2.391803\n",
      "Validation loss decreased (2.391909 --> 2.391803).  Saving model ...\n",
      "Epoch: 3725 \tTraining Loss: 2.319846 \tValidation Loss: 2.391790\n",
      "Validation loss decreased (2.391803 --> 2.391790).  Saving model ...\n",
      "Epoch: 3726 \tTraining Loss: 2.309994 \tValidation Loss: 2.391812\n",
      "Epoch: 3727 \tTraining Loss: 2.309866 \tValidation Loss: 2.391679\n",
      "Validation loss decreased (2.391790 --> 2.391679).  Saving model ...\n",
      "Epoch: 3728 \tTraining Loss: 2.339670 \tValidation Loss: 2.391701\n",
      "Epoch: 3729 \tTraining Loss: 2.331929 \tValidation Loss: 2.391732\n",
      "Epoch: 3730 \tTraining Loss: 2.323612 \tValidation Loss: 2.391636\n",
      "Validation loss decreased (2.391679 --> 2.391636).  Saving model ...\n",
      "Epoch: 3731 \tTraining Loss: 2.314529 \tValidation Loss: 2.391711\n",
      "Epoch: 3732 \tTraining Loss: 2.316253 \tValidation Loss: 2.391835\n",
      "Epoch: 3733 \tTraining Loss: 2.313304 \tValidation Loss: 2.391801\n",
      "Epoch: 3734 \tTraining Loss: 2.326922 \tValidation Loss: 2.391733\n",
      "Epoch: 3735 \tTraining Loss: 2.314223 \tValidation Loss: 2.391668\n",
      "Epoch: 3736 \tTraining Loss: 2.311751 \tValidation Loss: 2.391695\n",
      "Epoch: 3737 \tTraining Loss: 2.320892 \tValidation Loss: 2.391682\n",
      "Epoch: 3738 \tTraining Loss: 2.311015 \tValidation Loss: 2.391582\n",
      "Validation loss decreased (2.391636 --> 2.391582).  Saving model ...\n",
      "Epoch: 3739 \tTraining Loss: 2.306867 \tValidation Loss: 2.391536\n",
      "Validation loss decreased (2.391582 --> 2.391536).  Saving model ...\n",
      "Epoch: 3740 \tTraining Loss: 2.332080 \tValidation Loss: 2.391463\n",
      "Validation loss decreased (2.391536 --> 2.391463).  Saving model ...\n",
      "Epoch: 3741 \tTraining Loss: 2.321871 \tValidation Loss: 2.391427\n",
      "Validation loss decreased (2.391463 --> 2.391427).  Saving model ...\n",
      "Epoch: 3742 \tTraining Loss: 2.325371 \tValidation Loss: 2.391377\n",
      "Validation loss decreased (2.391427 --> 2.391377).  Saving model ...\n",
      "Epoch: 3743 \tTraining Loss: 2.331699 \tValidation Loss: 2.391472\n",
      "Epoch: 3744 \tTraining Loss: 2.321704 \tValidation Loss: 2.391443\n",
      "Epoch: 3745 \tTraining Loss: 2.310140 \tValidation Loss: 2.391418\n",
      "Epoch: 3746 \tTraining Loss: 2.310087 \tValidation Loss: 2.391428\n",
      "Epoch: 3747 \tTraining Loss: 2.322829 \tValidation Loss: 2.391528\n",
      "Epoch: 3748 \tTraining Loss: 2.309119 \tValidation Loss: 2.391412\n",
      "Epoch: 3749 \tTraining Loss: 2.318784 \tValidation Loss: 2.391319\n",
      "Validation loss decreased (2.391377 --> 2.391319).  Saving model ...\n",
      "Epoch: 3750 \tTraining Loss: 2.322593 \tValidation Loss: 2.391262\n",
      "Validation loss decreased (2.391319 --> 2.391262).  Saving model ...\n",
      "Epoch: 3751 \tTraining Loss: 2.301966 \tValidation Loss: 2.391243\n",
      "Validation loss decreased (2.391262 --> 2.391243).  Saving model ...\n",
      "Epoch: 3752 \tTraining Loss: 2.320329 \tValidation Loss: 2.391145\n",
      "Validation loss decreased (2.391243 --> 2.391145).  Saving model ...\n",
      "Epoch: 3753 \tTraining Loss: 2.320127 \tValidation Loss: 2.391165\n",
      "Epoch: 3754 \tTraining Loss: 2.318456 \tValidation Loss: 2.391158\n",
      "Epoch: 3755 \tTraining Loss: 2.322732 \tValidation Loss: 2.391098\n",
      "Validation loss decreased (2.391145 --> 2.391098).  Saving model ...\n",
      "Epoch: 3756 \tTraining Loss: 2.319331 \tValidation Loss: 2.391165\n",
      "Epoch: 3757 \tTraining Loss: 2.332646 \tValidation Loss: 2.391048\n",
      "Validation loss decreased (2.391098 --> 2.391048).  Saving model ...\n",
      "Epoch: 3758 \tTraining Loss: 2.315454 \tValidation Loss: 2.391033\n",
      "Validation loss decreased (2.391048 --> 2.391033).  Saving model ...\n",
      "Epoch: 3759 \tTraining Loss: 2.321071 \tValidation Loss: 2.390981\n",
      "Validation loss decreased (2.391033 --> 2.390981).  Saving model ...\n",
      "Epoch: 3760 \tTraining Loss: 2.314528 \tValidation Loss: 2.390828\n",
      "Validation loss decreased (2.390981 --> 2.390828).  Saving model ...\n",
      "Epoch: 3761 \tTraining Loss: 2.316598 \tValidation Loss: 2.390808\n",
      "Validation loss decreased (2.390828 --> 2.390808).  Saving model ...\n",
      "Epoch: 3762 \tTraining Loss: 2.310893 \tValidation Loss: 2.390725\n",
      "Validation loss decreased (2.390808 --> 2.390725).  Saving model ...\n",
      "Epoch: 3763 \tTraining Loss: 2.314830 \tValidation Loss: 2.390748\n",
      "Epoch: 3764 \tTraining Loss: 2.307017 \tValidation Loss: 2.390632\n",
      "Validation loss decreased (2.390725 --> 2.390632).  Saving model ...\n",
      "Epoch: 3765 \tTraining Loss: 2.316321 \tValidation Loss: 2.390639\n",
      "Epoch: 3766 \tTraining Loss: 2.311601 \tValidation Loss: 2.390660\n",
      "Epoch: 3767 \tTraining Loss: 2.305459 \tValidation Loss: 2.390578\n",
      "Validation loss decreased (2.390632 --> 2.390578).  Saving model ...\n",
      "Epoch: 3768 \tTraining Loss: 2.304518 \tValidation Loss: 2.390623\n",
      "Epoch: 3769 \tTraining Loss: 2.321357 \tValidation Loss: 2.390763\n",
      "Epoch: 3770 \tTraining Loss: 2.313959 \tValidation Loss: 2.390620\n",
      "Epoch: 3771 \tTraining Loss: 2.317158 \tValidation Loss: 2.390598\n",
      "Epoch: 3772 \tTraining Loss: 2.323292 \tValidation Loss: 2.390524\n",
      "Validation loss decreased (2.390578 --> 2.390524).  Saving model ...\n",
      "Epoch: 3773 \tTraining Loss: 2.308307 \tValidation Loss: 2.390453\n",
      "Validation loss decreased (2.390524 --> 2.390453).  Saving model ...\n",
      "Epoch: 3774 \tTraining Loss: 2.309231 \tValidation Loss: 2.390446\n",
      "Validation loss decreased (2.390453 --> 2.390446).  Saving model ...\n",
      "Epoch: 3775 \tTraining Loss: 2.325341 \tValidation Loss: 2.390282\n",
      "Validation loss decreased (2.390446 --> 2.390282).  Saving model ...\n",
      "Epoch: 3776 \tTraining Loss: 2.298931 \tValidation Loss: 2.390307\n",
      "Epoch: 3777 \tTraining Loss: 2.307079 \tValidation Loss: 2.390479\n",
      "Epoch: 3778 \tTraining Loss: 2.316673 \tValidation Loss: 2.390568\n",
      "Epoch: 3779 \tTraining Loss: 2.320539 \tValidation Loss: 2.390638\n",
      "Epoch: 3780 \tTraining Loss: 2.305372 \tValidation Loss: 2.390619\n",
      "Epoch: 3781 \tTraining Loss: 2.319699 \tValidation Loss: 2.390634\n",
      "Epoch: 3782 \tTraining Loss: 2.304649 \tValidation Loss: 2.390534\n",
      "Epoch: 3783 \tTraining Loss: 2.322220 \tValidation Loss: 2.390484\n",
      "Epoch: 3784 \tTraining Loss: 2.299067 \tValidation Loss: 2.390395\n",
      "Epoch: 3785 \tTraining Loss: 2.315835 \tValidation Loss: 2.390303\n",
      "Epoch: 3786 \tTraining Loss: 2.329863 \tValidation Loss: 2.390292\n",
      "Epoch: 3787 \tTraining Loss: 2.304904 \tValidation Loss: 2.390257\n",
      "Validation loss decreased (2.390282 --> 2.390257).  Saving model ...\n",
      "Epoch: 3788 \tTraining Loss: 2.314904 \tValidation Loss: 2.390204\n",
      "Validation loss decreased (2.390257 --> 2.390204).  Saving model ...\n",
      "Epoch: 3789 \tTraining Loss: 2.319564 \tValidation Loss: 2.390285\n",
      "Epoch: 3790 \tTraining Loss: 2.307466 \tValidation Loss: 2.390301\n",
      "Epoch: 3791 \tTraining Loss: 2.307563 \tValidation Loss: 2.390163\n",
      "Validation loss decreased (2.390204 --> 2.390163).  Saving model ...\n",
      "Epoch: 3792 \tTraining Loss: 2.326186 \tValidation Loss: 2.390350\n",
      "Epoch: 3793 \tTraining Loss: 2.304082 \tValidation Loss: 2.390414\n",
      "Epoch: 3794 \tTraining Loss: 2.327192 \tValidation Loss: 2.390455\n",
      "Epoch: 3795 \tTraining Loss: 2.308640 \tValidation Loss: 2.390434\n",
      "Epoch: 3796 \tTraining Loss: 2.310840 \tValidation Loss: 2.390426\n",
      "Epoch: 3797 \tTraining Loss: 2.323762 \tValidation Loss: 2.390444\n",
      "Epoch: 3798 \tTraining Loss: 2.324404 \tValidation Loss: 2.390508\n",
      "Epoch: 3799 \tTraining Loss: 2.335129 \tValidation Loss: 2.390590\n",
      "Epoch: 3800 \tTraining Loss: 2.304702 \tValidation Loss: 2.390472\n",
      "Epoch: 3801 \tTraining Loss: 2.311293 \tValidation Loss: 2.390452\n",
      "Epoch: 3802 \tTraining Loss: 2.333911 \tValidation Loss: 2.390529\n",
      "Epoch: 3803 \tTraining Loss: 2.323838 \tValidation Loss: 2.390433\n",
      "Epoch: 3804 \tTraining Loss: 2.324806 \tValidation Loss: 2.390481\n",
      "Epoch: 3805 \tTraining Loss: 2.323208 \tValidation Loss: 2.390382\n",
      "Epoch: 3806 \tTraining Loss: 2.299600 \tValidation Loss: 2.390217\n",
      "Epoch: 3807 \tTraining Loss: 2.304770 \tValidation Loss: 2.389965\n",
      "Validation loss decreased (2.390163 --> 2.389965).  Saving model ...\n",
      "Epoch: 3808 \tTraining Loss: 2.298066 \tValidation Loss: 2.389902\n",
      "Validation loss decreased (2.389965 --> 2.389902).  Saving model ...\n",
      "Epoch: 3809 \tTraining Loss: 2.317838 \tValidation Loss: 2.389938\n",
      "Epoch: 3810 \tTraining Loss: 2.306749 \tValidation Loss: 2.389847\n",
      "Validation loss decreased (2.389902 --> 2.389847).  Saving model ...\n",
      "Epoch: 3811 \tTraining Loss: 2.324284 \tValidation Loss: 2.389754\n",
      "Validation loss decreased (2.389847 --> 2.389754).  Saving model ...\n",
      "Epoch: 3812 \tTraining Loss: 2.303538 \tValidation Loss: 2.389656\n",
      "Validation loss decreased (2.389754 --> 2.389656).  Saving model ...\n",
      "Epoch: 3813 \tTraining Loss: 2.311797 \tValidation Loss: 2.389678\n",
      "Epoch: 3814 \tTraining Loss: 2.313383 \tValidation Loss: 2.389588\n",
      "Validation loss decreased (2.389656 --> 2.389588).  Saving model ...\n",
      "Epoch: 3815 \tTraining Loss: 2.304900 \tValidation Loss: 2.389533\n",
      "Validation loss decreased (2.389588 --> 2.389533).  Saving model ...\n",
      "Epoch: 3816 \tTraining Loss: 2.304163 \tValidation Loss: 2.389450\n",
      "Validation loss decreased (2.389533 --> 2.389450).  Saving model ...\n",
      "Epoch: 3817 \tTraining Loss: 2.309162 \tValidation Loss: 2.389474\n",
      "Epoch: 3818 \tTraining Loss: 2.305139 \tValidation Loss: 2.389405\n",
      "Validation loss decreased (2.389450 --> 2.389405).  Saving model ...\n",
      "Epoch: 3819 \tTraining Loss: 2.316413 \tValidation Loss: 2.389545\n",
      "Epoch: 3820 \tTraining Loss: 2.317091 \tValidation Loss: 2.389433\n",
      "Epoch: 3821 \tTraining Loss: 2.308611 \tValidation Loss: 2.389324\n",
      "Validation loss decreased (2.389405 --> 2.389324).  Saving model ...\n",
      "Epoch: 3822 \tTraining Loss: 2.318947 \tValidation Loss: 2.389381\n",
      "Epoch: 3823 \tTraining Loss: 2.286338 \tValidation Loss: 2.389310\n",
      "Validation loss decreased (2.389324 --> 2.389310).  Saving model ...\n",
      "Epoch: 3824 \tTraining Loss: 2.299894 \tValidation Loss: 2.389473\n",
      "Epoch: 3825 \tTraining Loss: 2.325023 \tValidation Loss: 2.389478\n",
      "Epoch: 3826 \tTraining Loss: 2.313603 \tValidation Loss: 2.389424\n",
      "Epoch: 3827 \tTraining Loss: 2.309436 \tValidation Loss: 2.389496\n",
      "Epoch: 3828 \tTraining Loss: 2.311956 \tValidation Loss: 2.389462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3829 \tTraining Loss: 2.309772 \tValidation Loss: 2.389413\n",
      "Epoch: 3830 \tTraining Loss: 2.314187 \tValidation Loss: 2.389505\n",
      "Epoch: 3831 \tTraining Loss: 2.297458 \tValidation Loss: 2.389428\n",
      "Epoch: 3832 \tTraining Loss: 2.320236 \tValidation Loss: 2.389343\n",
      "Epoch: 3833 \tTraining Loss: 2.320032 \tValidation Loss: 2.389298\n",
      "Validation loss decreased (2.389310 --> 2.389298).  Saving model ...\n",
      "Epoch: 3834 \tTraining Loss: 2.305458 \tValidation Loss: 2.389164\n",
      "Validation loss decreased (2.389298 --> 2.389164).  Saving model ...\n",
      "Epoch: 3835 \tTraining Loss: 2.320971 \tValidation Loss: 2.389167\n",
      "Epoch: 3836 \tTraining Loss: 2.313479 \tValidation Loss: 2.389122\n",
      "Validation loss decreased (2.389164 --> 2.389122).  Saving model ...\n",
      "Epoch: 3837 \tTraining Loss: 2.308517 \tValidation Loss: 2.389088\n",
      "Validation loss decreased (2.389122 --> 2.389088).  Saving model ...\n",
      "Epoch: 3838 \tTraining Loss: 2.322495 \tValidation Loss: 2.389232\n",
      "Epoch: 3839 \tTraining Loss: 2.311749 \tValidation Loss: 2.389148\n",
      "Epoch: 3840 \tTraining Loss: 2.320940 \tValidation Loss: 2.389136\n",
      "Epoch: 3841 \tTraining Loss: 2.306876 \tValidation Loss: 2.389097\n",
      "Epoch: 3842 \tTraining Loss: 2.312161 \tValidation Loss: 2.389137\n",
      "Epoch: 3843 \tTraining Loss: 2.305216 \tValidation Loss: 2.389027\n",
      "Validation loss decreased (2.389088 --> 2.389027).  Saving model ...\n",
      "Epoch: 3844 \tTraining Loss: 2.321898 \tValidation Loss: 2.389003\n",
      "Validation loss decreased (2.389027 --> 2.389003).  Saving model ...\n",
      "Epoch: 3845 \tTraining Loss: 2.315017 \tValidation Loss: 2.388927\n",
      "Validation loss decreased (2.389003 --> 2.388927).  Saving model ...\n",
      "Epoch: 3846 \tTraining Loss: 2.300030 \tValidation Loss: 2.388905\n",
      "Validation loss decreased (2.388927 --> 2.388905).  Saving model ...\n",
      "Epoch: 3847 \tTraining Loss: 2.323024 \tValidation Loss: 2.388969\n",
      "Epoch: 3848 \tTraining Loss: 2.324968 \tValidation Loss: 2.389163\n",
      "Epoch: 3849 \tTraining Loss: 2.329276 \tValidation Loss: 2.389425\n",
      "Epoch: 3850 \tTraining Loss: 2.304150 \tValidation Loss: 2.389398\n",
      "Epoch: 3851 \tTraining Loss: 2.319635 \tValidation Loss: 2.389247\n",
      "Epoch: 3852 \tTraining Loss: 2.320844 \tValidation Loss: 2.389325\n",
      "Epoch: 3853 \tTraining Loss: 2.316030 \tValidation Loss: 2.389416\n",
      "Epoch: 3854 \tTraining Loss: 2.319231 \tValidation Loss: 2.389399\n",
      "Epoch: 3855 \tTraining Loss: 2.301784 \tValidation Loss: 2.389243\n",
      "Epoch: 3856 \tTraining Loss: 2.304079 \tValidation Loss: 2.389175\n",
      "Epoch: 3857 \tTraining Loss: 2.311893 \tValidation Loss: 2.389160\n",
      "Epoch: 3858 \tTraining Loss: 2.317156 \tValidation Loss: 2.389123\n",
      "Epoch: 3859 \tTraining Loss: 2.293075 \tValidation Loss: 2.388949\n",
      "Epoch: 3860 \tTraining Loss: 2.309893 \tValidation Loss: 2.388934\n",
      "Epoch: 3861 \tTraining Loss: 2.311728 \tValidation Loss: 2.388933\n",
      "Epoch: 3862 \tTraining Loss: 2.312060 \tValidation Loss: 2.388764\n",
      "Validation loss decreased (2.388905 --> 2.388764).  Saving model ...\n",
      "Epoch: 3863 \tTraining Loss: 2.308511 \tValidation Loss: 2.388711\n",
      "Validation loss decreased (2.388764 --> 2.388711).  Saving model ...\n",
      "Epoch: 3864 \tTraining Loss: 2.310087 \tValidation Loss: 2.388675\n",
      "Validation loss decreased (2.388711 --> 2.388675).  Saving model ...\n",
      "Epoch: 3865 \tTraining Loss: 2.304600 \tValidation Loss: 2.388599\n",
      "Validation loss decreased (2.388675 --> 2.388599).  Saving model ...\n",
      "Epoch: 3866 \tTraining Loss: 2.316399 \tValidation Loss: 2.388699\n",
      "Epoch: 3867 \tTraining Loss: 2.309252 \tValidation Loss: 2.388683\n",
      "Epoch: 3868 \tTraining Loss: 2.302410 \tValidation Loss: 2.388605\n",
      "Epoch: 3869 \tTraining Loss: 2.317934 \tValidation Loss: 2.388532\n",
      "Validation loss decreased (2.388599 --> 2.388532).  Saving model ...\n",
      "Epoch: 3870 \tTraining Loss: 2.303300 \tValidation Loss: 2.388446\n",
      "Validation loss decreased (2.388532 --> 2.388446).  Saving model ...\n",
      "Epoch: 3871 \tTraining Loss: 2.309450 \tValidation Loss: 2.388491\n",
      "Epoch: 3872 \tTraining Loss: 2.306344 \tValidation Loss: 2.388346\n",
      "Validation loss decreased (2.388446 --> 2.388346).  Saving model ...\n",
      "Epoch: 3873 \tTraining Loss: 2.307584 \tValidation Loss: 2.388388\n",
      "Epoch: 3874 \tTraining Loss: 2.315469 \tValidation Loss: 2.388343\n",
      "Validation loss decreased (2.388346 --> 2.388343).  Saving model ...\n",
      "Epoch: 3875 \tTraining Loss: 2.317578 \tValidation Loss: 2.388324\n",
      "Validation loss decreased (2.388343 --> 2.388324).  Saving model ...\n",
      "Epoch: 3876 \tTraining Loss: 2.308781 \tValidation Loss: 2.388322\n",
      "Validation loss decreased (2.388324 --> 2.388322).  Saving model ...\n",
      "Epoch: 3877 \tTraining Loss: 2.306988 \tValidation Loss: 2.388237\n",
      "Validation loss decreased (2.388322 --> 2.388237).  Saving model ...\n",
      "Epoch: 3878 \tTraining Loss: 2.306308 \tValidation Loss: 2.388156\n",
      "Validation loss decreased (2.388237 --> 2.388156).  Saving model ...\n",
      "Epoch: 3879 \tTraining Loss: 2.304687 \tValidation Loss: 2.388260\n",
      "Epoch: 3880 \tTraining Loss: 2.304812 \tValidation Loss: 2.388132\n",
      "Validation loss decreased (2.388156 --> 2.388132).  Saving model ...\n",
      "Epoch: 3881 \tTraining Loss: 2.308933 \tValidation Loss: 2.388050\n",
      "Validation loss decreased (2.388132 --> 2.388050).  Saving model ...\n",
      "Epoch: 3882 \tTraining Loss: 2.301848 \tValidation Loss: 2.387929\n",
      "Validation loss decreased (2.388050 --> 2.387929).  Saving model ...\n",
      "Epoch: 3883 \tTraining Loss: 2.335747 \tValidation Loss: 2.388166\n",
      "Epoch: 3884 \tTraining Loss: 2.304111 \tValidation Loss: 2.388025\n",
      "Epoch: 3885 \tTraining Loss: 2.308396 \tValidation Loss: 2.387845\n",
      "Validation loss decreased (2.387929 --> 2.387845).  Saving model ...\n",
      "Epoch: 3886 \tTraining Loss: 2.316278 \tValidation Loss: 2.387865\n",
      "Epoch: 3887 \tTraining Loss: 2.309814 \tValidation Loss: 2.387760\n",
      "Validation loss decreased (2.387845 --> 2.387760).  Saving model ...\n",
      "Epoch: 3888 \tTraining Loss: 2.309615 \tValidation Loss: 2.387587\n",
      "Validation loss decreased (2.387760 --> 2.387587).  Saving model ...\n",
      "Epoch: 3889 \tTraining Loss: 2.301028 \tValidation Loss: 2.387525\n",
      "Validation loss decreased (2.387587 --> 2.387525).  Saving model ...\n",
      "Epoch: 3890 \tTraining Loss: 2.315779 \tValidation Loss: 2.387539\n",
      "Epoch: 3891 \tTraining Loss: 2.318053 \tValidation Loss: 2.387715\n",
      "Epoch: 3892 \tTraining Loss: 2.305232 \tValidation Loss: 2.387548\n",
      "Epoch: 3893 \tTraining Loss: 2.313147 \tValidation Loss: 2.387512\n",
      "Validation loss decreased (2.387525 --> 2.387512).  Saving model ...\n",
      "Epoch: 3894 \tTraining Loss: 2.313136 \tValidation Loss: 2.387491\n",
      "Validation loss decreased (2.387512 --> 2.387491).  Saving model ...\n",
      "Epoch: 3895 \tTraining Loss: 2.313555 \tValidation Loss: 2.387567\n",
      "Epoch: 3896 \tTraining Loss: 2.313839 \tValidation Loss: 2.387453\n",
      "Validation loss decreased (2.387491 --> 2.387453).  Saving model ...\n",
      "Epoch: 3897 \tTraining Loss: 2.319739 \tValidation Loss: 2.387563\n",
      "Epoch: 3898 \tTraining Loss: 2.314160 \tValidation Loss: 2.387467\n",
      "Epoch: 3899 \tTraining Loss: 2.308112 \tValidation Loss: 2.387640\n",
      "Epoch: 3900 \tTraining Loss: 2.296601 \tValidation Loss: 2.387487\n",
      "Epoch: 3901 \tTraining Loss: 2.313964 \tValidation Loss: 2.387533\n",
      "Epoch: 3902 \tTraining Loss: 2.304298 \tValidation Loss: 2.387472\n",
      "Epoch: 3903 \tTraining Loss: 2.320281 \tValidation Loss: 2.387308\n",
      "Validation loss decreased (2.387453 --> 2.387308).  Saving model ...\n",
      "Epoch: 3904 \tTraining Loss: 2.294696 \tValidation Loss: 2.387271\n",
      "Validation loss decreased (2.387308 --> 2.387271).  Saving model ...\n",
      "Epoch: 3905 \tTraining Loss: 2.296915 \tValidation Loss: 2.387155\n",
      "Validation loss decreased (2.387271 --> 2.387155).  Saving model ...\n",
      "Epoch: 3906 \tTraining Loss: 2.304568 \tValidation Loss: 2.387139\n",
      "Validation loss decreased (2.387155 --> 2.387139).  Saving model ...\n",
      "Epoch: 3907 \tTraining Loss: 2.317187 \tValidation Loss: 2.387176\n",
      "Epoch: 3908 \tTraining Loss: 2.311207 \tValidation Loss: 2.387154\n",
      "Epoch: 3909 \tTraining Loss: 2.307303 \tValidation Loss: 2.387055\n",
      "Validation loss decreased (2.387139 --> 2.387055).  Saving model ...\n",
      "Epoch: 3910 \tTraining Loss: 2.312347 \tValidation Loss: 2.387105\n",
      "Epoch: 3911 \tTraining Loss: 2.316090 \tValidation Loss: 2.387083\n",
      "Epoch: 3912 \tTraining Loss: 2.299287 \tValidation Loss: 2.387237\n",
      "Epoch: 3913 \tTraining Loss: 2.304109 \tValidation Loss: 2.387245\n",
      "Epoch: 3914 \tTraining Loss: 2.298378 \tValidation Loss: 2.387237\n",
      "Epoch: 3915 \tTraining Loss: 2.306862 \tValidation Loss: 2.387186\n",
      "Epoch: 3916 \tTraining Loss: 2.305728 \tValidation Loss: 2.387156\n",
      "Epoch: 3917 \tTraining Loss: 2.314748 \tValidation Loss: 2.387058\n",
      "Epoch: 3918 \tTraining Loss: 2.308908 \tValidation Loss: 2.387088\n",
      "Epoch: 3919 \tTraining Loss: 2.309371 \tValidation Loss: 2.387075\n",
      "Epoch: 3920 \tTraining Loss: 2.299463 \tValidation Loss: 2.387076\n",
      "Epoch: 3921 \tTraining Loss: 2.286497 \tValidation Loss: 2.386939\n",
      "Validation loss decreased (2.387055 --> 2.386939).  Saving model ...\n",
      "Epoch: 3922 \tTraining Loss: 2.317070 \tValidation Loss: 2.387064\n",
      "Epoch: 3923 \tTraining Loss: 2.312118 \tValidation Loss: 2.386858\n",
      "Validation loss decreased (2.386939 --> 2.386858).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3924 \tTraining Loss: 2.290749 \tValidation Loss: 2.386792\n",
      "Validation loss decreased (2.386858 --> 2.386792).  Saving model ...\n",
      "Epoch: 3925 \tTraining Loss: 2.301896 \tValidation Loss: 2.386785\n",
      "Validation loss decreased (2.386792 --> 2.386785).  Saving model ...\n",
      "Epoch: 3926 \tTraining Loss: 2.285747 \tValidation Loss: 2.386684\n",
      "Validation loss decreased (2.386785 --> 2.386684).  Saving model ...\n",
      "Epoch: 3927 \tTraining Loss: 2.316745 \tValidation Loss: 2.386694\n",
      "Epoch: 3928 \tTraining Loss: 2.324760 \tValidation Loss: 2.386505\n",
      "Validation loss decreased (2.386684 --> 2.386505).  Saving model ...\n",
      "Epoch: 3929 \tTraining Loss: 2.319666 \tValidation Loss: 2.386590\n",
      "Epoch: 3930 \tTraining Loss: 2.305382 \tValidation Loss: 2.386582\n",
      "Epoch: 3931 \tTraining Loss: 2.309196 \tValidation Loss: 2.386559\n",
      "Epoch: 3932 \tTraining Loss: 2.304013 \tValidation Loss: 2.386616\n",
      "Epoch: 3933 \tTraining Loss: 2.310903 \tValidation Loss: 2.386607\n",
      "Epoch: 3934 \tTraining Loss: 2.310244 \tValidation Loss: 2.386459\n",
      "Validation loss decreased (2.386505 --> 2.386459).  Saving model ...\n",
      "Epoch: 3935 \tTraining Loss: 2.307099 \tValidation Loss: 2.386440\n",
      "Validation loss decreased (2.386459 --> 2.386440).  Saving model ...\n",
      "Epoch: 3936 \tTraining Loss: 2.305758 \tValidation Loss: 2.386445\n",
      "Epoch: 3937 \tTraining Loss: 2.302786 \tValidation Loss: 2.386237\n",
      "Validation loss decreased (2.386440 --> 2.386237).  Saving model ...\n",
      "Epoch: 3938 \tTraining Loss: 2.309710 \tValidation Loss: 2.386313\n",
      "Epoch: 3939 \tTraining Loss: 2.309486 \tValidation Loss: 2.386277\n",
      "Epoch: 3940 \tTraining Loss: 2.308576 \tValidation Loss: 2.386375\n",
      "Epoch: 3941 \tTraining Loss: 2.318265 \tValidation Loss: 2.386409\n",
      "Epoch: 3942 \tTraining Loss: 2.324998 \tValidation Loss: 2.386521\n",
      "Epoch: 3943 \tTraining Loss: 2.307361 \tValidation Loss: 2.386551\n",
      "Epoch: 3944 \tTraining Loss: 2.296877 \tValidation Loss: 2.386323\n",
      "Epoch: 3945 \tTraining Loss: 2.304605 \tValidation Loss: 2.386322\n",
      "Epoch: 3946 \tTraining Loss: 2.282585 \tValidation Loss: 2.386175\n",
      "Validation loss decreased (2.386237 --> 2.386175).  Saving model ...\n",
      "Epoch: 3947 \tTraining Loss: 2.300399 \tValidation Loss: 2.386113\n",
      "Validation loss decreased (2.386175 --> 2.386113).  Saving model ...\n",
      "Epoch: 3948 \tTraining Loss: 2.313777 \tValidation Loss: 2.386081\n",
      "Validation loss decreased (2.386113 --> 2.386081).  Saving model ...\n",
      "Epoch: 3949 \tTraining Loss: 2.304602 \tValidation Loss: 2.386226\n",
      "Epoch: 3950 \tTraining Loss: 2.310324 \tValidation Loss: 2.386227\n",
      "Epoch: 3951 \tTraining Loss: 2.312834 \tValidation Loss: 2.386382\n",
      "Epoch: 3952 \tTraining Loss: 2.302118 \tValidation Loss: 2.386459\n",
      "Epoch: 3953 \tTraining Loss: 2.313308 \tValidation Loss: 2.386468\n",
      "Epoch: 3954 \tTraining Loss: 2.301126 \tValidation Loss: 2.386317\n",
      "Epoch: 3955 \tTraining Loss: 2.305794 \tValidation Loss: 2.386422\n",
      "Epoch: 3956 \tTraining Loss: 2.303892 \tValidation Loss: 2.386437\n",
      "Epoch: 3957 \tTraining Loss: 2.312949 \tValidation Loss: 2.386309\n",
      "Epoch: 3958 \tTraining Loss: 2.308977 \tValidation Loss: 2.386379\n",
      "Epoch: 3959 \tTraining Loss: 2.298862 \tValidation Loss: 2.386470\n",
      "Epoch: 3960 \tTraining Loss: 2.307274 \tValidation Loss: 2.386309\n",
      "Epoch: 3961 \tTraining Loss: 2.297292 \tValidation Loss: 2.386270\n",
      "Epoch: 3962 \tTraining Loss: 2.307812 \tValidation Loss: 2.386125\n",
      "Epoch: 3963 \tTraining Loss: 2.313420 \tValidation Loss: 2.386130\n",
      "Epoch: 3964 \tTraining Loss: 2.314845 \tValidation Loss: 2.386198\n",
      "Epoch: 3965 \tTraining Loss: 2.311110 \tValidation Loss: 2.386262\n",
      "Epoch: 3966 \tTraining Loss: 2.292205 \tValidation Loss: 2.386076\n",
      "Validation loss decreased (2.386081 --> 2.386076).  Saving model ...\n",
      "Epoch: 3967 \tTraining Loss: 2.317220 \tValidation Loss: 2.386070\n",
      "Validation loss decreased (2.386076 --> 2.386070).  Saving model ...\n",
      "Epoch: 3968 \tTraining Loss: 2.306623 \tValidation Loss: 2.385896\n",
      "Validation loss decreased (2.386070 --> 2.385896).  Saving model ...\n",
      "Epoch: 3969 \tTraining Loss: 2.302066 \tValidation Loss: 2.385850\n",
      "Validation loss decreased (2.385896 --> 2.385850).  Saving model ...\n",
      "Epoch: 3970 \tTraining Loss: 2.298075 \tValidation Loss: 2.385791\n",
      "Validation loss decreased (2.385850 --> 2.385791).  Saving model ...\n",
      "Epoch: 3971 \tTraining Loss: 2.318088 \tValidation Loss: 2.385758\n",
      "Validation loss decreased (2.385791 --> 2.385758).  Saving model ...\n",
      "Epoch: 3972 \tTraining Loss: 2.298972 \tValidation Loss: 2.385781\n",
      "Epoch: 3973 \tTraining Loss: 2.303506 \tValidation Loss: 2.385646\n",
      "Validation loss decreased (2.385758 --> 2.385646).  Saving model ...\n",
      "Epoch: 3974 \tTraining Loss: 2.302681 \tValidation Loss: 2.385548\n",
      "Validation loss decreased (2.385646 --> 2.385548).  Saving model ...\n",
      "Epoch: 3975 \tTraining Loss: 2.331273 \tValidation Loss: 2.385672\n",
      "Epoch: 3976 \tTraining Loss: 2.296183 \tValidation Loss: 2.385610\n",
      "Epoch: 3977 \tTraining Loss: 2.288565 \tValidation Loss: 2.385569\n",
      "Epoch: 3978 \tTraining Loss: 2.314511 \tValidation Loss: 2.385644\n",
      "Epoch: 3979 \tTraining Loss: 2.293911 \tValidation Loss: 2.385644\n",
      "Epoch: 3980 \tTraining Loss: 2.297263 \tValidation Loss: 2.385719\n",
      "Epoch: 3981 \tTraining Loss: 2.314809 \tValidation Loss: 2.385670\n",
      "Epoch: 3982 \tTraining Loss: 2.308843 \tValidation Loss: 2.385728\n",
      "Epoch: 3983 \tTraining Loss: 2.319033 \tValidation Loss: 2.385769\n",
      "Epoch: 3984 \tTraining Loss: 2.295071 \tValidation Loss: 2.385556\n",
      "Epoch: 3985 \tTraining Loss: 2.318226 \tValidation Loss: 2.385475\n",
      "Validation loss decreased (2.385548 --> 2.385475).  Saving model ...\n",
      "Epoch: 3986 \tTraining Loss: 2.300400 \tValidation Loss: 2.385430\n",
      "Validation loss decreased (2.385475 --> 2.385430).  Saving model ...\n",
      "Epoch: 3987 \tTraining Loss: 2.317115 \tValidation Loss: 2.385458\n",
      "Epoch: 3988 \tTraining Loss: 2.310707 \tValidation Loss: 2.385425\n",
      "Validation loss decreased (2.385430 --> 2.385425).  Saving model ...\n",
      "Epoch: 3989 \tTraining Loss: 2.320291 \tValidation Loss: 2.385327\n",
      "Validation loss decreased (2.385425 --> 2.385327).  Saving model ...\n",
      "Epoch: 3990 \tTraining Loss: 2.305941 \tValidation Loss: 2.385260\n",
      "Validation loss decreased (2.385327 --> 2.385260).  Saving model ...\n",
      "Epoch: 3991 \tTraining Loss: 2.322368 \tValidation Loss: 2.385242\n",
      "Validation loss decreased (2.385260 --> 2.385242).  Saving model ...\n",
      "Epoch: 3992 \tTraining Loss: 2.311327 \tValidation Loss: 2.385267\n",
      "Epoch: 3993 \tTraining Loss: 2.312230 \tValidation Loss: 2.385291\n",
      "Epoch: 3994 \tTraining Loss: 2.308261 \tValidation Loss: 2.385175\n",
      "Validation loss decreased (2.385242 --> 2.385175).  Saving model ...\n",
      "Epoch: 3995 \tTraining Loss: 2.301981 \tValidation Loss: 2.385176\n",
      "Epoch: 3996 \tTraining Loss: 2.302574 \tValidation Loss: 2.385049\n",
      "Validation loss decreased (2.385175 --> 2.385049).  Saving model ...\n",
      "Epoch: 3997 \tTraining Loss: 2.298077 \tValidation Loss: 2.384991\n",
      "Validation loss decreased (2.385049 --> 2.384991).  Saving model ...\n",
      "Epoch: 3998 \tTraining Loss: 2.298643 \tValidation Loss: 2.384928\n",
      "Validation loss decreased (2.384991 --> 2.384928).  Saving model ...\n",
      "Epoch: 3999 \tTraining Loss: 2.309417 \tValidation Loss: 2.384881\n",
      "Validation loss decreased (2.384928 --> 2.384881).  Saving model ...\n",
      "Epoch: 4000 \tTraining Loss: 2.295565 \tValidation Loss: 2.384935\n",
      "Epoch: 4001 \tTraining Loss: 2.301504 \tValidation Loss: 2.384953\n",
      "Epoch: 4002 \tTraining Loss: 2.291511 \tValidation Loss: 2.384834\n",
      "Validation loss decreased (2.384881 --> 2.384834).  Saving model ...\n",
      "Epoch: 4003 \tTraining Loss: 2.303241 \tValidation Loss: 2.384645\n",
      "Validation loss decreased (2.384834 --> 2.384645).  Saving model ...\n",
      "Epoch: 4004 \tTraining Loss: 2.304135 \tValidation Loss: 2.384757\n",
      "Epoch: 4005 \tTraining Loss: 2.308354 \tValidation Loss: 2.384849\n",
      "Epoch: 4006 \tTraining Loss: 2.311979 \tValidation Loss: 2.384978\n",
      "Epoch: 4007 \tTraining Loss: 2.311488 \tValidation Loss: 2.384882\n",
      "Epoch: 4008 \tTraining Loss: 2.293625 \tValidation Loss: 2.384816\n",
      "Epoch: 4009 \tTraining Loss: 2.309984 \tValidation Loss: 2.384898\n",
      "Epoch: 4010 \tTraining Loss: 2.313647 \tValidation Loss: 2.384884\n",
      "Epoch: 4011 \tTraining Loss: 2.302032 \tValidation Loss: 2.384993\n",
      "Epoch: 4012 \tTraining Loss: 2.319695 \tValidation Loss: 2.385026\n",
      "Epoch: 4013 \tTraining Loss: 2.318292 \tValidation Loss: 2.384963\n",
      "Epoch: 4014 \tTraining Loss: 2.309067 \tValidation Loss: 2.384928\n",
      "Epoch: 4015 \tTraining Loss: 2.308873 \tValidation Loss: 2.384839\n",
      "Epoch: 4016 \tTraining Loss: 2.288960 \tValidation Loss: 2.384806\n",
      "Epoch: 4017 \tTraining Loss: 2.295074 \tValidation Loss: 2.384754\n",
      "Epoch: 4018 \tTraining Loss: 2.300901 \tValidation Loss: 2.384705\n",
      "Epoch: 4019 \tTraining Loss: 2.305238 \tValidation Loss: 2.384552\n",
      "Validation loss decreased (2.384645 --> 2.384552).  Saving model ...\n",
      "Epoch: 4020 \tTraining Loss: 2.302270 \tValidation Loss: 2.384554\n",
      "Epoch: 4021 \tTraining Loss: 2.294995 \tValidation Loss: 2.384532\n",
      "Validation loss decreased (2.384552 --> 2.384532).  Saving model ...\n",
      "Epoch: 4022 \tTraining Loss: 2.310520 \tValidation Loss: 2.384564\n",
      "Epoch: 4023 \tTraining Loss: 2.312535 \tValidation Loss: 2.384586\n",
      "Epoch: 4024 \tTraining Loss: 2.306211 \tValidation Loss: 2.384419\n",
      "Validation loss decreased (2.384532 --> 2.384419).  Saving model ...\n",
      "Epoch: 4025 \tTraining Loss: 2.300648 \tValidation Loss: 2.384308\n",
      "Validation loss decreased (2.384419 --> 2.384308).  Saving model ...\n",
      "Epoch: 4026 \tTraining Loss: 2.292860 \tValidation Loss: 2.384347\n",
      "Epoch: 4027 \tTraining Loss: 2.295439 \tValidation Loss: 2.384398\n",
      "Epoch: 4028 \tTraining Loss: 2.301990 \tValidation Loss: 2.384425\n",
      "Epoch: 4029 \tTraining Loss: 2.299548 \tValidation Loss: 2.384577\n",
      "Epoch: 4030 \tTraining Loss: 2.303640 \tValidation Loss: 2.384587\n",
      "Epoch: 4031 \tTraining Loss: 2.302181 \tValidation Loss: 2.384586\n",
      "Epoch: 4032 \tTraining Loss: 2.303546 \tValidation Loss: 2.384512\n",
      "Epoch: 4033 \tTraining Loss: 2.294022 \tValidation Loss: 2.384353\n",
      "Epoch: 4034 \tTraining Loss: 2.284614 \tValidation Loss: 2.384279\n",
      "Validation loss decreased (2.384308 --> 2.384279).  Saving model ...\n",
      "Epoch: 4035 \tTraining Loss: 2.311564 \tValidation Loss: 2.384218\n",
      "Validation loss decreased (2.384279 --> 2.384218).  Saving model ...\n",
      "Epoch: 4036 \tTraining Loss: 2.308920 \tValidation Loss: 2.384242\n",
      "Epoch: 4037 \tTraining Loss: 2.295877 \tValidation Loss: 2.384167\n",
      "Validation loss decreased (2.384218 --> 2.384167).  Saving model ...\n",
      "Epoch: 4038 \tTraining Loss: 2.294173 \tValidation Loss: 2.383958\n",
      "Validation loss decreased (2.384167 --> 2.383958).  Saving model ...\n",
      "Epoch: 4039 \tTraining Loss: 2.315073 \tValidation Loss: 2.383905\n",
      "Validation loss decreased (2.383958 --> 2.383905).  Saving model ...\n",
      "Epoch: 4040 \tTraining Loss: 2.300621 \tValidation Loss: 2.383903\n",
      "Validation loss decreased (2.383905 --> 2.383903).  Saving model ...\n",
      "Epoch: 4041 \tTraining Loss: 2.294109 \tValidation Loss: 2.383996\n",
      "Epoch: 4042 \tTraining Loss: 2.290362 \tValidation Loss: 2.383956\n",
      "Epoch: 4043 \tTraining Loss: 2.308707 \tValidation Loss: 2.383911\n",
      "Epoch: 4044 \tTraining Loss: 2.285630 \tValidation Loss: 2.383751\n",
      "Validation loss decreased (2.383903 --> 2.383751).  Saving model ...\n",
      "Epoch: 4045 \tTraining Loss: 2.318115 \tValidation Loss: 2.383772\n",
      "Epoch: 4046 \tTraining Loss: 2.295956 \tValidation Loss: 2.383684\n",
      "Validation loss decreased (2.383751 --> 2.383684).  Saving model ...\n",
      "Epoch: 4047 \tTraining Loss: 2.311530 \tValidation Loss: 2.383717\n",
      "Epoch: 4048 \tTraining Loss: 2.301428 \tValidation Loss: 2.383616\n",
      "Validation loss decreased (2.383684 --> 2.383616).  Saving model ...\n",
      "Epoch: 4049 \tTraining Loss: 2.294784 \tValidation Loss: 2.383521\n",
      "Validation loss decreased (2.383616 --> 2.383521).  Saving model ...\n",
      "Epoch: 4050 \tTraining Loss: 2.301014 \tValidation Loss: 2.383647\n",
      "Epoch: 4051 \tTraining Loss: 2.297891 \tValidation Loss: 2.383527\n",
      "Epoch: 4052 \tTraining Loss: 2.295304 \tValidation Loss: 2.383615\n",
      "Epoch: 4053 \tTraining Loss: 2.307485 \tValidation Loss: 2.383615\n",
      "Epoch: 4054 \tTraining Loss: 2.297644 \tValidation Loss: 2.383563\n",
      "Epoch: 4055 \tTraining Loss: 2.291844 \tValidation Loss: 2.383523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4056 \tTraining Loss: 2.302573 \tValidation Loss: 2.383535\n",
      "Epoch: 4057 \tTraining Loss: 2.304809 \tValidation Loss: 2.383440\n",
      "Validation loss decreased (2.383521 --> 2.383440).  Saving model ...\n",
      "Epoch: 4058 \tTraining Loss: 2.295334 \tValidation Loss: 2.383332\n",
      "Validation loss decreased (2.383440 --> 2.383332).  Saving model ...\n",
      "Epoch: 4059 \tTraining Loss: 2.313081 \tValidation Loss: 2.383274\n",
      "Validation loss decreased (2.383332 --> 2.383274).  Saving model ...\n",
      "Epoch: 4060 \tTraining Loss: 2.293526 \tValidation Loss: 2.383337\n",
      "Epoch: 4061 \tTraining Loss: 2.292318 \tValidation Loss: 2.383324\n",
      "Epoch: 4062 \tTraining Loss: 2.294392 \tValidation Loss: 2.383244\n",
      "Validation loss decreased (2.383274 --> 2.383244).  Saving model ...\n",
      "Epoch: 4063 \tTraining Loss: 2.301726 \tValidation Loss: 2.383303\n",
      "Epoch: 4064 \tTraining Loss: 2.303424 \tValidation Loss: 2.383255\n",
      "Epoch: 4065 \tTraining Loss: 2.300756 \tValidation Loss: 2.383229\n",
      "Validation loss decreased (2.383244 --> 2.383229).  Saving model ...\n",
      "Epoch: 4066 \tTraining Loss: 2.293913 \tValidation Loss: 2.383066\n",
      "Validation loss decreased (2.383229 --> 2.383066).  Saving model ...\n",
      "Epoch: 4067 \tTraining Loss: 2.292206 \tValidation Loss: 2.383034\n",
      "Validation loss decreased (2.383066 --> 2.383034).  Saving model ...\n",
      "Epoch: 4068 \tTraining Loss: 2.310256 \tValidation Loss: 2.383001\n",
      "Validation loss decreased (2.383034 --> 2.383001).  Saving model ...\n",
      "Epoch: 4069 \tTraining Loss: 2.311406 \tValidation Loss: 2.383115\n",
      "Epoch: 4070 \tTraining Loss: 2.312606 \tValidation Loss: 2.383206\n",
      "Epoch: 4071 \tTraining Loss: 2.300783 \tValidation Loss: 2.383278\n",
      "Epoch: 4072 \tTraining Loss: 2.310968 \tValidation Loss: 2.383286\n",
      "Epoch: 4073 \tTraining Loss: 2.297587 \tValidation Loss: 2.383296\n",
      "Epoch: 4074 \tTraining Loss: 2.306121 \tValidation Loss: 2.383229\n",
      "Epoch: 4075 \tTraining Loss: 2.297914 \tValidation Loss: 2.383134\n",
      "Epoch: 4076 \tTraining Loss: 2.292227 \tValidation Loss: 2.383130\n",
      "Epoch: 4077 \tTraining Loss: 2.288262 \tValidation Loss: 2.383200\n",
      "Epoch: 4078 \tTraining Loss: 2.292711 \tValidation Loss: 2.383168\n",
      "Epoch: 4079 \tTraining Loss: 2.299835 \tValidation Loss: 2.383164\n",
      "Epoch: 4080 \tTraining Loss: 2.291256 \tValidation Loss: 2.383247\n",
      "Epoch: 4081 \tTraining Loss: 2.296385 \tValidation Loss: 2.383243\n",
      "Epoch: 4082 \tTraining Loss: 2.297662 \tValidation Loss: 2.383145\n",
      "Epoch: 4083 \tTraining Loss: 2.287173 \tValidation Loss: 2.383168\n",
      "Epoch: 4084 \tTraining Loss: 2.287110 \tValidation Loss: 2.383270\n",
      "Epoch: 4085 \tTraining Loss: 2.286825 \tValidation Loss: 2.383366\n",
      "Epoch: 4086 \tTraining Loss: 2.293449 \tValidation Loss: 2.383382\n",
      "Epoch: 4087 \tTraining Loss: 2.303899 \tValidation Loss: 2.383429\n",
      "Epoch: 4088 \tTraining Loss: 2.284290 \tValidation Loss: 2.383260\n",
      "Epoch: 4089 \tTraining Loss: 2.297311 \tValidation Loss: 2.383138\n",
      "Epoch: 4090 \tTraining Loss: 2.318369 \tValidation Loss: 2.383221\n",
      "Epoch: 4091 \tTraining Loss: 2.286284 \tValidation Loss: 2.383046\n",
      "Epoch: 4092 \tTraining Loss: 2.302560 \tValidation Loss: 2.383041\n",
      "Epoch: 4093 \tTraining Loss: 2.288766 \tValidation Loss: 2.382963\n",
      "Validation loss decreased (2.383001 --> 2.382963).  Saving model ...\n",
      "Epoch: 4094 \tTraining Loss: 2.312494 \tValidation Loss: 2.383090\n",
      "Epoch: 4095 \tTraining Loss: 2.293484 \tValidation Loss: 2.383021\n",
      "Epoch: 4096 \tTraining Loss: 2.315490 \tValidation Loss: 2.383020\n",
      "Epoch: 4097 \tTraining Loss: 2.303521 \tValidation Loss: 2.382948\n",
      "Validation loss decreased (2.382963 --> 2.382948).  Saving model ...\n",
      "Epoch: 4098 \tTraining Loss: 2.309526 \tValidation Loss: 2.382928\n",
      "Validation loss decreased (2.382948 --> 2.382928).  Saving model ...\n",
      "Epoch: 4099 \tTraining Loss: 2.296759 \tValidation Loss: 2.382963\n",
      "Epoch: 4100 \tTraining Loss: 2.295234 \tValidation Loss: 2.382919\n",
      "Validation loss decreased (2.382928 --> 2.382919).  Saving model ...\n",
      "Epoch: 4101 \tTraining Loss: 2.297048 \tValidation Loss: 2.382797\n",
      "Validation loss decreased (2.382919 --> 2.382797).  Saving model ...\n",
      "Epoch: 4102 \tTraining Loss: 2.298936 \tValidation Loss: 2.382850\n",
      "Epoch: 4103 \tTraining Loss: 2.301255 \tValidation Loss: 2.382854\n",
      "Epoch: 4104 \tTraining Loss: 2.301471 \tValidation Loss: 2.382842\n",
      "Epoch: 4105 \tTraining Loss: 2.293814 \tValidation Loss: 2.382745\n",
      "Validation loss decreased (2.382797 --> 2.382745).  Saving model ...\n",
      "Epoch: 4106 \tTraining Loss: 2.300875 \tValidation Loss: 2.382762\n",
      "Epoch: 4107 \tTraining Loss: 2.304253 \tValidation Loss: 2.382758\n",
      "Epoch: 4108 \tTraining Loss: 2.282715 \tValidation Loss: 2.382759\n",
      "Epoch: 4109 \tTraining Loss: 2.312106 \tValidation Loss: 2.382844\n",
      "Epoch: 4110 \tTraining Loss: 2.286930 \tValidation Loss: 2.382752\n",
      "Epoch: 4111 \tTraining Loss: 2.288862 \tValidation Loss: 2.382597\n",
      "Validation loss decreased (2.382745 --> 2.382597).  Saving model ...\n",
      "Epoch: 4112 \tTraining Loss: 2.282016 \tValidation Loss: 2.382621\n",
      "Epoch: 4113 \tTraining Loss: 2.289168 \tValidation Loss: 2.382680\n",
      "Epoch: 4114 \tTraining Loss: 2.292629 \tValidation Loss: 2.382676\n",
      "Epoch: 4115 \tTraining Loss: 2.288804 \tValidation Loss: 2.382554\n",
      "Validation loss decreased (2.382597 --> 2.382554).  Saving model ...\n",
      "Epoch: 4116 \tTraining Loss: 2.285717 \tValidation Loss: 2.382568\n",
      "Epoch: 4117 \tTraining Loss: 2.293365 \tValidation Loss: 2.382381\n",
      "Validation loss decreased (2.382554 --> 2.382381).  Saving model ...\n",
      "Epoch: 4118 \tTraining Loss: 2.279883 \tValidation Loss: 2.382244\n",
      "Validation loss decreased (2.382381 --> 2.382244).  Saving model ...\n",
      "Epoch: 4119 \tTraining Loss: 2.280234 \tValidation Loss: 2.382318\n",
      "Epoch: 4120 \tTraining Loss: 2.295142 \tValidation Loss: 2.382267\n",
      "Epoch: 4121 \tTraining Loss: 2.282019 \tValidation Loss: 2.382205\n",
      "Validation loss decreased (2.382244 --> 2.382205).  Saving model ...\n",
      "Epoch: 4122 \tTraining Loss: 2.294116 \tValidation Loss: 2.382100\n",
      "Validation loss decreased (2.382205 --> 2.382100).  Saving model ...\n",
      "Epoch: 4123 \tTraining Loss: 2.312825 \tValidation Loss: 2.382087\n",
      "Validation loss decreased (2.382100 --> 2.382087).  Saving model ...\n",
      "Epoch: 4124 \tTraining Loss: 2.290129 \tValidation Loss: 2.381874\n",
      "Validation loss decreased (2.382087 --> 2.381874).  Saving model ...\n",
      "Epoch: 4125 \tTraining Loss: 2.295876 \tValidation Loss: 2.381880\n",
      "Epoch: 4126 \tTraining Loss: 2.293579 \tValidation Loss: 2.381957\n",
      "Epoch: 4127 \tTraining Loss: 2.297128 \tValidation Loss: 2.381865\n",
      "Validation loss decreased (2.381874 --> 2.381865).  Saving model ...\n",
      "Epoch: 4128 \tTraining Loss: 2.307273 \tValidation Loss: 2.382022\n",
      "Epoch: 4129 \tTraining Loss: 2.298049 \tValidation Loss: 2.381947\n",
      "Epoch: 4130 \tTraining Loss: 2.297458 \tValidation Loss: 2.381966\n",
      "Epoch: 4131 \tTraining Loss: 2.309942 \tValidation Loss: 2.381981\n",
      "Epoch: 4132 \tTraining Loss: 2.283103 \tValidation Loss: 2.381973\n",
      "Epoch: 4133 \tTraining Loss: 2.305206 \tValidation Loss: 2.382045\n",
      "Epoch: 4134 \tTraining Loss: 2.302928 \tValidation Loss: 2.381945\n",
      "Epoch: 4135 \tTraining Loss: 2.308964 \tValidation Loss: 2.381864\n",
      "Validation loss decreased (2.381865 --> 2.381864).  Saving model ...\n",
      "Epoch: 4136 \tTraining Loss: 2.283056 \tValidation Loss: 2.381944\n",
      "Epoch: 4137 \tTraining Loss: 2.301399 \tValidation Loss: 2.381903\n",
      "Epoch: 4138 \tTraining Loss: 2.301007 \tValidation Loss: 2.381765\n",
      "Validation loss decreased (2.381864 --> 2.381765).  Saving model ...\n",
      "Epoch: 4139 \tTraining Loss: 2.303854 \tValidation Loss: 2.381715\n",
      "Validation loss decreased (2.381765 --> 2.381715).  Saving model ...\n",
      "Epoch: 4140 \tTraining Loss: 2.285398 \tValidation Loss: 2.381740\n",
      "Epoch: 4141 \tTraining Loss: 2.298257 \tValidation Loss: 2.381706\n",
      "Validation loss decreased (2.381715 --> 2.381706).  Saving model ...\n",
      "Epoch: 4142 \tTraining Loss: 2.302004 \tValidation Loss: 2.381659\n",
      "Validation loss decreased (2.381706 --> 2.381659).  Saving model ...\n",
      "Epoch: 4143 \tTraining Loss: 2.285028 \tValidation Loss: 2.381629\n",
      "Validation loss decreased (2.381659 --> 2.381629).  Saving model ...\n",
      "Epoch: 4144 \tTraining Loss: 2.288857 \tValidation Loss: 2.381582\n",
      "Validation loss decreased (2.381629 --> 2.381582).  Saving model ...\n",
      "Epoch: 4145 \tTraining Loss: 2.314136 \tValidation Loss: 2.381666\n",
      "Epoch: 4146 \tTraining Loss: 2.296429 \tValidation Loss: 2.381766\n",
      "Epoch: 4147 \tTraining Loss: 2.277544 \tValidation Loss: 2.381679\n",
      "Epoch: 4148 \tTraining Loss: 2.287073 \tValidation Loss: 2.381625\n",
      "Epoch: 4149 \tTraining Loss: 2.294483 \tValidation Loss: 2.381613\n",
      "Epoch: 4150 \tTraining Loss: 2.293214 \tValidation Loss: 2.381653\n",
      "Epoch: 4151 \tTraining Loss: 2.297974 \tValidation Loss: 2.381607\n",
      "Epoch: 4152 \tTraining Loss: 2.290113 \tValidation Loss: 2.381561\n",
      "Validation loss decreased (2.381582 --> 2.381561).  Saving model ...\n",
      "Epoch: 4153 \tTraining Loss: 2.286307 \tValidation Loss: 2.381454\n",
      "Validation loss decreased (2.381561 --> 2.381454).  Saving model ...\n",
      "Epoch: 4154 \tTraining Loss: 2.303768 \tValidation Loss: 2.381557\n",
      "Epoch: 4155 \tTraining Loss: 2.280268 \tValidation Loss: 2.381575\n",
      "Epoch: 4156 \tTraining Loss: 2.308788 \tValidation Loss: 2.381447\n",
      "Validation loss decreased (2.381454 --> 2.381447).  Saving model ...\n",
      "Epoch: 4157 \tTraining Loss: 2.291358 \tValidation Loss: 2.381314\n",
      "Validation loss decreased (2.381447 --> 2.381314).  Saving model ...\n",
      "Epoch: 4158 \tTraining Loss: 2.294484 \tValidation Loss: 2.381440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4159 \tTraining Loss: 2.304030 \tValidation Loss: 2.381366\n",
      "Epoch: 4160 \tTraining Loss: 2.308745 \tValidation Loss: 2.381234\n",
      "Validation loss decreased (2.381314 --> 2.381234).  Saving model ...\n",
      "Epoch: 4161 \tTraining Loss: 2.285080 \tValidation Loss: 2.381104\n",
      "Validation loss decreased (2.381234 --> 2.381104).  Saving model ...\n",
      "Epoch: 4162 \tTraining Loss: 2.299909 \tValidation Loss: 2.381231\n",
      "Epoch: 4163 \tTraining Loss: 2.287289 \tValidation Loss: 2.381241\n",
      "Epoch: 4164 \tTraining Loss: 2.302688 \tValidation Loss: 2.381253\n",
      "Epoch: 4165 \tTraining Loss: 2.294611 \tValidation Loss: 2.381305\n",
      "Epoch: 4166 \tTraining Loss: 2.294696 \tValidation Loss: 2.381194\n",
      "Epoch: 4167 \tTraining Loss: 2.290279 \tValidation Loss: 2.381266\n",
      "Epoch: 4168 \tTraining Loss: 2.274276 \tValidation Loss: 2.381295\n",
      "Epoch: 4169 \tTraining Loss: 2.281455 \tValidation Loss: 2.381217\n",
      "Epoch: 4170 \tTraining Loss: 2.311471 \tValidation Loss: 2.381278\n",
      "Epoch: 4171 \tTraining Loss: 2.287105 \tValidation Loss: 2.381435\n",
      "Epoch: 4172 \tTraining Loss: 2.295189 \tValidation Loss: 2.381481\n",
      "Epoch: 4173 \tTraining Loss: 2.293015 \tValidation Loss: 2.381537\n",
      "Epoch: 4174 \tTraining Loss: 2.294951 \tValidation Loss: 2.381404\n",
      "Epoch: 4175 \tTraining Loss: 2.295672 \tValidation Loss: 2.381454\n",
      "Epoch: 4176 \tTraining Loss: 2.275177 \tValidation Loss: 2.381411\n",
      "Epoch: 4177 \tTraining Loss: 2.297336 \tValidation Loss: 2.381435\n",
      "Epoch: 4178 \tTraining Loss: 2.286379 \tValidation Loss: 2.381366\n",
      "Epoch: 4179 \tTraining Loss: 2.289613 \tValidation Loss: 2.381461\n",
      "Epoch: 4180 \tTraining Loss: 2.292794 \tValidation Loss: 2.381439\n",
      "Epoch: 4181 \tTraining Loss: 2.291445 \tValidation Loss: 2.381341\n",
      "Epoch: 4182 \tTraining Loss: 2.293405 \tValidation Loss: 2.381506\n",
      "Epoch: 4183 \tTraining Loss: 2.288074 \tValidation Loss: 2.381531\n",
      "Epoch: 4184 \tTraining Loss: 2.299408 \tValidation Loss: 2.381529\n",
      "Epoch: 4185 \tTraining Loss: 2.298902 \tValidation Loss: 2.381407\n",
      "Epoch: 4186 \tTraining Loss: 2.295307 \tValidation Loss: 2.381333\n",
      "Epoch: 4187 \tTraining Loss: 2.297961 \tValidation Loss: 2.381443\n",
      "Epoch: 4188 \tTraining Loss: 2.299917 \tValidation Loss: 2.381301\n",
      "Epoch: 4189 \tTraining Loss: 2.285639 \tValidation Loss: 2.381117\n",
      "Epoch: 4190 \tTraining Loss: 2.294025 \tValidation Loss: 2.381102\n",
      "Validation loss decreased (2.381104 --> 2.381102).  Saving model ...\n",
      "Epoch: 4191 \tTraining Loss: 2.279233 \tValidation Loss: 2.381050\n",
      "Validation loss decreased (2.381102 --> 2.381050).  Saving model ...\n",
      "Epoch: 4192 \tTraining Loss: 2.296190 \tValidation Loss: 2.381065\n",
      "Epoch: 4193 \tTraining Loss: 2.283471 \tValidation Loss: 2.381090\n",
      "Epoch: 4194 \tTraining Loss: 2.294079 \tValidation Loss: 2.381139\n",
      "Epoch: 4195 \tTraining Loss: 2.291713 \tValidation Loss: 2.381070\n",
      "Epoch: 4196 \tTraining Loss: 2.286422 \tValidation Loss: 2.381173\n",
      "Epoch: 4197 \tTraining Loss: 2.301533 \tValidation Loss: 2.381189\n",
      "Epoch: 4198 \tTraining Loss: 2.310352 \tValidation Loss: 2.381202\n",
      "Epoch: 4199 \tTraining Loss: 2.296209 \tValidation Loss: 2.381289\n",
      "Epoch: 4200 \tTraining Loss: 2.282425 \tValidation Loss: 2.381156\n",
      "Epoch: 4201 \tTraining Loss: 2.303108 \tValidation Loss: 2.380950\n",
      "Validation loss decreased (2.381050 --> 2.380950).  Saving model ...\n",
      "Epoch: 4202 \tTraining Loss: 2.302978 \tValidation Loss: 2.381128\n",
      "Epoch: 4203 \tTraining Loss: 2.293375 \tValidation Loss: 2.381052\n",
      "Epoch: 4204 \tTraining Loss: 2.296250 \tValidation Loss: 2.381042\n",
      "Epoch: 4205 \tTraining Loss: 2.289595 \tValidation Loss: 2.381000\n",
      "Epoch: 4206 \tTraining Loss: 2.298006 \tValidation Loss: 2.380999\n",
      "Epoch: 4207 \tTraining Loss: 2.300984 \tValidation Loss: 2.381010\n",
      "Epoch: 4208 \tTraining Loss: 2.288920 \tValidation Loss: 2.380815\n",
      "Validation loss decreased (2.380950 --> 2.380815).  Saving model ...\n",
      "Epoch: 4209 \tTraining Loss: 2.291736 \tValidation Loss: 2.380824\n",
      "Epoch: 4210 \tTraining Loss: 2.281357 \tValidation Loss: 2.380682\n",
      "Validation loss decreased (2.380815 --> 2.380682).  Saving model ...\n",
      "Epoch: 4211 \tTraining Loss: 2.278921 \tValidation Loss: 2.380609\n",
      "Validation loss decreased (2.380682 --> 2.380609).  Saving model ...\n",
      "Epoch: 4212 \tTraining Loss: 2.279343 \tValidation Loss: 2.380661\n",
      "Epoch: 4213 \tTraining Loss: 2.280525 \tValidation Loss: 2.380635\n",
      "Epoch: 4214 \tTraining Loss: 2.295425 \tValidation Loss: 2.380654\n",
      "Epoch: 4215 \tTraining Loss: 2.291578 \tValidation Loss: 2.380549\n",
      "Validation loss decreased (2.380609 --> 2.380549).  Saving model ...\n",
      "Epoch: 4216 \tTraining Loss: 2.288524 \tValidation Loss: 2.380650\n",
      "Epoch: 4217 \tTraining Loss: 2.276896 \tValidation Loss: 2.380660\n",
      "Epoch: 4218 \tTraining Loss: 2.288294 \tValidation Loss: 2.380499\n",
      "Validation loss decreased (2.380549 --> 2.380499).  Saving model ...\n",
      "Epoch: 4219 \tTraining Loss: 2.279623 \tValidation Loss: 2.380447\n",
      "Validation loss decreased (2.380499 --> 2.380447).  Saving model ...\n",
      "Epoch: 4220 \tTraining Loss: 2.280695 \tValidation Loss: 2.380343\n",
      "Validation loss decreased (2.380447 --> 2.380343).  Saving model ...\n",
      "Epoch: 4221 \tTraining Loss: 2.288654 \tValidation Loss: 2.380334\n",
      "Validation loss decreased (2.380343 --> 2.380334).  Saving model ...\n",
      "Epoch: 4222 \tTraining Loss: 2.291402 \tValidation Loss: 2.380485\n",
      "Epoch: 4223 \tTraining Loss: 2.287209 \tValidation Loss: 2.380533\n",
      "Epoch: 4224 \tTraining Loss: 2.294186 \tValidation Loss: 2.380450\n",
      "Epoch: 4225 \tTraining Loss: 2.289886 \tValidation Loss: 2.380381\n",
      "Epoch: 4226 \tTraining Loss: 2.274704 \tValidation Loss: 2.380320\n",
      "Validation loss decreased (2.380334 --> 2.380320).  Saving model ...\n",
      "Epoch: 4227 \tTraining Loss: 2.289572 \tValidation Loss: 2.380344\n",
      "Epoch: 4228 \tTraining Loss: 2.276873 \tValidation Loss: 2.380128\n",
      "Validation loss decreased (2.380320 --> 2.380128).  Saving model ...\n",
      "Epoch: 4229 \tTraining Loss: 2.283766 \tValidation Loss: 2.380169\n",
      "Epoch: 4230 \tTraining Loss: 2.291968 \tValidation Loss: 2.380121\n",
      "Validation loss decreased (2.380128 --> 2.380121).  Saving model ...\n",
      "Epoch: 4231 \tTraining Loss: 2.303488 \tValidation Loss: 2.379971\n",
      "Validation loss decreased (2.380121 --> 2.379971).  Saving model ...\n",
      "Epoch: 4232 \tTraining Loss: 2.287894 \tValidation Loss: 2.380022\n",
      "Epoch: 4233 \tTraining Loss: 2.290736 \tValidation Loss: 2.379867\n",
      "Validation loss decreased (2.379971 --> 2.379867).  Saving model ...\n",
      "Epoch: 4234 \tTraining Loss: 2.294082 \tValidation Loss: 2.379738\n",
      "Validation loss decreased (2.379867 --> 2.379738).  Saving model ...\n",
      "Epoch: 4235 \tTraining Loss: 2.300931 \tValidation Loss: 2.379901\n",
      "Epoch: 4236 \tTraining Loss: 2.302045 \tValidation Loss: 2.379878\n",
      "Epoch: 4237 \tTraining Loss: 2.290295 \tValidation Loss: 2.379892\n",
      "Epoch: 4238 \tTraining Loss: 2.287526 \tValidation Loss: 2.379982\n",
      "Epoch: 4239 \tTraining Loss: 2.290412 \tValidation Loss: 2.379875\n",
      "Epoch: 4240 \tTraining Loss: 2.278522 \tValidation Loss: 2.379914\n",
      "Epoch: 4241 \tTraining Loss: 2.300529 \tValidation Loss: 2.379900\n",
      "Epoch: 4242 \tTraining Loss: 2.301842 \tValidation Loss: 2.379953\n",
      "Epoch: 4243 \tTraining Loss: 2.289591 \tValidation Loss: 2.380003\n",
      "Epoch: 4244 \tTraining Loss: 2.313922 \tValidation Loss: 2.380021\n",
      "Epoch: 4245 \tTraining Loss: 2.277365 \tValidation Loss: 2.379980\n",
      "Epoch: 4246 \tTraining Loss: 2.304668 \tValidation Loss: 2.380006\n",
      "Epoch: 4247 \tTraining Loss: 2.278228 \tValidation Loss: 2.379996\n",
      "Epoch: 4248 \tTraining Loss: 2.279869 \tValidation Loss: 2.379973\n",
      "Epoch: 4249 \tTraining Loss: 2.286490 \tValidation Loss: 2.379911\n",
      "Epoch: 4250 \tTraining Loss: 2.285782 \tValidation Loss: 2.379886\n",
      "Epoch: 4251 \tTraining Loss: 2.294249 \tValidation Loss: 2.379830\n",
      "Epoch: 4252 \tTraining Loss: 2.272088 \tValidation Loss: 2.379792\n",
      "Epoch: 4253 \tTraining Loss: 2.310680 \tValidation Loss: 2.379803\n",
      "Epoch: 4254 \tTraining Loss: 2.286456 \tValidation Loss: 2.379803\n",
      "Epoch: 4255 \tTraining Loss: 2.300951 \tValidation Loss: 2.379834\n",
      "Epoch: 4256 \tTraining Loss: 2.296818 \tValidation Loss: 2.379672\n",
      "Validation loss decreased (2.379738 --> 2.379672).  Saving model ...\n",
      "Epoch: 4257 \tTraining Loss: 2.275224 \tValidation Loss: 2.379601\n",
      "Validation loss decreased (2.379672 --> 2.379601).  Saving model ...\n",
      "Epoch: 4258 \tTraining Loss: 2.293411 \tValidation Loss: 2.379633\n",
      "Epoch: 4259 \tTraining Loss: 2.297192 \tValidation Loss: 2.379619\n",
      "Epoch: 4260 \tTraining Loss: 2.292898 \tValidation Loss: 2.379668\n",
      "Epoch: 4261 \tTraining Loss: 2.287436 \tValidation Loss: 2.379667\n",
      "Epoch: 4262 \tTraining Loss: 2.282161 \tValidation Loss: 2.379657\n",
      "Epoch: 4263 \tTraining Loss: 2.290765 \tValidation Loss: 2.379597\n",
      "Validation loss decreased (2.379601 --> 2.379597).  Saving model ...\n",
      "Epoch: 4264 \tTraining Loss: 2.292792 \tValidation Loss: 2.379563\n",
      "Validation loss decreased (2.379597 --> 2.379563).  Saving model ...\n",
      "Epoch: 4265 \tTraining Loss: 2.299364 \tValidation Loss: 2.379556\n",
      "Validation loss decreased (2.379563 --> 2.379556).  Saving model ...\n",
      "Epoch: 4266 \tTraining Loss: 2.295166 \tValidation Loss: 2.379461\n",
      "Validation loss decreased (2.379556 --> 2.379461).  Saving model ...\n",
      "Epoch: 4267 \tTraining Loss: 2.274680 \tValidation Loss: 2.379323\n",
      "Validation loss decreased (2.379461 --> 2.379323).  Saving model ...\n",
      "Epoch: 4268 \tTraining Loss: 2.291277 \tValidation Loss: 2.379311\n",
      "Validation loss decreased (2.379323 --> 2.379311).  Saving model ...\n",
      "Epoch: 4269 \tTraining Loss: 2.288057 \tValidation Loss: 2.379328\n",
      "Epoch: 4270 \tTraining Loss: 2.293494 \tValidation Loss: 2.379170\n",
      "Validation loss decreased (2.379311 --> 2.379170).  Saving model ...\n",
      "Epoch: 4271 \tTraining Loss: 2.277786 \tValidation Loss: 2.379026\n",
      "Validation loss decreased (2.379170 --> 2.379026).  Saving model ...\n",
      "Epoch: 4272 \tTraining Loss: 2.278598 \tValidation Loss: 2.379013\n",
      "Validation loss decreased (2.379026 --> 2.379013).  Saving model ...\n",
      "Epoch: 4273 \tTraining Loss: 2.293992 \tValidation Loss: 2.379041\n",
      "Epoch: 4274 \tTraining Loss: 2.299001 \tValidation Loss: 2.379149\n",
      "Epoch: 4275 \tTraining Loss: 2.301242 \tValidation Loss: 2.379261\n",
      "Epoch: 4276 \tTraining Loss: 2.281400 \tValidation Loss: 2.379124\n",
      "Epoch: 4277 \tTraining Loss: 2.288459 \tValidation Loss: 2.379076\n",
      "Epoch: 4278 \tTraining Loss: 2.293962 \tValidation Loss: 2.379114\n",
      "Epoch: 4279 \tTraining Loss: 2.296757 \tValidation Loss: 2.379044\n",
      "Epoch: 4280 \tTraining Loss: 2.293324 \tValidation Loss: 2.378977\n",
      "Validation loss decreased (2.379013 --> 2.378977).  Saving model ...\n",
      "Epoch: 4281 \tTraining Loss: 2.291845 \tValidation Loss: 2.379169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4282 \tTraining Loss: 2.284141 \tValidation Loss: 2.379156\n",
      "Epoch: 4283 \tTraining Loss: 2.299394 \tValidation Loss: 2.379154\n",
      "Epoch: 4284 \tTraining Loss: 2.287003 \tValidation Loss: 2.379047\n",
      "Epoch: 4285 \tTraining Loss: 2.275432 \tValidation Loss: 2.378964\n",
      "Validation loss decreased (2.378977 --> 2.378964).  Saving model ...\n",
      "Epoch: 4286 \tTraining Loss: 2.300251 \tValidation Loss: 2.379120\n",
      "Epoch: 4287 \tTraining Loss: 2.300971 \tValidation Loss: 2.379159\n",
      "Epoch: 4288 \tTraining Loss: 2.302102 \tValidation Loss: 2.379277\n",
      "Epoch: 4289 \tTraining Loss: 2.295587 \tValidation Loss: 2.379271\n",
      "Epoch: 4290 \tTraining Loss: 2.291388 \tValidation Loss: 2.379134\n",
      "Epoch: 4291 \tTraining Loss: 2.281578 \tValidation Loss: 2.379008\n",
      "Epoch: 4292 \tTraining Loss: 2.278880 \tValidation Loss: 2.378969\n",
      "Epoch: 4293 \tTraining Loss: 2.284555 \tValidation Loss: 2.378918\n",
      "Validation loss decreased (2.378964 --> 2.378918).  Saving model ...\n",
      "Epoch: 4294 \tTraining Loss: 2.283100 \tValidation Loss: 2.378788\n",
      "Validation loss decreased (2.378918 --> 2.378788).  Saving model ...\n",
      "Epoch: 4295 \tTraining Loss: 2.289357 \tValidation Loss: 2.378753\n",
      "Validation loss decreased (2.378788 --> 2.378753).  Saving model ...\n",
      "Epoch: 4296 \tTraining Loss: 2.290464 \tValidation Loss: 2.378793\n",
      "Epoch: 4297 \tTraining Loss: 2.287205 \tValidation Loss: 2.378701\n",
      "Validation loss decreased (2.378753 --> 2.378701).  Saving model ...\n",
      "Epoch: 4298 \tTraining Loss: 2.298229 \tValidation Loss: 2.378778\n",
      "Epoch: 4299 \tTraining Loss: 2.277039 \tValidation Loss: 2.378410\n",
      "Validation loss decreased (2.378701 --> 2.378410).  Saving model ...\n",
      "Epoch: 4300 \tTraining Loss: 2.291705 \tValidation Loss: 2.378448\n",
      "Epoch: 4301 \tTraining Loss: 2.278003 \tValidation Loss: 2.378311\n",
      "Validation loss decreased (2.378410 --> 2.378311).  Saving model ...\n",
      "Epoch: 4302 \tTraining Loss: 2.282664 \tValidation Loss: 2.378252\n",
      "Validation loss decreased (2.378311 --> 2.378252).  Saving model ...\n",
      "Epoch: 4303 \tTraining Loss: 2.301436 \tValidation Loss: 2.378407\n",
      "Epoch: 4304 \tTraining Loss: 2.302539 \tValidation Loss: 2.378391\n",
      "Epoch: 4305 \tTraining Loss: 2.288054 \tValidation Loss: 2.378323\n",
      "Epoch: 4306 \tTraining Loss: 2.275568 \tValidation Loss: 2.378194\n",
      "Validation loss decreased (2.378252 --> 2.378194).  Saving model ...\n",
      "Epoch: 4307 \tTraining Loss: 2.284080 \tValidation Loss: 2.378223\n",
      "Epoch: 4308 \tTraining Loss: 2.283694 \tValidation Loss: 2.378291\n",
      "Epoch: 4309 \tTraining Loss: 2.288635 \tValidation Loss: 2.378432\n",
      "Epoch: 4310 \tTraining Loss: 2.291854 \tValidation Loss: 2.378387\n",
      "Epoch: 4311 \tTraining Loss: 2.285968 \tValidation Loss: 2.378324\n",
      "Epoch: 4312 \tTraining Loss: 2.291425 \tValidation Loss: 2.378487\n",
      "Epoch: 4313 \tTraining Loss: 2.271601 \tValidation Loss: 2.378373\n",
      "Epoch: 4314 \tTraining Loss: 2.289662 \tValidation Loss: 2.378532\n",
      "Epoch: 4315 \tTraining Loss: 2.282979 \tValidation Loss: 2.378644\n",
      "Epoch: 4316 \tTraining Loss: 2.283637 \tValidation Loss: 2.378493\n",
      "Epoch: 4317 \tTraining Loss: 2.291918 \tValidation Loss: 2.378543\n",
      "Epoch: 4318 \tTraining Loss: 2.286424 \tValidation Loss: 2.378744\n",
      "Epoch: 4319 \tTraining Loss: 2.298204 \tValidation Loss: 2.378785\n",
      "Epoch: 4320 \tTraining Loss: 2.287922 \tValidation Loss: 2.378886\n",
      "Epoch: 4321 \tTraining Loss: 2.290952 \tValidation Loss: 2.378721\n",
      "Epoch: 4322 \tTraining Loss: 2.307886 \tValidation Loss: 2.378819\n",
      "Epoch: 4323 \tTraining Loss: 2.289811 \tValidation Loss: 2.378798\n",
      "Epoch: 4324 \tTraining Loss: 2.272892 \tValidation Loss: 2.378807\n",
      "Epoch: 4325 \tTraining Loss: 2.285726 \tValidation Loss: 2.378742\n",
      "Epoch: 4326 \tTraining Loss: 2.287552 \tValidation Loss: 2.378773\n",
      "Epoch: 4327 \tTraining Loss: 2.278443 \tValidation Loss: 2.378809\n",
      "Epoch: 4328 \tTraining Loss: 2.289774 \tValidation Loss: 2.378754\n",
      "Epoch: 4329 \tTraining Loss: 2.303327 \tValidation Loss: 2.378623\n",
      "Epoch: 4330 \tTraining Loss: 2.280831 \tValidation Loss: 2.378500\n",
      "Epoch: 4331 \tTraining Loss: 2.279029 \tValidation Loss: 2.378340\n",
      "Epoch: 4332 \tTraining Loss: 2.296185 \tValidation Loss: 2.378329\n",
      "Epoch: 4333 \tTraining Loss: 2.281451 \tValidation Loss: 2.378349\n",
      "Epoch: 4334 \tTraining Loss: 2.281229 \tValidation Loss: 2.378222\n",
      "Epoch: 4335 \tTraining Loss: 2.271674 \tValidation Loss: 2.378134\n",
      "Validation loss decreased (2.378194 --> 2.378134).  Saving model ...\n",
      "Epoch: 4336 \tTraining Loss: 2.274885 \tValidation Loss: 2.378122\n",
      "Validation loss decreased (2.378134 --> 2.378122).  Saving model ...\n",
      "Epoch: 4337 \tTraining Loss: 2.276144 \tValidation Loss: 2.378087\n",
      "Validation loss decreased (2.378122 --> 2.378087).  Saving model ...\n",
      "Epoch: 4338 \tTraining Loss: 2.286612 \tValidation Loss: 2.377954\n",
      "Validation loss decreased (2.378087 --> 2.377954).  Saving model ...\n",
      "Epoch: 4339 \tTraining Loss: 2.275780 \tValidation Loss: 2.377835\n",
      "Validation loss decreased (2.377954 --> 2.377835).  Saving model ...\n",
      "Epoch: 4340 \tTraining Loss: 2.280446 \tValidation Loss: 2.377622\n",
      "Validation loss decreased (2.377835 --> 2.377622).  Saving model ...\n",
      "Epoch: 4341 \tTraining Loss: 2.285402 \tValidation Loss: 2.377623\n",
      "Epoch: 4342 \tTraining Loss: 2.280675 \tValidation Loss: 2.377465\n",
      "Validation loss decreased (2.377622 --> 2.377465).  Saving model ...\n",
      "Epoch: 4343 \tTraining Loss: 2.267577 \tValidation Loss: 2.377358\n",
      "Validation loss decreased (2.377465 --> 2.377358).  Saving model ...\n",
      "Epoch: 4344 \tTraining Loss: 2.298598 \tValidation Loss: 2.377568\n",
      "Epoch: 4345 \tTraining Loss: 2.303442 \tValidation Loss: 2.377558\n",
      "Epoch: 4346 \tTraining Loss: 2.284836 \tValidation Loss: 2.377500\n",
      "Epoch: 4347 \tTraining Loss: 2.279104 \tValidation Loss: 2.377506\n",
      "Epoch: 4348 \tTraining Loss: 2.294381 \tValidation Loss: 2.377466\n",
      "Epoch: 4349 \tTraining Loss: 2.291125 \tValidation Loss: 2.377627\n",
      "Epoch: 4350 \tTraining Loss: 2.295070 \tValidation Loss: 2.377767\n",
      "Epoch: 4351 \tTraining Loss: 2.297967 \tValidation Loss: 2.377730\n",
      "Epoch: 4352 \tTraining Loss: 2.286964 \tValidation Loss: 2.377895\n",
      "Epoch: 4353 \tTraining Loss: 2.311101 \tValidation Loss: 2.377988\n",
      "Epoch: 4354 \tTraining Loss: 2.286760 \tValidation Loss: 2.378047\n",
      "Epoch: 4355 \tTraining Loss: 2.267550 \tValidation Loss: 2.377848\n",
      "Epoch: 4356 \tTraining Loss: 2.289956 \tValidation Loss: 2.377804\n",
      "Epoch: 4357 \tTraining Loss: 2.272596 \tValidation Loss: 2.377784\n",
      "Epoch: 4358 \tTraining Loss: 2.294213 \tValidation Loss: 2.377753\n",
      "Epoch: 4359 \tTraining Loss: 2.271842 \tValidation Loss: 2.377751\n",
      "Epoch: 4360 \tTraining Loss: 2.266526 \tValidation Loss: 2.377657\n",
      "Epoch: 4361 \tTraining Loss: 2.278541 \tValidation Loss: 2.377472\n",
      "Epoch: 4362 \tTraining Loss: 2.275195 \tValidation Loss: 2.377409\n",
      "Epoch: 4363 \tTraining Loss: 2.283415 \tValidation Loss: 2.377445\n",
      "Epoch: 4364 \tTraining Loss: 2.271469 \tValidation Loss: 2.377246\n",
      "Validation loss decreased (2.377358 --> 2.377246).  Saving model ...\n",
      "Epoch: 4365 \tTraining Loss: 2.270394 \tValidation Loss: 2.377290\n",
      "Epoch: 4366 \tTraining Loss: 2.273820 \tValidation Loss: 2.377112\n",
      "Validation loss decreased (2.377246 --> 2.377112).  Saving model ...\n",
      "Epoch: 4367 \tTraining Loss: 2.286510 \tValidation Loss: 2.377075\n",
      "Validation loss decreased (2.377112 --> 2.377075).  Saving model ...\n",
      "Epoch: 4368 \tTraining Loss: 2.278776 \tValidation Loss: 2.377248\n",
      "Epoch: 4369 \tTraining Loss: 2.281769 \tValidation Loss: 2.377158\n",
      "Epoch: 4370 \tTraining Loss: 2.277765 \tValidation Loss: 2.377196\n",
      "Epoch: 4371 \tTraining Loss: 2.271353 \tValidation Loss: 2.376986\n",
      "Validation loss decreased (2.377075 --> 2.376986).  Saving model ...\n",
      "Epoch: 4372 \tTraining Loss: 2.294863 \tValidation Loss: 2.376940\n",
      "Validation loss decreased (2.376986 --> 2.376940).  Saving model ...\n",
      "Epoch: 4373 \tTraining Loss: 2.278233 \tValidation Loss: 2.376823\n",
      "Validation loss decreased (2.376940 --> 2.376823).  Saving model ...\n",
      "Epoch: 4374 \tTraining Loss: 2.269567 \tValidation Loss: 2.376843\n",
      "Epoch: 4375 \tTraining Loss: 2.299046 \tValidation Loss: 2.376932\n",
      "Epoch: 4376 \tTraining Loss: 2.292545 \tValidation Loss: 2.376966\n",
      "Epoch: 4377 \tTraining Loss: 2.271091 \tValidation Loss: 2.377009\n",
      "Epoch: 4378 \tTraining Loss: 2.282482 \tValidation Loss: 2.376799\n",
      "Validation loss decreased (2.376823 --> 2.376799).  Saving model ...\n",
      "Epoch: 4379 \tTraining Loss: 2.267419 \tValidation Loss: 2.376845\n",
      "Epoch: 4380 \tTraining Loss: 2.279286 \tValidation Loss: 2.376834\n",
      "Epoch: 4381 \tTraining Loss: 2.289723 \tValidation Loss: 2.376951\n",
      "Epoch: 4382 \tTraining Loss: 2.286835 \tValidation Loss: 2.376959\n",
      "Epoch: 4383 \tTraining Loss: 2.280512 \tValidation Loss: 2.376982\n",
      "Epoch: 4384 \tTraining Loss: 2.269920 \tValidation Loss: 2.376881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4385 \tTraining Loss: 2.272064 \tValidation Loss: 2.376754\n",
      "Validation loss decreased (2.376799 --> 2.376754).  Saving model ...\n",
      "Epoch: 4386 \tTraining Loss: 2.271510 \tValidation Loss: 2.376800\n",
      "Epoch: 4387 \tTraining Loss: 2.287309 \tValidation Loss: 2.376669\n",
      "Validation loss decreased (2.376754 --> 2.376669).  Saving model ...\n",
      "Epoch: 4388 \tTraining Loss: 2.266618 \tValidation Loss: 2.376550\n",
      "Validation loss decreased (2.376669 --> 2.376550).  Saving model ...\n",
      "Epoch: 4389 \tTraining Loss: 2.267948 \tValidation Loss: 2.376404\n",
      "Validation loss decreased (2.376550 --> 2.376404).  Saving model ...\n",
      "Epoch: 4390 \tTraining Loss: 2.270448 \tValidation Loss: 2.376469\n",
      "Epoch: 4391 \tTraining Loss: 2.284648 \tValidation Loss: 2.376472\n",
      "Epoch: 4392 \tTraining Loss: 2.277313 \tValidation Loss: 2.376414\n",
      "Epoch: 4393 \tTraining Loss: 2.273657 \tValidation Loss: 2.376428\n",
      "Epoch: 4394 \tTraining Loss: 2.288084 \tValidation Loss: 2.376386\n",
      "Validation loss decreased (2.376404 --> 2.376386).  Saving model ...\n",
      "Epoch: 4395 \tTraining Loss: 2.281934 \tValidation Loss: 2.376381\n",
      "Validation loss decreased (2.376386 --> 2.376381).  Saving model ...\n",
      "Epoch: 4396 \tTraining Loss: 2.292915 \tValidation Loss: 2.376638\n",
      "Epoch: 4397 \tTraining Loss: 2.269632 \tValidation Loss: 2.376603\n",
      "Epoch: 4398 \tTraining Loss: 2.275698 \tValidation Loss: 2.376742\n",
      "Epoch: 4399 \tTraining Loss: 2.278672 \tValidation Loss: 2.376833\n",
      "Epoch: 4400 \tTraining Loss: 2.284431 \tValidation Loss: 2.376826\n",
      "Epoch: 4401 \tTraining Loss: 2.277613 \tValidation Loss: 2.376729\n",
      "Epoch: 4402 \tTraining Loss: 2.259491 \tValidation Loss: 2.376585\n",
      "Epoch: 4403 \tTraining Loss: 2.275076 \tValidation Loss: 2.376504\n",
      "Epoch: 4404 \tTraining Loss: 2.265741 \tValidation Loss: 2.376389\n",
      "Epoch: 4405 \tTraining Loss: 2.278363 \tValidation Loss: 2.376303\n",
      "Validation loss decreased (2.376381 --> 2.376303).  Saving model ...\n",
      "Epoch: 4406 \tTraining Loss: 2.284031 \tValidation Loss: 2.376174\n",
      "Validation loss decreased (2.376303 --> 2.376174).  Saving model ...\n",
      "Epoch: 4407 \tTraining Loss: 2.297153 \tValidation Loss: 2.376115\n",
      "Validation loss decreased (2.376174 --> 2.376115).  Saving model ...\n",
      "Epoch: 4408 \tTraining Loss: 2.266505 \tValidation Loss: 2.376217\n",
      "Epoch: 4409 \tTraining Loss: 2.302118 \tValidation Loss: 2.376231\n",
      "Epoch: 4410 \tTraining Loss: 2.287884 \tValidation Loss: 2.376236\n",
      "Epoch: 4411 \tTraining Loss: 2.287108 \tValidation Loss: 2.376133\n",
      "Epoch: 4412 \tTraining Loss: 2.272117 \tValidation Loss: 2.376084\n",
      "Validation loss decreased (2.376115 --> 2.376084).  Saving model ...\n",
      "Epoch: 4413 \tTraining Loss: 2.279382 \tValidation Loss: 2.376027\n",
      "Validation loss decreased (2.376084 --> 2.376027).  Saving model ...\n",
      "Epoch: 4414 \tTraining Loss: 2.276395 \tValidation Loss: 2.375866\n",
      "Validation loss decreased (2.376027 --> 2.375866).  Saving model ...\n",
      "Epoch: 4415 \tTraining Loss: 2.274009 \tValidation Loss: 2.375906\n",
      "Epoch: 4416 \tTraining Loss: 2.282893 \tValidation Loss: 2.375976\n",
      "Epoch: 4417 \tTraining Loss: 2.277510 \tValidation Loss: 2.375799\n",
      "Validation loss decreased (2.375866 --> 2.375799).  Saving model ...\n",
      "Epoch: 4418 \tTraining Loss: 2.277313 \tValidation Loss: 2.375884\n",
      "Epoch: 4419 \tTraining Loss: 2.269265 \tValidation Loss: 2.375735\n",
      "Validation loss decreased (2.375799 --> 2.375735).  Saving model ...\n",
      "Epoch: 4420 \tTraining Loss: 2.288999 \tValidation Loss: 2.375772\n",
      "Epoch: 4421 \tTraining Loss: 2.286227 \tValidation Loss: 2.375982\n",
      "Epoch: 4422 \tTraining Loss: 2.280850 \tValidation Loss: 2.375763\n",
      "Epoch: 4423 \tTraining Loss: 2.257715 \tValidation Loss: 2.375716\n",
      "Validation loss decreased (2.375735 --> 2.375716).  Saving model ...\n",
      "Epoch: 4424 \tTraining Loss: 2.287210 \tValidation Loss: 2.375765\n",
      "Epoch: 4425 \tTraining Loss: 2.278033 \tValidation Loss: 2.375782\n",
      "Epoch: 4426 \tTraining Loss: 2.275361 \tValidation Loss: 2.375800\n",
      "Epoch: 4427 \tTraining Loss: 2.275864 \tValidation Loss: 2.375830\n",
      "Epoch: 4428 \tTraining Loss: 2.280811 \tValidation Loss: 2.375909\n",
      "Epoch: 4429 \tTraining Loss: 2.265630 \tValidation Loss: 2.375802\n",
      "Epoch: 4430 \tTraining Loss: 2.293518 \tValidation Loss: 2.375887\n",
      "Epoch: 4431 \tTraining Loss: 2.267925 \tValidation Loss: 2.375818\n",
      "Epoch: 4432 \tTraining Loss: 2.275755 \tValidation Loss: 2.375639\n",
      "Validation loss decreased (2.375716 --> 2.375639).  Saving model ...\n",
      "Epoch: 4433 \tTraining Loss: 2.278537 \tValidation Loss: 2.375713\n",
      "Epoch: 4434 \tTraining Loss: 2.274814 \tValidation Loss: 2.375639\n",
      "Validation loss decreased (2.375639 --> 2.375639).  Saving model ...\n",
      "Epoch: 4435 \tTraining Loss: 2.265388 \tValidation Loss: 2.375527\n",
      "Validation loss decreased (2.375639 --> 2.375527).  Saving model ...\n",
      "Epoch: 4436 \tTraining Loss: 2.274783 \tValidation Loss: 2.375488\n",
      "Validation loss decreased (2.375527 --> 2.375488).  Saving model ...\n",
      "Epoch: 4437 \tTraining Loss: 2.259955 \tValidation Loss: 2.375537\n",
      "Epoch: 4438 \tTraining Loss: 2.280167 \tValidation Loss: 2.375670\n",
      "Epoch: 4439 \tTraining Loss: 2.264716 \tValidation Loss: 2.375575\n",
      "Epoch: 4440 \tTraining Loss: 2.278452 \tValidation Loss: 2.375647\n",
      "Epoch: 4441 \tTraining Loss: 2.275789 \tValidation Loss: 2.375621\n",
      "Epoch: 4442 \tTraining Loss: 2.287735 \tValidation Loss: 2.375613\n",
      "Epoch: 4443 \tTraining Loss: 2.266760 \tValidation Loss: 2.375533\n",
      "Epoch: 4444 \tTraining Loss: 2.279959 \tValidation Loss: 2.375458\n",
      "Validation loss decreased (2.375488 --> 2.375458).  Saving model ...\n",
      "Epoch: 4445 \tTraining Loss: 2.274218 \tValidation Loss: 2.375446\n",
      "Validation loss decreased (2.375458 --> 2.375446).  Saving model ...\n",
      "Epoch: 4446 \tTraining Loss: 2.303623 \tValidation Loss: 2.375485\n",
      "Epoch: 4447 \tTraining Loss: 2.285979 \tValidation Loss: 2.375530\n",
      "Epoch: 4448 \tTraining Loss: 2.275577 \tValidation Loss: 2.375348\n",
      "Validation loss decreased (2.375446 --> 2.375348).  Saving model ...\n",
      "Epoch: 4449 \tTraining Loss: 2.294093 \tValidation Loss: 2.375479\n",
      "Epoch: 4450 \tTraining Loss: 2.272309 \tValidation Loss: 2.375622\n",
      "Epoch: 4451 \tTraining Loss: 2.283969 \tValidation Loss: 2.375800\n",
      "Epoch: 4452 \tTraining Loss: 2.299366 \tValidation Loss: 2.375984\n",
      "Epoch: 4453 \tTraining Loss: 2.262420 \tValidation Loss: 2.375861\n",
      "Epoch: 4454 \tTraining Loss: 2.277848 \tValidation Loss: 2.375639\n",
      "Epoch: 4455 \tTraining Loss: 2.278725 \tValidation Loss: 2.375675\n",
      "Epoch: 4456 \tTraining Loss: 2.285439 \tValidation Loss: 2.375716\n",
      "Epoch: 4457 \tTraining Loss: 2.278352 \tValidation Loss: 2.375609\n",
      "Epoch: 4458 \tTraining Loss: 2.278714 \tValidation Loss: 2.375627\n",
      "Epoch: 4459 \tTraining Loss: 2.269492 \tValidation Loss: 2.375764\n",
      "Epoch: 4460 \tTraining Loss: 2.284536 \tValidation Loss: 2.375608\n",
      "Epoch: 4461 \tTraining Loss: 2.287370 \tValidation Loss: 2.375688\n",
      "Epoch: 4462 \tTraining Loss: 2.272475 \tValidation Loss: 2.375519\n",
      "Epoch: 4463 \tTraining Loss: 2.275796 \tValidation Loss: 2.375493\n",
      "Epoch: 4464 \tTraining Loss: 2.271730 \tValidation Loss: 2.375493\n",
      "Epoch: 4465 \tTraining Loss: 2.281539 \tValidation Loss: 2.375266\n",
      "Validation loss decreased (2.375348 --> 2.375266).  Saving model ...\n",
      "Epoch: 4466 \tTraining Loss: 2.288419 \tValidation Loss: 2.375452\n",
      "Epoch: 4467 \tTraining Loss: 2.279977 \tValidation Loss: 2.375252\n",
      "Validation loss decreased (2.375266 --> 2.375252).  Saving model ...\n",
      "Epoch: 4468 \tTraining Loss: 2.277877 \tValidation Loss: 2.375447\n",
      "Epoch: 4469 \tTraining Loss: 2.270170 \tValidation Loss: 2.375505\n",
      "Epoch: 4470 \tTraining Loss: 2.264443 \tValidation Loss: 2.375466\n",
      "Epoch: 4471 \tTraining Loss: 2.273930 \tValidation Loss: 2.375375\n",
      "Epoch: 4472 \tTraining Loss: 2.292089 \tValidation Loss: 2.375311\n",
      "Epoch: 4473 \tTraining Loss: 2.268175 \tValidation Loss: 2.375117\n",
      "Validation loss decreased (2.375252 --> 2.375117).  Saving model ...\n",
      "Epoch: 4474 \tTraining Loss: 2.266157 \tValidation Loss: 2.375155\n",
      "Epoch: 4475 \tTraining Loss: 2.281369 \tValidation Loss: 2.375059\n",
      "Validation loss decreased (2.375117 --> 2.375059).  Saving model ...\n",
      "Epoch: 4476 \tTraining Loss: 2.272519 \tValidation Loss: 2.374957\n",
      "Validation loss decreased (2.375059 --> 2.374957).  Saving model ...\n",
      "Epoch: 4477 \tTraining Loss: 2.300571 \tValidation Loss: 2.375028\n",
      "Epoch: 4478 \tTraining Loss: 2.275457 \tValidation Loss: 2.375198\n",
      "Epoch: 4479 \tTraining Loss: 2.272961 \tValidation Loss: 2.375166\n",
      "Epoch: 4480 \tTraining Loss: 2.265679 \tValidation Loss: 2.375021\n",
      "Epoch: 4481 \tTraining Loss: 2.274311 \tValidation Loss: 2.375081\n",
      "Epoch: 4482 \tTraining Loss: 2.281711 \tValidation Loss: 2.375001\n",
      "Epoch: 4483 \tTraining Loss: 2.261423 \tValidation Loss: 2.375128\n",
      "Epoch: 4484 \tTraining Loss: 2.280466 \tValidation Loss: 2.374978\n",
      "Epoch: 4485 \tTraining Loss: 2.284013 \tValidation Loss: 2.374981\n",
      "Epoch: 4486 \tTraining Loss: 2.271362 \tValidation Loss: 2.374990\n",
      "Epoch: 4487 \tTraining Loss: 2.267532 \tValidation Loss: 2.374832\n",
      "Validation loss decreased (2.374957 --> 2.374832).  Saving model ...\n",
      "Epoch: 4488 \tTraining Loss: 2.289775 \tValidation Loss: 2.374800\n",
      "Validation loss decreased (2.374832 --> 2.374800).  Saving model ...\n",
      "Epoch: 4489 \tTraining Loss: 2.266179 \tValidation Loss: 2.374669\n",
      "Validation loss decreased (2.374800 --> 2.374669).  Saving model ...\n",
      "Epoch: 4490 \tTraining Loss: 2.282418 \tValidation Loss: 2.374659\n",
      "Validation loss decreased (2.374669 --> 2.374659).  Saving model ...\n",
      "Epoch: 4491 \tTraining Loss: 2.280888 \tValidation Loss: 2.374811\n",
      "Epoch: 4492 \tTraining Loss: 2.270742 \tValidation Loss: 2.374750\n",
      "Epoch: 4493 \tTraining Loss: 2.278956 \tValidation Loss: 2.374630\n",
      "Validation loss decreased (2.374659 --> 2.374630).  Saving model ...\n",
      "Epoch: 4494 \tTraining Loss: 2.246127 \tValidation Loss: 2.374407\n",
      "Validation loss decreased (2.374630 --> 2.374407).  Saving model ...\n",
      "Epoch: 4495 \tTraining Loss: 2.261276 \tValidation Loss: 2.374264\n",
      "Validation loss decreased (2.374407 --> 2.374264).  Saving model ...\n",
      "Epoch: 4496 \tTraining Loss: 2.289445 \tValidation Loss: 2.374169\n",
      "Validation loss decreased (2.374264 --> 2.374169).  Saving model ...\n",
      "Epoch: 4497 \tTraining Loss: 2.291739 \tValidation Loss: 2.374061\n",
      "Validation loss decreased (2.374169 --> 2.374061).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4498 \tTraining Loss: 2.287301 \tValidation Loss: 2.374205\n",
      "Epoch: 4499 \tTraining Loss: 2.266820 \tValidation Loss: 2.374085\n",
      "Epoch: 4500 \tTraining Loss: 2.272168 \tValidation Loss: 2.374159\n",
      "Epoch: 4501 \tTraining Loss: 2.267833 \tValidation Loss: 2.373984\n",
      "Validation loss decreased (2.374061 --> 2.373984).  Saving model ...\n",
      "Epoch: 4502 \tTraining Loss: 2.272418 \tValidation Loss: 2.373824\n",
      "Validation loss decreased (2.373984 --> 2.373824).  Saving model ...\n",
      "Epoch: 4503 \tTraining Loss: 2.270710 \tValidation Loss: 2.373755\n",
      "Validation loss decreased (2.373824 --> 2.373755).  Saving model ...\n",
      "Epoch: 4504 \tTraining Loss: 2.287315 \tValidation Loss: 2.373934\n",
      "Epoch: 4505 \tTraining Loss: 2.266300 \tValidation Loss: 2.374059\n",
      "Epoch: 4506 \tTraining Loss: 2.269259 \tValidation Loss: 2.373999\n",
      "Epoch: 4507 \tTraining Loss: 2.276454 \tValidation Loss: 2.373995\n",
      "Epoch: 4508 \tTraining Loss: 2.269468 \tValidation Loss: 2.374039\n",
      "Epoch: 4509 \tTraining Loss: 2.278355 \tValidation Loss: 2.374014\n",
      "Epoch: 4510 \tTraining Loss: 2.270979 \tValidation Loss: 2.374015\n",
      "Epoch: 4511 \tTraining Loss: 2.270742 \tValidation Loss: 2.374016\n",
      "Epoch: 4512 \tTraining Loss: 2.268982 \tValidation Loss: 2.373912\n",
      "Epoch: 4513 \tTraining Loss: 2.277569 \tValidation Loss: 2.374031\n",
      "Epoch: 4514 \tTraining Loss: 2.273849 \tValidation Loss: 2.373956\n",
      "Epoch: 4515 \tTraining Loss: 2.271110 \tValidation Loss: 2.373775\n",
      "Epoch: 4516 \tTraining Loss: 2.274439 \tValidation Loss: 2.373695\n",
      "Validation loss decreased (2.373755 --> 2.373695).  Saving model ...\n",
      "Epoch: 4517 \tTraining Loss: 2.277378 \tValidation Loss: 2.373411\n",
      "Validation loss decreased (2.373695 --> 2.373411).  Saving model ...\n",
      "Epoch: 4518 \tTraining Loss: 2.272163 \tValidation Loss: 2.373432\n",
      "Epoch: 4519 \tTraining Loss: 2.262205 \tValidation Loss: 2.373290\n",
      "Validation loss decreased (2.373411 --> 2.373290).  Saving model ...\n",
      "Epoch: 4520 \tTraining Loss: 2.279951 \tValidation Loss: 2.373360\n",
      "Epoch: 4521 \tTraining Loss: 2.307129 \tValidation Loss: 2.373533\n",
      "Epoch: 4522 \tTraining Loss: 2.276706 \tValidation Loss: 2.373516\n",
      "Epoch: 4523 \tTraining Loss: 2.284207 \tValidation Loss: 2.373437\n",
      "Epoch: 4524 \tTraining Loss: 2.281707 \tValidation Loss: 2.373481\n",
      "Epoch: 4525 \tTraining Loss: 2.279898 \tValidation Loss: 2.373554\n",
      "Epoch: 4526 \tTraining Loss: 2.267395 \tValidation Loss: 2.373522\n",
      "Epoch: 4527 \tTraining Loss: 2.280026 \tValidation Loss: 2.373602\n",
      "Epoch: 4528 \tTraining Loss: 2.269349 \tValidation Loss: 2.373747\n",
      "Epoch: 4529 \tTraining Loss: 2.284487 \tValidation Loss: 2.373665\n",
      "Epoch: 4530 \tTraining Loss: 2.262762 \tValidation Loss: 2.373732\n",
      "Epoch: 4531 \tTraining Loss: 2.281342 \tValidation Loss: 2.373814\n",
      "Epoch: 4532 \tTraining Loss: 2.275733 \tValidation Loss: 2.373955\n",
      "Epoch: 4533 \tTraining Loss: 2.282119 \tValidation Loss: 2.373922\n",
      "Epoch: 4534 \tTraining Loss: 2.278274 \tValidation Loss: 2.374064\n",
      "Epoch: 4535 \tTraining Loss: 2.266505 \tValidation Loss: 2.373880\n",
      "Epoch: 4536 \tTraining Loss: 2.271621 \tValidation Loss: 2.373978\n",
      "Epoch: 4537 \tTraining Loss: 2.283502 \tValidation Loss: 2.373908\n",
      "Epoch: 4538 \tTraining Loss: 2.262612 \tValidation Loss: 2.373881\n",
      "Epoch: 4539 \tTraining Loss: 2.266805 \tValidation Loss: 2.373800\n",
      "Epoch: 4540 \tTraining Loss: 2.276606 \tValidation Loss: 2.373984\n",
      "Epoch: 4541 \tTraining Loss: 2.259946 \tValidation Loss: 2.374020\n",
      "Epoch: 4542 \tTraining Loss: 2.279443 \tValidation Loss: 2.373939\n",
      "Epoch: 4543 \tTraining Loss: 2.267557 \tValidation Loss: 2.373923\n",
      "Epoch: 4544 \tTraining Loss: 2.255970 \tValidation Loss: 2.373767\n",
      "Epoch: 4545 \tTraining Loss: 2.270620 \tValidation Loss: 2.373772\n",
      "Epoch: 4546 \tTraining Loss: 2.260075 \tValidation Loss: 2.373735\n",
      "Epoch: 4547 \tTraining Loss: 2.270285 \tValidation Loss: 2.373667\n",
      "Epoch: 4548 \tTraining Loss: 2.282274 \tValidation Loss: 2.373700\n",
      "Epoch: 4549 \tTraining Loss: 2.280988 \tValidation Loss: 2.373745\n",
      "Epoch: 4550 \tTraining Loss: 2.295463 \tValidation Loss: 2.373771\n",
      "Epoch: 4551 \tTraining Loss: 2.270498 \tValidation Loss: 2.373781\n",
      "Epoch: 4552 \tTraining Loss: 2.271951 \tValidation Loss: 2.373663\n",
      "Epoch: 4553 \tTraining Loss: 2.263010 \tValidation Loss: 2.373528\n",
      "Epoch: 4554 \tTraining Loss: 2.289490 \tValidation Loss: 2.373520\n",
      "Epoch: 4555 \tTraining Loss: 2.290283 \tValidation Loss: 2.373541\n",
      "Epoch: 4556 \tTraining Loss: 2.280885 \tValidation Loss: 2.373656\n",
      "Epoch: 4557 \tTraining Loss: 2.259606 \tValidation Loss: 2.373438\n",
      "Epoch: 4558 \tTraining Loss: 2.278069 \tValidation Loss: 2.373407\n",
      "Epoch: 4559 \tTraining Loss: 2.279844 \tValidation Loss: 2.373325\n",
      "Epoch: 4560 \tTraining Loss: 2.264726 \tValidation Loss: 2.373183\n",
      "Validation loss decreased (2.373290 --> 2.373183).  Saving model ...\n",
      "Epoch: 4561 \tTraining Loss: 2.293247 \tValidation Loss: 2.373135\n",
      "Validation loss decreased (2.373183 --> 2.373135).  Saving model ...\n",
      "Epoch: 4562 \tTraining Loss: 2.265148 \tValidation Loss: 2.372940\n",
      "Validation loss decreased (2.373135 --> 2.372940).  Saving model ...\n",
      "Epoch: 4563 \tTraining Loss: 2.280369 \tValidation Loss: 2.372991\n",
      "Epoch: 4564 \tTraining Loss: 2.261740 \tValidation Loss: 2.372887\n",
      "Validation loss decreased (2.372940 --> 2.372887).  Saving model ...\n",
      "Epoch: 4565 \tTraining Loss: 2.267188 \tValidation Loss: 2.372948\n",
      "Epoch: 4566 \tTraining Loss: 2.272633 \tValidation Loss: 2.372871\n",
      "Validation loss decreased (2.372887 --> 2.372871).  Saving model ...\n",
      "Epoch: 4567 \tTraining Loss: 2.284857 \tValidation Loss: 2.372829\n",
      "Validation loss decreased (2.372871 --> 2.372829).  Saving model ...\n",
      "Epoch: 4568 \tTraining Loss: 2.269365 \tValidation Loss: 2.372823\n",
      "Validation loss decreased (2.372829 --> 2.372823).  Saving model ...\n",
      "Epoch: 4569 \tTraining Loss: 2.264682 \tValidation Loss: 2.372938\n",
      "Epoch: 4570 \tTraining Loss: 2.265570 \tValidation Loss: 2.372859\n",
      "Epoch: 4571 \tTraining Loss: 2.267128 \tValidation Loss: 2.372761\n",
      "Validation loss decreased (2.372823 --> 2.372761).  Saving model ...\n",
      "Epoch: 4572 \tTraining Loss: 2.271388 \tValidation Loss: 2.372773\n",
      "Epoch: 4573 \tTraining Loss: 2.261674 \tValidation Loss: 2.372794\n",
      "Epoch: 4574 \tTraining Loss: 2.263544 \tValidation Loss: 2.372687\n",
      "Validation loss decreased (2.372761 --> 2.372687).  Saving model ...\n",
      "Epoch: 4575 \tTraining Loss: 2.275661 \tValidation Loss: 2.372620\n",
      "Validation loss decreased (2.372687 --> 2.372620).  Saving model ...\n",
      "Epoch: 4576 \tTraining Loss: 2.275050 \tValidation Loss: 2.372439\n",
      "Validation loss decreased (2.372620 --> 2.372439).  Saving model ...\n",
      "Epoch: 4577 \tTraining Loss: 2.267261 \tValidation Loss: 2.372511\n",
      "Epoch: 4578 \tTraining Loss: 2.265104 \tValidation Loss: 2.372397\n",
      "Validation loss decreased (2.372439 --> 2.372397).  Saving model ...\n",
      "Epoch: 4579 \tTraining Loss: 2.282638 \tValidation Loss: 2.372315\n",
      "Validation loss decreased (2.372397 --> 2.372315).  Saving model ...\n",
      "Epoch: 4580 \tTraining Loss: 2.272655 \tValidation Loss: 2.372130\n",
      "Validation loss decreased (2.372315 --> 2.372130).  Saving model ...\n",
      "Epoch: 4581 \tTraining Loss: 2.267486 \tValidation Loss: 2.372262\n",
      "Epoch: 4582 \tTraining Loss: 2.268061 \tValidation Loss: 2.372271\n",
      "Epoch: 4583 \tTraining Loss: 2.261477 \tValidation Loss: 2.372311\n",
      "Epoch: 4584 \tTraining Loss: 2.267311 \tValidation Loss: 2.372404\n",
      "Epoch: 4585 \tTraining Loss: 2.268668 \tValidation Loss: 2.372504\n",
      "Epoch: 4586 \tTraining Loss: 2.262481 \tValidation Loss: 2.372743\n",
      "Epoch: 4587 \tTraining Loss: 2.266505 \tValidation Loss: 2.372713\n",
      "Epoch: 4588 \tTraining Loss: 2.286383 \tValidation Loss: 2.372670\n",
      "Epoch: 4589 \tTraining Loss: 2.275449 \tValidation Loss: 2.372713\n",
      "Epoch: 4590 \tTraining Loss: 2.266842 \tValidation Loss: 2.372547\n",
      "Epoch: 4591 \tTraining Loss: 2.263083 \tValidation Loss: 2.372480\n",
      "Epoch: 4592 \tTraining Loss: 2.288917 \tValidation Loss: 2.372416\n",
      "Epoch: 4593 \tTraining Loss: 2.284101 \tValidation Loss: 2.372458\n",
      "Epoch: 4594 \tTraining Loss: 2.276625 \tValidation Loss: 2.372361\n",
      "Epoch: 4595 \tTraining Loss: 2.280452 \tValidation Loss: 2.372295\n",
      "Epoch: 4596 \tTraining Loss: 2.252716 \tValidation Loss: 2.372262\n",
      "Epoch: 4597 \tTraining Loss: 2.261196 \tValidation Loss: 2.372249\n",
      "Epoch: 4598 \tTraining Loss: 2.269171 \tValidation Loss: 2.372114\n",
      "Validation loss decreased (2.372130 --> 2.372114).  Saving model ...\n",
      "Epoch: 4599 \tTraining Loss: 2.274335 \tValidation Loss: 2.372018\n",
      "Validation loss decreased (2.372114 --> 2.372018).  Saving model ...\n",
      "Epoch: 4600 \tTraining Loss: 2.259676 \tValidation Loss: 2.372101\n",
      "Epoch: 4601 \tTraining Loss: 2.263776 \tValidation Loss: 2.372129\n",
      "Epoch: 4602 \tTraining Loss: 2.268264 \tValidation Loss: 2.372218\n",
      "Epoch: 4603 \tTraining Loss: 2.298427 \tValidation Loss: 2.372313\n",
      "Epoch: 4604 \tTraining Loss: 2.259151 \tValidation Loss: 2.372301\n",
      "Epoch: 4605 \tTraining Loss: 2.273771 \tValidation Loss: 2.372366\n",
      "Epoch: 4606 \tTraining Loss: 2.288258 \tValidation Loss: 2.372482\n",
      "Epoch: 4607 \tTraining Loss: 2.281105 \tValidation Loss: 2.372509\n",
      "Epoch: 4608 \tTraining Loss: 2.250900 \tValidation Loss: 2.372317\n",
      "Epoch: 4609 \tTraining Loss: 2.277880 \tValidation Loss: 2.372287\n",
      "Epoch: 4610 \tTraining Loss: 2.261494 \tValidation Loss: 2.372285\n",
      "Epoch: 4611 \tTraining Loss: 2.260235 \tValidation Loss: 2.372151\n",
      "Epoch: 4612 \tTraining Loss: 2.271131 \tValidation Loss: 2.372255\n",
      "Epoch: 4613 \tTraining Loss: 2.270844 \tValidation Loss: 2.372248\n",
      "Epoch: 4614 \tTraining Loss: 2.295115 \tValidation Loss: 2.372236\n",
      "Epoch: 4615 \tTraining Loss: 2.271129 \tValidation Loss: 2.372193\n",
      "Epoch: 4616 \tTraining Loss: 2.278912 \tValidation Loss: 2.372216\n",
      "Epoch: 4617 \tTraining Loss: 2.259589 \tValidation Loss: 2.372082\n",
      "Epoch: 4618 \tTraining Loss: 2.266711 \tValidation Loss: 2.371957\n",
      "Validation loss decreased (2.372018 --> 2.371957).  Saving model ...\n",
      "Epoch: 4619 \tTraining Loss: 2.249701 \tValidation Loss: 2.371993\n",
      "Epoch: 4620 \tTraining Loss: 2.269862 \tValidation Loss: 2.372015\n",
      "Epoch: 4621 \tTraining Loss: 2.260247 \tValidation Loss: 2.371876\n",
      "Validation loss decreased (2.371957 --> 2.371876).  Saving model ...\n",
      "Epoch: 4622 \tTraining Loss: 2.271826 \tValidation Loss: 2.371922\n",
      "Epoch: 4623 \tTraining Loss: 2.261260 \tValidation Loss: 2.371675\n",
      "Validation loss decreased (2.371876 --> 2.371675).  Saving model ...\n",
      "Epoch: 4624 \tTraining Loss: 2.278834 \tValidation Loss: 2.371747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4625 \tTraining Loss: 2.271305 \tValidation Loss: 2.371917\n",
      "Epoch: 4626 \tTraining Loss: 2.269847 \tValidation Loss: 2.371911\n",
      "Epoch: 4627 \tTraining Loss: 2.276285 \tValidation Loss: 2.371903\n",
      "Epoch: 4628 \tTraining Loss: 2.260692 \tValidation Loss: 2.371778\n",
      "Epoch: 4629 \tTraining Loss: 2.256348 \tValidation Loss: 2.371671\n",
      "Validation loss decreased (2.371675 --> 2.371671).  Saving model ...\n",
      "Epoch: 4630 \tTraining Loss: 2.269405 \tValidation Loss: 2.371541\n",
      "Validation loss decreased (2.371671 --> 2.371541).  Saving model ...\n",
      "Epoch: 4631 \tTraining Loss: 2.265507 \tValidation Loss: 2.371470\n",
      "Validation loss decreased (2.371541 --> 2.371470).  Saving model ...\n",
      "Epoch: 4632 \tTraining Loss: 2.271863 \tValidation Loss: 2.371445\n",
      "Validation loss decreased (2.371470 --> 2.371445).  Saving model ...\n",
      "Epoch: 4633 \tTraining Loss: 2.271274 \tValidation Loss: 2.371379\n",
      "Validation loss decreased (2.371445 --> 2.371379).  Saving model ...\n",
      "Epoch: 4634 \tTraining Loss: 2.254248 \tValidation Loss: 2.371145\n",
      "Validation loss decreased (2.371379 --> 2.371145).  Saving model ...\n",
      "Epoch: 4635 \tTraining Loss: 2.272686 \tValidation Loss: 2.371100\n",
      "Validation loss decreased (2.371145 --> 2.371100).  Saving model ...\n",
      "Epoch: 4636 \tTraining Loss: 2.269808 \tValidation Loss: 2.371107\n",
      "Epoch: 4637 \tTraining Loss: 2.255248 \tValidation Loss: 2.370952\n",
      "Validation loss decreased (2.371100 --> 2.370952).  Saving model ...\n",
      "Epoch: 4638 \tTraining Loss: 2.262995 \tValidation Loss: 2.370922\n",
      "Validation loss decreased (2.370952 --> 2.370922).  Saving model ...\n",
      "Epoch: 4639 \tTraining Loss: 2.267676 \tValidation Loss: 2.370915\n",
      "Validation loss decreased (2.370922 --> 2.370915).  Saving model ...\n",
      "Epoch: 4640 \tTraining Loss: 2.270411 \tValidation Loss: 2.371049\n",
      "Epoch: 4641 \tTraining Loss: 2.254638 \tValidation Loss: 2.370756\n",
      "Validation loss decreased (2.370915 --> 2.370756).  Saving model ...\n",
      "Epoch: 4642 \tTraining Loss: 2.265800 \tValidation Loss: 2.370735\n",
      "Validation loss decreased (2.370756 --> 2.370735).  Saving model ...\n",
      "Epoch: 4643 \tTraining Loss: 2.248256 \tValidation Loss: 2.370582\n",
      "Validation loss decreased (2.370735 --> 2.370582).  Saving model ...\n",
      "Epoch: 4644 \tTraining Loss: 2.261448 \tValidation Loss: 2.370508\n",
      "Validation loss decreased (2.370582 --> 2.370508).  Saving model ...\n",
      "Epoch: 4645 \tTraining Loss: 2.264508 \tValidation Loss: 2.370423\n",
      "Validation loss decreased (2.370508 --> 2.370423).  Saving model ...\n",
      "Epoch: 4646 \tTraining Loss: 2.270161 \tValidation Loss: 2.370368\n",
      "Validation loss decreased (2.370423 --> 2.370368).  Saving model ...\n",
      "Epoch: 4647 \tTraining Loss: 2.255248 \tValidation Loss: 2.370434\n",
      "Epoch: 4648 \tTraining Loss: 2.267170 \tValidation Loss: 2.370363\n",
      "Validation loss decreased (2.370368 --> 2.370363).  Saving model ...\n",
      "Epoch: 4649 \tTraining Loss: 2.275154 \tValidation Loss: 2.370267\n",
      "Validation loss decreased (2.370363 --> 2.370267).  Saving model ...\n",
      "Epoch: 4650 \tTraining Loss: 2.267485 \tValidation Loss: 2.370341\n",
      "Epoch: 4651 \tTraining Loss: 2.285420 \tValidation Loss: 2.370313\n",
      "Epoch: 4652 \tTraining Loss: 2.263185 \tValidation Loss: 2.370325\n",
      "Epoch: 4653 \tTraining Loss: 2.283194 \tValidation Loss: 2.370328\n",
      "Epoch: 4654 \tTraining Loss: 2.253065 \tValidation Loss: 2.370373\n",
      "Epoch: 4655 \tTraining Loss: 2.268817 \tValidation Loss: 2.370377\n",
      "Epoch: 4656 \tTraining Loss: 2.270350 \tValidation Loss: 2.370358\n",
      "Epoch: 4657 \tTraining Loss: 2.292320 \tValidation Loss: 2.370457\n",
      "Epoch: 4658 \tTraining Loss: 2.267350 \tValidation Loss: 2.370404\n",
      "Epoch: 4659 \tTraining Loss: 2.273700 \tValidation Loss: 2.370403\n",
      "Epoch: 4660 \tTraining Loss: 2.259019 \tValidation Loss: 2.370268\n",
      "Epoch: 4661 \tTraining Loss: 2.269744 \tValidation Loss: 2.370216\n",
      "Validation loss decreased (2.370267 --> 2.370216).  Saving model ...\n",
      "Epoch: 4662 \tTraining Loss: 2.263709 \tValidation Loss: 2.370295\n",
      "Epoch: 4663 \tTraining Loss: 2.260243 \tValidation Loss: 2.370437\n",
      "Epoch: 4664 \tTraining Loss: 2.266692 \tValidation Loss: 2.370392\n",
      "Epoch: 4665 \tTraining Loss: 2.269600 \tValidation Loss: 2.370373\n",
      "Epoch: 4666 \tTraining Loss: 2.276397 \tValidation Loss: 2.370327\n",
      "Epoch: 4667 \tTraining Loss: 2.253235 \tValidation Loss: 2.370366\n",
      "Epoch: 4668 \tTraining Loss: 2.273254 \tValidation Loss: 2.370355\n",
      "Epoch: 4669 \tTraining Loss: 2.275703 \tValidation Loss: 2.370314\n",
      "Epoch: 4670 \tTraining Loss: 2.270928 \tValidation Loss: 2.370337\n",
      "Epoch: 4671 \tTraining Loss: 2.254777 \tValidation Loss: 2.370367\n",
      "Epoch: 4672 \tTraining Loss: 2.264997 \tValidation Loss: 2.370367\n",
      "Epoch: 4673 \tTraining Loss: 2.272933 \tValidation Loss: 2.370334\n",
      "Epoch: 4674 \tTraining Loss: 2.267688 \tValidation Loss: 2.370499\n",
      "Epoch: 4675 \tTraining Loss: 2.253758 \tValidation Loss: 2.370442\n",
      "Epoch: 4676 \tTraining Loss: 2.261209 \tValidation Loss: 2.370407\n",
      "Epoch: 4677 \tTraining Loss: 2.274274 \tValidation Loss: 2.370329\n",
      "Epoch: 4678 \tTraining Loss: 2.271941 \tValidation Loss: 2.370364\n",
      "Epoch: 4679 \tTraining Loss: 2.265702 \tValidation Loss: 2.370391\n",
      "Epoch: 4680 \tTraining Loss: 2.265253 \tValidation Loss: 2.370362\n",
      "Epoch: 4681 \tTraining Loss: 2.258492 \tValidation Loss: 2.370440\n",
      "Epoch: 4682 \tTraining Loss: 2.275411 \tValidation Loss: 2.370379\n",
      "Epoch: 4683 \tTraining Loss: 2.261156 \tValidation Loss: 2.370380\n",
      "Epoch: 4684 \tTraining Loss: 2.266148 \tValidation Loss: 2.370345\n",
      "Epoch: 4685 \tTraining Loss: 2.264184 \tValidation Loss: 2.370374\n",
      "Epoch: 4686 \tTraining Loss: 2.268244 \tValidation Loss: 2.370330\n",
      "Epoch: 4687 \tTraining Loss: 2.281016 \tValidation Loss: 2.370434\n",
      "Epoch: 4688 \tTraining Loss: 2.272108 \tValidation Loss: 2.370512\n",
      "Epoch: 4689 \tTraining Loss: 2.244433 \tValidation Loss: 2.370484\n",
      "Epoch: 4690 \tTraining Loss: 2.264381 \tValidation Loss: 2.370489\n",
      "Epoch: 4691 \tTraining Loss: 2.262098 \tValidation Loss: 2.370444\n",
      "Epoch: 4692 \tTraining Loss: 2.291395 \tValidation Loss: 2.370685\n",
      "Epoch: 4693 \tTraining Loss: 2.269939 \tValidation Loss: 2.370562\n",
      "Epoch: 4694 \tTraining Loss: 2.258057 \tValidation Loss: 2.370661\n",
      "Epoch: 4695 \tTraining Loss: 2.252448 \tValidation Loss: 2.370567\n",
      "Epoch: 4696 \tTraining Loss: 2.282631 \tValidation Loss: 2.370498\n",
      "Epoch: 4697 \tTraining Loss: 2.265899 \tValidation Loss: 2.370435\n",
      "Epoch: 4698 \tTraining Loss: 2.242672 \tValidation Loss: 2.370426\n",
      "Epoch: 4699 \tTraining Loss: 2.270092 \tValidation Loss: 2.370578\n",
      "Epoch: 4700 \tTraining Loss: 2.259340 \tValidation Loss: 2.370461\n",
      "Epoch: 4701 \tTraining Loss: 2.247253 \tValidation Loss: 2.370303\n",
      "Epoch: 4702 \tTraining Loss: 2.272104 \tValidation Loss: 2.370233\n",
      "Epoch: 4703 \tTraining Loss: 2.244142 \tValidation Loss: 2.370013\n",
      "Validation loss decreased (2.370216 --> 2.370013).  Saving model ...\n",
      "Epoch: 4704 \tTraining Loss: 2.273729 \tValidation Loss: 2.369958\n",
      "Validation loss decreased (2.370013 --> 2.369958).  Saving model ...\n",
      "Epoch: 4705 \tTraining Loss: 2.251337 \tValidation Loss: 2.369926\n",
      "Validation loss decreased (2.369958 --> 2.369926).  Saving model ...\n",
      "Epoch: 4706 \tTraining Loss: 2.245907 \tValidation Loss: 2.369716\n",
      "Validation loss decreased (2.369926 --> 2.369716).  Saving model ...\n",
      "Epoch: 4707 \tTraining Loss: 2.255569 \tValidation Loss: 2.369764\n",
      "Epoch: 4708 \tTraining Loss: 2.252135 \tValidation Loss: 2.369655\n",
      "Validation loss decreased (2.369716 --> 2.369655).  Saving model ...\n",
      "Epoch: 4709 \tTraining Loss: 2.257205 \tValidation Loss: 2.369690\n",
      "Epoch: 4710 \tTraining Loss: 2.259763 \tValidation Loss: 2.369612\n",
      "Validation loss decreased (2.369655 --> 2.369612).  Saving model ...\n",
      "Epoch: 4711 \tTraining Loss: 2.267305 \tValidation Loss: 2.369596\n",
      "Validation loss decreased (2.369612 --> 2.369596).  Saving model ...\n",
      "Epoch: 4712 \tTraining Loss: 2.257992 \tValidation Loss: 2.369514\n",
      "Validation loss decreased (2.369596 --> 2.369514).  Saving model ...\n",
      "Epoch: 4713 \tTraining Loss: 2.256416 \tValidation Loss: 2.369540\n",
      "Epoch: 4714 \tTraining Loss: 2.244278 \tValidation Loss: 2.369640\n",
      "Epoch: 4715 \tTraining Loss: 2.266289 \tValidation Loss: 2.369565\n",
      "Epoch: 4716 \tTraining Loss: 2.239505 \tValidation Loss: 2.369369\n",
      "Validation loss decreased (2.369514 --> 2.369369).  Saving model ...\n",
      "Epoch: 4717 \tTraining Loss: 2.261514 \tValidation Loss: 2.369276\n",
      "Validation loss decreased (2.369369 --> 2.369276).  Saving model ...\n",
      "Epoch: 4718 \tTraining Loss: 2.246058 \tValidation Loss: 2.369085\n",
      "Validation loss decreased (2.369276 --> 2.369085).  Saving model ...\n",
      "Epoch: 4719 \tTraining Loss: 2.268462 \tValidation Loss: 2.369029\n",
      "Validation loss decreased (2.369085 --> 2.369029).  Saving model ...\n",
      "Epoch: 4720 \tTraining Loss: 2.272586 \tValidation Loss: 2.369114\n",
      "Epoch: 4721 \tTraining Loss: 2.248178 \tValidation Loss: 2.369059\n",
      "Epoch: 4722 \tTraining Loss: 2.251812 \tValidation Loss: 2.369056\n",
      "Epoch: 4723 \tTraining Loss: 2.259426 \tValidation Loss: 2.369045\n",
      "Epoch: 4724 \tTraining Loss: 2.276154 \tValidation Loss: 2.369032\n",
      "Epoch: 4725 \tTraining Loss: 2.267553 \tValidation Loss: 2.369058\n",
      "Epoch: 4726 \tTraining Loss: 2.255647 \tValidation Loss: 2.369171\n",
      "Epoch: 4727 \tTraining Loss: 2.256818 \tValidation Loss: 2.369102\n",
      "Epoch: 4728 \tTraining Loss: 2.260201 \tValidation Loss: 2.368895\n",
      "Validation loss decreased (2.369029 --> 2.368895).  Saving model ...\n",
      "Epoch: 4729 \tTraining Loss: 2.248009 \tValidation Loss: 2.368918\n",
      "Epoch: 4730 \tTraining Loss: 2.263607 \tValidation Loss: 2.369047\n",
      "Epoch: 4731 \tTraining Loss: 2.297529 \tValidation Loss: 2.369118\n",
      "Epoch: 4732 \tTraining Loss: 2.258427 \tValidation Loss: 2.369035\n",
      "Epoch: 4733 \tTraining Loss: 2.255994 \tValidation Loss: 2.369089\n",
      "Epoch: 4734 \tTraining Loss: 2.264952 \tValidation Loss: 2.368824\n",
      "Validation loss decreased (2.368895 --> 2.368824).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4735 \tTraining Loss: 2.255545 \tValidation Loss: 2.368871\n",
      "Epoch: 4736 \tTraining Loss: 2.262740 \tValidation Loss: 2.368879\n",
      "Epoch: 4737 \tTraining Loss: 2.248529 \tValidation Loss: 2.368772\n",
      "Validation loss decreased (2.368824 --> 2.368772).  Saving model ...\n",
      "Epoch: 4738 \tTraining Loss: 2.257681 \tValidation Loss: 2.368885\n",
      "Epoch: 4739 \tTraining Loss: 2.248038 \tValidation Loss: 2.368976\n",
      "Epoch: 4740 \tTraining Loss: 2.264791 \tValidation Loss: 2.368903\n",
      "Epoch: 4741 \tTraining Loss: 2.245067 \tValidation Loss: 2.368859\n",
      "Epoch: 4742 \tTraining Loss: 2.256965 \tValidation Loss: 2.368950\n",
      "Epoch: 4743 \tTraining Loss: 2.267300 \tValidation Loss: 2.368827\n",
      "Epoch: 4744 \tTraining Loss: 2.270493 \tValidation Loss: 2.368847\n",
      "Epoch: 4745 \tTraining Loss: 2.253958 \tValidation Loss: 2.368899\n",
      "Epoch: 4746 \tTraining Loss: 2.254683 \tValidation Loss: 2.368793\n",
      "Epoch: 4747 \tTraining Loss: 2.252342 \tValidation Loss: 2.368587\n",
      "Validation loss decreased (2.368772 --> 2.368587).  Saving model ...\n",
      "Epoch: 4748 \tTraining Loss: 2.252767 \tValidation Loss: 2.368408\n",
      "Validation loss decreased (2.368587 --> 2.368408).  Saving model ...\n",
      "Epoch: 4749 \tTraining Loss: 2.259660 \tValidation Loss: 2.368538\n",
      "Epoch: 4750 \tTraining Loss: 2.275766 \tValidation Loss: 2.368705\n",
      "Epoch: 4751 \tTraining Loss: 2.251579 \tValidation Loss: 2.368531\n",
      "Epoch: 4752 \tTraining Loss: 2.248010 \tValidation Loss: 2.368679\n",
      "Epoch: 4753 \tTraining Loss: 2.243803 \tValidation Loss: 2.368649\n",
      "Epoch: 4754 \tTraining Loss: 2.259272 \tValidation Loss: 2.368525\n",
      "Epoch: 4755 \tTraining Loss: 2.276210 \tValidation Loss: 2.368775\n",
      "Epoch: 4756 \tTraining Loss: 2.268633 \tValidation Loss: 2.368678\n",
      "Epoch: 4757 \tTraining Loss: 2.258163 \tValidation Loss: 2.368643\n",
      "Epoch: 4758 \tTraining Loss: 2.243503 \tValidation Loss: 2.368699\n",
      "Epoch: 4759 \tTraining Loss: 2.256318 \tValidation Loss: 2.368529\n",
      "Epoch: 4760 \tTraining Loss: 2.271188 \tValidation Loss: 2.368587\n",
      "Epoch: 4761 \tTraining Loss: 2.262081 \tValidation Loss: 2.368441\n",
      "Epoch: 4762 \tTraining Loss: 2.260949 \tValidation Loss: 2.368142\n",
      "Validation loss decreased (2.368408 --> 2.368142).  Saving model ...\n",
      "Epoch: 4763 \tTraining Loss: 2.273568 \tValidation Loss: 2.368207\n",
      "Epoch: 4764 \tTraining Loss: 2.270352 \tValidation Loss: 2.368345\n",
      "Epoch: 4765 \tTraining Loss: 2.252144 \tValidation Loss: 2.368340\n",
      "Epoch: 4766 \tTraining Loss: 2.264621 \tValidation Loss: 2.368347\n",
      "Epoch: 4767 \tTraining Loss: 2.247710 \tValidation Loss: 2.368292\n",
      "Epoch: 4768 \tTraining Loss: 2.255679 \tValidation Loss: 2.368364\n",
      "Epoch: 4769 \tTraining Loss: 2.272314 \tValidation Loss: 2.368376\n",
      "Epoch: 4770 \tTraining Loss: 2.258264 \tValidation Loss: 2.368496\n",
      "Epoch: 4771 \tTraining Loss: 2.252714 \tValidation Loss: 2.368410\n",
      "Epoch: 4772 \tTraining Loss: 2.264441 \tValidation Loss: 2.368573\n",
      "Epoch: 4773 \tTraining Loss: 2.257615 \tValidation Loss: 2.368628\n",
      "Epoch: 4774 \tTraining Loss: 2.252482 \tValidation Loss: 2.368634\n",
      "Epoch: 4775 \tTraining Loss: 2.269684 \tValidation Loss: 2.368590\n",
      "Epoch: 4776 \tTraining Loss: 2.253649 \tValidation Loss: 2.368606\n",
      "Epoch: 4777 \tTraining Loss: 2.248158 \tValidation Loss: 2.368548\n",
      "Epoch: 4778 \tTraining Loss: 2.255928 \tValidation Loss: 2.368525\n",
      "Epoch: 4779 \tTraining Loss: 2.254282 \tValidation Loss: 2.368401\n",
      "Epoch: 4780 \tTraining Loss: 2.252771 \tValidation Loss: 2.368273\n",
      "Epoch: 4781 \tTraining Loss: 2.250401 \tValidation Loss: 2.368323\n",
      "Epoch: 4782 \tTraining Loss: 2.262707 \tValidation Loss: 2.368289\n",
      "Epoch: 4783 \tTraining Loss: 2.246609 \tValidation Loss: 2.368235\n",
      "Epoch: 4784 \tTraining Loss: 2.255184 \tValidation Loss: 2.368208\n",
      "Epoch: 4785 \tTraining Loss: 2.273038 \tValidation Loss: 2.368191\n",
      "Epoch: 4786 \tTraining Loss: 2.276645 \tValidation Loss: 2.368231\n",
      "Epoch: 4787 \tTraining Loss: 2.269951 \tValidation Loss: 2.368163\n",
      "Epoch: 4788 \tTraining Loss: 2.247178 \tValidation Loss: 2.368064\n",
      "Validation loss decreased (2.368142 --> 2.368064).  Saving model ...\n",
      "Epoch: 4789 \tTraining Loss: 2.266025 \tValidation Loss: 2.368046\n",
      "Validation loss decreased (2.368064 --> 2.368046).  Saving model ...\n",
      "Epoch: 4790 \tTraining Loss: 2.239820 \tValidation Loss: 2.367808\n",
      "Validation loss decreased (2.368046 --> 2.367808).  Saving model ...\n",
      "Epoch: 4791 \tTraining Loss: 2.255541 \tValidation Loss: 2.367687\n",
      "Validation loss decreased (2.367808 --> 2.367687).  Saving model ...\n",
      "Epoch: 4792 \tTraining Loss: 2.242780 \tValidation Loss: 2.367573\n",
      "Validation loss decreased (2.367687 --> 2.367573).  Saving model ...\n",
      "Epoch: 4793 \tTraining Loss: 2.263436 \tValidation Loss: 2.367604\n",
      "Epoch: 4794 \tTraining Loss: 2.242989 \tValidation Loss: 2.367600\n",
      "Epoch: 4795 \tTraining Loss: 2.235414 \tValidation Loss: 2.367556\n",
      "Validation loss decreased (2.367573 --> 2.367556).  Saving model ...\n",
      "Epoch: 4796 \tTraining Loss: 2.260682 \tValidation Loss: 2.367598\n",
      "Epoch: 4797 \tTraining Loss: 2.277685 \tValidation Loss: 2.367572\n",
      "Epoch: 4798 \tTraining Loss: 2.242390 \tValidation Loss: 2.367219\n",
      "Validation loss decreased (2.367556 --> 2.367219).  Saving model ...\n",
      "Epoch: 4799 \tTraining Loss: 2.251517 \tValidation Loss: 2.366999\n",
      "Validation loss decreased (2.367219 --> 2.366999).  Saving model ...\n",
      "Epoch: 4800 \tTraining Loss: 2.253294 \tValidation Loss: 2.366932\n",
      "Validation loss decreased (2.366999 --> 2.366932).  Saving model ...\n",
      "Epoch: 4801 \tTraining Loss: 2.244719 \tValidation Loss: 2.367023\n",
      "Epoch: 4802 \tTraining Loss: 2.284124 \tValidation Loss: 2.367061\n",
      "Epoch: 4803 \tTraining Loss: 2.249625 \tValidation Loss: 2.367160\n",
      "Epoch: 4804 \tTraining Loss: 2.266863 \tValidation Loss: 2.367249\n",
      "Epoch: 4805 \tTraining Loss: 2.254166 \tValidation Loss: 2.367326\n",
      "Epoch: 4806 \tTraining Loss: 2.263906 \tValidation Loss: 2.367357\n",
      "Epoch: 4807 \tTraining Loss: 2.275080 \tValidation Loss: 2.367424\n",
      "Epoch: 4808 \tTraining Loss: 2.255873 \tValidation Loss: 2.367448\n",
      "Epoch: 4809 \tTraining Loss: 2.268998 \tValidation Loss: 2.367552\n",
      "Epoch: 4810 \tTraining Loss: 2.245354 \tValidation Loss: 2.367261\n",
      "Epoch: 4811 \tTraining Loss: 2.242777 \tValidation Loss: 2.367213\n",
      "Epoch: 4812 \tTraining Loss: 2.243455 \tValidation Loss: 2.367157\n",
      "Epoch: 4813 \tTraining Loss: 2.257951 \tValidation Loss: 2.367071\n",
      "Epoch: 4814 \tTraining Loss: 2.238172 \tValidation Loss: 2.366908\n",
      "Validation loss decreased (2.366932 --> 2.366908).  Saving model ...\n",
      "Epoch: 4815 \tTraining Loss: 2.271854 \tValidation Loss: 2.366823\n",
      "Validation loss decreased (2.366908 --> 2.366823).  Saving model ...\n",
      "Epoch: 4816 \tTraining Loss: 2.268927 \tValidation Loss: 2.366918\n",
      "Epoch: 4817 \tTraining Loss: 2.245746 \tValidation Loss: 2.366882\n",
      "Epoch: 4818 \tTraining Loss: 2.270752 \tValidation Loss: 2.367164\n",
      "Epoch: 4819 \tTraining Loss: 2.254358 \tValidation Loss: 2.367133\n",
      "Epoch: 4820 \tTraining Loss: 2.263386 \tValidation Loss: 2.367127\n",
      "Epoch: 4821 \tTraining Loss: 2.263463 \tValidation Loss: 2.367234\n",
      "Epoch: 4822 \tTraining Loss: 2.246195 \tValidation Loss: 2.367221\n",
      "Epoch: 4823 \tTraining Loss: 2.266023 \tValidation Loss: 2.367290\n",
      "Epoch: 4824 \tTraining Loss: 2.235720 \tValidation Loss: 2.367084\n",
      "Epoch: 4825 \tTraining Loss: 2.251799 \tValidation Loss: 2.366991\n",
      "Epoch: 4826 \tTraining Loss: 2.259154 \tValidation Loss: 2.367212\n",
      "Epoch: 4827 \tTraining Loss: 2.255543 \tValidation Loss: 2.367198\n",
      "Epoch: 4828 \tTraining Loss: 2.244943 \tValidation Loss: 2.367139\n",
      "Epoch: 4829 \tTraining Loss: 2.253574 \tValidation Loss: 2.367281\n",
      "Epoch: 4830 \tTraining Loss: 2.247619 \tValidation Loss: 2.367246\n",
      "Epoch: 4831 \tTraining Loss: 2.270953 \tValidation Loss: 2.367264\n",
      "Epoch: 4832 \tTraining Loss: 2.244401 \tValidation Loss: 2.367102\n",
      "Epoch: 4833 \tTraining Loss: 2.243445 \tValidation Loss: 2.367146\n",
      "Epoch: 4834 \tTraining Loss: 2.241059 \tValidation Loss: 2.367184\n",
      "Epoch: 4835 \tTraining Loss: 2.241023 \tValidation Loss: 2.367181\n",
      "Epoch: 4836 \tTraining Loss: 2.243169 \tValidation Loss: 2.367025\n",
      "Epoch: 4837 \tTraining Loss: 2.260545 \tValidation Loss: 2.366870\n",
      "Epoch: 4838 \tTraining Loss: 2.236970 \tValidation Loss: 2.366772\n",
      "Validation loss decreased (2.366823 --> 2.366772).  Saving model ...\n",
      "Epoch: 4839 \tTraining Loss: 2.254112 \tValidation Loss: 2.366755\n",
      "Validation loss decreased (2.366772 --> 2.366755).  Saving model ...\n",
      "Epoch: 4840 \tTraining Loss: 2.246136 \tValidation Loss: 2.366800\n",
      "Epoch: 4841 \tTraining Loss: 2.251051 \tValidation Loss: 2.366844\n",
      "Epoch: 4842 \tTraining Loss: 2.251170 \tValidation Loss: 2.366585\n",
      "Validation loss decreased (2.366755 --> 2.366585).  Saving model ...\n",
      "Epoch: 4843 \tTraining Loss: 2.244579 \tValidation Loss: 2.366563\n",
      "Validation loss decreased (2.366585 --> 2.366563).  Saving model ...\n",
      "Epoch: 4844 \tTraining Loss: 2.251827 \tValidation Loss: 2.366693\n",
      "Epoch: 4845 \tTraining Loss: 2.280355 \tValidation Loss: 2.366647\n",
      "Epoch: 4846 \tTraining Loss: 2.258809 \tValidation Loss: 2.366884\n",
      "Epoch: 4847 \tTraining Loss: 2.248024 \tValidation Loss: 2.366747\n",
      "Epoch: 4848 \tTraining Loss: 2.259022 \tValidation Loss: 2.366630\n",
      "Epoch: 4849 \tTraining Loss: 2.257430 \tValidation Loss: 2.366488\n",
      "Validation loss decreased (2.366563 --> 2.366488).  Saving model ...\n",
      "Epoch: 4850 \tTraining Loss: 2.245776 \tValidation Loss: 2.366626\n",
      "Epoch: 4851 \tTraining Loss: 2.254883 \tValidation Loss: 2.366560\n",
      "Epoch: 4852 \tTraining Loss: 2.245125 \tValidation Loss: 2.366279\n",
      "Validation loss decreased (2.366488 --> 2.366279).  Saving model ...\n",
      "Epoch: 4853 \tTraining Loss: 2.254317 \tValidation Loss: 2.366462\n",
      "Epoch: 4854 \tTraining Loss: 2.257851 \tValidation Loss: 2.366568\n",
      "Epoch: 4855 \tTraining Loss: 2.238281 \tValidation Loss: 2.366321\n",
      "Epoch: 4856 \tTraining Loss: 2.259664 \tValidation Loss: 2.366307\n",
      "Epoch: 4857 \tTraining Loss: 2.233316 \tValidation Loss: 2.366152\n",
      "Validation loss decreased (2.366279 --> 2.366152).  Saving model ...\n",
      "Epoch: 4858 \tTraining Loss: 2.269789 \tValidation Loss: 2.366129\n",
      "Validation loss decreased (2.366152 --> 2.366129).  Saving model ...\n",
      "Epoch: 4859 \tTraining Loss: 2.267025 \tValidation Loss: 2.365961\n",
      "Validation loss decreased (2.366129 --> 2.365961).  Saving model ...\n",
      "Epoch: 4860 \tTraining Loss: 2.220804 \tValidation Loss: 2.365836\n",
      "Validation loss decreased (2.365961 --> 2.365836).  Saving model ...\n",
      "Epoch: 4861 \tTraining Loss: 2.247245 \tValidation Loss: 2.366008\n",
      "Epoch: 4862 \tTraining Loss: 2.260173 \tValidation Loss: 2.365969\n",
      "Epoch: 4863 \tTraining Loss: 2.232794 \tValidation Loss: 2.365972\n",
      "Epoch: 4864 \tTraining Loss: 2.260647 \tValidation Loss: 2.365738\n",
      "Validation loss decreased (2.365836 --> 2.365738).  Saving model ...\n",
      "Epoch: 4865 \tTraining Loss: 2.253877 \tValidation Loss: 2.365760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4866 \tTraining Loss: 2.248221 \tValidation Loss: 2.365410\n",
      "Validation loss decreased (2.365738 --> 2.365410).  Saving model ...\n",
      "Epoch: 4867 \tTraining Loss: 2.262552 \tValidation Loss: 2.365320\n",
      "Validation loss decreased (2.365410 --> 2.365320).  Saving model ...\n",
      "Epoch: 4868 \tTraining Loss: 2.241600 \tValidation Loss: 2.365197\n",
      "Validation loss decreased (2.365320 --> 2.365197).  Saving model ...\n",
      "Epoch: 4869 \tTraining Loss: 2.272723 \tValidation Loss: 2.365137\n",
      "Validation loss decreased (2.365197 --> 2.365137).  Saving model ...\n",
      "Epoch: 4870 \tTraining Loss: 2.260221 \tValidation Loss: 2.365241\n",
      "Epoch: 4871 \tTraining Loss: 2.250389 \tValidation Loss: 2.365353\n",
      "Epoch: 4872 \tTraining Loss: 2.261390 \tValidation Loss: 2.365585\n",
      "Epoch: 4873 \tTraining Loss: 2.251994 \tValidation Loss: 2.365605\n",
      "Epoch: 4874 \tTraining Loss: 2.264745 \tValidation Loss: 2.365694\n",
      "Epoch: 4875 \tTraining Loss: 2.267178 \tValidation Loss: 2.365929\n",
      "Epoch: 4876 \tTraining Loss: 2.260044 \tValidation Loss: 2.366005\n",
      "Epoch: 4877 \tTraining Loss: 2.250149 \tValidation Loss: 2.366078\n",
      "Epoch: 4878 \tTraining Loss: 2.246786 \tValidation Loss: 2.365970\n",
      "Epoch: 4879 \tTraining Loss: 2.257153 \tValidation Loss: 2.365997\n",
      "Epoch: 4880 \tTraining Loss: 2.254053 \tValidation Loss: 2.366005\n",
      "Epoch: 4881 \tTraining Loss: 2.241592 \tValidation Loss: 2.365978\n",
      "Epoch: 4882 \tTraining Loss: 2.251282 \tValidation Loss: 2.365895\n",
      "Epoch: 4883 \tTraining Loss: 2.232976 \tValidation Loss: 2.365804\n",
      "Epoch: 4884 \tTraining Loss: 2.274516 \tValidation Loss: 2.365857\n",
      "Epoch: 4885 \tTraining Loss: 2.248178 \tValidation Loss: 2.365871\n",
      "Epoch: 4886 \tTraining Loss: 2.247210 \tValidation Loss: 2.365894\n",
      "Epoch: 4887 \tTraining Loss: 2.271078 \tValidation Loss: 2.365967\n",
      "Epoch: 4888 \tTraining Loss: 2.262046 \tValidation Loss: 2.366063\n",
      "Epoch: 4889 \tTraining Loss: 2.251616 \tValidation Loss: 2.366113\n",
      "Epoch: 4890 \tTraining Loss: 2.258863 \tValidation Loss: 2.366068\n",
      "Epoch: 4891 \tTraining Loss: 2.254871 \tValidation Loss: 2.365843\n",
      "Epoch: 4892 \tTraining Loss: 2.251195 \tValidation Loss: 2.365847\n",
      "Epoch: 4893 \tTraining Loss: 2.255741 \tValidation Loss: 2.365802\n",
      "Epoch: 4894 \tTraining Loss: 2.261738 \tValidation Loss: 2.365840\n",
      "Epoch: 4895 \tTraining Loss: 2.254950 \tValidation Loss: 2.365708\n",
      "Epoch: 4896 \tTraining Loss: 2.248354 \tValidation Loss: 2.365634\n",
      "Epoch: 4897 \tTraining Loss: 2.249905 \tValidation Loss: 2.365487\n",
      "Epoch: 4898 \tTraining Loss: 2.257385 \tValidation Loss: 2.365732\n",
      "Epoch: 4899 \tTraining Loss: 2.235081 \tValidation Loss: 2.365543\n",
      "Epoch: 4900 \tTraining Loss: 2.242059 \tValidation Loss: 2.365561\n",
      "Epoch: 4901 \tTraining Loss: 2.242779 \tValidation Loss: 2.365508\n",
      "Epoch: 4902 \tTraining Loss: 2.247865 \tValidation Loss: 2.365414\n",
      "Epoch: 4903 \tTraining Loss: 2.242522 \tValidation Loss: 2.365357\n",
      "Epoch: 4904 \tTraining Loss: 2.261908 \tValidation Loss: 2.365565\n",
      "Epoch: 4905 \tTraining Loss: 2.259849 \tValidation Loss: 2.365510\n",
      "Epoch: 4906 \tTraining Loss: 2.257072 \tValidation Loss: 2.365558\n",
      "Epoch: 4907 \tTraining Loss: 2.250175 \tValidation Loss: 2.365619\n",
      "Epoch: 4908 \tTraining Loss: 2.267362 \tValidation Loss: 2.365657\n",
      "Epoch: 4909 \tTraining Loss: 2.247901 \tValidation Loss: 2.365475\n",
      "Epoch: 4910 \tTraining Loss: 2.262781 \tValidation Loss: 2.365475\n",
      "Epoch: 4911 \tTraining Loss: 2.243121 \tValidation Loss: 2.365323\n",
      "Epoch: 4912 \tTraining Loss: 2.255017 \tValidation Loss: 2.365485\n",
      "Epoch: 4913 \tTraining Loss: 2.257375 \tValidation Loss: 2.365511\n",
      "Epoch: 4914 \tTraining Loss: 2.250969 \tValidation Loss: 2.365476\n",
      "Epoch: 4915 \tTraining Loss: 2.244184 \tValidation Loss: 2.365368\n",
      "Epoch: 4916 \tTraining Loss: 2.239477 \tValidation Loss: 2.365292\n",
      "Epoch: 4917 \tTraining Loss: 2.249078 \tValidation Loss: 2.365294\n",
      "Epoch: 4918 \tTraining Loss: 2.262136 \tValidation Loss: 2.365294\n",
      "Epoch: 4919 \tTraining Loss: 2.249408 \tValidation Loss: 2.365177\n",
      "Epoch: 4920 \tTraining Loss: 2.242258 \tValidation Loss: 2.364904\n",
      "Validation loss decreased (2.365137 --> 2.364904).  Saving model ...\n",
      "Epoch: 4921 \tTraining Loss: 2.237466 \tValidation Loss: 2.364676\n",
      "Validation loss decreased (2.364904 --> 2.364676).  Saving model ...\n",
      "Epoch: 4922 \tTraining Loss: 2.259103 \tValidation Loss: 2.364652\n",
      "Validation loss decreased (2.364676 --> 2.364652).  Saving model ...\n",
      "Epoch: 4923 \tTraining Loss: 2.249466 \tValidation Loss: 2.364572\n",
      "Validation loss decreased (2.364652 --> 2.364572).  Saving model ...\n",
      "Epoch: 4924 \tTraining Loss: 2.244940 \tValidation Loss: 2.364601\n",
      "Epoch: 4925 \tTraining Loss: 2.240071 \tValidation Loss: 2.364428\n",
      "Validation loss decreased (2.364572 --> 2.364428).  Saving model ...\n",
      "Epoch: 4926 \tTraining Loss: 2.250664 \tValidation Loss: 2.364463\n",
      "Epoch: 4927 \tTraining Loss: 2.248650 \tValidation Loss: 2.364605\n",
      "Epoch: 4928 \tTraining Loss: 2.256502 \tValidation Loss: 2.364662\n",
      "Epoch: 4929 \tTraining Loss: 2.242842 \tValidation Loss: 2.364608\n",
      "Epoch: 4930 \tTraining Loss: 2.261721 \tValidation Loss: 2.364590\n",
      "Epoch: 4931 \tTraining Loss: 2.257263 \tValidation Loss: 2.364478\n",
      "Epoch: 4932 \tTraining Loss: 2.261359 \tValidation Loss: 2.364527\n",
      "Epoch: 4933 \tTraining Loss: 2.257216 \tValidation Loss: 2.364580\n",
      "Epoch: 4934 \tTraining Loss: 2.267365 \tValidation Loss: 2.364703\n",
      "Epoch: 4935 \tTraining Loss: 2.251523 \tValidation Loss: 2.364939\n",
      "Epoch: 4936 \tTraining Loss: 2.247245 \tValidation Loss: 2.364810\n",
      "Epoch: 4937 \tTraining Loss: 2.256492 \tValidation Loss: 2.364769\n",
      "Epoch: 4938 \tTraining Loss: 2.268974 \tValidation Loss: 2.364652\n",
      "Epoch: 4939 \tTraining Loss: 2.246612 \tValidation Loss: 2.364647\n",
      "Epoch: 4940 \tTraining Loss: 2.257323 \tValidation Loss: 2.364777\n",
      "Epoch: 4941 \tTraining Loss: 2.240826 \tValidation Loss: 2.364702\n",
      "Epoch: 4942 \tTraining Loss: 2.245567 \tValidation Loss: 2.364680\n",
      "Epoch: 4943 \tTraining Loss: 2.243781 \tValidation Loss: 2.364767\n",
      "Epoch: 4944 \tTraining Loss: 2.254282 \tValidation Loss: 2.364676\n",
      "Epoch: 4945 \tTraining Loss: 2.234079 \tValidation Loss: 2.364524\n",
      "Epoch: 4946 \tTraining Loss: 2.222375 \tValidation Loss: 2.364456\n",
      "Epoch: 4947 \tTraining Loss: 2.250021 \tValidation Loss: 2.364413\n",
      "Validation loss decreased (2.364428 --> 2.364413).  Saving model ...\n",
      "Epoch: 4948 \tTraining Loss: 2.242424 \tValidation Loss: 2.364350\n",
      "Validation loss decreased (2.364413 --> 2.364350).  Saving model ...\n",
      "Epoch: 4949 \tTraining Loss: 2.252164 \tValidation Loss: 2.364308\n",
      "Validation loss decreased (2.364350 --> 2.364308).  Saving model ...\n",
      "Epoch: 4950 \tTraining Loss: 2.239640 \tValidation Loss: 2.364507\n",
      "Epoch: 4951 \tTraining Loss: 2.252558 \tValidation Loss: 2.364449\n",
      "Epoch: 4952 \tTraining Loss: 2.255111 \tValidation Loss: 2.364380\n",
      "Epoch: 4953 \tTraining Loss: 2.251053 \tValidation Loss: 2.364471\n",
      "Epoch: 4954 \tTraining Loss: 2.240924 \tValidation Loss: 2.364283\n",
      "Validation loss decreased (2.364308 --> 2.364283).  Saving model ...\n",
      "Epoch: 4955 \tTraining Loss: 2.254325 \tValidation Loss: 2.364313\n",
      "Epoch: 4956 \tTraining Loss: 2.231241 \tValidation Loss: 2.364372\n",
      "Epoch: 4957 \tTraining Loss: 2.258473 \tValidation Loss: 2.364279\n",
      "Validation loss decreased (2.364283 --> 2.364279).  Saving model ...\n",
      "Epoch: 4958 \tTraining Loss: 2.246064 \tValidation Loss: 2.364079\n",
      "Validation loss decreased (2.364279 --> 2.364079).  Saving model ...\n",
      "Epoch: 4959 \tTraining Loss: 2.240371 \tValidation Loss: 2.364087\n",
      "Epoch: 4960 \tTraining Loss: 2.238622 \tValidation Loss: 2.364086\n",
      "Epoch: 4961 \tTraining Loss: 2.236508 \tValidation Loss: 2.364066\n",
      "Validation loss decreased (2.364079 --> 2.364066).  Saving model ...\n",
      "Epoch: 4962 \tTraining Loss: 2.243251 \tValidation Loss: 2.364066\n",
      "Epoch: 4963 \tTraining Loss: 2.264117 \tValidation Loss: 2.364155\n",
      "Epoch: 4964 \tTraining Loss: 2.236613 \tValidation Loss: 2.364123\n",
      "Epoch: 4965 \tTraining Loss: 2.222679 \tValidation Loss: 2.363907\n",
      "Validation loss decreased (2.364066 --> 2.363907).  Saving model ...\n",
      "Epoch: 4966 \tTraining Loss: 2.252027 \tValidation Loss: 2.363881\n",
      "Validation loss decreased (2.363907 --> 2.363881).  Saving model ...\n",
      "Epoch: 4967 \tTraining Loss: 2.223106 \tValidation Loss: 2.363726\n",
      "Validation loss decreased (2.363881 --> 2.363726).  Saving model ...\n",
      "Epoch: 4968 \tTraining Loss: 2.239958 \tValidation Loss: 2.363749\n",
      "Epoch: 4969 \tTraining Loss: 2.242035 \tValidation Loss: 2.363668\n",
      "Validation loss decreased (2.363726 --> 2.363668).  Saving model ...\n",
      "Epoch: 4970 \tTraining Loss: 2.250061 \tValidation Loss: 2.363910\n",
      "Epoch: 4971 \tTraining Loss: 2.249072 \tValidation Loss: 2.363917\n",
      "Epoch: 4972 \tTraining Loss: 2.247549 \tValidation Loss: 2.363981\n",
      "Epoch: 4973 \tTraining Loss: 2.239104 \tValidation Loss: 2.363823\n",
      "Epoch: 4974 \tTraining Loss: 2.242418 \tValidation Loss: 2.363559\n",
      "Validation loss decreased (2.363668 --> 2.363559).  Saving model ...\n",
      "Epoch: 4975 \tTraining Loss: 2.248238 \tValidation Loss: 2.363837\n",
      "Epoch: 4976 \tTraining Loss: 2.244901 \tValidation Loss: 2.363786\n",
      "Epoch: 4977 \tTraining Loss: 2.249237 \tValidation Loss: 2.363787\n",
      "Epoch: 4978 \tTraining Loss: 2.246344 \tValidation Loss: 2.363859\n",
      "Epoch: 4979 \tTraining Loss: 2.241825 \tValidation Loss: 2.363699\n",
      "Epoch: 4980 \tTraining Loss: 2.247050 \tValidation Loss: 2.363964\n",
      "Epoch: 4981 \tTraining Loss: 2.257628 \tValidation Loss: 2.364091\n",
      "Epoch: 4982 \tTraining Loss: 2.245924 \tValidation Loss: 2.363991\n",
      "Epoch: 4983 \tTraining Loss: 2.247606 \tValidation Loss: 2.363823\n",
      "Epoch: 4984 \tTraining Loss: 2.235701 \tValidation Loss: 2.363681\n",
      "Epoch: 4985 \tTraining Loss: 2.258692 \tValidation Loss: 2.363685\n",
      "Epoch: 4986 \tTraining Loss: 2.215232 \tValidation Loss: 2.363503\n",
      "Validation loss decreased (2.363559 --> 2.363503).  Saving model ...\n",
      "Epoch: 4987 \tTraining Loss: 2.255059 \tValidation Loss: 2.363636\n",
      "Epoch: 4988 \tTraining Loss: 2.248009 \tValidation Loss: 2.363573\n",
      "Epoch: 4989 \tTraining Loss: 2.245012 \tValidation Loss: 2.363544\n",
      "Epoch: 4990 \tTraining Loss: 2.241867 \tValidation Loss: 2.363638\n",
      "Epoch: 4991 \tTraining Loss: 2.241270 \tValidation Loss: 2.363687\n",
      "Epoch: 4992 \tTraining Loss: 2.252976 \tValidation Loss: 2.363593\n",
      "Epoch: 4993 \tTraining Loss: 2.236283 \tValidation Loss: 2.363541\n",
      "Epoch: 4994 \tTraining Loss: 2.243448 \tValidation Loss: 2.363499\n",
      "Validation loss decreased (2.363503 --> 2.363499).  Saving model ...\n",
      "Epoch: 4995 \tTraining Loss: 2.250779 \tValidation Loss: 2.363543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4996 \tTraining Loss: 2.232507 \tValidation Loss: 2.363592\n",
      "Epoch: 4997 \tTraining Loss: 2.246177 \tValidation Loss: 2.363436\n",
      "Validation loss decreased (2.363499 --> 2.363436).  Saving model ...\n",
      "Epoch: 4998 \tTraining Loss: 2.257093 \tValidation Loss: 2.363379\n",
      "Validation loss decreased (2.363436 --> 2.363379).  Saving model ...\n",
      "Epoch: 4999 \tTraining Loss: 2.237417 \tValidation Loss: 2.363378\n",
      "Validation loss decreased (2.363379 --> 2.363378).  Saving model ...\n",
      "Epoch: 5000 \tTraining Loss: 2.238280 \tValidation Loss: 2.363320\n",
      "Validation loss decreased (2.363378 --> 2.363320).  Saving model ...\n",
      "Epoch: 5001 \tTraining Loss: 2.239387 \tValidation Loss: 2.363156\n",
      "Validation loss decreased (2.363320 --> 2.363156).  Saving model ...\n",
      "Epoch: 5002 \tTraining Loss: 2.237255 \tValidation Loss: 2.363039\n",
      "Validation loss decreased (2.363156 --> 2.363039).  Saving model ...\n",
      "Epoch: 5003 \tTraining Loss: 2.223516 \tValidation Loss: 2.363191\n",
      "Epoch: 5004 \tTraining Loss: 2.267187 \tValidation Loss: 2.363173\n",
      "Epoch: 5005 \tTraining Loss: 2.251805 \tValidation Loss: 2.363068\n",
      "Epoch: 5006 \tTraining Loss: 2.252572 \tValidation Loss: 2.363062\n",
      "Epoch: 5007 \tTraining Loss: 2.237664 \tValidation Loss: 2.363017\n",
      "Validation loss decreased (2.363039 --> 2.363017).  Saving model ...\n",
      "Epoch: 5008 \tTraining Loss: 2.254216 \tValidation Loss: 2.363086\n",
      "Epoch: 5009 \tTraining Loss: 2.238240 \tValidation Loss: 2.363016\n",
      "Validation loss decreased (2.363017 --> 2.363016).  Saving model ...\n",
      "Epoch: 5010 \tTraining Loss: 2.237645 \tValidation Loss: 2.363033\n",
      "Epoch: 5011 \tTraining Loss: 2.240465 \tValidation Loss: 2.362977\n",
      "Validation loss decreased (2.363016 --> 2.362977).  Saving model ...\n",
      "Epoch: 5012 \tTraining Loss: 2.244779 \tValidation Loss: 2.362994\n",
      "Epoch: 5013 \tTraining Loss: 2.228563 \tValidation Loss: 2.363031\n",
      "Epoch: 5014 \tTraining Loss: 2.265361 \tValidation Loss: 2.363223\n",
      "Epoch: 5015 \tTraining Loss: 2.230850 \tValidation Loss: 2.363222\n",
      "Epoch: 5016 \tTraining Loss: 2.241820 \tValidation Loss: 2.363191\n",
      "Epoch: 5017 \tTraining Loss: 2.246367 \tValidation Loss: 2.362907\n",
      "Validation loss decreased (2.362977 --> 2.362907).  Saving model ...\n",
      "Epoch: 5018 \tTraining Loss: 2.238252 \tValidation Loss: 2.362924\n",
      "Epoch: 5019 \tTraining Loss: 2.233939 \tValidation Loss: 2.362755\n",
      "Validation loss decreased (2.362907 --> 2.362755).  Saving model ...\n",
      "Epoch: 5020 \tTraining Loss: 2.266787 \tValidation Loss: 2.362824\n",
      "Epoch: 5021 \tTraining Loss: 2.235165 \tValidation Loss: 2.362605\n",
      "Validation loss decreased (2.362755 --> 2.362605).  Saving model ...\n",
      "Epoch: 5022 \tTraining Loss: 2.238400 \tValidation Loss: 2.362364\n",
      "Validation loss decreased (2.362605 --> 2.362364).  Saving model ...\n",
      "Epoch: 5023 \tTraining Loss: 2.234921 \tValidation Loss: 2.362448\n",
      "Epoch: 5024 \tTraining Loss: 2.250054 \tValidation Loss: 2.362266\n",
      "Validation loss decreased (2.362364 --> 2.362266).  Saving model ...\n",
      "Epoch: 5025 \tTraining Loss: 2.240914 \tValidation Loss: 2.362280\n",
      "Epoch: 5026 \tTraining Loss: 2.229504 \tValidation Loss: 2.362134\n",
      "Validation loss decreased (2.362266 --> 2.362134).  Saving model ...\n",
      "Epoch: 5027 \tTraining Loss: 2.230081 \tValidation Loss: 2.361933\n",
      "Validation loss decreased (2.362134 --> 2.361933).  Saving model ...\n",
      "Epoch: 5028 \tTraining Loss: 2.244497 \tValidation Loss: 2.361872\n",
      "Validation loss decreased (2.361933 --> 2.361872).  Saving model ...\n",
      "Epoch: 5029 \tTraining Loss: 2.247078 \tValidation Loss: 2.361795\n",
      "Validation loss decreased (2.361872 --> 2.361795).  Saving model ...\n",
      "Epoch: 5030 \tTraining Loss: 2.250301 \tValidation Loss: 2.361733\n",
      "Validation loss decreased (2.361795 --> 2.361733).  Saving model ...\n",
      "Epoch: 5031 \tTraining Loss: 2.234720 \tValidation Loss: 2.361848\n",
      "Epoch: 5032 \tTraining Loss: 2.239383 \tValidation Loss: 2.361827\n",
      "Epoch: 5033 \tTraining Loss: 2.231532 \tValidation Loss: 2.361706\n",
      "Validation loss decreased (2.361733 --> 2.361706).  Saving model ...\n",
      "Epoch: 5034 \tTraining Loss: 2.231762 \tValidation Loss: 2.361618\n",
      "Validation loss decreased (2.361706 --> 2.361618).  Saving model ...\n",
      "Epoch: 5035 \tTraining Loss: 2.267867 \tValidation Loss: 2.361738\n",
      "Epoch: 5036 \tTraining Loss: 2.238877 \tValidation Loss: 2.361738\n",
      "Epoch: 5037 \tTraining Loss: 2.219427 \tValidation Loss: 2.361677\n",
      "Epoch: 5038 \tTraining Loss: 2.234974 \tValidation Loss: 2.361586\n",
      "Validation loss decreased (2.361618 --> 2.361586).  Saving model ...\n",
      "Epoch: 5039 \tTraining Loss: 2.259180 \tValidation Loss: 2.361618\n",
      "Epoch: 5040 \tTraining Loss: 2.247846 \tValidation Loss: 2.361533\n",
      "Validation loss decreased (2.361586 --> 2.361533).  Saving model ...\n",
      "Epoch: 5041 \tTraining Loss: 2.237333 \tValidation Loss: 2.361572\n",
      "Epoch: 5042 \tTraining Loss: 2.236695 \tValidation Loss: 2.361598\n",
      "Epoch: 5043 \tTraining Loss: 2.240887 \tValidation Loss: 2.361628\n",
      "Epoch: 5044 \tTraining Loss: 2.244540 \tValidation Loss: 2.361683\n",
      "Epoch: 5045 \tTraining Loss: 2.232890 \tValidation Loss: 2.361596\n",
      "Epoch: 5046 \tTraining Loss: 2.223482 \tValidation Loss: 2.361750\n",
      "Epoch: 5047 \tTraining Loss: 2.240451 \tValidation Loss: 2.362011\n",
      "Epoch: 5048 \tTraining Loss: 2.233736 \tValidation Loss: 2.362013\n",
      "Epoch: 5049 \tTraining Loss: 2.243960 \tValidation Loss: 2.361964\n",
      "Epoch: 5050 \tTraining Loss: 2.246935 \tValidation Loss: 2.362093\n",
      "Epoch: 5051 \tTraining Loss: 2.274445 \tValidation Loss: 2.362079\n",
      "Epoch: 5052 \tTraining Loss: 2.232229 \tValidation Loss: 2.362197\n",
      "Epoch: 5053 \tTraining Loss: 2.243983 \tValidation Loss: 2.362374\n",
      "Epoch: 5054 \tTraining Loss: 2.246604 \tValidation Loss: 2.362481\n",
      "Epoch: 5055 \tTraining Loss: 2.249614 \tValidation Loss: 2.362532\n",
      "Epoch: 5056 \tTraining Loss: 2.245120 \tValidation Loss: 2.362590\n",
      "Epoch: 5057 \tTraining Loss: 2.247447 \tValidation Loss: 2.362409\n",
      "Epoch: 5058 \tTraining Loss: 2.241482 \tValidation Loss: 2.362339\n",
      "Epoch: 5059 \tTraining Loss: 2.241916 \tValidation Loss: 2.362442\n",
      "Epoch: 5060 \tTraining Loss: 2.254417 \tValidation Loss: 2.362340\n",
      "Epoch: 5061 \tTraining Loss: 2.238762 \tValidation Loss: 2.362387\n",
      "Epoch: 5062 \tTraining Loss: 2.246000 \tValidation Loss: 2.362400\n",
      "Epoch: 5063 \tTraining Loss: 2.243858 \tValidation Loss: 2.362272\n",
      "Epoch: 5064 \tTraining Loss: 2.227754 \tValidation Loss: 2.362154\n",
      "Epoch: 5065 \tTraining Loss: 2.247709 \tValidation Loss: 2.362284\n",
      "Epoch: 5066 \tTraining Loss: 2.230321 \tValidation Loss: 2.362206\n",
      "Epoch: 5067 \tTraining Loss: 2.225057 \tValidation Loss: 2.362067\n",
      "Epoch: 5068 \tTraining Loss: 2.243443 \tValidation Loss: 2.362140\n",
      "Epoch: 5069 \tTraining Loss: 2.240099 \tValidation Loss: 2.362054\n",
      "Epoch: 5070 \tTraining Loss: 2.234144 \tValidation Loss: 2.361956\n",
      "Epoch: 5071 \tTraining Loss: 2.257400 \tValidation Loss: 2.361961\n",
      "Epoch: 5072 \tTraining Loss: 2.239770 \tValidation Loss: 2.361844\n",
      "Epoch: 5073 \tTraining Loss: 2.230380 \tValidation Loss: 2.361886\n",
      "Epoch: 5074 \tTraining Loss: 2.240429 \tValidation Loss: 2.361931\n",
      "Epoch: 5075 \tTraining Loss: 2.232364 \tValidation Loss: 2.361928\n",
      "Epoch: 5076 \tTraining Loss: 2.227952 \tValidation Loss: 2.361719\n",
      "Epoch: 5077 \tTraining Loss: 2.251209 \tValidation Loss: 2.361373\n",
      "Validation loss decreased (2.361533 --> 2.361373).  Saving model ...\n",
      "Epoch: 5078 \tTraining Loss: 2.249330 \tValidation Loss: 2.361519\n",
      "Epoch: 5079 \tTraining Loss: 2.235170 \tValidation Loss: 2.361584\n",
      "Epoch: 5080 \tTraining Loss: 2.228549 \tValidation Loss: 2.361559\n",
      "Epoch: 5081 \tTraining Loss: 2.241260 \tValidation Loss: 2.361574\n",
      "Epoch: 5082 \tTraining Loss: 2.239610 \tValidation Loss: 2.361533\n",
      "Epoch: 5083 \tTraining Loss: 2.258935 \tValidation Loss: 2.361496\n",
      "Epoch: 5084 \tTraining Loss: 2.238719 \tValidation Loss: 2.361439\n",
      "Epoch: 5085 \tTraining Loss: 2.238882 \tValidation Loss: 2.361467\n",
      "Epoch: 5086 \tTraining Loss: 2.228330 \tValidation Loss: 2.361274\n",
      "Validation loss decreased (2.361373 --> 2.361274).  Saving model ...\n",
      "Epoch: 5087 \tTraining Loss: 2.240083 \tValidation Loss: 2.361023\n",
      "Validation loss decreased (2.361274 --> 2.361023).  Saving model ...\n",
      "Epoch: 5088 \tTraining Loss: 2.241454 \tValidation Loss: 2.360938\n",
      "Validation loss decreased (2.361023 --> 2.360938).  Saving model ...\n",
      "Epoch: 5089 \tTraining Loss: 2.234399 \tValidation Loss: 2.361138\n",
      "Epoch: 5090 \tTraining Loss: 2.243260 \tValidation Loss: 2.361154\n",
      "Epoch: 5091 \tTraining Loss: 2.223685 \tValidation Loss: 2.361058\n",
      "Epoch: 5092 \tTraining Loss: 2.236975 \tValidation Loss: 2.361345\n",
      "Epoch: 5093 \tTraining Loss: 2.228463 \tValidation Loss: 2.361383\n",
      "Epoch: 5094 \tTraining Loss: 2.235427 \tValidation Loss: 2.361615\n",
      "Epoch: 5095 \tTraining Loss: 2.247829 \tValidation Loss: 2.361807\n",
      "Epoch: 5096 \tTraining Loss: 2.239404 \tValidation Loss: 2.361770\n",
      "Epoch: 5097 \tTraining Loss: 2.238247 \tValidation Loss: 2.361846\n",
      "Epoch: 5098 \tTraining Loss: 2.251905 \tValidation Loss: 2.361814\n",
      "Epoch: 5099 \tTraining Loss: 2.243107 \tValidation Loss: 2.361600\n",
      "Epoch: 5100 \tTraining Loss: 2.232344 \tValidation Loss: 2.361454\n",
      "Epoch: 5101 \tTraining Loss: 2.260934 \tValidation Loss: 2.361391\n",
      "Epoch: 5102 \tTraining Loss: 2.232167 \tValidation Loss: 2.361356\n",
      "Epoch: 5103 \tTraining Loss: 2.225833 \tValidation Loss: 2.361213\n",
      "Epoch: 5104 \tTraining Loss: 2.235082 \tValidation Loss: 2.361103\n",
      "Epoch: 5105 \tTraining Loss: 2.227047 \tValidation Loss: 2.361307\n",
      "Epoch: 5106 \tTraining Loss: 2.245352 \tValidation Loss: 2.361461\n",
      "Epoch: 5107 \tTraining Loss: 2.230801 \tValidation Loss: 2.361220\n",
      "Epoch: 5108 \tTraining Loss: 2.218287 \tValidation Loss: 2.361359\n",
      "Epoch: 5109 \tTraining Loss: 2.239435 \tValidation Loss: 2.361395\n",
      "Epoch: 5110 \tTraining Loss: 2.230235 \tValidation Loss: 2.361308\n",
      "Epoch: 5111 \tTraining Loss: 2.234077 \tValidation Loss: 2.361367\n",
      "Epoch: 5112 \tTraining Loss: 2.227095 \tValidation Loss: 2.361231\n",
      "Epoch: 5113 \tTraining Loss: 2.237928 \tValidation Loss: 2.361178\n",
      "Epoch: 5114 \tTraining Loss: 2.259973 \tValidation Loss: 2.361162\n",
      "Epoch: 5115 \tTraining Loss: 2.225844 \tValidation Loss: 2.361054\n",
      "Epoch: 5116 \tTraining Loss: 2.244096 \tValidation Loss: 2.361056\n",
      "Epoch: 5117 \tTraining Loss: 2.236329 \tValidation Loss: 2.361185\n",
      "Epoch: 5118 \tTraining Loss: 2.246357 \tValidation Loss: 2.361273\n",
      "Epoch: 5119 \tTraining Loss: 2.229035 \tValidation Loss: 2.361202\n",
      "Epoch: 5120 \tTraining Loss: 2.249499 \tValidation Loss: 2.361238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5121 \tTraining Loss: 2.246219 \tValidation Loss: 2.361011\n",
      "Epoch: 5122 \tTraining Loss: 2.250729 \tValidation Loss: 2.361152\n",
      "Epoch: 5123 \tTraining Loss: 2.260019 \tValidation Loss: 2.361185\n",
      "Epoch: 5124 \tTraining Loss: 2.241373 \tValidation Loss: 2.361276\n",
      "Epoch: 5125 \tTraining Loss: 2.234083 \tValidation Loss: 2.361434\n",
      "Epoch: 5126 \tTraining Loss: 2.245907 \tValidation Loss: 2.361542\n",
      "Epoch: 5127 \tTraining Loss: 2.238039 \tValidation Loss: 2.361527\n",
      "Epoch: 5128 \tTraining Loss: 2.219029 \tValidation Loss: 2.361486\n",
      "Epoch: 5129 \tTraining Loss: 2.252015 \tValidation Loss: 2.361381\n",
      "Epoch: 5130 \tTraining Loss: 2.242292 \tValidation Loss: 2.361292\n",
      "Epoch: 5131 \tTraining Loss: 2.252017 \tValidation Loss: 2.361367\n",
      "Epoch: 5132 \tTraining Loss: 2.229753 \tValidation Loss: 2.361249\n",
      "Epoch: 5133 \tTraining Loss: 2.240354 \tValidation Loss: 2.361314\n",
      "Epoch: 5134 \tTraining Loss: 2.253458 \tValidation Loss: 2.361538\n",
      "Epoch: 5135 \tTraining Loss: 2.243787 \tValidation Loss: 2.361529\n",
      "Epoch: 5136 \tTraining Loss: 2.233600 \tValidation Loss: 2.361583\n",
      "Epoch: 5137 \tTraining Loss: 2.240241 \tValidation Loss: 2.361542\n",
      "Epoch: 5138 \tTraining Loss: 2.222746 \tValidation Loss: 2.361516\n",
      "Epoch: 5139 \tTraining Loss: 2.244353 \tValidation Loss: 2.361526\n",
      "Epoch: 5140 \tTraining Loss: 2.236018 \tValidation Loss: 2.361454\n",
      "Epoch: 5141 \tTraining Loss: 2.230339 \tValidation Loss: 2.361414\n",
      "Epoch: 5142 \tTraining Loss: 2.239931 \tValidation Loss: 2.361303\n",
      "Epoch: 5143 \tTraining Loss: 2.245709 \tValidation Loss: 2.361282\n",
      "Epoch: 5144 \tTraining Loss: 2.220514 \tValidation Loss: 2.361219\n",
      "Epoch: 5145 \tTraining Loss: 2.242441 \tValidation Loss: 2.361017\n",
      "Epoch: 5146 \tTraining Loss: 2.228643 \tValidation Loss: 2.361095\n",
      "Epoch: 5147 \tTraining Loss: 2.239416 \tValidation Loss: 2.361047\n",
      "Epoch: 5148 \tTraining Loss: 2.239253 \tValidation Loss: 2.361180\n",
      "Epoch: 5149 \tTraining Loss: 2.214241 \tValidation Loss: 2.361120\n",
      "Epoch: 5150 \tTraining Loss: 2.240218 \tValidation Loss: 2.361269\n",
      "Epoch: 5151 \tTraining Loss: 2.234755 \tValidation Loss: 2.361065\n",
      "Epoch: 5152 \tTraining Loss: 2.254574 \tValidation Loss: 2.361209\n",
      "Epoch: 5153 \tTraining Loss: 2.253505 \tValidation Loss: 2.361237\n",
      "Epoch: 5154 \tTraining Loss: 2.234806 \tValidation Loss: 2.361029\n",
      "Epoch: 5155 \tTraining Loss: 2.239797 \tValidation Loss: 2.361048\n",
      "Epoch: 5156 \tTraining Loss: 2.230517 \tValidation Loss: 2.360920\n",
      "Validation loss decreased (2.360938 --> 2.360920).  Saving model ...\n",
      "Epoch: 5157 \tTraining Loss: 2.244450 \tValidation Loss: 2.360813\n",
      "Validation loss decreased (2.360920 --> 2.360813).  Saving model ...\n",
      "Epoch: 5158 \tTraining Loss: 2.235689 \tValidation Loss: 2.360752\n",
      "Validation loss decreased (2.360813 --> 2.360752).  Saving model ...\n",
      "Epoch: 5159 \tTraining Loss: 2.227068 \tValidation Loss: 2.360669\n",
      "Validation loss decreased (2.360752 --> 2.360669).  Saving model ...\n",
      "Epoch: 5160 \tTraining Loss: 2.230631 \tValidation Loss: 2.360452\n",
      "Validation loss decreased (2.360669 --> 2.360452).  Saving model ...\n",
      "Epoch: 5161 \tTraining Loss: 2.219311 \tValidation Loss: 2.360435\n",
      "Validation loss decreased (2.360452 --> 2.360435).  Saving model ...\n",
      "Epoch: 5162 \tTraining Loss: 2.225536 \tValidation Loss: 2.360408\n",
      "Validation loss decreased (2.360435 --> 2.360408).  Saving model ...\n",
      "Epoch: 5163 \tTraining Loss: 2.229510 \tValidation Loss: 2.360527\n",
      "Epoch: 5164 \tTraining Loss: 2.248943 \tValidation Loss: 2.360682\n",
      "Epoch: 5165 \tTraining Loss: 2.210560 \tValidation Loss: 2.360767\n",
      "Epoch: 5166 \tTraining Loss: 2.233274 \tValidation Loss: 2.360743\n",
      "Epoch: 5167 \tTraining Loss: 2.241622 \tValidation Loss: 2.360682\n",
      "Epoch: 5168 \tTraining Loss: 2.236461 \tValidation Loss: 2.360764\n",
      "Epoch: 5169 \tTraining Loss: 2.234077 \tValidation Loss: 2.360799\n",
      "Epoch: 5170 \tTraining Loss: 2.247474 \tValidation Loss: 2.360939\n",
      "Epoch: 5171 \tTraining Loss: 2.239421 \tValidation Loss: 2.360969\n",
      "Epoch: 5172 \tTraining Loss: 2.215753 \tValidation Loss: 2.360881\n",
      "Epoch: 5173 \tTraining Loss: 2.224754 \tValidation Loss: 2.360994\n",
      "Epoch: 5174 \tTraining Loss: 2.239168 \tValidation Loss: 2.360916\n",
      "Epoch: 5175 \tTraining Loss: 2.225508 \tValidation Loss: 2.360746\n",
      "Epoch: 5176 \tTraining Loss: 2.238615 \tValidation Loss: 2.360931\n",
      "Epoch: 5177 \tTraining Loss: 2.229267 \tValidation Loss: 2.360978\n",
      "Epoch: 5178 \tTraining Loss: 2.234561 \tValidation Loss: 2.360965\n",
      "Epoch: 5179 \tTraining Loss: 2.235139 \tValidation Loss: 2.360790\n",
      "Epoch: 5180 \tTraining Loss: 2.248474 \tValidation Loss: 2.360671\n",
      "Epoch: 5181 \tTraining Loss: 2.218581 \tValidation Loss: 2.360566\n",
      "Epoch: 5182 \tTraining Loss: 2.246280 \tValidation Loss: 2.360408\n",
      "Validation loss decreased (2.360408 --> 2.360408).  Saving model ...\n",
      "Epoch: 5183 \tTraining Loss: 2.237102 \tValidation Loss: 2.360346\n",
      "Validation loss decreased (2.360408 --> 2.360346).  Saving model ...\n",
      "Epoch: 5184 \tTraining Loss: 2.237982 \tValidation Loss: 2.360051\n",
      "Validation loss decreased (2.360346 --> 2.360051).  Saving model ...\n",
      "Epoch: 5185 \tTraining Loss: 2.236120 \tValidation Loss: 2.359887\n",
      "Validation loss decreased (2.360051 --> 2.359887).  Saving model ...\n",
      "Epoch: 5186 \tTraining Loss: 2.226981 \tValidation Loss: 2.359835\n",
      "Validation loss decreased (2.359887 --> 2.359835).  Saving model ...\n",
      "Epoch: 5187 \tTraining Loss: 2.223857 \tValidation Loss: 2.359876\n",
      "Epoch: 5188 \tTraining Loss: 2.254457 \tValidation Loss: 2.360016\n",
      "Epoch: 5189 \tTraining Loss: 2.234368 \tValidation Loss: 2.360161\n",
      "Epoch: 5190 \tTraining Loss: 2.234928 \tValidation Loss: 2.360090\n",
      "Epoch: 5191 \tTraining Loss: 2.236442 \tValidation Loss: 2.360157\n",
      "Epoch: 5192 \tTraining Loss: 2.243495 \tValidation Loss: 2.360331\n",
      "Epoch: 5193 \tTraining Loss: 2.223140 \tValidation Loss: 2.360113\n",
      "Epoch: 5194 \tTraining Loss: 2.232237 \tValidation Loss: 2.360329\n",
      "Epoch: 5195 \tTraining Loss: 2.231783 \tValidation Loss: 2.360212\n",
      "Epoch: 5196 \tTraining Loss: 2.225542 \tValidation Loss: 2.359994\n",
      "Epoch: 5197 \tTraining Loss: 2.236756 \tValidation Loss: 2.359798\n",
      "Validation loss decreased (2.359835 --> 2.359798).  Saving model ...\n",
      "Epoch: 5198 \tTraining Loss: 2.231214 \tValidation Loss: 2.359818\n",
      "Epoch: 5199 \tTraining Loss: 2.231806 \tValidation Loss: 2.359615\n",
      "Validation loss decreased (2.359798 --> 2.359615).  Saving model ...\n",
      "Epoch: 5200 \tTraining Loss: 2.236297 \tValidation Loss: 2.359635\n",
      "Epoch: 5201 \tTraining Loss: 2.215771 \tValidation Loss: 2.359312\n",
      "Validation loss decreased (2.359615 --> 2.359312).  Saving model ...\n",
      "Epoch: 5202 \tTraining Loss: 2.228299 \tValidation Loss: 2.359323\n",
      "Epoch: 5203 \tTraining Loss: 2.230537 \tValidation Loss: 2.359425\n",
      "Epoch: 5204 \tTraining Loss: 2.223007 \tValidation Loss: 2.359441\n",
      "Epoch: 5205 \tTraining Loss: 2.247586 \tValidation Loss: 2.359576\n",
      "Epoch: 5206 \tTraining Loss: 2.228845 \tValidation Loss: 2.359587\n",
      "Epoch: 5207 \tTraining Loss: 2.241796 \tValidation Loss: 2.359537\n",
      "Epoch: 5208 \tTraining Loss: 2.228116 \tValidation Loss: 2.359464\n",
      "Epoch: 5209 \tTraining Loss: 2.218550 \tValidation Loss: 2.359455\n",
      "Epoch: 5210 \tTraining Loss: 2.252226 \tValidation Loss: 2.359428\n",
      "Epoch: 5211 \tTraining Loss: 2.218212 \tValidation Loss: 2.359496\n",
      "Epoch: 5212 \tTraining Loss: 2.233354 \tValidation Loss: 2.359296\n",
      "Validation loss decreased (2.359312 --> 2.359296).  Saving model ...\n",
      "Epoch: 5213 \tTraining Loss: 2.230843 \tValidation Loss: 2.359249\n",
      "Validation loss decreased (2.359296 --> 2.359249).  Saving model ...\n",
      "Epoch: 5214 \tTraining Loss: 2.240620 \tValidation Loss: 2.359190\n",
      "Validation loss decreased (2.359249 --> 2.359190).  Saving model ...\n",
      "Epoch: 5215 \tTraining Loss: 2.224713 \tValidation Loss: 2.359304\n",
      "Epoch: 5216 \tTraining Loss: 2.222783 \tValidation Loss: 2.359424\n",
      "Epoch: 5217 \tTraining Loss: 2.209543 \tValidation Loss: 2.359480\n",
      "Epoch: 5218 \tTraining Loss: 2.223516 \tValidation Loss: 2.359308\n",
      "Epoch: 5219 \tTraining Loss: 2.222521 \tValidation Loss: 2.359425\n",
      "Epoch: 5220 \tTraining Loss: 2.239558 \tValidation Loss: 2.359351\n",
      "Epoch: 5221 \tTraining Loss: 2.240219 \tValidation Loss: 2.359534\n",
      "Epoch: 5222 \tTraining Loss: 2.244687 \tValidation Loss: 2.359355\n",
      "Epoch: 5223 \tTraining Loss: 2.232193 \tValidation Loss: 2.359196\n",
      "Epoch: 5224 \tTraining Loss: 2.217147 \tValidation Loss: 2.358940\n",
      "Validation loss decreased (2.359190 --> 2.358940).  Saving model ...\n",
      "Epoch: 5225 \tTraining Loss: 2.231992 \tValidation Loss: 2.358920\n",
      "Validation loss decreased (2.358940 --> 2.358920).  Saving model ...\n",
      "Epoch: 5226 \tTraining Loss: 2.241352 \tValidation Loss: 2.358830\n",
      "Validation loss decreased (2.358920 --> 2.358830).  Saving model ...\n",
      "Epoch: 5227 \tTraining Loss: 2.227500 \tValidation Loss: 2.358714\n",
      "Validation loss decreased (2.358830 --> 2.358714).  Saving model ...\n",
      "Epoch: 5228 \tTraining Loss: 2.226459 \tValidation Loss: 2.359013\n",
      "Epoch: 5229 \tTraining Loss: 2.233620 \tValidation Loss: 2.359014\n",
      "Epoch: 5230 \tTraining Loss: 2.223628 \tValidation Loss: 2.358840\n",
      "Epoch: 5231 \tTraining Loss: 2.222834 \tValidation Loss: 2.358780\n",
      "Epoch: 5232 \tTraining Loss: 2.217859 \tValidation Loss: 2.358854\n",
      "Epoch: 5233 \tTraining Loss: 2.234078 \tValidation Loss: 2.358982\n",
      "Epoch: 5234 \tTraining Loss: 2.224074 \tValidation Loss: 2.358986\n",
      "Epoch: 5235 \tTraining Loss: 2.242267 \tValidation Loss: 2.359045\n",
      "Epoch: 5236 \tTraining Loss: 2.249957 \tValidation Loss: 2.359131\n",
      "Epoch: 5237 \tTraining Loss: 2.242993 \tValidation Loss: 2.359143\n",
      "Epoch: 5238 \tTraining Loss: 2.213761 \tValidation Loss: 2.358968\n",
      "Epoch: 5239 \tTraining Loss: 2.267506 \tValidation Loss: 2.359004\n",
      "Epoch: 5240 \tTraining Loss: 2.227834 \tValidation Loss: 2.358996\n",
      "Epoch: 5241 \tTraining Loss: 2.249671 \tValidation Loss: 2.358819\n",
      "Epoch: 5242 \tTraining Loss: 2.220517 \tValidation Loss: 2.359086\n",
      "Epoch: 5243 \tTraining Loss: 2.234572 \tValidation Loss: 2.359110\n",
      "Epoch: 5244 \tTraining Loss: 2.212227 \tValidation Loss: 2.358945\n",
      "Epoch: 5245 \tTraining Loss: 2.212812 \tValidation Loss: 2.358829\n",
      "Epoch: 5246 \tTraining Loss: 2.236172 \tValidation Loss: 2.358769\n",
      "Epoch: 5247 \tTraining Loss: 2.218341 \tValidation Loss: 2.358777\n",
      "Epoch: 5248 \tTraining Loss: 2.221333 \tValidation Loss: 2.358680\n",
      "Validation loss decreased (2.358714 --> 2.358680).  Saving model ...\n",
      "Epoch: 5249 \tTraining Loss: 2.231514 \tValidation Loss: 2.358552\n",
      "Validation loss decreased (2.358680 --> 2.358552).  Saving model ...\n",
      "Epoch: 5250 \tTraining Loss: 2.240469 \tValidation Loss: 2.358620\n",
      "Epoch: 5251 \tTraining Loss: 2.230847 \tValidation Loss: 2.358698\n",
      "Epoch: 5252 \tTraining Loss: 2.233504 \tValidation Loss: 2.358752\n",
      "Epoch: 5253 \tTraining Loss: 2.233084 \tValidation Loss: 2.358685\n",
      "Epoch: 5254 \tTraining Loss: 2.212902 \tValidation Loss: 2.358571\n",
      "Epoch: 5255 \tTraining Loss: 2.219093 \tValidation Loss: 2.358764\n",
      "Epoch: 5256 \tTraining Loss: 2.240430 \tValidation Loss: 2.358899\n",
      "Epoch: 5257 \tTraining Loss: 2.225050 \tValidation Loss: 2.358639\n",
      "Epoch: 5258 \tTraining Loss: 2.259193 \tValidation Loss: 2.358716\n",
      "Epoch: 5259 \tTraining Loss: 2.244040 \tValidation Loss: 2.358712\n",
      "Epoch: 5260 \tTraining Loss: 2.245744 \tValidation Loss: 2.358972\n",
      "Epoch: 5261 \tTraining Loss: 2.219906 \tValidation Loss: 2.358843\n",
      "Epoch: 5262 \tTraining Loss: 2.242317 \tValidation Loss: 2.358979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5263 \tTraining Loss: 2.205469 \tValidation Loss: 2.358926\n",
      "Epoch: 5264 \tTraining Loss: 2.232910 \tValidation Loss: 2.359073\n",
      "Epoch: 5265 \tTraining Loss: 2.227165 \tValidation Loss: 2.358947\n",
      "Epoch: 5266 \tTraining Loss: 2.233807 \tValidation Loss: 2.358953\n",
      "Epoch: 5267 \tTraining Loss: 2.212240 \tValidation Loss: 2.358900\n",
      "Epoch: 5268 \tTraining Loss: 2.214464 \tValidation Loss: 2.358728\n",
      "Epoch: 5269 \tTraining Loss: 2.221770 \tValidation Loss: 2.358793\n",
      "Epoch: 5270 \tTraining Loss: 2.235511 \tValidation Loss: 2.358770\n",
      "Epoch: 5271 \tTraining Loss: 2.239973 \tValidation Loss: 2.358620\n",
      "Epoch: 5272 \tTraining Loss: 2.216021 \tValidation Loss: 2.358601\n",
      "Epoch: 5273 \tTraining Loss: 2.221083 \tValidation Loss: 2.358647\n",
      "Epoch: 5274 \tTraining Loss: 2.247530 \tValidation Loss: 2.358567\n",
      "Epoch: 5275 \tTraining Loss: 2.241839 \tValidation Loss: 2.358804\n",
      "Epoch: 5276 \tTraining Loss: 2.237103 \tValidation Loss: 2.358689\n",
      "Epoch: 5277 \tTraining Loss: 2.225300 \tValidation Loss: 2.358554\n",
      "Epoch: 5278 \tTraining Loss: 2.243386 \tValidation Loss: 2.358348\n",
      "Validation loss decreased (2.358552 --> 2.358348).  Saving model ...\n",
      "Epoch: 5279 \tTraining Loss: 2.226963 \tValidation Loss: 2.358130\n",
      "Validation loss decreased (2.358348 --> 2.358130).  Saving model ...\n",
      "Epoch: 5280 \tTraining Loss: 2.224635 \tValidation Loss: 2.358348\n",
      "Epoch: 5281 \tTraining Loss: 2.230574 \tValidation Loss: 2.358424\n",
      "Epoch: 5282 \tTraining Loss: 2.238148 \tValidation Loss: 2.358438\n",
      "Epoch: 5283 \tTraining Loss: 2.220211 \tValidation Loss: 2.358527\n",
      "Epoch: 5284 \tTraining Loss: 2.231430 \tValidation Loss: 2.358642\n",
      "Epoch: 5285 \tTraining Loss: 2.215816 \tValidation Loss: 2.358460\n",
      "Epoch: 5286 \tTraining Loss: 2.220804 \tValidation Loss: 2.358343\n",
      "Epoch: 5287 \tTraining Loss: 2.234017 \tValidation Loss: 2.358420\n",
      "Epoch: 5288 \tTraining Loss: 2.232115 \tValidation Loss: 2.358364\n",
      "Epoch: 5289 \tTraining Loss: 2.215443 \tValidation Loss: 2.358104\n",
      "Validation loss decreased (2.358130 --> 2.358104).  Saving model ...\n",
      "Epoch: 5290 \tTraining Loss: 2.222105 \tValidation Loss: 2.358078\n",
      "Validation loss decreased (2.358104 --> 2.358078).  Saving model ...\n",
      "Epoch: 5291 \tTraining Loss: 2.224720 \tValidation Loss: 2.358171\n",
      "Epoch: 5292 \tTraining Loss: 2.237362 \tValidation Loss: 2.358286\n",
      "Epoch: 5293 \tTraining Loss: 2.238554 \tValidation Loss: 2.358349\n",
      "Epoch: 5294 \tTraining Loss: 2.206671 \tValidation Loss: 2.358260\n",
      "Epoch: 5295 \tTraining Loss: 2.228270 \tValidation Loss: 2.358328\n",
      "Epoch: 5296 \tTraining Loss: 2.225520 \tValidation Loss: 2.358317\n",
      "Epoch: 5297 \tTraining Loss: 2.217011 \tValidation Loss: 2.358138\n",
      "Epoch: 5298 \tTraining Loss: 2.226021 \tValidation Loss: 2.358266\n",
      "Epoch: 5299 \tTraining Loss: 2.229584 \tValidation Loss: 2.357974\n",
      "Validation loss decreased (2.358078 --> 2.357974).  Saving model ...\n",
      "Epoch: 5300 \tTraining Loss: 2.224612 \tValidation Loss: 2.357982\n",
      "Epoch: 5301 \tTraining Loss: 2.224165 \tValidation Loss: 2.358047\n",
      "Epoch: 5302 \tTraining Loss: 2.231703 \tValidation Loss: 2.358016\n",
      "Epoch: 5303 \tTraining Loss: 2.209296 \tValidation Loss: 2.357934\n",
      "Validation loss decreased (2.357974 --> 2.357934).  Saving model ...\n",
      "Epoch: 5304 \tTraining Loss: 2.244848 \tValidation Loss: 2.357973\n",
      "Epoch: 5305 \tTraining Loss: 2.210802 \tValidation Loss: 2.357742\n",
      "Validation loss decreased (2.357934 --> 2.357742).  Saving model ...\n",
      "Epoch: 5306 \tTraining Loss: 2.209332 \tValidation Loss: 2.357463\n",
      "Validation loss decreased (2.357742 --> 2.357463).  Saving model ...\n",
      "Epoch: 5307 \tTraining Loss: 2.208639 \tValidation Loss: 2.357337\n",
      "Validation loss decreased (2.357463 --> 2.357337).  Saving model ...\n",
      "Epoch: 5308 \tTraining Loss: 2.218704 \tValidation Loss: 2.357306\n",
      "Validation loss decreased (2.357337 --> 2.357306).  Saving model ...\n",
      "Epoch: 5309 \tTraining Loss: 2.227317 \tValidation Loss: 2.357393\n",
      "Epoch: 5310 \tTraining Loss: 2.225938 \tValidation Loss: 2.357482\n",
      "Epoch: 5311 \tTraining Loss: 2.221558 \tValidation Loss: 2.357363\n",
      "Epoch: 5312 \tTraining Loss: 2.243431 \tValidation Loss: 2.357493\n",
      "Epoch: 5313 \tTraining Loss: 2.225626 \tValidation Loss: 2.357450\n",
      "Epoch: 5314 \tTraining Loss: 2.234627 \tValidation Loss: 2.357287\n",
      "Validation loss decreased (2.357306 --> 2.357287).  Saving model ...\n",
      "Epoch: 5315 \tTraining Loss: 2.199389 \tValidation Loss: 2.357018\n",
      "Validation loss decreased (2.357287 --> 2.357018).  Saving model ...\n",
      "Epoch: 5316 \tTraining Loss: 2.212000 \tValidation Loss: 2.357218\n",
      "Epoch: 5317 \tTraining Loss: 2.216956 \tValidation Loss: 2.357083\n",
      "Epoch: 5318 \tTraining Loss: 2.226713 \tValidation Loss: 2.356991\n",
      "Validation loss decreased (2.357018 --> 2.356991).  Saving model ...\n",
      "Epoch: 5319 \tTraining Loss: 2.236651 \tValidation Loss: 2.357128\n",
      "Epoch: 5320 \tTraining Loss: 2.224431 \tValidation Loss: 2.357018\n",
      "Epoch: 5321 \tTraining Loss: 2.219172 \tValidation Loss: 2.356886\n",
      "Validation loss decreased (2.356991 --> 2.356886).  Saving model ...\n",
      "Epoch: 5322 \tTraining Loss: 2.224165 \tValidation Loss: 2.356938\n",
      "Epoch: 5323 \tTraining Loss: 2.215459 \tValidation Loss: 2.356913\n",
      "Epoch: 5324 \tTraining Loss: 2.240089 \tValidation Loss: 2.356991\n",
      "Epoch: 5325 \tTraining Loss: 2.222707 \tValidation Loss: 2.356836\n",
      "Validation loss decreased (2.356886 --> 2.356836).  Saving model ...\n",
      "Epoch: 5326 \tTraining Loss: 2.234422 \tValidation Loss: 2.356664\n",
      "Validation loss decreased (2.356836 --> 2.356664).  Saving model ...\n",
      "Epoch: 5327 \tTraining Loss: 2.228074 \tValidation Loss: 2.356907\n",
      "Epoch: 5328 \tTraining Loss: 2.223154 \tValidation Loss: 2.356991\n",
      "Epoch: 5329 \tTraining Loss: 2.222733 \tValidation Loss: 2.357137\n",
      "Epoch: 5330 \tTraining Loss: 2.209104 \tValidation Loss: 2.357039\n",
      "Epoch: 5331 \tTraining Loss: 2.234480 \tValidation Loss: 2.356916\n",
      "Epoch: 5332 \tTraining Loss: 2.237449 \tValidation Loss: 2.356683\n",
      "Epoch: 5333 \tTraining Loss: 2.230663 \tValidation Loss: 2.356621\n",
      "Validation loss decreased (2.356664 --> 2.356621).  Saving model ...\n",
      "Epoch: 5334 \tTraining Loss: 2.203759 \tValidation Loss: 2.356649\n",
      "Epoch: 5335 \tTraining Loss: 2.252111 \tValidation Loss: 2.356745\n",
      "Epoch: 5336 \tTraining Loss: 2.245692 \tValidation Loss: 2.356767\n",
      "Epoch: 5337 \tTraining Loss: 2.228995 \tValidation Loss: 2.356543\n",
      "Validation loss decreased (2.356621 --> 2.356543).  Saving model ...\n",
      "Epoch: 5338 \tTraining Loss: 2.210248 \tValidation Loss: 2.356590\n",
      "Epoch: 5339 \tTraining Loss: 2.226285 \tValidation Loss: 2.356510\n",
      "Validation loss decreased (2.356543 --> 2.356510).  Saving model ...\n",
      "Epoch: 5340 \tTraining Loss: 2.230970 \tValidation Loss: 2.356482\n",
      "Validation loss decreased (2.356510 --> 2.356482).  Saving model ...\n",
      "Epoch: 5341 \tTraining Loss: 2.210471 \tValidation Loss: 2.356399\n",
      "Validation loss decreased (2.356482 --> 2.356399).  Saving model ...\n",
      "Epoch: 5342 \tTraining Loss: 2.213640 \tValidation Loss: 2.356431\n",
      "Epoch: 5343 \tTraining Loss: 2.223635 \tValidation Loss: 2.356416\n",
      "Epoch: 5344 \tTraining Loss: 2.235396 \tValidation Loss: 2.356559\n",
      "Epoch: 5345 \tTraining Loss: 2.216131 \tValidation Loss: 2.356495\n",
      "Epoch: 5346 \tTraining Loss: 2.226187 \tValidation Loss: 2.356406\n",
      "Epoch: 5347 \tTraining Loss: 2.243640 \tValidation Loss: 2.356301\n",
      "Validation loss decreased (2.356399 --> 2.356301).  Saving model ...\n",
      "Epoch: 5348 \tTraining Loss: 2.235341 \tValidation Loss: 2.356169\n",
      "Validation loss decreased (2.356301 --> 2.356169).  Saving model ...\n",
      "Epoch: 5349 \tTraining Loss: 2.224790 \tValidation Loss: 2.356287\n",
      "Epoch: 5350 \tTraining Loss: 2.205111 \tValidation Loss: 2.356217\n",
      "Epoch: 5351 \tTraining Loss: 2.217175 \tValidation Loss: 2.356218\n",
      "Epoch: 5352 \tTraining Loss: 2.213399 \tValidation Loss: 2.356239\n",
      "Epoch: 5353 \tTraining Loss: 2.209278 \tValidation Loss: 2.356232\n",
      "Epoch: 5354 \tTraining Loss: 2.206011 \tValidation Loss: 2.356222\n",
      "Epoch: 5355 \tTraining Loss: 2.214207 \tValidation Loss: 2.356042\n",
      "Validation loss decreased (2.356169 --> 2.356042).  Saving model ...\n",
      "Epoch: 5356 \tTraining Loss: 2.237731 \tValidation Loss: 2.355897\n",
      "Validation loss decreased (2.356042 --> 2.355897).  Saving model ...\n",
      "Epoch: 5357 \tTraining Loss: 2.240706 \tValidation Loss: 2.355823\n",
      "Validation loss decreased (2.355897 --> 2.355823).  Saving model ...\n",
      "Epoch: 5358 \tTraining Loss: 2.212891 \tValidation Loss: 2.355677\n",
      "Validation loss decreased (2.355823 --> 2.355677).  Saving model ...\n",
      "Epoch: 5359 \tTraining Loss: 2.212810 \tValidation Loss: 2.355867\n",
      "Epoch: 5360 \tTraining Loss: 2.218519 \tValidation Loss: 2.355868\n",
      "Epoch: 5361 \tTraining Loss: 2.204732 \tValidation Loss: 2.355823\n",
      "Epoch: 5362 \tTraining Loss: 2.209146 \tValidation Loss: 2.355865\n",
      "Epoch: 5363 \tTraining Loss: 2.222358 \tValidation Loss: 2.356087\n",
      "Epoch: 5364 \tTraining Loss: 2.206845 \tValidation Loss: 2.356095\n",
      "Epoch: 5365 \tTraining Loss: 2.218498 \tValidation Loss: 2.356200\n",
      "Epoch: 5366 \tTraining Loss: 2.233373 \tValidation Loss: 2.356039\n",
      "Epoch: 5367 \tTraining Loss: 2.210716 \tValidation Loss: 2.356054\n",
      "Epoch: 5368 \tTraining Loss: 2.220234 \tValidation Loss: 2.356028\n",
      "Epoch: 5369 \tTraining Loss: 2.221430 \tValidation Loss: 2.356076\n",
      "Epoch: 5370 \tTraining Loss: 2.226614 \tValidation Loss: 2.356327\n",
      "Epoch: 5371 \tTraining Loss: 2.212151 \tValidation Loss: 2.356200\n",
      "Epoch: 5372 \tTraining Loss: 2.222666 \tValidation Loss: 2.356000\n",
      "Epoch: 5373 \tTraining Loss: 2.223227 \tValidation Loss: 2.355965\n",
      "Epoch: 5374 \tTraining Loss: 2.230543 \tValidation Loss: 2.356055\n",
      "Epoch: 5375 \tTraining Loss: 2.214197 \tValidation Loss: 2.356060\n",
      "Epoch: 5376 \tTraining Loss: 2.225630 \tValidation Loss: 2.356074\n",
      "Epoch: 5377 \tTraining Loss: 2.232708 \tValidation Loss: 2.356254\n",
      "Epoch: 5378 \tTraining Loss: 2.236136 \tValidation Loss: 2.356383\n",
      "Epoch: 5379 \tTraining Loss: 2.249139 \tValidation Loss: 2.356362\n",
      "Epoch: 5380 \tTraining Loss: 2.214125 \tValidation Loss: 2.356235\n",
      "Epoch: 5381 \tTraining Loss: 2.228534 \tValidation Loss: 2.356194\n",
      "Epoch: 5382 \tTraining Loss: 2.210685 \tValidation Loss: 2.356112\n",
      "Epoch: 5383 \tTraining Loss: 2.219459 \tValidation Loss: 2.356009\n",
      "Epoch: 5384 \tTraining Loss: 2.217528 \tValidation Loss: 2.356076\n",
      "Epoch: 5385 \tTraining Loss: 2.228787 \tValidation Loss: 2.355923\n",
      "Epoch: 5386 \tTraining Loss: 2.215177 \tValidation Loss: 2.355853\n",
      "Epoch: 5387 \tTraining Loss: 2.230832 \tValidation Loss: 2.355936\n",
      "Epoch: 5388 \tTraining Loss: 2.213828 \tValidation Loss: 2.356026\n",
      "Epoch: 5389 \tTraining Loss: 2.218052 \tValidation Loss: 2.355866\n",
      "Epoch: 5390 \tTraining Loss: 2.202455 \tValidation Loss: 2.355892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5391 \tTraining Loss: 2.221485 \tValidation Loss: 2.355991\n",
      "Epoch: 5392 \tTraining Loss: 2.230448 \tValidation Loss: 2.355912\n",
      "Epoch: 5393 \tTraining Loss: 2.224007 \tValidation Loss: 2.355644\n",
      "Validation loss decreased (2.355677 --> 2.355644).  Saving model ...\n",
      "Epoch: 5394 \tTraining Loss: 2.223224 \tValidation Loss: 2.355414\n",
      "Validation loss decreased (2.355644 --> 2.355414).  Saving model ...\n",
      "Epoch: 5395 \tTraining Loss: 2.219089 \tValidation Loss: 2.355250\n",
      "Validation loss decreased (2.355414 --> 2.355250).  Saving model ...\n",
      "Epoch: 5396 \tTraining Loss: 2.219092 \tValidation Loss: 2.355255\n",
      "Epoch: 5397 \tTraining Loss: 2.209825 \tValidation Loss: 2.355110\n",
      "Validation loss decreased (2.355250 --> 2.355110).  Saving model ...\n",
      "Epoch: 5398 \tTraining Loss: 2.220131 \tValidation Loss: 2.354958\n",
      "Validation loss decreased (2.355110 --> 2.354958).  Saving model ...\n",
      "Epoch: 5399 \tTraining Loss: 2.226685 \tValidation Loss: 2.355048\n",
      "Epoch: 5400 \tTraining Loss: 2.214180 \tValidation Loss: 2.355240\n",
      "Epoch: 5401 \tTraining Loss: 2.229901 \tValidation Loss: 2.355327\n",
      "Epoch: 5402 \tTraining Loss: 2.239178 \tValidation Loss: 2.355532\n",
      "Epoch: 5403 \tTraining Loss: 2.230793 \tValidation Loss: 2.355613\n",
      "Epoch: 5404 \tTraining Loss: 2.230983 \tValidation Loss: 2.355729\n",
      "Epoch: 5405 \tTraining Loss: 2.238638 \tValidation Loss: 2.355638\n",
      "Epoch: 5406 \tTraining Loss: 2.213571 \tValidation Loss: 2.355455\n",
      "Epoch: 5407 \tTraining Loss: 2.212912 \tValidation Loss: 2.355366\n",
      "Epoch: 5408 \tTraining Loss: 2.207804 \tValidation Loss: 2.355436\n",
      "Epoch: 5409 \tTraining Loss: 2.214669 \tValidation Loss: 2.355360\n",
      "Epoch: 5410 \tTraining Loss: 2.210150 \tValidation Loss: 2.355260\n",
      "Epoch: 5411 \tTraining Loss: 2.228537 \tValidation Loss: 2.355306\n",
      "Epoch: 5412 \tTraining Loss: 2.211186 \tValidation Loss: 2.355468\n",
      "Epoch: 5413 \tTraining Loss: 2.218901 \tValidation Loss: 2.355313\n",
      "Epoch: 5414 \tTraining Loss: 2.214903 \tValidation Loss: 2.355163\n",
      "Epoch: 5415 \tTraining Loss: 2.222226 \tValidation Loss: 2.355197\n",
      "Epoch: 5416 \tTraining Loss: 2.210989 \tValidation Loss: 2.355124\n",
      "Epoch: 5417 \tTraining Loss: 2.214600 \tValidation Loss: 2.355182\n",
      "Epoch: 5418 \tTraining Loss: 2.241802 \tValidation Loss: 2.354992\n",
      "Epoch: 5419 \tTraining Loss: 2.235189 \tValidation Loss: 2.354695\n",
      "Validation loss decreased (2.354958 --> 2.354695).  Saving model ...\n",
      "Epoch: 5420 \tTraining Loss: 2.222287 \tValidation Loss: 2.354694\n",
      "Validation loss decreased (2.354695 --> 2.354694).  Saving model ...\n",
      "Epoch: 5421 \tTraining Loss: 2.206593 \tValidation Loss: 2.354577\n",
      "Validation loss decreased (2.354694 --> 2.354577).  Saving model ...\n",
      "Epoch: 5422 \tTraining Loss: 2.195983 \tValidation Loss: 2.354454\n",
      "Validation loss decreased (2.354577 --> 2.354454).  Saving model ...\n",
      "Epoch: 5423 \tTraining Loss: 2.213153 \tValidation Loss: 2.354389\n",
      "Validation loss decreased (2.354454 --> 2.354389).  Saving model ...\n",
      "Epoch: 5424 \tTraining Loss: 2.219682 \tValidation Loss: 2.354450\n",
      "Epoch: 5425 \tTraining Loss: 2.220329 \tValidation Loss: 2.354447\n",
      "Epoch: 5426 \tTraining Loss: 2.210422 \tValidation Loss: 2.354656\n",
      "Epoch: 5427 \tTraining Loss: 2.207542 \tValidation Loss: 2.354883\n",
      "Epoch: 5428 \tTraining Loss: 2.198933 \tValidation Loss: 2.354687\n",
      "Epoch: 5429 \tTraining Loss: 2.235829 \tValidation Loss: 2.354868\n",
      "Epoch: 5430 \tTraining Loss: 2.248297 \tValidation Loss: 2.354928\n",
      "Epoch: 5431 \tTraining Loss: 2.232654 \tValidation Loss: 2.355070\n",
      "Epoch: 5432 \tTraining Loss: 2.224956 \tValidation Loss: 2.355038\n",
      "Epoch: 5433 \tTraining Loss: 2.220632 \tValidation Loss: 2.354849\n",
      "Epoch: 5434 \tTraining Loss: 2.223710 \tValidation Loss: 2.354749\n",
      "Epoch: 5435 \tTraining Loss: 2.245321 \tValidation Loss: 2.354769\n",
      "Epoch: 5436 \tTraining Loss: 2.222819 \tValidation Loss: 2.354762\n",
      "Epoch: 5437 \tTraining Loss: 2.221691 \tValidation Loss: 2.354697\n",
      "Epoch: 5438 \tTraining Loss: 2.226447 \tValidation Loss: 2.354686\n",
      "Epoch: 5439 \tTraining Loss: 2.217824 \tValidation Loss: 2.354607\n",
      "Epoch: 5440 \tTraining Loss: 2.211907 \tValidation Loss: 2.354609\n",
      "Epoch: 5441 \tTraining Loss: 2.212412 \tValidation Loss: 2.354835\n",
      "Epoch: 5442 \tTraining Loss: 2.235918 \tValidation Loss: 2.355122\n",
      "Epoch: 5443 \tTraining Loss: 2.207947 \tValidation Loss: 2.355010\n",
      "Epoch: 5444 \tTraining Loss: 2.191388 \tValidation Loss: 2.354904\n",
      "Epoch: 5445 \tTraining Loss: 2.220276 \tValidation Loss: 2.354836\n",
      "Epoch: 5446 \tTraining Loss: 2.201498 \tValidation Loss: 2.354787\n",
      "Epoch: 5447 \tTraining Loss: 2.200883 \tValidation Loss: 2.354676\n",
      "Epoch: 5448 \tTraining Loss: 2.205823 \tValidation Loss: 2.354794\n",
      "Epoch: 5449 \tTraining Loss: 2.211879 \tValidation Loss: 2.354813\n",
      "Epoch: 5450 \tTraining Loss: 2.206483 \tValidation Loss: 2.354746\n",
      "Epoch: 5451 \tTraining Loss: 2.221139 \tValidation Loss: 2.354814\n",
      "Epoch: 5452 \tTraining Loss: 2.211121 \tValidation Loss: 2.354556\n",
      "Epoch: 5453 \tTraining Loss: 2.198736 \tValidation Loss: 2.354430\n",
      "Epoch: 5454 \tTraining Loss: 2.235797 \tValidation Loss: 2.354491\n",
      "Epoch: 5455 \tTraining Loss: 2.217437 \tValidation Loss: 2.354391\n",
      "Epoch: 5456 \tTraining Loss: 2.198369 \tValidation Loss: 2.354257\n",
      "Validation loss decreased (2.354389 --> 2.354257).  Saving model ...\n",
      "Epoch: 5457 \tTraining Loss: 2.221106 \tValidation Loss: 2.354173\n",
      "Validation loss decreased (2.354257 --> 2.354173).  Saving model ...\n",
      "Epoch: 5458 \tTraining Loss: 2.205798 \tValidation Loss: 2.354272\n",
      "Epoch: 5459 \tTraining Loss: 2.207415 \tValidation Loss: 2.354376\n",
      "Epoch: 5460 \tTraining Loss: 2.202724 \tValidation Loss: 2.354547\n",
      "Epoch: 5461 \tTraining Loss: 2.203615 \tValidation Loss: 2.354707\n",
      "Epoch: 5462 \tTraining Loss: 2.219422 \tValidation Loss: 2.354802\n",
      "Epoch: 5463 \tTraining Loss: 2.230363 \tValidation Loss: 2.354867\n",
      "Epoch: 5464 \tTraining Loss: 2.231060 \tValidation Loss: 2.354971\n",
      "Epoch: 5465 \tTraining Loss: 2.208509 \tValidation Loss: 2.354992\n",
      "Epoch: 5466 \tTraining Loss: 2.218654 \tValidation Loss: 2.355019\n",
      "Epoch: 5467 \tTraining Loss: 2.206838 \tValidation Loss: 2.354527\n",
      "Epoch: 5468 \tTraining Loss: 2.212433 \tValidation Loss: 2.354255\n",
      "Epoch: 5469 \tTraining Loss: 2.213092 \tValidation Loss: 2.354088\n",
      "Validation loss decreased (2.354173 --> 2.354088).  Saving model ...\n",
      "Epoch: 5470 \tTraining Loss: 2.206163 \tValidation Loss: 2.353827\n",
      "Validation loss decreased (2.354088 --> 2.353827).  Saving model ...\n",
      "Epoch: 5471 \tTraining Loss: 2.223943 \tValidation Loss: 2.353990\n",
      "Epoch: 5472 \tTraining Loss: 2.226169 \tValidation Loss: 2.354065\n",
      "Epoch: 5473 \tTraining Loss: 2.220466 \tValidation Loss: 2.354247\n",
      "Epoch: 5474 \tTraining Loss: 2.230484 \tValidation Loss: 2.354372\n",
      "Epoch: 5475 \tTraining Loss: 2.222518 \tValidation Loss: 2.354278\n",
      "Epoch: 5476 \tTraining Loss: 2.203269 \tValidation Loss: 2.354190\n",
      "Epoch: 5477 \tTraining Loss: 2.209653 \tValidation Loss: 2.354173\n",
      "Epoch: 5478 \tTraining Loss: 2.223736 \tValidation Loss: 2.354152\n",
      "Epoch: 5479 \tTraining Loss: 2.206608 \tValidation Loss: 2.353950\n",
      "Epoch: 5480 \tTraining Loss: 2.229915 \tValidation Loss: 2.354042\n",
      "Epoch: 5481 \tTraining Loss: 2.201864 \tValidation Loss: 2.353725\n",
      "Validation loss decreased (2.353827 --> 2.353725).  Saving model ...\n",
      "Epoch: 5482 \tTraining Loss: 2.213596 \tValidation Loss: 2.353711\n",
      "Validation loss decreased (2.353725 --> 2.353711).  Saving model ...\n",
      "Epoch: 5483 \tTraining Loss: 2.217600 \tValidation Loss: 2.353699\n",
      "Validation loss decreased (2.353711 --> 2.353699).  Saving model ...\n",
      "Epoch: 5484 \tTraining Loss: 2.212179 \tValidation Loss: 2.353702\n",
      "Epoch: 5485 \tTraining Loss: 2.222349 \tValidation Loss: 2.353724\n",
      "Epoch: 5486 \tTraining Loss: 2.196690 \tValidation Loss: 2.353654\n",
      "Validation loss decreased (2.353699 --> 2.353654).  Saving model ...\n",
      "Epoch: 5487 \tTraining Loss: 2.220571 \tValidation Loss: 2.353748\n",
      "Epoch: 5488 \tTraining Loss: 2.209780 \tValidation Loss: 2.353728\n",
      "Epoch: 5489 \tTraining Loss: 2.215056 \tValidation Loss: 2.353714\n",
      "Epoch: 5490 \tTraining Loss: 2.199688 \tValidation Loss: 2.353930\n",
      "Epoch: 5491 \tTraining Loss: 2.219685 \tValidation Loss: 2.353960\n",
      "Epoch: 5492 \tTraining Loss: 2.218628 \tValidation Loss: 2.354053\n",
      "Epoch: 5493 \tTraining Loss: 2.206559 \tValidation Loss: 2.354012\n",
      "Epoch: 5494 \tTraining Loss: 2.208660 \tValidation Loss: 2.353785\n",
      "Epoch: 5495 \tTraining Loss: 2.206414 \tValidation Loss: 2.353774\n",
      "Epoch: 5496 \tTraining Loss: 2.224898 \tValidation Loss: 2.353841\n",
      "Epoch: 5497 \tTraining Loss: 2.199806 \tValidation Loss: 2.353725\n",
      "Epoch: 5498 \tTraining Loss: 2.217661 \tValidation Loss: 2.353492\n",
      "Validation loss decreased (2.353654 --> 2.353492).  Saving model ...\n",
      "Epoch: 5499 \tTraining Loss: 2.220508 \tValidation Loss: 2.353420\n",
      "Validation loss decreased (2.353492 --> 2.353420).  Saving model ...\n",
      "Epoch: 5500 \tTraining Loss: 2.235041 \tValidation Loss: 2.353372\n",
      "Validation loss decreased (2.353420 --> 2.353372).  Saving model ...\n",
      "Epoch: 5501 \tTraining Loss: 2.229153 \tValidation Loss: 2.353457\n",
      "Epoch: 5502 \tTraining Loss: 2.224744 \tValidation Loss: 2.353545\n",
      "Epoch: 5503 \tTraining Loss: 2.201391 \tValidation Loss: 2.353511\n",
      "Epoch: 5504 \tTraining Loss: 2.213188 \tValidation Loss: 2.353561\n",
      "Epoch: 5505 \tTraining Loss: 2.217109 \tValidation Loss: 2.353553\n",
      "Epoch: 5506 \tTraining Loss: 2.209928 \tValidation Loss: 2.353391\n",
      "Epoch: 5507 \tTraining Loss: 2.227539 \tValidation Loss: 2.353550\n",
      "Epoch: 5508 \tTraining Loss: 2.196235 \tValidation Loss: 2.353516\n",
      "Epoch: 5509 \tTraining Loss: 2.228676 \tValidation Loss: 2.353705\n",
      "Epoch: 5510 \tTraining Loss: 2.201297 \tValidation Loss: 2.353663\n",
      "Epoch: 5511 \tTraining Loss: 2.220243 \tValidation Loss: 2.353549\n",
      "Epoch: 5512 \tTraining Loss: 2.213195 \tValidation Loss: 2.353452\n",
      "Epoch: 5513 \tTraining Loss: 2.197199 \tValidation Loss: 2.353272\n",
      "Validation loss decreased (2.353372 --> 2.353272).  Saving model ...\n",
      "Epoch: 5514 \tTraining Loss: 2.211193 \tValidation Loss: 2.353308\n",
      "Epoch: 5515 \tTraining Loss: 2.230264 \tValidation Loss: 2.353482\n",
      "Epoch: 5516 \tTraining Loss: 2.220571 \tValidation Loss: 2.353605\n",
      "Epoch: 5517 \tTraining Loss: 2.225146 \tValidation Loss: 2.353668\n",
      "Epoch: 5518 \tTraining Loss: 2.227550 \tValidation Loss: 2.353642\n",
      "Epoch: 5519 \tTraining Loss: 2.205141 \tValidation Loss: 2.353508\n",
      "Epoch: 5520 \tTraining Loss: 2.228383 \tValidation Loss: 2.353460\n",
      "Epoch: 5521 \tTraining Loss: 2.194906 \tValidation Loss: 2.353189\n",
      "Validation loss decreased (2.353272 --> 2.353189).  Saving model ...\n",
      "Epoch: 5522 \tTraining Loss: 2.220562 \tValidation Loss: 2.353098\n",
      "Validation loss decreased (2.353189 --> 2.353098).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5523 \tTraining Loss: 2.226685 \tValidation Loss: 2.353164\n",
      "Epoch: 5524 \tTraining Loss: 2.227154 \tValidation Loss: 2.353124\n",
      "Epoch: 5525 \tTraining Loss: 2.209337 \tValidation Loss: 2.352985\n",
      "Validation loss decreased (2.353098 --> 2.352985).  Saving model ...\n",
      "Epoch: 5526 \tTraining Loss: 2.204349 \tValidation Loss: 2.352790\n",
      "Validation loss decreased (2.352985 --> 2.352790).  Saving model ...\n",
      "Epoch: 5527 \tTraining Loss: 2.214312 \tValidation Loss: 2.352532\n",
      "Validation loss decreased (2.352790 --> 2.352532).  Saving model ...\n",
      "Epoch: 5528 \tTraining Loss: 2.218024 \tValidation Loss: 2.352578\n",
      "Epoch: 5529 \tTraining Loss: 2.216223 \tValidation Loss: 2.352401\n",
      "Validation loss decreased (2.352532 --> 2.352401).  Saving model ...\n",
      "Epoch: 5530 \tTraining Loss: 2.225346 \tValidation Loss: 2.352546\n",
      "Epoch: 5531 \tTraining Loss: 2.233153 \tValidation Loss: 2.352553\n",
      "Epoch: 5532 \tTraining Loss: 2.216348 \tValidation Loss: 2.352431\n",
      "Epoch: 5533 \tTraining Loss: 2.191614 \tValidation Loss: 2.352164\n",
      "Validation loss decreased (2.352401 --> 2.352164).  Saving model ...\n",
      "Epoch: 5534 \tTraining Loss: 2.207578 \tValidation Loss: 2.352403\n",
      "Epoch: 5535 \tTraining Loss: 2.209932 \tValidation Loss: 2.352431\n",
      "Epoch: 5536 \tTraining Loss: 2.225969 \tValidation Loss: 2.352704\n",
      "Epoch: 5537 \tTraining Loss: 2.227108 \tValidation Loss: 2.352858\n",
      "Epoch: 5538 \tTraining Loss: 2.208084 \tValidation Loss: 2.352899\n",
      "Epoch: 5539 \tTraining Loss: 2.211312 \tValidation Loss: 2.352734\n",
      "Epoch: 5540 \tTraining Loss: 2.217874 \tValidation Loss: 2.352665\n",
      "Epoch: 5541 \tTraining Loss: 2.201582 \tValidation Loss: 2.352596\n",
      "Epoch: 5542 \tTraining Loss: 2.201048 \tValidation Loss: 2.352936\n",
      "Epoch: 5543 \tTraining Loss: 2.216084 \tValidation Loss: 2.352941\n",
      "Epoch: 5544 \tTraining Loss: 2.220423 \tValidation Loss: 2.352587\n",
      "Epoch: 5545 \tTraining Loss: 2.229012 \tValidation Loss: 2.352812\n",
      "Epoch: 5546 \tTraining Loss: 2.205784 \tValidation Loss: 2.352922\n",
      "Epoch: 5547 \tTraining Loss: 2.214224 \tValidation Loss: 2.352957\n",
      "Epoch: 5548 \tTraining Loss: 2.214034 \tValidation Loss: 2.352855\n",
      "Epoch: 5549 \tTraining Loss: 2.209393 \tValidation Loss: 2.352961\n",
      "Epoch: 5550 \tTraining Loss: 2.213434 \tValidation Loss: 2.352779\n",
      "Epoch: 5551 \tTraining Loss: 2.215261 \tValidation Loss: 2.352756\n",
      "Epoch: 5552 \tTraining Loss: 2.196233 \tValidation Loss: 2.352723\n",
      "Epoch: 5553 \tTraining Loss: 2.201653 \tValidation Loss: 2.352765\n",
      "Epoch: 5554 \tTraining Loss: 2.200262 \tValidation Loss: 2.352735\n",
      "Epoch: 5555 \tTraining Loss: 2.213318 \tValidation Loss: 2.352654\n",
      "Epoch: 5556 \tTraining Loss: 2.185543 \tValidation Loss: 2.352309\n",
      "Epoch: 5557 \tTraining Loss: 2.225304 \tValidation Loss: 2.352350\n",
      "Epoch: 5558 \tTraining Loss: 2.191979 \tValidation Loss: 2.352377\n",
      "Epoch: 5559 \tTraining Loss: 2.190636 \tValidation Loss: 2.352268\n",
      "Epoch: 5560 \tTraining Loss: 2.196529 \tValidation Loss: 2.352127\n",
      "Validation loss decreased (2.352164 --> 2.352127).  Saving model ...\n",
      "Epoch: 5561 \tTraining Loss: 2.198059 \tValidation Loss: 2.352149\n",
      "Epoch: 5562 \tTraining Loss: 2.228105 \tValidation Loss: 2.352162\n",
      "Epoch: 5563 \tTraining Loss: 2.210096 \tValidation Loss: 2.351986\n",
      "Validation loss decreased (2.352127 --> 2.351986).  Saving model ...\n",
      "Epoch: 5564 \tTraining Loss: 2.193816 \tValidation Loss: 2.352005\n",
      "Epoch: 5565 \tTraining Loss: 2.222571 \tValidation Loss: 2.352122\n",
      "Epoch: 5566 \tTraining Loss: 2.194809 \tValidation Loss: 2.352074\n",
      "Epoch: 5567 \tTraining Loss: 2.189194 \tValidation Loss: 2.351856\n",
      "Validation loss decreased (2.351986 --> 2.351856).  Saving model ...\n",
      "Epoch: 5568 \tTraining Loss: 2.216575 \tValidation Loss: 2.351769\n",
      "Validation loss decreased (2.351856 --> 2.351769).  Saving model ...\n",
      "Epoch: 5569 \tTraining Loss: 2.214041 \tValidation Loss: 2.351630\n",
      "Validation loss decreased (2.351769 --> 2.351630).  Saving model ...\n",
      "Epoch: 5570 \tTraining Loss: 2.221343 \tValidation Loss: 2.351551\n",
      "Validation loss decreased (2.351630 --> 2.351551).  Saving model ...\n",
      "Epoch: 5571 \tTraining Loss: 2.212194 \tValidation Loss: 2.351453\n",
      "Validation loss decreased (2.351551 --> 2.351453).  Saving model ...\n",
      "Epoch: 5572 \tTraining Loss: 2.213376 \tValidation Loss: 2.351382\n",
      "Validation loss decreased (2.351453 --> 2.351382).  Saving model ...\n",
      "Epoch: 5573 \tTraining Loss: 2.184116 \tValidation Loss: 2.351367\n",
      "Validation loss decreased (2.351382 --> 2.351367).  Saving model ...\n",
      "Epoch: 5574 \tTraining Loss: 2.230247 \tValidation Loss: 2.351382\n",
      "Epoch: 5575 \tTraining Loss: 2.232743 \tValidation Loss: 2.351396\n",
      "Epoch: 5576 \tTraining Loss: 2.220684 \tValidation Loss: 2.351343\n",
      "Validation loss decreased (2.351367 --> 2.351343).  Saving model ...\n",
      "Epoch: 5577 \tTraining Loss: 2.205392 \tValidation Loss: 2.351420\n",
      "Epoch: 5578 \tTraining Loss: 2.186980 \tValidation Loss: 2.351387\n",
      "Epoch: 5579 \tTraining Loss: 2.210491 \tValidation Loss: 2.351334\n",
      "Validation loss decreased (2.351343 --> 2.351334).  Saving model ...\n",
      "Epoch: 5580 \tTraining Loss: 2.206348 \tValidation Loss: 2.351218\n",
      "Validation loss decreased (2.351334 --> 2.351218).  Saving model ...\n",
      "Epoch: 5581 \tTraining Loss: 2.200544 \tValidation Loss: 2.351008\n",
      "Validation loss decreased (2.351218 --> 2.351008).  Saving model ...\n",
      "Epoch: 5582 \tTraining Loss: 2.217640 \tValidation Loss: 2.351212\n",
      "Epoch: 5583 \tTraining Loss: 2.222013 \tValidation Loss: 2.351094\n",
      "Epoch: 5584 \tTraining Loss: 2.235305 \tValidation Loss: 2.351073\n",
      "Epoch: 5585 \tTraining Loss: 2.219202 \tValidation Loss: 2.351268\n",
      "Epoch: 5586 \tTraining Loss: 2.212716 \tValidation Loss: 2.351187\n",
      "Epoch: 5587 \tTraining Loss: 2.226757 \tValidation Loss: 2.351470\n",
      "Epoch: 5588 \tTraining Loss: 2.213305 \tValidation Loss: 2.351398\n",
      "Epoch: 5589 \tTraining Loss: 2.190696 \tValidation Loss: 2.351370\n",
      "Epoch: 5590 \tTraining Loss: 2.214786 \tValidation Loss: 2.351481\n",
      "Epoch: 5591 \tTraining Loss: 2.188931 \tValidation Loss: 2.351422\n",
      "Epoch: 5592 \tTraining Loss: 2.207024 \tValidation Loss: 2.351404\n",
      "Epoch: 5593 \tTraining Loss: 2.215531 \tValidation Loss: 2.351276\n",
      "Epoch: 5594 \tTraining Loss: 2.216023 \tValidation Loss: 2.351359\n",
      "Epoch: 5595 \tTraining Loss: 2.200142 \tValidation Loss: 2.351445\n",
      "Epoch: 5596 \tTraining Loss: 2.197193 \tValidation Loss: 2.351290\n",
      "Epoch: 5597 \tTraining Loss: 2.236342 \tValidation Loss: 2.351355\n",
      "Epoch: 5598 \tTraining Loss: 2.211661 \tValidation Loss: 2.351394\n",
      "Epoch: 5599 \tTraining Loss: 2.230691 \tValidation Loss: 2.351465\n",
      "Epoch: 5600 \tTraining Loss: 2.218553 \tValidation Loss: 2.351535\n",
      "Epoch: 5601 \tTraining Loss: 2.200197 \tValidation Loss: 2.351660\n",
      "Epoch: 5602 \tTraining Loss: 2.216615 \tValidation Loss: 2.351739\n",
      "Epoch: 5603 \tTraining Loss: 2.214857 \tValidation Loss: 2.351674\n",
      "Epoch: 5604 \tTraining Loss: 2.207319 \tValidation Loss: 2.351706\n",
      "Epoch: 5605 \tTraining Loss: 2.227479 \tValidation Loss: 2.351578\n",
      "Epoch: 5606 \tTraining Loss: 2.212716 \tValidation Loss: 2.351637\n",
      "Epoch: 5607 \tTraining Loss: 2.213044 \tValidation Loss: 2.351619\n",
      "Epoch: 5608 \tTraining Loss: 2.196339 \tValidation Loss: 2.351290\n",
      "Epoch: 5609 \tTraining Loss: 2.217355 \tValidation Loss: 2.351431\n",
      "Epoch: 5610 \tTraining Loss: 2.208112 \tValidation Loss: 2.351337\n",
      "Epoch: 5611 \tTraining Loss: 2.217224 \tValidation Loss: 2.351373\n",
      "Epoch: 5612 \tTraining Loss: 2.202425 \tValidation Loss: 2.351485\n",
      "Epoch: 5613 \tTraining Loss: 2.222861 \tValidation Loss: 2.351319\n",
      "Epoch: 5614 \tTraining Loss: 2.200074 \tValidation Loss: 2.351317\n",
      "Epoch: 5615 \tTraining Loss: 2.209385 \tValidation Loss: 2.351218\n",
      "Epoch: 5616 \tTraining Loss: 2.200356 \tValidation Loss: 2.351065\n",
      "Epoch: 5617 \tTraining Loss: 2.215091 \tValidation Loss: 2.351113\n",
      "Epoch: 5618 \tTraining Loss: 2.185396 \tValidation Loss: 2.351208\n",
      "Epoch: 5619 \tTraining Loss: 2.217848 \tValidation Loss: 2.351053\n",
      "Epoch: 5620 \tTraining Loss: 2.215003 \tValidation Loss: 2.350974\n",
      "Validation loss decreased (2.351008 --> 2.350974).  Saving model ...\n",
      "Epoch: 5621 \tTraining Loss: 2.221073 \tValidation Loss: 2.350927\n",
      "Validation loss decreased (2.350974 --> 2.350927).  Saving model ...\n",
      "Epoch: 5622 \tTraining Loss: 2.200911 \tValidation Loss: 2.350794\n",
      "Validation loss decreased (2.350927 --> 2.350794).  Saving model ...\n",
      "Epoch: 5623 \tTraining Loss: 2.227492 \tValidation Loss: 2.350910\n",
      "Epoch: 5624 \tTraining Loss: 2.196039 \tValidation Loss: 2.350738\n",
      "Validation loss decreased (2.350794 --> 2.350738).  Saving model ...\n",
      "Epoch: 5625 \tTraining Loss: 2.194383 \tValidation Loss: 2.350746\n",
      "Epoch: 5626 \tTraining Loss: 2.217058 \tValidation Loss: 2.350785\n",
      "Epoch: 5627 \tTraining Loss: 2.200559 \tValidation Loss: 2.350773\n",
      "Epoch: 5628 \tTraining Loss: 2.190640 \tValidation Loss: 2.350577\n",
      "Validation loss decreased (2.350738 --> 2.350577).  Saving model ...\n",
      "Epoch: 5629 \tTraining Loss: 2.188126 \tValidation Loss: 2.350831\n",
      "Epoch: 5630 \tTraining Loss: 2.206306 \tValidation Loss: 2.350789\n",
      "Epoch: 5631 \tTraining Loss: 2.206680 \tValidation Loss: 2.350616\n",
      "Epoch: 5632 \tTraining Loss: 2.196402 \tValidation Loss: 2.350673\n",
      "Epoch: 5633 \tTraining Loss: 2.203167 \tValidation Loss: 2.350827\n",
      "Epoch: 5634 \tTraining Loss: 2.202126 \tValidation Loss: 2.350631\n",
      "Epoch: 5635 \tTraining Loss: 2.208459 \tValidation Loss: 2.350705\n",
      "Epoch: 5636 \tTraining Loss: 2.209284 \tValidation Loss: 2.350823\n",
      "Epoch: 5637 \tTraining Loss: 2.201866 \tValidation Loss: 2.351013\n",
      "Epoch: 5638 \tTraining Loss: 2.203162 \tValidation Loss: 2.351102\n",
      "Epoch: 5639 \tTraining Loss: 2.211518 \tValidation Loss: 2.351326\n",
      "Epoch: 5640 \tTraining Loss: 2.188680 \tValidation Loss: 2.350937\n",
      "Epoch: 5641 \tTraining Loss: 2.199232 \tValidation Loss: 2.350735\n",
      "Epoch: 5642 \tTraining Loss: 2.199531 \tValidation Loss: 2.350695\n",
      "Epoch: 5643 \tTraining Loss: 2.211543 \tValidation Loss: 2.350475\n",
      "Validation loss decreased (2.350577 --> 2.350475).  Saving model ...\n",
      "Epoch: 5644 \tTraining Loss: 2.180311 \tValidation Loss: 2.350641\n",
      "Epoch: 5645 \tTraining Loss: 2.224184 \tValidation Loss: 2.350744\n",
      "Epoch: 5646 \tTraining Loss: 2.205490 \tValidation Loss: 2.350906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5647 \tTraining Loss: 2.194185 \tValidation Loss: 2.350700\n",
      "Epoch: 5648 \tTraining Loss: 2.228681 \tValidation Loss: 2.350923\n",
      "Epoch: 5649 \tTraining Loss: 2.207444 \tValidation Loss: 2.350956\n",
      "Epoch: 5650 \tTraining Loss: 2.204222 \tValidation Loss: 2.350999\n",
      "Epoch: 5651 \tTraining Loss: 2.206571 \tValidation Loss: 2.350991\n",
      "Epoch: 5652 \tTraining Loss: 2.232218 \tValidation Loss: 2.350827\n",
      "Epoch: 5653 \tTraining Loss: 2.205458 \tValidation Loss: 2.350862\n",
      "Epoch: 5654 \tTraining Loss: 2.213969 \tValidation Loss: 2.350738\n",
      "Epoch: 5655 \tTraining Loss: 2.230278 \tValidation Loss: 2.350755\n",
      "Epoch: 5656 \tTraining Loss: 2.208481 \tValidation Loss: 2.350903\n",
      "Epoch: 5657 \tTraining Loss: 2.199136 \tValidation Loss: 2.350974\n",
      "Epoch: 5658 \tTraining Loss: 2.200477 \tValidation Loss: 2.350930\n",
      "Epoch: 5659 \tTraining Loss: 2.212828 \tValidation Loss: 2.350844\n",
      "Epoch: 5660 \tTraining Loss: 2.196224 \tValidation Loss: 2.350707\n",
      "Epoch: 5661 \tTraining Loss: 2.187351 \tValidation Loss: 2.350418\n",
      "Validation loss decreased (2.350475 --> 2.350418).  Saving model ...\n",
      "Epoch: 5662 \tTraining Loss: 2.190821 \tValidation Loss: 2.350409\n",
      "Validation loss decreased (2.350418 --> 2.350409).  Saving model ...\n",
      "Epoch: 5663 \tTraining Loss: 2.180794 \tValidation Loss: 2.350275\n",
      "Validation loss decreased (2.350409 --> 2.350275).  Saving model ...\n",
      "Epoch: 5664 \tTraining Loss: 2.200045 \tValidation Loss: 2.349995\n",
      "Validation loss decreased (2.350275 --> 2.349995).  Saving model ...\n",
      "Epoch: 5665 \tTraining Loss: 2.192347 \tValidation Loss: 2.349919\n",
      "Validation loss decreased (2.349995 --> 2.349919).  Saving model ...\n",
      "Epoch: 5666 \tTraining Loss: 2.194155 \tValidation Loss: 2.349898\n",
      "Validation loss decreased (2.349919 --> 2.349898).  Saving model ...\n",
      "Epoch: 5667 \tTraining Loss: 2.210346 \tValidation Loss: 2.349700\n",
      "Validation loss decreased (2.349898 --> 2.349700).  Saving model ...\n",
      "Epoch: 5668 \tTraining Loss: 2.183021 \tValidation Loss: 2.349599\n",
      "Validation loss decreased (2.349700 --> 2.349599).  Saving model ...\n",
      "Epoch: 5669 \tTraining Loss: 2.200802 \tValidation Loss: 2.349736\n",
      "Epoch: 5670 \tTraining Loss: 2.196472 \tValidation Loss: 2.349522\n",
      "Validation loss decreased (2.349599 --> 2.349522).  Saving model ...\n",
      "Epoch: 5671 \tTraining Loss: 2.189969 \tValidation Loss: 2.349435\n",
      "Validation loss decreased (2.349522 --> 2.349435).  Saving model ...\n",
      "Epoch: 5672 \tTraining Loss: 2.204509 \tValidation Loss: 2.349471\n",
      "Epoch: 5673 \tTraining Loss: 2.190874 \tValidation Loss: 2.349412\n",
      "Validation loss decreased (2.349435 --> 2.349412).  Saving model ...\n",
      "Epoch: 5674 \tTraining Loss: 2.205938 \tValidation Loss: 2.349398\n",
      "Validation loss decreased (2.349412 --> 2.349398).  Saving model ...\n",
      "Epoch: 5675 \tTraining Loss: 2.207384 \tValidation Loss: 2.349243\n",
      "Validation loss decreased (2.349398 --> 2.349243).  Saving model ...\n",
      "Epoch: 5676 \tTraining Loss: 2.178940 \tValidation Loss: 2.349110\n",
      "Validation loss decreased (2.349243 --> 2.349110).  Saving model ...\n",
      "Epoch: 5677 \tTraining Loss: 2.183967 \tValidation Loss: 2.349006\n",
      "Validation loss decreased (2.349110 --> 2.349006).  Saving model ...\n",
      "Epoch: 5678 \tTraining Loss: 2.211288 \tValidation Loss: 2.348903\n",
      "Validation loss decreased (2.349006 --> 2.348903).  Saving model ...\n",
      "Epoch: 5679 \tTraining Loss: 2.193198 \tValidation Loss: 2.348938\n",
      "Epoch: 5680 \tTraining Loss: 2.198016 \tValidation Loss: 2.348918\n",
      "Epoch: 5681 \tTraining Loss: 2.211836 \tValidation Loss: 2.349123\n",
      "Epoch: 5682 \tTraining Loss: 2.213677 \tValidation Loss: 2.349128\n",
      "Epoch: 5683 \tTraining Loss: 2.210568 \tValidation Loss: 2.349021\n",
      "Epoch: 5684 \tTraining Loss: 2.191133 \tValidation Loss: 2.348945\n",
      "Epoch: 5685 \tTraining Loss: 2.200898 \tValidation Loss: 2.348927\n",
      "Epoch: 5686 \tTraining Loss: 2.219087 \tValidation Loss: 2.348918\n",
      "Epoch: 5687 \tTraining Loss: 2.197905 \tValidation Loss: 2.349003\n",
      "Epoch: 5688 \tTraining Loss: 2.196543 \tValidation Loss: 2.349091\n",
      "Epoch: 5689 \tTraining Loss: 2.195692 \tValidation Loss: 2.348894\n",
      "Validation loss decreased (2.348903 --> 2.348894).  Saving model ...\n",
      "Epoch: 5690 \tTraining Loss: 2.200328 \tValidation Loss: 2.349108\n",
      "Epoch: 5691 \tTraining Loss: 2.200387 \tValidation Loss: 2.349090\n",
      "Epoch: 5692 \tTraining Loss: 2.206311 \tValidation Loss: 2.349404\n",
      "Epoch: 5693 \tTraining Loss: 2.227719 \tValidation Loss: 2.349472\n",
      "Epoch: 5694 \tTraining Loss: 2.209982 \tValidation Loss: 2.349419\n",
      "Epoch: 5695 \tTraining Loss: 2.165263 \tValidation Loss: 2.349272\n",
      "Epoch: 5696 \tTraining Loss: 2.207530 \tValidation Loss: 2.349294\n",
      "Epoch: 5697 \tTraining Loss: 2.200213 \tValidation Loss: 2.349239\n",
      "Epoch: 5698 \tTraining Loss: 2.215363 \tValidation Loss: 2.349329\n",
      "Epoch: 5699 \tTraining Loss: 2.182446 \tValidation Loss: 2.349329\n",
      "Epoch: 5700 \tTraining Loss: 2.203728 \tValidation Loss: 2.349350\n",
      "Epoch: 5701 \tTraining Loss: 2.209154 \tValidation Loss: 2.349360\n",
      "Epoch: 5702 \tTraining Loss: 2.205640 \tValidation Loss: 2.349437\n",
      "Epoch: 5703 \tTraining Loss: 2.209983 \tValidation Loss: 2.349456\n",
      "Epoch: 5704 \tTraining Loss: 2.199584 \tValidation Loss: 2.349418\n",
      "Epoch: 5705 \tTraining Loss: 2.194554 \tValidation Loss: 2.349457\n",
      "Epoch: 5706 \tTraining Loss: 2.210923 \tValidation Loss: 2.349274\n",
      "Epoch: 5707 \tTraining Loss: 2.207994 \tValidation Loss: 2.349329\n",
      "Epoch: 5708 \tTraining Loss: 2.199372 \tValidation Loss: 2.349124\n",
      "Epoch: 5709 \tTraining Loss: 2.199930 \tValidation Loss: 2.349129\n",
      "Epoch: 5710 \tTraining Loss: 2.202756 \tValidation Loss: 2.349257\n",
      "Epoch: 5711 \tTraining Loss: 2.218330 \tValidation Loss: 2.349280\n",
      "Epoch: 5712 \tTraining Loss: 2.198405 \tValidation Loss: 2.349330\n",
      "Epoch: 5713 \tTraining Loss: 2.211981 \tValidation Loss: 2.349159\n",
      "Epoch: 5714 \tTraining Loss: 2.181870 \tValidation Loss: 2.348921\n",
      "Epoch: 5715 \tTraining Loss: 2.205344 \tValidation Loss: 2.348917\n",
      "Epoch: 5716 \tTraining Loss: 2.197095 \tValidation Loss: 2.348794\n",
      "Validation loss decreased (2.348894 --> 2.348794).  Saving model ...\n",
      "Epoch: 5717 \tTraining Loss: 2.188926 \tValidation Loss: 2.348736\n",
      "Validation loss decreased (2.348794 --> 2.348736).  Saving model ...\n",
      "Epoch: 5718 \tTraining Loss: 2.183758 \tValidation Loss: 2.348708\n",
      "Validation loss decreased (2.348736 --> 2.348708).  Saving model ...\n",
      "Epoch: 5719 \tTraining Loss: 2.196861 \tValidation Loss: 2.348844\n",
      "Epoch: 5720 \tTraining Loss: 2.220073 \tValidation Loss: 2.348780\n",
      "Epoch: 5721 \tTraining Loss: 2.221609 \tValidation Loss: 2.348797\n",
      "Epoch: 5722 \tTraining Loss: 2.215860 \tValidation Loss: 2.348831\n",
      "Epoch: 5723 \tTraining Loss: 2.205243 \tValidation Loss: 2.348618\n",
      "Validation loss decreased (2.348708 --> 2.348618).  Saving model ...\n",
      "Epoch: 5724 \tTraining Loss: 2.203099 \tValidation Loss: 2.348513\n",
      "Validation loss decreased (2.348618 --> 2.348513).  Saving model ...\n",
      "Epoch: 5725 \tTraining Loss: 2.185288 \tValidation Loss: 2.348387\n",
      "Validation loss decreased (2.348513 --> 2.348387).  Saving model ...\n",
      "Epoch: 5726 \tTraining Loss: 2.201714 \tValidation Loss: 2.348432\n",
      "Epoch: 5727 \tTraining Loss: 2.206788 \tValidation Loss: 2.348535\n",
      "Epoch: 5728 \tTraining Loss: 2.210290 \tValidation Loss: 2.348480\n",
      "Epoch: 5729 \tTraining Loss: 2.189329 \tValidation Loss: 2.348551\n",
      "Epoch: 5730 \tTraining Loss: 2.210000 \tValidation Loss: 2.348653\n",
      "Epoch: 5731 \tTraining Loss: 2.198518 \tValidation Loss: 2.348584\n",
      "Epoch: 5732 \tTraining Loss: 2.205568 \tValidation Loss: 2.348353\n",
      "Validation loss decreased (2.348387 --> 2.348353).  Saving model ...\n",
      "Epoch: 5733 \tTraining Loss: 2.177751 \tValidation Loss: 2.348344\n",
      "Validation loss decreased (2.348353 --> 2.348344).  Saving model ...\n",
      "Epoch: 5734 \tTraining Loss: 2.194735 \tValidation Loss: 2.348289\n",
      "Validation loss decreased (2.348344 --> 2.348289).  Saving model ...\n",
      "Epoch: 5735 \tTraining Loss: 2.196833 \tValidation Loss: 2.348345\n",
      "Epoch: 5736 \tTraining Loss: 2.207483 \tValidation Loss: 2.348547\n",
      "Epoch: 5737 \tTraining Loss: 2.198359 \tValidation Loss: 2.348691\n",
      "Epoch: 5738 \tTraining Loss: 2.210530 \tValidation Loss: 2.348699\n",
      "Epoch: 5739 \tTraining Loss: 2.192749 \tValidation Loss: 2.348540\n",
      "Epoch: 5740 \tTraining Loss: 2.218582 \tValidation Loss: 2.348597\n",
      "Epoch: 5741 \tTraining Loss: 2.209199 \tValidation Loss: 2.348465\n",
      "Epoch: 5742 \tTraining Loss: 2.197493 \tValidation Loss: 2.348412\n",
      "Epoch: 5743 \tTraining Loss: 2.203717 \tValidation Loss: 2.348495\n",
      "Epoch: 5744 \tTraining Loss: 2.200016 \tValidation Loss: 2.348443\n",
      "Epoch: 5745 \tTraining Loss: 2.213675 \tValidation Loss: 2.348576\n",
      "Epoch: 5746 \tTraining Loss: 2.213150 \tValidation Loss: 2.348540\n",
      "Epoch: 5747 \tTraining Loss: 2.191227 \tValidation Loss: 2.348479\n",
      "Epoch: 5748 \tTraining Loss: 2.198463 \tValidation Loss: 2.348592\n",
      "Epoch: 5749 \tTraining Loss: 2.185454 \tValidation Loss: 2.348617\n",
      "Epoch: 5750 \tTraining Loss: 2.185423 \tValidation Loss: 2.348279\n",
      "Validation loss decreased (2.348289 --> 2.348279).  Saving model ...\n",
      "Epoch: 5751 \tTraining Loss: 2.201569 \tValidation Loss: 2.348078\n",
      "Validation loss decreased (2.348279 --> 2.348078).  Saving model ...\n",
      "Epoch: 5752 \tTraining Loss: 2.192170 \tValidation Loss: 2.348166\n",
      "Epoch: 5753 \tTraining Loss: 2.174346 \tValidation Loss: 2.348023\n",
      "Validation loss decreased (2.348078 --> 2.348023).  Saving model ...\n",
      "Epoch: 5754 \tTraining Loss: 2.217063 \tValidation Loss: 2.348210\n",
      "Epoch: 5755 \tTraining Loss: 2.200616 \tValidation Loss: 2.348334\n",
      "Epoch: 5756 \tTraining Loss: 2.188098 \tValidation Loss: 2.348384\n",
      "Epoch: 5757 \tTraining Loss: 2.203007 \tValidation Loss: 2.348381\n",
      "Epoch: 5758 \tTraining Loss: 2.197191 \tValidation Loss: 2.348312\n",
      "Epoch: 5759 \tTraining Loss: 2.175498 \tValidation Loss: 2.348144\n",
      "Epoch: 5760 \tTraining Loss: 2.203565 \tValidation Loss: 2.347978\n",
      "Validation loss decreased (2.348023 --> 2.347978).  Saving model ...\n",
      "Epoch: 5761 \tTraining Loss: 2.196246 \tValidation Loss: 2.348035\n",
      "Epoch: 5762 \tTraining Loss: 2.201874 \tValidation Loss: 2.348011\n",
      "Epoch: 5763 \tTraining Loss: 2.204591 \tValidation Loss: 2.348263\n",
      "Epoch: 5764 \tTraining Loss: 2.194458 \tValidation Loss: 2.348130\n",
      "Epoch: 5765 \tTraining Loss: 2.169606 \tValidation Loss: 2.348102\n",
      "Epoch: 5766 \tTraining Loss: 2.185050 \tValidation Loss: 2.347999\n",
      "Epoch: 5767 \tTraining Loss: 2.197846 \tValidation Loss: 2.348030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5768 \tTraining Loss: 2.210670 \tValidation Loss: 2.348172\n",
      "Epoch: 5769 \tTraining Loss: 2.190269 \tValidation Loss: 2.348166\n",
      "Epoch: 5770 \tTraining Loss: 2.193595 \tValidation Loss: 2.347919\n",
      "Validation loss decreased (2.347978 --> 2.347919).  Saving model ...\n",
      "Epoch: 5771 \tTraining Loss: 2.179708 \tValidation Loss: 2.347651\n",
      "Validation loss decreased (2.347919 --> 2.347651).  Saving model ...\n",
      "Epoch: 5772 \tTraining Loss: 2.194036 \tValidation Loss: 2.347659\n",
      "Epoch: 5773 \tTraining Loss: 2.215097 \tValidation Loss: 2.347883\n",
      "Epoch: 5774 \tTraining Loss: 2.196041 \tValidation Loss: 2.347675\n",
      "Epoch: 5775 \tTraining Loss: 2.212862 \tValidation Loss: 2.348001\n",
      "Epoch: 5776 \tTraining Loss: 2.190712 \tValidation Loss: 2.347974\n",
      "Epoch: 5777 \tTraining Loss: 2.218313 \tValidation Loss: 2.347999\n",
      "Epoch: 5778 \tTraining Loss: 2.216834 \tValidation Loss: 2.348039\n",
      "Epoch: 5779 \tTraining Loss: 2.189339 \tValidation Loss: 2.348232\n",
      "Epoch: 5780 \tTraining Loss: 2.201988 \tValidation Loss: 2.348305\n",
      "Epoch: 5781 \tTraining Loss: 2.193456 \tValidation Loss: 2.348030\n",
      "Epoch: 5782 \tTraining Loss: 2.190030 \tValidation Loss: 2.347865\n",
      "Epoch: 5783 \tTraining Loss: 2.212955 \tValidation Loss: 2.348004\n",
      "Epoch: 5784 \tTraining Loss: 2.198528 \tValidation Loss: 2.347924\n",
      "Epoch: 5785 \tTraining Loss: 2.205071 \tValidation Loss: 2.347845\n",
      "Epoch: 5786 \tTraining Loss: 2.186825 \tValidation Loss: 2.347707\n",
      "Epoch: 5787 \tTraining Loss: 2.202934 \tValidation Loss: 2.347692\n",
      "Epoch: 5788 \tTraining Loss: 2.198715 \tValidation Loss: 2.347652\n",
      "Epoch: 5789 \tTraining Loss: 2.184359 \tValidation Loss: 2.347526\n",
      "Validation loss decreased (2.347651 --> 2.347526).  Saving model ...\n",
      "Epoch: 5790 \tTraining Loss: 2.176033 \tValidation Loss: 2.347329\n",
      "Validation loss decreased (2.347526 --> 2.347329).  Saving model ...\n",
      "Epoch: 5791 \tTraining Loss: 2.217671 \tValidation Loss: 2.347566\n",
      "Epoch: 5792 \tTraining Loss: 2.181422 \tValidation Loss: 2.347528\n",
      "Epoch: 5793 \tTraining Loss: 2.203536 \tValidation Loss: 2.347592\n",
      "Epoch: 5794 \tTraining Loss: 2.176172 \tValidation Loss: 2.347493\n",
      "Epoch: 5795 \tTraining Loss: 2.176078 \tValidation Loss: 2.347525\n",
      "Epoch: 5796 \tTraining Loss: 2.199842 \tValidation Loss: 2.347645\n",
      "Epoch: 5797 \tTraining Loss: 2.201581 \tValidation Loss: 2.347627\n",
      "Epoch: 5798 \tTraining Loss: 2.182087 \tValidation Loss: 2.347412\n",
      "Epoch: 5799 \tTraining Loss: 2.212531 \tValidation Loss: 2.347291\n",
      "Validation loss decreased (2.347329 --> 2.347291).  Saving model ...\n",
      "Epoch: 5800 \tTraining Loss: 2.189843 \tValidation Loss: 2.347251\n",
      "Validation loss decreased (2.347291 --> 2.347251).  Saving model ...\n",
      "Epoch: 5801 \tTraining Loss: 2.180919 \tValidation Loss: 2.347193\n",
      "Validation loss decreased (2.347251 --> 2.347193).  Saving model ...\n",
      "Epoch: 5802 \tTraining Loss: 2.201293 \tValidation Loss: 2.346979\n",
      "Validation loss decreased (2.347193 --> 2.346979).  Saving model ...\n",
      "Epoch: 5803 \tTraining Loss: 2.186502 \tValidation Loss: 2.346905\n",
      "Validation loss decreased (2.346979 --> 2.346905).  Saving model ...\n",
      "Epoch: 5804 \tTraining Loss: 2.220981 \tValidation Loss: 2.346687\n",
      "Validation loss decreased (2.346905 --> 2.346687).  Saving model ...\n",
      "Epoch: 5805 \tTraining Loss: 2.190915 \tValidation Loss: 2.346652\n",
      "Validation loss decreased (2.346687 --> 2.346652).  Saving model ...\n",
      "Epoch: 5806 \tTraining Loss: 2.208464 \tValidation Loss: 2.346763\n",
      "Epoch: 5807 \tTraining Loss: 2.197312 \tValidation Loss: 2.346757\n",
      "Epoch: 5808 \tTraining Loss: 2.177603 \tValidation Loss: 2.346622\n",
      "Validation loss decreased (2.346652 --> 2.346622).  Saving model ...\n",
      "Epoch: 5809 \tTraining Loss: 2.174927 \tValidation Loss: 2.346680\n",
      "Epoch: 5810 \tTraining Loss: 2.185119 \tValidation Loss: 2.346516\n",
      "Validation loss decreased (2.346622 --> 2.346516).  Saving model ...\n",
      "Epoch: 5811 \tTraining Loss: 2.202893 \tValidation Loss: 2.346695\n",
      "Epoch: 5812 \tTraining Loss: 2.181833 \tValidation Loss: 2.346866\n",
      "Epoch: 5813 \tTraining Loss: 2.180865 \tValidation Loss: 2.346653\n",
      "Epoch: 5814 \tTraining Loss: 2.184877 \tValidation Loss: 2.346694\n",
      "Epoch: 5815 \tTraining Loss: 2.190484 \tValidation Loss: 2.346638\n",
      "Epoch: 5816 \tTraining Loss: 2.182616 \tValidation Loss: 2.346392\n",
      "Validation loss decreased (2.346516 --> 2.346392).  Saving model ...\n",
      "Epoch: 5817 \tTraining Loss: 2.172394 \tValidation Loss: 2.346265\n",
      "Validation loss decreased (2.346392 --> 2.346265).  Saving model ...\n",
      "Epoch: 5818 \tTraining Loss: 2.195179 \tValidation Loss: 2.346233\n",
      "Validation loss decreased (2.346265 --> 2.346233).  Saving model ...\n",
      "Epoch: 5819 \tTraining Loss: 2.196998 \tValidation Loss: 2.346462\n",
      "Epoch: 5820 \tTraining Loss: 2.190836 \tValidation Loss: 2.346512\n",
      "Epoch: 5821 \tTraining Loss: 2.196905 \tValidation Loss: 2.346552\n",
      "Epoch: 5822 \tTraining Loss: 2.191777 \tValidation Loss: 2.346495\n",
      "Epoch: 5823 \tTraining Loss: 2.198125 \tValidation Loss: 2.346669\n",
      "Epoch: 5824 \tTraining Loss: 2.186543 \tValidation Loss: 2.346666\n",
      "Epoch: 5825 \tTraining Loss: 2.194192 \tValidation Loss: 2.346524\n",
      "Epoch: 5826 \tTraining Loss: 2.176710 \tValidation Loss: 2.346574\n",
      "Epoch: 5827 \tTraining Loss: 2.200063 \tValidation Loss: 2.346515\n",
      "Epoch: 5828 \tTraining Loss: 2.194078 \tValidation Loss: 2.346485\n",
      "Epoch: 5829 \tTraining Loss: 2.198974 \tValidation Loss: 2.346690\n",
      "Epoch: 5830 \tTraining Loss: 2.212700 \tValidation Loss: 2.346684\n",
      "Epoch: 5831 \tTraining Loss: 2.164084 \tValidation Loss: 2.346478\n",
      "Epoch: 5832 \tTraining Loss: 2.200238 \tValidation Loss: 2.346555\n",
      "Epoch: 5833 \tTraining Loss: 2.209463 \tValidation Loss: 2.346762\n",
      "Epoch: 5834 \tTraining Loss: 2.176054 \tValidation Loss: 2.346727\n",
      "Epoch: 5835 \tTraining Loss: 2.188999 \tValidation Loss: 2.346797\n",
      "Epoch: 5836 \tTraining Loss: 2.191829 \tValidation Loss: 2.346680\n",
      "Epoch: 5837 \tTraining Loss: 2.203497 \tValidation Loss: 2.346523\n",
      "Epoch: 5838 \tTraining Loss: 2.189627 \tValidation Loss: 2.346672\n",
      "Epoch: 5839 \tTraining Loss: 2.209149 \tValidation Loss: 2.346697\n",
      "Epoch: 5840 \tTraining Loss: 2.190481 \tValidation Loss: 2.346451\n",
      "Epoch: 5841 \tTraining Loss: 2.189468 \tValidation Loss: 2.346145\n",
      "Validation loss decreased (2.346233 --> 2.346145).  Saving model ...\n",
      "Epoch: 5842 \tTraining Loss: 2.185931 \tValidation Loss: 2.346284\n",
      "Epoch: 5843 \tTraining Loss: 2.195890 \tValidation Loss: 2.346131\n",
      "Validation loss decreased (2.346145 --> 2.346131).  Saving model ...\n",
      "Epoch: 5844 \tTraining Loss: 2.195013 \tValidation Loss: 2.346115\n",
      "Validation loss decreased (2.346131 --> 2.346115).  Saving model ...\n",
      "Epoch: 5845 \tTraining Loss: 2.191194 \tValidation Loss: 2.346084\n",
      "Validation loss decreased (2.346115 --> 2.346084).  Saving model ...\n",
      "Epoch: 5846 \tTraining Loss: 2.204214 \tValidation Loss: 2.346109\n",
      "Epoch: 5847 \tTraining Loss: 2.198322 \tValidation Loss: 2.345941\n",
      "Validation loss decreased (2.346084 --> 2.345941).  Saving model ...\n",
      "Epoch: 5848 \tTraining Loss: 2.200266 \tValidation Loss: 2.345899\n",
      "Validation loss decreased (2.345941 --> 2.345899).  Saving model ...\n",
      "Epoch: 5849 \tTraining Loss: 2.194515 \tValidation Loss: 2.345913\n",
      "Epoch: 5850 \tTraining Loss: 2.182314 \tValidation Loss: 2.346086\n",
      "Epoch: 5851 \tTraining Loss: 2.191155 \tValidation Loss: 2.345984\n",
      "Epoch: 5852 \tTraining Loss: 2.192773 \tValidation Loss: 2.345894\n",
      "Validation loss decreased (2.345899 --> 2.345894).  Saving model ...\n",
      "Epoch: 5853 \tTraining Loss: 2.168390 \tValidation Loss: 2.346047\n",
      "Epoch: 5854 \tTraining Loss: 2.191303 \tValidation Loss: 2.345957\n",
      "Epoch: 5855 \tTraining Loss: 2.182409 \tValidation Loss: 2.345757\n",
      "Validation loss decreased (2.345894 --> 2.345757).  Saving model ...\n",
      "Epoch: 5856 \tTraining Loss: 2.201802 \tValidation Loss: 2.345741\n",
      "Validation loss decreased (2.345757 --> 2.345741).  Saving model ...\n",
      "Epoch: 5857 \tTraining Loss: 2.191574 \tValidation Loss: 2.345899\n",
      "Epoch: 5858 \tTraining Loss: 2.184678 \tValidation Loss: 2.346185\n",
      "Epoch: 5859 \tTraining Loss: 2.187494 \tValidation Loss: 2.345996\n",
      "Epoch: 5860 \tTraining Loss: 2.179384 \tValidation Loss: 2.345932\n",
      "Epoch: 5861 \tTraining Loss: 2.171613 \tValidation Loss: 2.346026\n",
      "Epoch: 5862 \tTraining Loss: 2.176292 \tValidation Loss: 2.346014\n",
      "Epoch: 5863 \tTraining Loss: 2.190701 \tValidation Loss: 2.346070\n",
      "Epoch: 5864 \tTraining Loss: 2.192139 \tValidation Loss: 2.346030\n",
      "Epoch: 5865 \tTraining Loss: 2.212760 \tValidation Loss: 2.345938\n",
      "Epoch: 5866 \tTraining Loss: 2.175146 \tValidation Loss: 2.345946\n",
      "Epoch: 5867 \tTraining Loss: 2.203164 \tValidation Loss: 2.345868\n",
      "Epoch: 5868 \tTraining Loss: 2.168753 \tValidation Loss: 2.345530\n",
      "Validation loss decreased (2.345741 --> 2.345530).  Saving model ...\n",
      "Epoch: 5869 \tTraining Loss: 2.203626 \tValidation Loss: 2.345634\n",
      "Epoch: 5870 \tTraining Loss: 2.184114 \tValidation Loss: 2.345553\n",
      "Epoch: 5871 \tTraining Loss: 2.185129 \tValidation Loss: 2.345556\n",
      "Epoch: 5872 \tTraining Loss: 2.209024 \tValidation Loss: 2.345632\n",
      "Epoch: 5873 \tTraining Loss: 2.181678 \tValidation Loss: 2.345807\n",
      "Epoch: 5874 \tTraining Loss: 2.198953 \tValidation Loss: 2.345711\n",
      "Epoch: 5875 \tTraining Loss: 2.199326 \tValidation Loss: 2.345354\n",
      "Validation loss decreased (2.345530 --> 2.345354).  Saving model ...\n",
      "Epoch: 5876 \tTraining Loss: 2.202125 \tValidation Loss: 2.345679\n",
      "Epoch: 5877 \tTraining Loss: 2.203197 \tValidation Loss: 2.345860\n",
      "Epoch: 5878 \tTraining Loss: 2.196582 \tValidation Loss: 2.346002\n",
      "Epoch: 5879 \tTraining Loss: 2.189994 \tValidation Loss: 2.345702\n",
      "Epoch: 5880 \tTraining Loss: 2.177958 \tValidation Loss: 2.345587\n",
      "Epoch: 5881 \tTraining Loss: 2.160195 \tValidation Loss: 2.345464\n",
      "Epoch: 5882 \tTraining Loss: 2.209283 \tValidation Loss: 2.345587\n",
      "Epoch: 5883 \tTraining Loss: 2.204952 \tValidation Loss: 2.345462\n",
      "Epoch: 5884 \tTraining Loss: 2.185659 \tValidation Loss: 2.345513\n",
      "Epoch: 5885 \tTraining Loss: 2.197558 \tValidation Loss: 2.345842\n",
      "Epoch: 5886 \tTraining Loss: 2.168843 \tValidation Loss: 2.345655\n",
      "Epoch: 5887 \tTraining Loss: 2.193453 \tValidation Loss: 2.345902\n",
      "Epoch: 5888 \tTraining Loss: 2.202298 \tValidation Loss: 2.345714\n",
      "Epoch: 5889 \tTraining Loss: 2.193921 \tValidation Loss: 2.345469\n",
      "Epoch: 5890 \tTraining Loss: 2.175753 \tValidation Loss: 2.345666\n",
      "Epoch: 5891 \tTraining Loss: 2.186623 \tValidation Loss: 2.345640\n",
      "Epoch: 5892 \tTraining Loss: 2.174309 \tValidation Loss: 2.345277\n",
      "Validation loss decreased (2.345354 --> 2.345277).  Saving model ...\n",
      "Epoch: 5893 \tTraining Loss: 2.200779 \tValidation Loss: 2.345431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5894 \tTraining Loss: 2.182324 \tValidation Loss: 2.345374\n",
      "Epoch: 5895 \tTraining Loss: 2.209712 \tValidation Loss: 2.345327\n",
      "Epoch: 5896 \tTraining Loss: 2.190499 \tValidation Loss: 2.345125\n",
      "Validation loss decreased (2.345277 --> 2.345125).  Saving model ...\n",
      "Epoch: 5897 \tTraining Loss: 2.202011 \tValidation Loss: 2.345021\n",
      "Validation loss decreased (2.345125 --> 2.345021).  Saving model ...\n",
      "Epoch: 5898 \tTraining Loss: 2.185150 \tValidation Loss: 2.345171\n",
      "Epoch: 5899 \tTraining Loss: 2.205246 \tValidation Loss: 2.345391\n",
      "Epoch: 5900 \tTraining Loss: 2.183277 \tValidation Loss: 2.345253\n",
      "Epoch: 5901 \tTraining Loss: 2.207737 \tValidation Loss: 2.345202\n",
      "Epoch: 5902 \tTraining Loss: 2.189827 \tValidation Loss: 2.345232\n",
      "Epoch: 5903 \tTraining Loss: 2.198030 \tValidation Loss: 2.345321\n",
      "Epoch: 5904 \tTraining Loss: 2.180081 \tValidation Loss: 2.345212\n",
      "Epoch: 5905 \tTraining Loss: 2.222602 \tValidation Loss: 2.344954\n",
      "Validation loss decreased (2.345021 --> 2.344954).  Saving model ...\n",
      "Epoch: 5906 \tTraining Loss: 2.180985 \tValidation Loss: 2.344725\n",
      "Validation loss decreased (2.344954 --> 2.344725).  Saving model ...\n",
      "Epoch: 5907 \tTraining Loss: 2.194136 \tValidation Loss: 2.344847\n",
      "Epoch: 5908 \tTraining Loss: 2.176064 \tValidation Loss: 2.344650\n",
      "Validation loss decreased (2.344725 --> 2.344650).  Saving model ...\n",
      "Epoch: 5909 \tTraining Loss: 2.185014 \tValidation Loss: 2.344402\n",
      "Validation loss decreased (2.344650 --> 2.344402).  Saving model ...\n",
      "Epoch: 5910 \tTraining Loss: 2.200277 \tValidation Loss: 2.344387\n",
      "Validation loss decreased (2.344402 --> 2.344387).  Saving model ...\n",
      "Epoch: 5911 \tTraining Loss: 2.187819 \tValidation Loss: 2.344532\n",
      "Epoch: 5912 \tTraining Loss: 2.184260 \tValidation Loss: 2.344606\n",
      "Epoch: 5913 \tTraining Loss: 2.202583 \tValidation Loss: 2.344387\n",
      "Validation loss decreased (2.344387 --> 2.344387).  Saving model ...\n",
      "Epoch: 5914 \tTraining Loss: 2.189201 \tValidation Loss: 2.344137\n",
      "Validation loss decreased (2.344387 --> 2.344137).  Saving model ...\n",
      "Epoch: 5915 \tTraining Loss: 2.179257 \tValidation Loss: 2.344270\n",
      "Epoch: 5916 \tTraining Loss: 2.179354 \tValidation Loss: 2.344392\n",
      "Epoch: 5917 \tTraining Loss: 2.168931 \tValidation Loss: 2.344429\n",
      "Epoch: 5918 \tTraining Loss: 2.179657 \tValidation Loss: 2.344316\n",
      "Epoch: 5919 \tTraining Loss: 2.179901 \tValidation Loss: 2.344258\n",
      "Epoch: 5920 \tTraining Loss: 2.200831 \tValidation Loss: 2.344321\n",
      "Epoch: 5921 \tTraining Loss: 2.189048 \tValidation Loss: 2.344312\n",
      "Epoch: 5922 \tTraining Loss: 2.196594 \tValidation Loss: 2.344323\n",
      "Epoch: 5923 \tTraining Loss: 2.179862 \tValidation Loss: 2.344377\n",
      "Epoch: 5924 \tTraining Loss: 2.170450 \tValidation Loss: 2.344331\n",
      "Epoch: 5925 \tTraining Loss: 2.178237 \tValidation Loss: 2.344446\n",
      "Epoch: 5926 \tTraining Loss: 2.178404 \tValidation Loss: 2.344489\n",
      "Epoch: 5927 \tTraining Loss: 2.194774 \tValidation Loss: 2.344475\n",
      "Epoch: 5928 \tTraining Loss: 2.196254 \tValidation Loss: 2.344257\n",
      "Epoch: 5929 \tTraining Loss: 2.177997 \tValidation Loss: 2.344345\n",
      "Epoch: 5930 \tTraining Loss: 2.180360 \tValidation Loss: 2.344149\n",
      "Epoch: 5931 \tTraining Loss: 2.164804 \tValidation Loss: 2.343912\n",
      "Validation loss decreased (2.344137 --> 2.343912).  Saving model ...\n",
      "Epoch: 5932 \tTraining Loss: 2.195379 \tValidation Loss: 2.343710\n",
      "Validation loss decreased (2.343912 --> 2.343710).  Saving model ...\n",
      "Epoch: 5933 \tTraining Loss: 2.170987 \tValidation Loss: 2.343680\n",
      "Validation loss decreased (2.343710 --> 2.343680).  Saving model ...\n",
      "Epoch: 5934 \tTraining Loss: 2.206404 \tValidation Loss: 2.343902\n",
      "Epoch: 5935 \tTraining Loss: 2.176360 \tValidation Loss: 2.344108\n",
      "Epoch: 5936 \tTraining Loss: 2.159859 \tValidation Loss: 2.344170\n",
      "Epoch: 5937 \tTraining Loss: 2.168668 \tValidation Loss: 2.344133\n",
      "Epoch: 5938 \tTraining Loss: 2.171968 \tValidation Loss: 2.343964\n",
      "Epoch: 5939 \tTraining Loss: 2.195101 \tValidation Loss: 2.344091\n",
      "Epoch: 5940 \tTraining Loss: 2.170508 \tValidation Loss: 2.344090\n",
      "Epoch: 5941 \tTraining Loss: 2.187490 \tValidation Loss: 2.343997\n",
      "Epoch: 5942 \tTraining Loss: 2.177578 \tValidation Loss: 2.343865\n",
      "Epoch: 5943 \tTraining Loss: 2.175258 \tValidation Loss: 2.343697\n",
      "Epoch: 5944 \tTraining Loss: 2.173641 \tValidation Loss: 2.343677\n",
      "Validation loss decreased (2.343680 --> 2.343677).  Saving model ...\n",
      "Epoch: 5945 \tTraining Loss: 2.179269 \tValidation Loss: 2.343797\n",
      "Epoch: 5946 \tTraining Loss: 2.174232 \tValidation Loss: 2.343649\n",
      "Validation loss decreased (2.343677 --> 2.343649).  Saving model ...\n",
      "Epoch: 5947 \tTraining Loss: 2.177197 \tValidation Loss: 2.343802\n",
      "Epoch: 5948 \tTraining Loss: 2.162520 \tValidation Loss: 2.343639\n",
      "Validation loss decreased (2.343649 --> 2.343639).  Saving model ...\n",
      "Epoch: 5949 \tTraining Loss: 2.185054 \tValidation Loss: 2.343818\n",
      "Epoch: 5950 \tTraining Loss: 2.172572 \tValidation Loss: 2.343770\n",
      "Epoch: 5951 \tTraining Loss: 2.159256 \tValidation Loss: 2.343704\n",
      "Epoch: 5952 \tTraining Loss: 2.196317 \tValidation Loss: 2.343739\n",
      "Epoch: 5953 \tTraining Loss: 2.179281 \tValidation Loss: 2.343554\n",
      "Validation loss decreased (2.343639 --> 2.343554).  Saving model ...\n",
      "Epoch: 5954 \tTraining Loss: 2.209207 \tValidation Loss: 2.343869\n",
      "Epoch: 5955 \tTraining Loss: 2.183234 \tValidation Loss: 2.343915\n",
      "Epoch: 5956 \tTraining Loss: 2.182306 \tValidation Loss: 2.344020\n",
      "Epoch: 5957 \tTraining Loss: 2.184119 \tValidation Loss: 2.344175\n",
      "Epoch: 5958 \tTraining Loss: 2.176566 \tValidation Loss: 2.344103\n",
      "Epoch: 5959 \tTraining Loss: 2.191931 \tValidation Loss: 2.344155\n",
      "Epoch: 5960 \tTraining Loss: 2.175296 \tValidation Loss: 2.344050\n",
      "Epoch: 5961 \tTraining Loss: 2.182356 \tValidation Loss: 2.343983\n",
      "Epoch: 5962 \tTraining Loss: 2.165966 \tValidation Loss: 2.343798\n",
      "Epoch: 5963 \tTraining Loss: 2.173503 \tValidation Loss: 2.343637\n",
      "Epoch: 5964 \tTraining Loss: 2.167528 \tValidation Loss: 2.343622\n",
      "Epoch: 5965 \tTraining Loss: 2.189015 \tValidation Loss: 2.343456\n",
      "Validation loss decreased (2.343554 --> 2.343456).  Saving model ...\n",
      "Epoch: 5966 \tTraining Loss: 2.176450 \tValidation Loss: 2.343518\n",
      "Epoch: 5967 \tTraining Loss: 2.176820 \tValidation Loss: 2.343485\n",
      "Epoch: 5968 \tTraining Loss: 2.186091 \tValidation Loss: 2.343520\n",
      "Epoch: 5969 \tTraining Loss: 2.181418 \tValidation Loss: 2.343534\n",
      "Epoch: 5970 \tTraining Loss: 2.214266 \tValidation Loss: 2.343615\n",
      "Epoch: 5971 \tTraining Loss: 2.172658 \tValidation Loss: 2.343719\n",
      "Epoch: 5972 \tTraining Loss: 2.175538 \tValidation Loss: 2.343746\n",
      "Epoch: 5973 \tTraining Loss: 2.193420 \tValidation Loss: 2.343632\n",
      "Epoch: 5974 \tTraining Loss: 2.162812 \tValidation Loss: 2.343569\n",
      "Epoch: 5975 \tTraining Loss: 2.185883 \tValidation Loss: 2.343573\n",
      "Epoch: 5976 \tTraining Loss: 2.177113 \tValidation Loss: 2.343560\n",
      "Epoch: 5977 \tTraining Loss: 2.192904 \tValidation Loss: 2.343624\n",
      "Epoch: 5978 \tTraining Loss: 2.177599 \tValidation Loss: 2.343726\n",
      "Epoch: 5979 \tTraining Loss: 2.167172 \tValidation Loss: 2.343816\n",
      "Epoch: 5980 \tTraining Loss: 2.187756 \tValidation Loss: 2.343639\n",
      "Epoch: 5981 \tTraining Loss: 2.199118 \tValidation Loss: 2.343951\n",
      "Epoch: 5982 \tTraining Loss: 2.212211 \tValidation Loss: 2.343970\n",
      "Epoch: 5983 \tTraining Loss: 2.188969 \tValidation Loss: 2.343826\n",
      "Epoch: 5984 \tTraining Loss: 2.174239 \tValidation Loss: 2.343676\n",
      "Epoch: 5985 \tTraining Loss: 2.158121 \tValidation Loss: 2.343481\n",
      "Epoch: 5986 \tTraining Loss: 2.194672 \tValidation Loss: 2.343596\n",
      "Epoch: 5987 \tTraining Loss: 2.169138 \tValidation Loss: 2.343610\n",
      "Epoch: 5988 \tTraining Loss: 2.194514 \tValidation Loss: 2.343521\n",
      "Epoch: 5989 \tTraining Loss: 2.197944 \tValidation Loss: 2.343460\n",
      "Epoch: 5990 \tTraining Loss: 2.168468 \tValidation Loss: 2.343379\n",
      "Validation loss decreased (2.343456 --> 2.343379).  Saving model ...\n",
      "Epoch: 5991 \tTraining Loss: 2.201674 \tValidation Loss: 2.343475\n",
      "Epoch: 5992 \tTraining Loss: 2.180440 \tValidation Loss: 2.343207\n",
      "Validation loss decreased (2.343379 --> 2.343207).  Saving model ...\n",
      "Epoch: 5993 \tTraining Loss: 2.183012 \tValidation Loss: 2.343098\n",
      "Validation loss decreased (2.343207 --> 2.343098).  Saving model ...\n",
      "Epoch: 5994 \tTraining Loss: 2.192284 \tValidation Loss: 2.343143\n",
      "Epoch: 5995 \tTraining Loss: 2.161372 \tValidation Loss: 2.342854\n",
      "Validation loss decreased (2.343098 --> 2.342854).  Saving model ...\n",
      "Epoch: 5996 \tTraining Loss: 2.179060 \tValidation Loss: 2.342870\n",
      "Epoch: 5997 \tTraining Loss: 2.183432 \tValidation Loss: 2.342782\n",
      "Validation loss decreased (2.342854 --> 2.342782).  Saving model ...\n",
      "Epoch: 5998 \tTraining Loss: 2.194641 \tValidation Loss: 2.342896\n",
      "Epoch: 5999 \tTraining Loss: 2.174715 \tValidation Loss: 2.342872\n",
      "Epoch: 6000 \tTraining Loss: 2.165075 \tValidation Loss: 2.342711\n",
      "Validation loss decreased (2.342782 --> 2.342711).  Saving model ...\n",
      "Epoch: 6001 \tTraining Loss: 2.161969 \tValidation Loss: 2.342595\n",
      "Validation loss decreased (2.342711 --> 2.342595).  Saving model ...\n",
      "Epoch: 6002 \tTraining Loss: 2.177154 \tValidation Loss: 2.342686\n",
      "Epoch: 6003 \tTraining Loss: 2.188862 \tValidation Loss: 2.342894\n",
      "Epoch: 6004 \tTraining Loss: 2.193048 \tValidation Loss: 2.342674\n",
      "Epoch: 6005 \tTraining Loss: 2.189010 \tValidation Loss: 2.342941\n",
      "Epoch: 6006 \tTraining Loss: 2.167341 \tValidation Loss: 2.342852\n",
      "Epoch: 6007 \tTraining Loss: 2.163134 \tValidation Loss: 2.342878\n",
      "Epoch: 6008 \tTraining Loss: 2.175113 \tValidation Loss: 2.342889\n",
      "Epoch: 6009 \tTraining Loss: 2.174723 \tValidation Loss: 2.342740\n",
      "Epoch: 6010 \tTraining Loss: 2.205532 \tValidation Loss: 2.342816\n",
      "Epoch: 6011 \tTraining Loss: 2.189337 \tValidation Loss: 2.342950\n",
      "Epoch: 6012 \tTraining Loss: 2.180776 \tValidation Loss: 2.343282\n",
      "Epoch: 6013 \tTraining Loss: 2.188219 \tValidation Loss: 2.343211\n",
      "Epoch: 6014 \tTraining Loss: 2.158821 \tValidation Loss: 2.342912\n",
      "Epoch: 6015 \tTraining Loss: 2.182919 \tValidation Loss: 2.342918\n",
      "Epoch: 6016 \tTraining Loss: 2.202559 \tValidation Loss: 2.343107\n",
      "Epoch: 6017 \tTraining Loss: 2.180258 \tValidation Loss: 2.342699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6018 \tTraining Loss: 2.187904 \tValidation Loss: 2.342826\n",
      "Epoch: 6019 \tTraining Loss: 2.205608 \tValidation Loss: 2.342694\n",
      "Epoch: 6020 \tTraining Loss: 2.191597 \tValidation Loss: 2.342559\n",
      "Validation loss decreased (2.342595 --> 2.342559).  Saving model ...\n",
      "Epoch: 6021 \tTraining Loss: 2.193965 \tValidation Loss: 2.342763\n",
      "Epoch: 6022 \tTraining Loss: 2.189989 \tValidation Loss: 2.342856\n",
      "Epoch: 6023 \tTraining Loss: 2.213611 \tValidation Loss: 2.343096\n",
      "Epoch: 6024 \tTraining Loss: 2.190871 \tValidation Loss: 2.343100\n",
      "Epoch: 6025 \tTraining Loss: 2.191254 \tValidation Loss: 2.343046\n",
      "Epoch: 6026 \tTraining Loss: 2.185938 \tValidation Loss: 2.343034\n",
      "Epoch: 6027 \tTraining Loss: 2.188131 \tValidation Loss: 2.342657\n",
      "Epoch: 6028 \tTraining Loss: 2.174521 \tValidation Loss: 2.342611\n",
      "Epoch: 6029 \tTraining Loss: 2.185576 \tValidation Loss: 2.342653\n",
      "Epoch: 6030 \tTraining Loss: 2.198509 \tValidation Loss: 2.342611\n",
      "Epoch: 6031 \tTraining Loss: 2.198360 \tValidation Loss: 2.342621\n",
      "Epoch: 6032 \tTraining Loss: 2.177840 \tValidation Loss: 2.342801\n",
      "Epoch: 6033 \tTraining Loss: 2.187817 \tValidation Loss: 2.342739\n",
      "Epoch: 6034 \tTraining Loss: 2.194300 \tValidation Loss: 2.342760\n",
      "Epoch: 6035 \tTraining Loss: 2.154372 \tValidation Loss: 2.342589\n",
      "Epoch: 6036 \tTraining Loss: 2.183820 \tValidation Loss: 2.342720\n",
      "Epoch: 6037 \tTraining Loss: 2.166506 \tValidation Loss: 2.342806\n",
      "Epoch: 6038 \tTraining Loss: 2.180848 \tValidation Loss: 2.342954\n",
      "Epoch: 6039 \tTraining Loss: 2.183196 \tValidation Loss: 2.343003\n",
      "Epoch: 6040 \tTraining Loss: 2.167009 \tValidation Loss: 2.342806\n",
      "Epoch: 6041 \tTraining Loss: 2.177353 \tValidation Loss: 2.342828\n",
      "Epoch: 6042 \tTraining Loss: 2.197511 \tValidation Loss: 2.342825\n",
      "Epoch: 6043 \tTraining Loss: 2.167129 \tValidation Loss: 2.342803\n",
      "Epoch: 6044 \tTraining Loss: 2.172662 \tValidation Loss: 2.342720\n",
      "Epoch: 6045 \tTraining Loss: 2.185956 \tValidation Loss: 2.342587\n",
      "Epoch: 6046 \tTraining Loss: 2.165429 \tValidation Loss: 2.342425\n",
      "Validation loss decreased (2.342559 --> 2.342425).  Saving model ...\n",
      "Epoch: 6047 \tTraining Loss: 2.191521 \tValidation Loss: 2.342328\n",
      "Validation loss decreased (2.342425 --> 2.342328).  Saving model ...\n",
      "Epoch: 6048 \tTraining Loss: 2.178391 \tValidation Loss: 2.342328\n",
      "Epoch: 6049 \tTraining Loss: 2.193585 \tValidation Loss: 2.342321\n",
      "Validation loss decreased (2.342328 --> 2.342321).  Saving model ...\n",
      "Epoch: 6050 \tTraining Loss: 2.178796 \tValidation Loss: 2.342325\n",
      "Epoch: 6051 \tTraining Loss: 2.173643 \tValidation Loss: 2.342237\n",
      "Validation loss decreased (2.342321 --> 2.342237).  Saving model ...\n",
      "Epoch: 6052 \tTraining Loss: 2.193682 \tValidation Loss: 2.342442\n",
      "Epoch: 6053 \tTraining Loss: 2.178281 \tValidation Loss: 2.342272\n",
      "Epoch: 6054 \tTraining Loss: 2.189460 \tValidation Loss: 2.342312\n",
      "Epoch: 6055 \tTraining Loss: 2.156637 \tValidation Loss: 2.342186\n",
      "Validation loss decreased (2.342237 --> 2.342186).  Saving model ...\n",
      "Epoch: 6056 \tTraining Loss: 2.195405 \tValidation Loss: 2.342478\n",
      "Epoch: 6057 \tTraining Loss: 2.199116 \tValidation Loss: 2.342401\n",
      "Epoch: 6058 \tTraining Loss: 2.176114 \tValidation Loss: 2.342128\n",
      "Validation loss decreased (2.342186 --> 2.342128).  Saving model ...\n",
      "Epoch: 6059 \tTraining Loss: 2.196481 \tValidation Loss: 2.342024\n",
      "Validation loss decreased (2.342128 --> 2.342024).  Saving model ...\n",
      "Epoch: 6060 \tTraining Loss: 2.171414 \tValidation Loss: 2.342045\n",
      "Epoch: 6061 \tTraining Loss: 2.174045 \tValidation Loss: 2.341994\n",
      "Validation loss decreased (2.342024 --> 2.341994).  Saving model ...\n",
      "Epoch: 6062 \tTraining Loss: 2.176726 \tValidation Loss: 2.342159\n",
      "Epoch: 6063 \tTraining Loss: 2.173586 \tValidation Loss: 2.342097\n",
      "Epoch: 6064 \tTraining Loss: 2.191501 \tValidation Loss: 2.342171\n",
      "Epoch: 6065 \tTraining Loss: 2.176147 \tValidation Loss: 2.342204\n",
      "Epoch: 6066 \tTraining Loss: 2.205138 \tValidation Loss: 2.342143\n",
      "Epoch: 6067 \tTraining Loss: 2.162708 \tValidation Loss: 2.342456\n",
      "Epoch: 6068 \tTraining Loss: 2.157318 \tValidation Loss: 2.342470\n",
      "Epoch: 6069 \tTraining Loss: 2.181903 \tValidation Loss: 2.342309\n",
      "Epoch: 6070 \tTraining Loss: 2.174394 \tValidation Loss: 2.342333\n",
      "Epoch: 6071 \tTraining Loss: 2.171255 \tValidation Loss: 2.342010\n",
      "Epoch: 6072 \tTraining Loss: 2.172583 \tValidation Loss: 2.341930\n",
      "Validation loss decreased (2.341994 --> 2.341930).  Saving model ...\n",
      "Epoch: 6073 \tTraining Loss: 2.170659 \tValidation Loss: 2.341919\n",
      "Validation loss decreased (2.341930 --> 2.341919).  Saving model ...\n",
      "Epoch: 6074 \tTraining Loss: 2.180852 \tValidation Loss: 2.342046\n",
      "Epoch: 6075 \tTraining Loss: 2.170490 \tValidation Loss: 2.341993\n",
      "Epoch: 6076 \tTraining Loss: 2.193785 \tValidation Loss: 2.342055\n",
      "Epoch: 6077 \tTraining Loss: 2.200789 \tValidation Loss: 2.341869\n",
      "Validation loss decreased (2.341919 --> 2.341869).  Saving model ...\n",
      "Epoch: 6078 \tTraining Loss: 2.160939 \tValidation Loss: 2.341634\n",
      "Validation loss decreased (2.341869 --> 2.341634).  Saving model ...\n",
      "Epoch: 6079 \tTraining Loss: 2.182688 \tValidation Loss: 2.341733\n",
      "Epoch: 6080 \tTraining Loss: 2.153732 \tValidation Loss: 2.341731\n",
      "Epoch: 6081 \tTraining Loss: 2.179838 \tValidation Loss: 2.341833\n",
      "Epoch: 6082 \tTraining Loss: 2.183499 \tValidation Loss: 2.341805\n",
      "Epoch: 6083 \tTraining Loss: 2.167592 \tValidation Loss: 2.341910\n",
      "Epoch: 6084 \tTraining Loss: 2.183218 \tValidation Loss: 2.341836\n",
      "Epoch: 6085 \tTraining Loss: 2.184523 \tValidation Loss: 2.341590\n",
      "Validation loss decreased (2.341634 --> 2.341590).  Saving model ...\n",
      "Epoch: 6086 \tTraining Loss: 2.195201 \tValidation Loss: 2.341769\n",
      "Epoch: 6087 \tTraining Loss: 2.152719 \tValidation Loss: 2.341430\n",
      "Validation loss decreased (2.341590 --> 2.341430).  Saving model ...\n",
      "Epoch: 6088 \tTraining Loss: 2.191552 \tValidation Loss: 2.341486\n",
      "Epoch: 6089 \tTraining Loss: 2.175402 \tValidation Loss: 2.341680\n",
      "Epoch: 6090 \tTraining Loss: 2.182954 \tValidation Loss: 2.341801\n",
      "Epoch: 6091 \tTraining Loss: 2.165215 \tValidation Loss: 2.341942\n",
      "Epoch: 6092 \tTraining Loss: 2.197703 \tValidation Loss: 2.342022\n",
      "Epoch: 6093 \tTraining Loss: 2.186394 \tValidation Loss: 2.341931\n",
      "Epoch: 6094 \tTraining Loss: 2.186004 \tValidation Loss: 2.341722\n",
      "Epoch: 6095 \tTraining Loss: 2.166537 \tValidation Loss: 2.341649\n",
      "Epoch: 6096 \tTraining Loss: 2.155506 \tValidation Loss: 2.341459\n",
      "Epoch: 6097 \tTraining Loss: 2.178231 \tValidation Loss: 2.341421\n",
      "Validation loss decreased (2.341430 --> 2.341421).  Saving model ...\n",
      "Epoch: 6098 \tTraining Loss: 2.187543 \tValidation Loss: 2.341567\n",
      "Epoch: 6099 \tTraining Loss: 2.184194 \tValidation Loss: 2.341572\n",
      "Epoch: 6100 \tTraining Loss: 2.148887 \tValidation Loss: 2.341279\n",
      "Validation loss decreased (2.341421 --> 2.341279).  Saving model ...\n",
      "Epoch: 6101 \tTraining Loss: 2.160611 \tValidation Loss: 2.341221\n",
      "Validation loss decreased (2.341279 --> 2.341221).  Saving model ...\n",
      "Epoch: 6102 \tTraining Loss: 2.178822 \tValidation Loss: 2.341331\n",
      "Epoch: 6103 \tTraining Loss: 2.175809 \tValidation Loss: 2.341264\n",
      "Epoch: 6104 \tTraining Loss: 2.161115 \tValidation Loss: 2.341129\n",
      "Validation loss decreased (2.341221 --> 2.341129).  Saving model ...\n",
      "Epoch: 6105 \tTraining Loss: 2.164966 \tValidation Loss: 2.341349\n",
      "Epoch: 6106 \tTraining Loss: 2.194419 \tValidation Loss: 2.341487\n",
      "Epoch: 6107 \tTraining Loss: 2.177998 \tValidation Loss: 2.341509\n",
      "Epoch: 6108 \tTraining Loss: 2.183064 \tValidation Loss: 2.341520\n",
      "Epoch: 6109 \tTraining Loss: 2.193883 \tValidation Loss: 2.341579\n",
      "Epoch: 6110 \tTraining Loss: 2.183112 \tValidation Loss: 2.341728\n",
      "Epoch: 6111 \tTraining Loss: 2.164263 \tValidation Loss: 2.341841\n",
      "Epoch: 6112 \tTraining Loss: 2.175375 \tValidation Loss: 2.341649\n",
      "Epoch: 6113 \tTraining Loss: 2.159595 \tValidation Loss: 2.341766\n",
      "Epoch: 6114 \tTraining Loss: 2.168101 \tValidation Loss: 2.341403\n",
      "Epoch: 6115 \tTraining Loss: 2.156716 \tValidation Loss: 2.341440\n",
      "Epoch: 6116 \tTraining Loss: 2.167087 \tValidation Loss: 2.341205\n",
      "Epoch: 6117 \tTraining Loss: 2.141867 \tValidation Loss: 2.341342\n",
      "Epoch: 6118 \tTraining Loss: 2.188480 \tValidation Loss: 2.341269\n",
      "Epoch: 6119 \tTraining Loss: 2.179610 \tValidation Loss: 2.341419\n",
      "Epoch: 6120 \tTraining Loss: 2.182953 \tValidation Loss: 2.341326\n",
      "Epoch: 6121 \tTraining Loss: 2.165951 \tValidation Loss: 2.341175\n",
      "Epoch: 6122 \tTraining Loss: 2.202221 \tValidation Loss: 2.341209\n",
      "Epoch: 6123 \tTraining Loss: 2.152577 \tValidation Loss: 2.341244\n",
      "Epoch: 6124 \tTraining Loss: 2.189375 \tValidation Loss: 2.341140\n",
      "Epoch: 6125 \tTraining Loss: 2.167800 \tValidation Loss: 2.341040\n",
      "Validation loss decreased (2.341129 --> 2.341040).  Saving model ...\n",
      "Epoch: 6126 \tTraining Loss: 2.173246 \tValidation Loss: 2.340960\n",
      "Validation loss decreased (2.341040 --> 2.340960).  Saving model ...\n",
      "Epoch: 6127 \tTraining Loss: 2.153214 \tValidation Loss: 2.340925\n",
      "Validation loss decreased (2.340960 --> 2.340925).  Saving model ...\n",
      "Epoch: 6128 \tTraining Loss: 2.153725 \tValidation Loss: 2.340652\n",
      "Validation loss decreased (2.340925 --> 2.340652).  Saving model ...\n",
      "Epoch: 6129 \tTraining Loss: 2.166554 \tValidation Loss: 2.340594\n",
      "Validation loss decreased (2.340652 --> 2.340594).  Saving model ...\n",
      "Epoch: 6130 \tTraining Loss: 2.173057 \tValidation Loss: 2.340519\n",
      "Validation loss decreased (2.340594 --> 2.340519).  Saving model ...\n",
      "Epoch: 6131 \tTraining Loss: 2.166447 \tValidation Loss: 2.340528\n",
      "Epoch: 6132 \tTraining Loss: 2.171188 \tValidation Loss: 2.340633\n",
      "Epoch: 6133 \tTraining Loss: 2.186208 \tValidation Loss: 2.341052\n",
      "Epoch: 6134 \tTraining Loss: 2.185067 \tValidation Loss: 2.340876\n",
      "Epoch: 6135 \tTraining Loss: 2.192594 \tValidation Loss: 2.340693\n",
      "Epoch: 6136 \tTraining Loss: 2.166711 \tValidation Loss: 2.340694\n",
      "Epoch: 6137 \tTraining Loss: 2.169350 \tValidation Loss: 2.340503\n",
      "Validation loss decreased (2.340519 --> 2.340503).  Saving model ...\n",
      "Epoch: 6138 \tTraining Loss: 2.180425 \tValidation Loss: 2.340588\n",
      "Epoch: 6139 \tTraining Loss: 2.190615 \tValidation Loss: 2.340555\n",
      "Epoch: 6140 \tTraining Loss: 2.190540 \tValidation Loss: 2.340564\n",
      "Epoch: 6141 \tTraining Loss: 2.169526 \tValidation Loss: 2.340473\n",
      "Validation loss decreased (2.340503 --> 2.340473).  Saving model ...\n",
      "Epoch: 6142 \tTraining Loss: 2.183782 \tValidation Loss: 2.340644\n",
      "Epoch: 6143 \tTraining Loss: 2.178871 \tValidation Loss: 2.340520\n",
      "Epoch: 6144 \tTraining Loss: 2.181849 \tValidation Loss: 2.340804\n",
      "Epoch: 6145 \tTraining Loss: 2.180298 \tValidation Loss: 2.340837\n",
      "Epoch: 6146 \tTraining Loss: 2.176931 \tValidation Loss: 2.340639\n",
      "Epoch: 6147 \tTraining Loss: 2.172712 \tValidation Loss: 2.340638\n",
      "Epoch: 6148 \tTraining Loss: 2.200323 \tValidation Loss: 2.340755\n",
      "Epoch: 6149 \tTraining Loss: 2.165119 \tValidation Loss: 2.340796\n",
      "Epoch: 6150 \tTraining Loss: 2.164734 \tValidation Loss: 2.340545\n",
      "Epoch: 6151 \tTraining Loss: 2.185900 \tValidation Loss: 2.340767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6152 \tTraining Loss: 2.163880 \tValidation Loss: 2.340653\n",
      "Epoch: 6153 \tTraining Loss: 2.184850 \tValidation Loss: 2.340518\n",
      "Epoch: 6154 \tTraining Loss: 2.172774 \tValidation Loss: 2.340274\n",
      "Validation loss decreased (2.340473 --> 2.340274).  Saving model ...\n",
      "Epoch: 6155 \tTraining Loss: 2.173503 \tValidation Loss: 2.340427\n",
      "Epoch: 6156 \tTraining Loss: 2.171047 \tValidation Loss: 2.340166\n",
      "Validation loss decreased (2.340274 --> 2.340166).  Saving model ...\n",
      "Epoch: 6157 \tTraining Loss: 2.171688 \tValidation Loss: 2.340205\n",
      "Epoch: 6158 \tTraining Loss: 2.162041 \tValidation Loss: 2.340231\n",
      "Epoch: 6159 \tTraining Loss: 2.172849 \tValidation Loss: 2.340100\n",
      "Validation loss decreased (2.340166 --> 2.340100).  Saving model ...\n",
      "Epoch: 6160 \tTraining Loss: 2.187525 \tValidation Loss: 2.340125\n",
      "Epoch: 6161 \tTraining Loss: 2.188499 \tValidation Loss: 2.340206\n",
      "Epoch: 6162 \tTraining Loss: 2.170072 \tValidation Loss: 2.340382\n",
      "Epoch: 6163 \tTraining Loss: 2.155411 \tValidation Loss: 2.340343\n",
      "Epoch: 6164 \tTraining Loss: 2.171474 \tValidation Loss: 2.340204\n",
      "Epoch: 6165 \tTraining Loss: 2.164568 \tValidation Loss: 2.340269\n",
      "Epoch: 6166 \tTraining Loss: 2.189500 \tValidation Loss: 2.340391\n",
      "Epoch: 6167 \tTraining Loss: 2.182584 \tValidation Loss: 2.340124\n",
      "Epoch: 6168 \tTraining Loss: 2.175510 \tValidation Loss: 2.340258\n",
      "Epoch: 6169 \tTraining Loss: 2.186366 \tValidation Loss: 2.340069\n",
      "Validation loss decreased (2.340100 --> 2.340069).  Saving model ...\n",
      "Epoch: 6170 \tTraining Loss: 2.157526 \tValidation Loss: 2.339802\n",
      "Validation loss decreased (2.340069 --> 2.339802).  Saving model ...\n",
      "Epoch: 6171 \tTraining Loss: 2.165656 \tValidation Loss: 2.339919\n",
      "Epoch: 6172 \tTraining Loss: 2.190746 \tValidation Loss: 2.339986\n",
      "Epoch: 6173 \tTraining Loss: 2.173692 \tValidation Loss: 2.340028\n",
      "Epoch: 6174 \tTraining Loss: 2.161212 \tValidation Loss: 2.339767\n",
      "Validation loss decreased (2.339802 --> 2.339767).  Saving model ...\n",
      "Epoch: 6175 \tTraining Loss: 2.184629 \tValidation Loss: 2.339745\n",
      "Validation loss decreased (2.339767 --> 2.339745).  Saving model ...\n",
      "Epoch: 6176 \tTraining Loss: 2.165412 \tValidation Loss: 2.339684\n",
      "Validation loss decreased (2.339745 --> 2.339684).  Saving model ...\n",
      "Epoch: 6177 \tTraining Loss: 2.181103 \tValidation Loss: 2.339799\n",
      "Epoch: 6178 \tTraining Loss: 2.175556 \tValidation Loss: 2.340010\n",
      "Epoch: 6179 \tTraining Loss: 2.153542 \tValidation Loss: 2.339799\n",
      "Epoch: 6180 \tTraining Loss: 2.170951 \tValidation Loss: 2.339909\n",
      "Epoch: 6181 \tTraining Loss: 2.186088 \tValidation Loss: 2.339931\n",
      "Epoch: 6182 \tTraining Loss: 2.183990 \tValidation Loss: 2.340060\n",
      "Epoch: 6183 \tTraining Loss: 2.168918 \tValidation Loss: 2.340084\n",
      "Epoch: 6184 \tTraining Loss: 2.157347 \tValidation Loss: 2.339850\n",
      "Epoch: 6185 \tTraining Loss: 2.192521 \tValidation Loss: 2.339929\n",
      "Epoch: 6186 \tTraining Loss: 2.189640 \tValidation Loss: 2.340018\n",
      "Epoch: 6187 \tTraining Loss: 2.175395 \tValidation Loss: 2.339904\n",
      "Epoch: 6188 \tTraining Loss: 2.162977 \tValidation Loss: 2.339789\n",
      "Epoch: 6189 \tTraining Loss: 2.175082 \tValidation Loss: 2.339616\n",
      "Validation loss decreased (2.339684 --> 2.339616).  Saving model ...\n",
      "Epoch: 6190 \tTraining Loss: 2.145332 \tValidation Loss: 2.339253\n",
      "Validation loss decreased (2.339616 --> 2.339253).  Saving model ...\n",
      "Epoch: 6191 \tTraining Loss: 2.171569 \tValidation Loss: 2.339385\n",
      "Epoch: 6192 \tTraining Loss: 2.171138 \tValidation Loss: 2.339235\n",
      "Validation loss decreased (2.339253 --> 2.339235).  Saving model ...\n",
      "Epoch: 6193 \tTraining Loss: 2.183750 \tValidation Loss: 2.339418\n",
      "Epoch: 6194 \tTraining Loss: 2.157178 \tValidation Loss: 2.339521\n",
      "Epoch: 6195 \tTraining Loss: 2.162201 \tValidation Loss: 2.339516\n",
      "Epoch: 6196 \tTraining Loss: 2.170303 \tValidation Loss: 2.339531\n",
      "Epoch: 6197 \tTraining Loss: 2.172309 \tValidation Loss: 2.339589\n",
      "Epoch: 6198 \tTraining Loss: 2.155150 \tValidation Loss: 2.339589\n",
      "Epoch: 6199 \tTraining Loss: 2.174904 \tValidation Loss: 2.339892\n",
      "Epoch: 6200 \tTraining Loss: 2.178205 \tValidation Loss: 2.339586\n",
      "Epoch: 6201 \tTraining Loss: 2.171101 \tValidation Loss: 2.339662\n",
      "Epoch: 6202 \tTraining Loss: 2.182537 \tValidation Loss: 2.339500\n",
      "Epoch: 6203 \tTraining Loss: 2.173926 \tValidation Loss: 2.339666\n",
      "Epoch: 6204 \tTraining Loss: 2.172086 \tValidation Loss: 2.339307\n",
      "Epoch: 6205 \tTraining Loss: 2.156224 \tValidation Loss: 2.339150\n",
      "Validation loss decreased (2.339235 --> 2.339150).  Saving model ...\n",
      "Epoch: 6206 \tTraining Loss: 2.157300 \tValidation Loss: 2.339035\n",
      "Validation loss decreased (2.339150 --> 2.339035).  Saving model ...\n",
      "Epoch: 6207 \tTraining Loss: 2.152860 \tValidation Loss: 2.339085\n",
      "Epoch: 6208 \tTraining Loss: 2.169111 \tValidation Loss: 2.339248\n",
      "Epoch: 6209 \tTraining Loss: 2.159727 \tValidation Loss: 2.339257\n",
      "Epoch: 6210 \tTraining Loss: 2.176734 \tValidation Loss: 2.339415\n",
      "Epoch: 6211 \tTraining Loss: 2.170893 \tValidation Loss: 2.339082\n",
      "Epoch: 6212 \tTraining Loss: 2.164200 \tValidation Loss: 2.339154\n",
      "Epoch: 6213 \tTraining Loss: 2.158243 \tValidation Loss: 2.339063\n",
      "Epoch: 6214 \tTraining Loss: 2.139261 \tValidation Loss: 2.339192\n",
      "Epoch: 6215 \tTraining Loss: 2.178528 \tValidation Loss: 2.339254\n",
      "Epoch: 6216 \tTraining Loss: 2.175627 \tValidation Loss: 2.339383\n",
      "Epoch: 6217 \tTraining Loss: 2.182061 \tValidation Loss: 2.339308\n",
      "Epoch: 6218 \tTraining Loss: 2.166991 \tValidation Loss: 2.339222\n",
      "Epoch: 6219 \tTraining Loss: 2.157716 \tValidation Loss: 2.339213\n",
      "Epoch: 6220 \tTraining Loss: 2.182618 \tValidation Loss: 2.339249\n",
      "Epoch: 6221 \tTraining Loss: 2.159041 \tValidation Loss: 2.339267\n",
      "Epoch: 6222 \tTraining Loss: 2.179900 \tValidation Loss: 2.339122\n",
      "Epoch: 6223 \tTraining Loss: 2.179342 \tValidation Loss: 2.339216\n",
      "Epoch: 6224 \tTraining Loss: 2.171669 \tValidation Loss: 2.339183\n",
      "Epoch: 6225 \tTraining Loss: 2.158448 \tValidation Loss: 2.339395\n",
      "Epoch: 6226 \tTraining Loss: 2.169068 \tValidation Loss: 2.339330\n",
      "Epoch: 6227 \tTraining Loss: 2.158472 \tValidation Loss: 2.338947\n",
      "Validation loss decreased (2.339035 --> 2.338947).  Saving model ...\n",
      "Epoch: 6228 \tTraining Loss: 2.165992 \tValidation Loss: 2.338973\n",
      "Epoch: 6229 \tTraining Loss: 2.197590 \tValidation Loss: 2.339166\n",
      "Epoch: 6230 \tTraining Loss: 2.182504 \tValidation Loss: 2.339199\n",
      "Epoch: 6231 \tTraining Loss: 2.151629 \tValidation Loss: 2.338925\n",
      "Validation loss decreased (2.338947 --> 2.338925).  Saving model ...\n",
      "Epoch: 6232 \tTraining Loss: 2.161997 \tValidation Loss: 2.338951\n",
      "Epoch: 6233 \tTraining Loss: 2.168049 \tValidation Loss: 2.339055\n",
      "Epoch: 6234 \tTraining Loss: 2.178372 \tValidation Loss: 2.339097\n",
      "Epoch: 6235 \tTraining Loss: 2.165796 \tValidation Loss: 2.339072\n",
      "Epoch: 6236 \tTraining Loss: 2.168340 \tValidation Loss: 2.338945\n",
      "Epoch: 6237 \tTraining Loss: 2.161814 \tValidation Loss: 2.338783\n",
      "Validation loss decreased (2.338925 --> 2.338783).  Saving model ...\n",
      "Epoch: 6238 \tTraining Loss: 2.173400 \tValidation Loss: 2.338917\n",
      "Epoch: 6239 \tTraining Loss: 2.170143 \tValidation Loss: 2.338890\n",
      "Epoch: 6240 \tTraining Loss: 2.186870 \tValidation Loss: 2.339039\n",
      "Epoch: 6241 \tTraining Loss: 2.178641 \tValidation Loss: 2.339124\n",
      "Epoch: 6242 \tTraining Loss: 2.178713 \tValidation Loss: 2.339225\n",
      "Epoch: 6243 \tTraining Loss: 2.178071 \tValidation Loss: 2.339436\n",
      "Epoch: 6244 \tTraining Loss: 2.148360 \tValidation Loss: 2.339561\n",
      "Epoch: 6245 \tTraining Loss: 2.180447 \tValidation Loss: 2.339598\n",
      "Epoch: 6246 \tTraining Loss: 2.165363 \tValidation Loss: 2.339516\n",
      "Epoch: 6247 \tTraining Loss: 2.170674 \tValidation Loss: 2.339592\n",
      "Epoch: 6248 \tTraining Loss: 2.167964 \tValidation Loss: 2.339439\n",
      "Epoch: 6249 \tTraining Loss: 2.170665 \tValidation Loss: 2.339165\n",
      "Epoch: 6250 \tTraining Loss: 2.146987 \tValidation Loss: 2.338900\n",
      "Epoch: 6251 \tTraining Loss: 2.167727 \tValidation Loss: 2.339179\n",
      "Epoch: 6252 \tTraining Loss: 2.179061 \tValidation Loss: 2.339332\n",
      "Epoch: 6253 \tTraining Loss: 2.179104 \tValidation Loss: 2.338968\n",
      "Epoch: 6254 \tTraining Loss: 2.160011 \tValidation Loss: 2.338782\n",
      "Validation loss decreased (2.338783 --> 2.338782).  Saving model ...\n",
      "Epoch: 6255 \tTraining Loss: 2.165976 \tValidation Loss: 2.338890\n",
      "Epoch: 6256 \tTraining Loss: 2.180202 \tValidation Loss: 2.338985\n",
      "Epoch: 6257 \tTraining Loss: 2.154799 \tValidation Loss: 2.338514\n",
      "Validation loss decreased (2.338782 --> 2.338514).  Saving model ...\n",
      "Epoch: 6258 \tTraining Loss: 2.172399 \tValidation Loss: 2.338701\n",
      "Epoch: 6259 \tTraining Loss: 2.176009 \tValidation Loss: 2.338491\n",
      "Validation loss decreased (2.338514 --> 2.338491).  Saving model ...\n",
      "Epoch: 6260 \tTraining Loss: 2.145878 \tValidation Loss: 2.338500\n",
      "Epoch: 6261 \tTraining Loss: 2.174799 \tValidation Loss: 2.338645\n",
      "Epoch: 6262 \tTraining Loss: 2.162834 \tValidation Loss: 2.338720\n",
      "Epoch: 6263 \tTraining Loss: 2.173033 \tValidation Loss: 2.338594\n",
      "Epoch: 6264 \tTraining Loss: 2.163112 \tValidation Loss: 2.338303\n",
      "Validation loss decreased (2.338491 --> 2.338303).  Saving model ...\n",
      "Epoch: 6265 \tTraining Loss: 2.157192 \tValidation Loss: 2.338072\n",
      "Validation loss decreased (2.338303 --> 2.338072).  Saving model ...\n",
      "Epoch: 6266 \tTraining Loss: 2.175748 \tValidation Loss: 2.338255\n",
      "Epoch: 6267 \tTraining Loss: 2.158576 \tValidation Loss: 2.338388\n",
      "Epoch: 6268 \tTraining Loss: 2.163661 \tValidation Loss: 2.338367\n",
      "Epoch: 6269 \tTraining Loss: 2.145372 \tValidation Loss: 2.338406\n",
      "Epoch: 6270 \tTraining Loss: 2.159104 \tValidation Loss: 2.338385\n",
      "Epoch: 6271 \tTraining Loss: 2.167976 \tValidation Loss: 2.338272\n",
      "Epoch: 6272 \tTraining Loss: 2.160964 \tValidation Loss: 2.338299\n",
      "Epoch: 6273 \tTraining Loss: 2.160890 \tValidation Loss: 2.338293\n",
      "Epoch: 6274 \tTraining Loss: 2.159895 \tValidation Loss: 2.338213\n",
      "Epoch: 6275 \tTraining Loss: 2.187381 \tValidation Loss: 2.338222\n",
      "Epoch: 6276 \tTraining Loss: 2.154464 \tValidation Loss: 2.338400\n",
      "Epoch: 6277 \tTraining Loss: 2.170121 \tValidation Loss: 2.338522\n",
      "Epoch: 6278 \tTraining Loss: 2.174160 \tValidation Loss: 2.338592\n",
      "Epoch: 6279 \tTraining Loss: 2.186061 \tValidation Loss: 2.338733\n",
      "Epoch: 6280 \tTraining Loss: 2.172469 \tValidation Loss: 2.338724\n",
      "Epoch: 6281 \tTraining Loss: 2.169551 \tValidation Loss: 2.338770\n",
      "Epoch: 6282 \tTraining Loss: 2.171450 \tValidation Loss: 2.338681\n",
      "Epoch: 6283 \tTraining Loss: 2.170624 \tValidation Loss: 2.338512\n",
      "Epoch: 6284 \tTraining Loss: 2.171472 \tValidation Loss: 2.338387\n",
      "Epoch: 6285 \tTraining Loss: 2.180941 \tValidation Loss: 2.338430\n",
      "Epoch: 6286 \tTraining Loss: 2.170134 \tValidation Loss: 2.338588\n",
      "Epoch: 6287 \tTraining Loss: 2.154214 \tValidation Loss: 2.338733\n",
      "Epoch: 6288 \tTraining Loss: 2.170393 \tValidation Loss: 2.338753\n",
      "Epoch: 6289 \tTraining Loss: 2.167470 \tValidation Loss: 2.338643\n",
      "Epoch: 6290 \tTraining Loss: 2.181592 \tValidation Loss: 2.338587\n",
      "Epoch: 6291 \tTraining Loss: 2.172284 \tValidation Loss: 2.338619\n",
      "Epoch: 6292 \tTraining Loss: 2.160851 \tValidation Loss: 2.338488\n",
      "Epoch: 6293 \tTraining Loss: 2.155986 \tValidation Loss: 2.338581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6294 \tTraining Loss: 2.174199 \tValidation Loss: 2.338477\n",
      "Epoch: 6295 \tTraining Loss: 2.176856 \tValidation Loss: 2.338598\n",
      "Epoch: 6296 \tTraining Loss: 2.168011 \tValidation Loss: 2.338373\n",
      "Epoch: 6297 \tTraining Loss: 2.174831 \tValidation Loss: 2.338621\n",
      "Epoch: 6298 \tTraining Loss: 2.142041 \tValidation Loss: 2.338589\n",
      "Epoch: 6299 \tTraining Loss: 2.168717 \tValidation Loss: 2.338633\n",
      "Epoch: 6300 \tTraining Loss: 2.181857 \tValidation Loss: 2.338464\n",
      "Epoch: 6301 \tTraining Loss: 2.152094 \tValidation Loss: 2.338481\n",
      "Epoch: 6302 \tTraining Loss: 2.146832 \tValidation Loss: 2.338338\n",
      "Epoch: 6303 \tTraining Loss: 2.153787 \tValidation Loss: 2.338230\n",
      "Epoch: 6304 \tTraining Loss: 2.167526 \tValidation Loss: 2.338196\n",
      "Epoch: 6305 \tTraining Loss: 2.151282 \tValidation Loss: 2.338166\n",
      "Epoch: 6306 \tTraining Loss: 2.155841 \tValidation Loss: 2.338044\n",
      "Validation loss decreased (2.338072 --> 2.338044).  Saving model ...\n",
      "Epoch: 6307 \tTraining Loss: 2.177964 \tValidation Loss: 2.337776\n",
      "Validation loss decreased (2.338044 --> 2.337776).  Saving model ...\n",
      "Epoch: 6308 \tTraining Loss: 2.139807 \tValidation Loss: 2.337676\n",
      "Validation loss decreased (2.337776 --> 2.337676).  Saving model ...\n",
      "Epoch: 6309 \tTraining Loss: 2.157771 \tValidation Loss: 2.337844\n",
      "Epoch: 6310 \tTraining Loss: 2.162843 \tValidation Loss: 2.337939\n",
      "Epoch: 6311 \tTraining Loss: 2.152816 \tValidation Loss: 2.337864\n",
      "Epoch: 6312 \tTraining Loss: 2.180562 \tValidation Loss: 2.338005\n",
      "Epoch: 6313 \tTraining Loss: 2.174462 \tValidation Loss: 2.337750\n",
      "Epoch: 6314 \tTraining Loss: 2.179442 \tValidation Loss: 2.337952\n",
      "Epoch: 6315 \tTraining Loss: 2.170246 \tValidation Loss: 2.338160\n",
      "Epoch: 6316 \tTraining Loss: 2.154934 \tValidation Loss: 2.338062\n",
      "Epoch: 6317 \tTraining Loss: 2.139241 \tValidation Loss: 2.337843\n",
      "Epoch: 6318 \tTraining Loss: 2.173584 \tValidation Loss: 2.337957\n",
      "Epoch: 6319 \tTraining Loss: 2.193769 \tValidation Loss: 2.337988\n",
      "Epoch: 6320 \tTraining Loss: 2.164927 \tValidation Loss: 2.338128\n",
      "Epoch: 6321 \tTraining Loss: 2.169461 \tValidation Loss: 2.337961\n",
      "Epoch: 6322 \tTraining Loss: 2.167172 \tValidation Loss: 2.337994\n",
      "Epoch: 6323 \tTraining Loss: 2.180511 \tValidation Loss: 2.338045\n",
      "Epoch: 6324 \tTraining Loss: 2.176400 \tValidation Loss: 2.338123\n",
      "Epoch: 6325 \tTraining Loss: 2.151311 \tValidation Loss: 2.338250\n",
      "Epoch: 6326 \tTraining Loss: 2.164963 \tValidation Loss: 2.338453\n",
      "Epoch: 6327 \tTraining Loss: 2.135981 \tValidation Loss: 2.338376\n",
      "Epoch: 6328 \tTraining Loss: 2.161882 \tValidation Loss: 2.338464\n",
      "Epoch: 6329 \tTraining Loss: 2.153656 \tValidation Loss: 2.338127\n",
      "Epoch: 6330 \tTraining Loss: 2.141753 \tValidation Loss: 2.337985\n",
      "Epoch: 6331 \tTraining Loss: 2.155182 \tValidation Loss: 2.337802\n",
      "Epoch: 6332 \tTraining Loss: 2.159711 \tValidation Loss: 2.337946\n",
      "Epoch: 6333 \tTraining Loss: 2.155172 \tValidation Loss: 2.337870\n",
      "Epoch: 6334 \tTraining Loss: 2.155882 \tValidation Loss: 2.337849\n",
      "Epoch: 6335 \tTraining Loss: 2.154120 \tValidation Loss: 2.337469\n",
      "Validation loss decreased (2.337676 --> 2.337469).  Saving model ...\n",
      "Epoch: 6336 \tTraining Loss: 2.126773 \tValidation Loss: 2.337432\n",
      "Validation loss decreased (2.337469 --> 2.337432).  Saving model ...\n",
      "Epoch: 6337 \tTraining Loss: 2.156793 \tValidation Loss: 2.337521\n",
      "Epoch: 6338 \tTraining Loss: 2.159412 \tValidation Loss: 2.337352\n",
      "Validation loss decreased (2.337432 --> 2.337352).  Saving model ...\n",
      "Epoch: 6339 \tTraining Loss: 2.159568 \tValidation Loss: 2.337420\n",
      "Epoch: 6340 \tTraining Loss: 2.160182 \tValidation Loss: 2.337491\n",
      "Epoch: 6341 \tTraining Loss: 2.158778 \tValidation Loss: 2.337286\n",
      "Validation loss decreased (2.337352 --> 2.337286).  Saving model ...\n",
      "Epoch: 6342 \tTraining Loss: 2.176734 \tValidation Loss: 2.337470\n",
      "Epoch: 6343 \tTraining Loss: 2.156288 \tValidation Loss: 2.337411\n",
      "Epoch: 6344 \tTraining Loss: 2.148985 \tValidation Loss: 2.337689\n",
      "Epoch: 6345 \tTraining Loss: 2.153061 \tValidation Loss: 2.337695\n",
      "Epoch: 6346 \tTraining Loss: 2.155408 \tValidation Loss: 2.337730\n",
      "Epoch: 6347 \tTraining Loss: 2.177395 \tValidation Loss: 2.337846\n",
      "Epoch: 6348 \tTraining Loss: 2.153275 \tValidation Loss: 2.337708\n",
      "Epoch: 6349 \tTraining Loss: 2.175458 \tValidation Loss: 2.337951\n",
      "Epoch: 6350 \tTraining Loss: 2.184051 \tValidation Loss: 2.337887\n",
      "Epoch: 6351 \tTraining Loss: 2.183099 \tValidation Loss: 2.337979\n",
      "Epoch: 6352 \tTraining Loss: 2.161577 \tValidation Loss: 2.337737\n",
      "Epoch: 6353 \tTraining Loss: 2.154559 \tValidation Loss: 2.337906\n",
      "Epoch: 6354 \tTraining Loss: 2.161916 \tValidation Loss: 2.338194\n",
      "Epoch: 6355 \tTraining Loss: 2.155347 \tValidation Loss: 2.338268\n",
      "Epoch: 6356 \tTraining Loss: 2.135868 \tValidation Loss: 2.338023\n",
      "Epoch: 6357 \tTraining Loss: 2.152469 \tValidation Loss: 2.337760\n",
      "Epoch: 6358 \tTraining Loss: 2.174423 \tValidation Loss: 2.337486\n",
      "Epoch: 6359 \tTraining Loss: 2.145203 \tValidation Loss: 2.337374\n",
      "Epoch: 6360 \tTraining Loss: 2.172650 \tValidation Loss: 2.337602\n",
      "Epoch: 6361 \tTraining Loss: 2.174014 \tValidation Loss: 2.337511\n",
      "Epoch: 6362 \tTraining Loss: 2.187991 \tValidation Loss: 2.337281\n",
      "Validation loss decreased (2.337286 --> 2.337281).  Saving model ...\n",
      "Epoch: 6363 \tTraining Loss: 2.140265 \tValidation Loss: 2.337223\n",
      "Validation loss decreased (2.337281 --> 2.337223).  Saving model ...\n",
      "Epoch: 6364 \tTraining Loss: 2.167540 \tValidation Loss: 2.337088\n",
      "Validation loss decreased (2.337223 --> 2.337088).  Saving model ...\n",
      "Epoch: 6365 \tTraining Loss: 2.169239 \tValidation Loss: 2.337226\n",
      "Epoch: 6366 \tTraining Loss: 2.164377 \tValidation Loss: 2.337006\n",
      "Validation loss decreased (2.337088 --> 2.337006).  Saving model ...\n",
      "Epoch: 6367 \tTraining Loss: 2.174735 \tValidation Loss: 2.337026\n",
      "Epoch: 6368 \tTraining Loss: 2.146540 \tValidation Loss: 2.336831\n",
      "Validation loss decreased (2.337006 --> 2.336831).  Saving model ...\n",
      "Epoch: 6369 \tTraining Loss: 2.171901 \tValidation Loss: 2.337099\n",
      "Epoch: 6370 \tTraining Loss: 2.134076 \tValidation Loss: 2.336816\n",
      "Validation loss decreased (2.336831 --> 2.336816).  Saving model ...\n",
      "Epoch: 6371 \tTraining Loss: 2.140924 \tValidation Loss: 2.336876\n",
      "Epoch: 6372 \tTraining Loss: 2.155351 \tValidation Loss: 2.336812\n",
      "Validation loss decreased (2.336816 --> 2.336812).  Saving model ...\n",
      "Epoch: 6373 \tTraining Loss: 2.167764 \tValidation Loss: 2.336626\n",
      "Validation loss decreased (2.336812 --> 2.336626).  Saving model ...\n",
      "Epoch: 6374 \tTraining Loss: 2.176069 \tValidation Loss: 2.336478\n",
      "Validation loss decreased (2.336626 --> 2.336478).  Saving model ...\n",
      "Epoch: 6375 \tTraining Loss: 2.169352 \tValidation Loss: 2.336517\n",
      "Epoch: 6376 \tTraining Loss: 2.179170 \tValidation Loss: 2.336568\n",
      "Epoch: 6377 \tTraining Loss: 2.160651 \tValidation Loss: 2.336395\n",
      "Validation loss decreased (2.336478 --> 2.336395).  Saving model ...\n",
      "Epoch: 6378 \tTraining Loss: 2.178917 \tValidation Loss: 2.336457\n",
      "Epoch: 6379 \tTraining Loss: 2.156854 \tValidation Loss: 2.336617\n",
      "Epoch: 6380 \tTraining Loss: 2.134115 \tValidation Loss: 2.336579\n",
      "Epoch: 6381 \tTraining Loss: 2.162101 \tValidation Loss: 2.336355\n",
      "Validation loss decreased (2.336395 --> 2.336355).  Saving model ...\n",
      "Epoch: 6382 \tTraining Loss: 2.168703 \tValidation Loss: 2.336274\n",
      "Validation loss decreased (2.336355 --> 2.336274).  Saving model ...\n",
      "Epoch: 6383 \tTraining Loss: 2.162593 \tValidation Loss: 2.336544\n",
      "Epoch: 6384 \tTraining Loss: 2.153842 \tValidation Loss: 2.336462\n",
      "Epoch: 6385 \tTraining Loss: 2.172025 \tValidation Loss: 2.336663\n",
      "Epoch: 6386 \tTraining Loss: 2.162442 \tValidation Loss: 2.336787\n",
      "Epoch: 6387 \tTraining Loss: 2.153143 \tValidation Loss: 2.336793\n",
      "Epoch: 6388 \tTraining Loss: 2.141909 \tValidation Loss: 2.336706\n",
      "Epoch: 6389 \tTraining Loss: 2.171350 \tValidation Loss: 2.336879\n",
      "Epoch: 6390 \tTraining Loss: 2.146236 \tValidation Loss: 2.336518\n",
      "Epoch: 6391 \tTraining Loss: 2.163883 \tValidation Loss: 2.336545\n",
      "Epoch: 6392 \tTraining Loss: 2.175505 \tValidation Loss: 2.336505\n",
      "Epoch: 6393 \tTraining Loss: 2.154235 \tValidation Loss: 2.336685\n",
      "Epoch: 6394 \tTraining Loss: 2.164478 \tValidation Loss: 2.336528\n",
      "Epoch: 6395 \tTraining Loss: 2.157556 \tValidation Loss: 2.336538\n",
      "Epoch: 6396 \tTraining Loss: 2.160657 \tValidation Loss: 2.336501\n",
      "Epoch: 6397 \tTraining Loss: 2.163644 \tValidation Loss: 2.336623\n",
      "Epoch: 6398 \tTraining Loss: 2.157799 \tValidation Loss: 2.336479\n",
      "Epoch: 6399 \tTraining Loss: 2.158732 \tValidation Loss: 2.336354\n",
      "Epoch: 6400 \tTraining Loss: 2.165725 \tValidation Loss: 2.336304\n",
      "Epoch: 6401 \tTraining Loss: 2.153878 \tValidation Loss: 2.336338\n",
      "Epoch: 6402 \tTraining Loss: 2.149770 \tValidation Loss: 2.336236\n",
      "Validation loss decreased (2.336274 --> 2.336236).  Saving model ...\n",
      "Epoch: 6403 \tTraining Loss: 2.152741 \tValidation Loss: 2.336236\n",
      "Validation loss decreased (2.336236 --> 2.336236).  Saving model ...\n",
      "Epoch: 6404 \tTraining Loss: 2.156443 \tValidation Loss: 2.335958\n",
      "Validation loss decreased (2.336236 --> 2.335958).  Saving model ...\n",
      "Epoch: 6405 \tTraining Loss: 2.151413 \tValidation Loss: 2.336020\n",
      "Epoch: 6406 \tTraining Loss: 2.159035 \tValidation Loss: 2.336100\n",
      "Epoch: 6407 \tTraining Loss: 2.152812 \tValidation Loss: 2.335901\n",
      "Validation loss decreased (2.335958 --> 2.335901).  Saving model ...\n",
      "Epoch: 6408 \tTraining Loss: 2.148901 \tValidation Loss: 2.335804\n",
      "Validation loss decreased (2.335901 --> 2.335804).  Saving model ...\n",
      "Epoch: 6409 \tTraining Loss: 2.120519 \tValidation Loss: 2.335996\n",
      "Epoch: 6410 \tTraining Loss: 2.145337 \tValidation Loss: 2.335912\n",
      "Epoch: 6411 \tTraining Loss: 2.161450 \tValidation Loss: 2.335780\n",
      "Validation loss decreased (2.335804 --> 2.335780).  Saving model ...\n",
      "Epoch: 6412 \tTraining Loss: 2.150372 \tValidation Loss: 2.335495\n",
      "Validation loss decreased (2.335780 --> 2.335495).  Saving model ...\n",
      "Epoch: 6413 \tTraining Loss: 2.150962 \tValidation Loss: 2.335357\n",
      "Validation loss decreased (2.335495 --> 2.335357).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6414 \tTraining Loss: 2.123824 \tValidation Loss: 2.335365\n",
      "Epoch: 6415 \tTraining Loss: 2.137056 \tValidation Loss: 2.335090\n",
      "Validation loss decreased (2.335357 --> 2.335090).  Saving model ...\n",
      "Epoch: 6416 \tTraining Loss: 2.143190 \tValidation Loss: 2.335023\n",
      "Validation loss decreased (2.335090 --> 2.335023).  Saving model ...\n",
      "Epoch: 6417 \tTraining Loss: 2.149906 \tValidation Loss: 2.335249\n",
      "Epoch: 6418 \tTraining Loss: 2.173447 \tValidation Loss: 2.335191\n",
      "Epoch: 6419 \tTraining Loss: 2.149969 \tValidation Loss: 2.334965\n",
      "Validation loss decreased (2.335023 --> 2.334965).  Saving model ...\n",
      "Epoch: 6420 \tTraining Loss: 2.174882 \tValidation Loss: 2.335051\n",
      "Epoch: 6421 \tTraining Loss: 2.143002 \tValidation Loss: 2.335215\n",
      "Epoch: 6422 \tTraining Loss: 2.133040 \tValidation Loss: 2.335148\n",
      "Epoch: 6423 \tTraining Loss: 2.135223 \tValidation Loss: 2.335078\n",
      "Epoch: 6424 \tTraining Loss: 2.165554 \tValidation Loss: 2.335078\n",
      "Epoch: 6425 \tTraining Loss: 2.156381 \tValidation Loss: 2.335146\n",
      "Epoch: 6426 \tTraining Loss: 2.156847 \tValidation Loss: 2.335103\n",
      "Epoch: 6427 \tTraining Loss: 2.162954 \tValidation Loss: 2.335379\n",
      "Epoch: 6428 \tTraining Loss: 2.165913 \tValidation Loss: 2.335229\n",
      "Epoch: 6429 \tTraining Loss: 2.166995 \tValidation Loss: 2.335073\n",
      "Epoch: 6430 \tTraining Loss: 2.147481 \tValidation Loss: 2.335142\n",
      "Epoch: 6431 \tTraining Loss: 2.151056 \tValidation Loss: 2.335262\n",
      "Epoch: 6432 \tTraining Loss: 2.179497 \tValidation Loss: 2.335221\n",
      "Epoch: 6433 \tTraining Loss: 2.143577 \tValidation Loss: 2.335064\n",
      "Epoch: 6434 \tTraining Loss: 2.157157 \tValidation Loss: 2.334854\n",
      "Validation loss decreased (2.334965 --> 2.334854).  Saving model ...\n",
      "Epoch: 6435 \tTraining Loss: 2.145877 \tValidation Loss: 2.334582\n",
      "Validation loss decreased (2.334854 --> 2.334582).  Saving model ...\n",
      "Epoch: 6436 \tTraining Loss: 2.148805 \tValidation Loss: 2.334841\n",
      "Epoch: 6437 \tTraining Loss: 2.142072 \tValidation Loss: 2.334921\n",
      "Epoch: 6438 \tTraining Loss: 2.171068 \tValidation Loss: 2.334923\n",
      "Epoch: 6439 \tTraining Loss: 2.165179 \tValidation Loss: 2.334736\n",
      "Epoch: 6440 \tTraining Loss: 2.168627 \tValidation Loss: 2.334820\n",
      "Epoch: 6441 \tTraining Loss: 2.147671 \tValidation Loss: 2.334995\n",
      "Epoch: 6442 \tTraining Loss: 2.161143 \tValidation Loss: 2.335134\n",
      "Epoch: 6443 \tTraining Loss: 2.176642 \tValidation Loss: 2.335280\n",
      "Epoch: 6444 \tTraining Loss: 2.171321 \tValidation Loss: 2.335284\n",
      "Epoch: 6445 \tTraining Loss: 2.170040 \tValidation Loss: 2.335276\n",
      "Epoch: 6446 \tTraining Loss: 2.152347 \tValidation Loss: 2.335025\n",
      "Epoch: 6447 \tTraining Loss: 2.164752 \tValidation Loss: 2.335229\n",
      "Epoch: 6448 \tTraining Loss: 2.148223 \tValidation Loss: 2.335333\n",
      "Epoch: 6449 \tTraining Loss: 2.161358 \tValidation Loss: 2.335100\n",
      "Epoch: 6450 \tTraining Loss: 2.151384 \tValidation Loss: 2.335166\n",
      "Epoch: 6451 \tTraining Loss: 2.167935 \tValidation Loss: 2.335054\n",
      "Epoch: 6452 \tTraining Loss: 2.171034 \tValidation Loss: 2.334771\n",
      "Epoch: 6453 \tTraining Loss: 2.165748 \tValidation Loss: 2.334852\n",
      "Epoch: 6454 \tTraining Loss: 2.136674 \tValidation Loss: 2.334925\n",
      "Epoch: 6455 \tTraining Loss: 2.168398 \tValidation Loss: 2.335011\n",
      "Epoch: 6456 \tTraining Loss: 2.153294 \tValidation Loss: 2.335200\n",
      "Epoch: 6457 \tTraining Loss: 2.145778 \tValidation Loss: 2.335116\n",
      "Epoch: 6458 \tTraining Loss: 2.146324 \tValidation Loss: 2.335123\n",
      "Epoch: 6459 \tTraining Loss: 2.149635 \tValidation Loss: 2.334954\n",
      "Epoch: 6460 \tTraining Loss: 2.154197 \tValidation Loss: 2.334862\n",
      "Epoch: 6461 \tTraining Loss: 2.163359 \tValidation Loss: 2.335160\n",
      "Epoch: 6462 \tTraining Loss: 2.181455 \tValidation Loss: 2.335246\n",
      "Epoch: 6463 \tTraining Loss: 2.140416 \tValidation Loss: 2.335351\n",
      "Epoch: 6464 \tTraining Loss: 2.136875 \tValidation Loss: 2.335497\n",
      "Epoch: 6465 \tTraining Loss: 2.163222 \tValidation Loss: 2.335545\n",
      "Epoch: 6466 \tTraining Loss: 2.149577 \tValidation Loss: 2.335229\n",
      "Epoch: 6467 \tTraining Loss: 2.141057 \tValidation Loss: 2.335271\n",
      "Epoch: 6468 \tTraining Loss: 2.162977 \tValidation Loss: 2.334851\n",
      "Epoch: 6469 \tTraining Loss: 2.139174 \tValidation Loss: 2.334673\n",
      "Epoch: 6470 \tTraining Loss: 2.157737 \tValidation Loss: 2.334882\n",
      "Epoch: 6471 \tTraining Loss: 2.144980 \tValidation Loss: 2.335015\n",
      "Epoch: 6472 \tTraining Loss: 2.167475 \tValidation Loss: 2.335014\n",
      "Epoch: 6473 \tTraining Loss: 2.163019 \tValidation Loss: 2.334625\n",
      "Epoch: 6474 \tTraining Loss: 2.164170 \tValidation Loss: 2.334301\n",
      "Validation loss decreased (2.334582 --> 2.334301).  Saving model ...\n",
      "Epoch: 6475 \tTraining Loss: 2.154677 \tValidation Loss: 2.334241\n",
      "Validation loss decreased (2.334301 --> 2.334241).  Saving model ...\n",
      "Epoch: 6476 \tTraining Loss: 2.129684 \tValidation Loss: 2.333994\n",
      "Validation loss decreased (2.334241 --> 2.333994).  Saving model ...\n",
      "Epoch: 6477 \tTraining Loss: 2.165980 \tValidation Loss: 2.334310\n",
      "Epoch: 6478 \tTraining Loss: 2.148553 \tValidation Loss: 2.334510\n",
      "Epoch: 6479 \tTraining Loss: 2.152470 \tValidation Loss: 2.334867\n",
      "Epoch: 6480 \tTraining Loss: 2.138827 \tValidation Loss: 2.334842\n",
      "Epoch: 6481 \tTraining Loss: 2.135631 \tValidation Loss: 2.334506\n",
      "Epoch: 6482 \tTraining Loss: 2.154718 \tValidation Loss: 2.334354\n",
      "Epoch: 6483 \tTraining Loss: 2.165045 \tValidation Loss: 2.334637\n",
      "Epoch: 6484 \tTraining Loss: 2.156529 \tValidation Loss: 2.334394\n",
      "Epoch: 6485 \tTraining Loss: 2.153365 \tValidation Loss: 2.334507\n",
      "Epoch: 6486 \tTraining Loss: 2.157656 \tValidation Loss: 2.334515\n",
      "Epoch: 6487 \tTraining Loss: 2.157616 \tValidation Loss: 2.334539\n",
      "Epoch: 6488 \tTraining Loss: 2.137225 \tValidation Loss: 2.334481\n",
      "Epoch: 6489 \tTraining Loss: 2.138354 \tValidation Loss: 2.334363\n",
      "Epoch: 6490 \tTraining Loss: 2.165018 \tValidation Loss: 2.334392\n",
      "Epoch: 6491 \tTraining Loss: 2.161874 \tValidation Loss: 2.334661\n",
      "Epoch: 6492 \tTraining Loss: 2.169643 \tValidation Loss: 2.334656\n",
      "Epoch: 6493 \tTraining Loss: 2.157400 \tValidation Loss: 2.334888\n",
      "Epoch: 6494 \tTraining Loss: 2.143878 \tValidation Loss: 2.334548\n",
      "Epoch: 6495 \tTraining Loss: 2.135940 \tValidation Loss: 2.334711\n",
      "Epoch: 6496 \tTraining Loss: 2.157624 \tValidation Loss: 2.334531\n",
      "Epoch: 6497 \tTraining Loss: 2.166688 \tValidation Loss: 2.334459\n",
      "Epoch: 6498 \tTraining Loss: 2.138894 \tValidation Loss: 2.334512\n",
      "Epoch: 6499 \tTraining Loss: 2.166311 \tValidation Loss: 2.334515\n",
      "Epoch: 6500 \tTraining Loss: 2.167495 \tValidation Loss: 2.334485\n",
      "Epoch: 6501 \tTraining Loss: 2.160094 \tValidation Loss: 2.334583\n",
      "Epoch: 6502 \tTraining Loss: 2.121793 \tValidation Loss: 2.334526\n",
      "Epoch: 6503 \tTraining Loss: 2.160006 \tValidation Loss: 2.334475\n",
      "Epoch: 6504 \tTraining Loss: 2.166541 \tValidation Loss: 2.334621\n",
      "Epoch: 6505 \tTraining Loss: 2.162801 \tValidation Loss: 2.334779\n",
      "Epoch: 6506 \tTraining Loss: 2.154962 \tValidation Loss: 2.334583\n",
      "Epoch: 6507 \tTraining Loss: 2.136158 \tValidation Loss: 2.334317\n",
      "Epoch: 6508 \tTraining Loss: 2.161564 \tValidation Loss: 2.334106\n",
      "Epoch: 6509 \tTraining Loss: 2.150681 \tValidation Loss: 2.334195\n",
      "Epoch: 6510 \tTraining Loss: 2.154178 \tValidation Loss: 2.334210\n",
      "Epoch: 6511 \tTraining Loss: 2.139552 \tValidation Loss: 2.334173\n",
      "Epoch: 6512 \tTraining Loss: 2.154743 \tValidation Loss: 2.334207\n",
      "Epoch: 6513 \tTraining Loss: 2.143652 \tValidation Loss: 2.334127\n",
      "Epoch: 6514 \tTraining Loss: 2.166481 \tValidation Loss: 2.334319\n",
      "Epoch: 6515 \tTraining Loss: 2.145228 \tValidation Loss: 2.334397\n",
      "Epoch: 6516 \tTraining Loss: 2.152292 \tValidation Loss: 2.334215\n",
      "Epoch: 6517 \tTraining Loss: 2.144860 \tValidation Loss: 2.334230\n",
      "Epoch: 6518 \tTraining Loss: 2.134746 \tValidation Loss: 2.334181\n",
      "Epoch: 6519 \tTraining Loss: 2.152171 \tValidation Loss: 2.334360\n",
      "Epoch: 6520 \tTraining Loss: 2.168341 \tValidation Loss: 2.334448\n",
      "Epoch: 6521 \tTraining Loss: 2.178388 \tValidation Loss: 2.334399\n",
      "Epoch: 6522 \tTraining Loss: 2.151345 \tValidation Loss: 2.334538\n",
      "Epoch: 6523 \tTraining Loss: 2.147507 \tValidation Loss: 2.334656\n",
      "Epoch: 6524 \tTraining Loss: 2.156164 \tValidation Loss: 2.334619\n",
      "Epoch: 6525 \tTraining Loss: 2.149839 \tValidation Loss: 2.334344\n",
      "Epoch: 6526 \tTraining Loss: 2.146407 \tValidation Loss: 2.334170\n",
      "Epoch: 6527 \tTraining Loss: 2.155948 \tValidation Loss: 2.334361\n",
      "Epoch: 6528 \tTraining Loss: 2.147116 \tValidation Loss: 2.334265\n",
      "Epoch: 6529 \tTraining Loss: 2.150652 \tValidation Loss: 2.334101\n",
      "Epoch: 6530 \tTraining Loss: 2.165684 \tValidation Loss: 2.333931\n",
      "Validation loss decreased (2.333994 --> 2.333931).  Saving model ...\n",
      "Epoch: 6531 \tTraining Loss: 2.151888 \tValidation Loss: 2.333947\n",
      "Epoch: 6532 \tTraining Loss: 2.149003 \tValidation Loss: 2.334008\n",
      "Epoch: 6533 \tTraining Loss: 2.141890 \tValidation Loss: 2.333865\n",
      "Validation loss decreased (2.333931 --> 2.333865).  Saving model ...\n",
      "Epoch: 6534 \tTraining Loss: 2.156718 \tValidation Loss: 2.333951\n",
      "Epoch: 6535 \tTraining Loss: 2.155835 \tValidation Loss: 2.334038\n",
      "Epoch: 6536 \tTraining Loss: 2.147273 \tValidation Loss: 2.334267\n",
      "Epoch: 6537 \tTraining Loss: 2.136060 \tValidation Loss: 2.334281\n",
      "Epoch: 6538 \tTraining Loss: 2.148054 \tValidation Loss: 2.334258\n",
      "Epoch: 6539 \tTraining Loss: 2.167236 \tValidation Loss: 2.334105\n",
      "Epoch: 6540 \tTraining Loss: 2.164960 \tValidation Loss: 2.334197\n",
      "Epoch: 6541 \tTraining Loss: 2.161592 \tValidation Loss: 2.334306\n",
      "Epoch: 6542 \tTraining Loss: 2.172143 \tValidation Loss: 2.334134\n",
      "Epoch: 6543 \tTraining Loss: 2.169046 \tValidation Loss: 2.334188\n",
      "Epoch: 6544 \tTraining Loss: 2.153978 \tValidation Loss: 2.334404\n",
      "Epoch: 6545 \tTraining Loss: 2.129904 \tValidation Loss: 2.334140\n",
      "Epoch: 6546 \tTraining Loss: 2.171651 \tValidation Loss: 2.334341\n",
      "Epoch: 6547 \tTraining Loss: 2.148429 \tValidation Loss: 2.334163\n",
      "Epoch: 6548 \tTraining Loss: 2.146067 \tValidation Loss: 2.334163\n",
      "Epoch: 6549 \tTraining Loss: 2.142672 \tValidation Loss: 2.334228\n",
      "Epoch: 6550 \tTraining Loss: 2.152084 \tValidation Loss: 2.334241\n",
      "Epoch: 6551 \tTraining Loss: 2.138732 \tValidation Loss: 2.334204\n",
      "Epoch: 6552 \tTraining Loss: 2.154839 \tValidation Loss: 2.334317\n",
      "Epoch: 6553 \tTraining Loss: 2.151939 \tValidation Loss: 2.333769\n",
      "Validation loss decreased (2.333865 --> 2.333769).  Saving model ...\n",
      "Epoch: 6554 \tTraining Loss: 2.139393 \tValidation Loss: 2.333775\n",
      "Epoch: 6555 \tTraining Loss: 2.142696 \tValidation Loss: 2.333634\n",
      "Validation loss decreased (2.333769 --> 2.333634).  Saving model ...\n",
      "Epoch: 6556 \tTraining Loss: 2.136418 \tValidation Loss: 2.333512\n",
      "Validation loss decreased (2.333634 --> 2.333512).  Saving model ...\n",
      "Epoch: 6557 \tTraining Loss: 2.170614 \tValidation Loss: 2.333611\n",
      "Epoch: 6558 \tTraining Loss: 2.147286 \tValidation Loss: 2.333915\n",
      "Epoch: 6559 \tTraining Loss: 2.151606 \tValidation Loss: 2.333846\n",
      "Epoch: 6560 \tTraining Loss: 2.137766 \tValidation Loss: 2.333968\n",
      "Epoch: 6561 \tTraining Loss: 2.163361 \tValidation Loss: 2.334153\n",
      "Epoch: 6562 \tTraining Loss: 2.132391 \tValidation Loss: 2.333829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6563 \tTraining Loss: 2.163642 \tValidation Loss: 2.333667\n",
      "Epoch: 6564 \tTraining Loss: 2.126737 \tValidation Loss: 2.333693\n",
      "Epoch: 6565 \tTraining Loss: 2.143316 \tValidation Loss: 2.333760\n",
      "Epoch: 6566 \tTraining Loss: 2.167807 \tValidation Loss: 2.333655\n",
      "Epoch: 6567 \tTraining Loss: 2.144086 \tValidation Loss: 2.333963\n",
      "Epoch: 6568 \tTraining Loss: 2.152638 \tValidation Loss: 2.334142\n",
      "Epoch: 6569 \tTraining Loss: 2.145557 \tValidation Loss: 2.333846\n",
      "Epoch: 6570 \tTraining Loss: 2.150059 \tValidation Loss: 2.333771\n",
      "Epoch: 6571 \tTraining Loss: 2.145786 \tValidation Loss: 2.333973\n",
      "Epoch: 6572 \tTraining Loss: 2.151634 \tValidation Loss: 2.333721\n",
      "Epoch: 6573 \tTraining Loss: 2.140435 \tValidation Loss: 2.333662\n",
      "Epoch: 6574 \tTraining Loss: 2.150443 \tValidation Loss: 2.333669\n",
      "Epoch: 6575 \tTraining Loss: 2.134366 \tValidation Loss: 2.333461\n",
      "Validation loss decreased (2.333512 --> 2.333461).  Saving model ...\n",
      "Epoch: 6576 \tTraining Loss: 2.139241 \tValidation Loss: 2.333442\n",
      "Validation loss decreased (2.333461 --> 2.333442).  Saving model ...\n",
      "Epoch: 6577 \tTraining Loss: 2.179651 \tValidation Loss: 2.333701\n",
      "Epoch: 6578 \tTraining Loss: 2.156256 \tValidation Loss: 2.333611\n",
      "Epoch: 6579 \tTraining Loss: 2.138368 \tValidation Loss: 2.333696\n",
      "Epoch: 6580 \tTraining Loss: 2.138849 \tValidation Loss: 2.333748\n",
      "Epoch: 6581 \tTraining Loss: 2.142001 \tValidation Loss: 2.333617\n",
      "Epoch: 6582 \tTraining Loss: 2.143407 \tValidation Loss: 2.333763\n",
      "Epoch: 6583 \tTraining Loss: 2.140419 \tValidation Loss: 2.333748\n",
      "Epoch: 6584 \tTraining Loss: 2.152668 \tValidation Loss: 2.333380\n",
      "Validation loss decreased (2.333442 --> 2.333380).  Saving model ...\n",
      "Epoch: 6585 \tTraining Loss: 2.153668 \tValidation Loss: 2.333370\n",
      "Validation loss decreased (2.333380 --> 2.333370).  Saving model ...\n",
      "Epoch: 6586 \tTraining Loss: 2.149636 \tValidation Loss: 2.333205\n",
      "Validation loss decreased (2.333370 --> 2.333205).  Saving model ...\n",
      "Epoch: 6587 \tTraining Loss: 2.143805 \tValidation Loss: 2.333214\n",
      "Epoch: 6588 \tTraining Loss: 2.135101 \tValidation Loss: 2.333251\n",
      "Epoch: 6589 \tTraining Loss: 2.146260 \tValidation Loss: 2.333349\n",
      "Epoch: 6590 \tTraining Loss: 2.151097 \tValidation Loss: 2.333493\n",
      "Epoch: 6591 \tTraining Loss: 2.133322 \tValidation Loss: 2.333367\n",
      "Epoch: 6592 \tTraining Loss: 2.142892 \tValidation Loss: 2.333557\n",
      "Epoch: 6593 \tTraining Loss: 2.132880 \tValidation Loss: 2.333604\n",
      "Epoch: 6594 \tTraining Loss: 2.143073 \tValidation Loss: 2.333903\n",
      "Epoch: 6595 \tTraining Loss: 2.151586 \tValidation Loss: 2.333956\n",
      "Epoch: 6596 \tTraining Loss: 2.154736 \tValidation Loss: 2.333936\n",
      "Epoch: 6597 \tTraining Loss: 2.162155 \tValidation Loss: 2.333956\n",
      "Epoch: 6598 \tTraining Loss: 2.151237 \tValidation Loss: 2.334011\n",
      "Epoch: 6599 \tTraining Loss: 2.129674 \tValidation Loss: 2.333875\n",
      "Epoch: 6600 \tTraining Loss: 2.164417 \tValidation Loss: 2.333573\n",
      "Epoch: 6601 \tTraining Loss: 2.145454 \tValidation Loss: 2.333309\n",
      "Epoch: 6602 \tTraining Loss: 2.169775 \tValidation Loss: 2.333223\n",
      "Epoch: 6603 \tTraining Loss: 2.140723 \tValidation Loss: 2.333371\n",
      "Epoch: 6604 \tTraining Loss: 2.132723 \tValidation Loss: 2.333009\n",
      "Validation loss decreased (2.333205 --> 2.333009).  Saving model ...\n",
      "Epoch: 6605 \tTraining Loss: 2.124974 \tValidation Loss: 2.332819\n",
      "Validation loss decreased (2.333009 --> 2.332819).  Saving model ...\n",
      "Epoch: 6606 \tTraining Loss: 2.127095 \tValidation Loss: 2.332698\n",
      "Validation loss decreased (2.332819 --> 2.332698).  Saving model ...\n",
      "Epoch: 6607 \tTraining Loss: 2.137707 \tValidation Loss: 2.332531\n",
      "Validation loss decreased (2.332698 --> 2.332531).  Saving model ...\n",
      "Epoch: 6608 \tTraining Loss: 2.144888 \tValidation Loss: 2.332454\n",
      "Validation loss decreased (2.332531 --> 2.332454).  Saving model ...\n",
      "Epoch: 6609 \tTraining Loss: 2.164901 \tValidation Loss: 2.332591\n",
      "Epoch: 6610 \tTraining Loss: 2.166738 \tValidation Loss: 2.332644\n",
      "Epoch: 6611 \tTraining Loss: 2.178143 \tValidation Loss: 2.332754\n",
      "Epoch: 6612 \tTraining Loss: 2.161591 \tValidation Loss: 2.332837\n",
      "Epoch: 6613 \tTraining Loss: 2.152288 \tValidation Loss: 2.332678\n",
      "Epoch: 6614 \tTraining Loss: 2.148711 \tValidation Loss: 2.332891\n",
      "Epoch: 6615 \tTraining Loss: 2.150278 \tValidation Loss: 2.332740\n",
      "Epoch: 6616 \tTraining Loss: 2.155740 \tValidation Loss: 2.333067\n",
      "Epoch: 6617 \tTraining Loss: 2.144648 \tValidation Loss: 2.333155\n",
      "Epoch: 6618 \tTraining Loss: 2.156732 \tValidation Loss: 2.333047\n",
      "Epoch: 6619 \tTraining Loss: 2.135941 \tValidation Loss: 2.333105\n",
      "Epoch: 6620 \tTraining Loss: 2.141061 \tValidation Loss: 2.332870\n",
      "Epoch: 6621 \tTraining Loss: 2.157593 \tValidation Loss: 2.332780\n",
      "Epoch: 6622 \tTraining Loss: 2.178060 \tValidation Loss: 2.332982\n",
      "Epoch: 6623 \tTraining Loss: 2.169729 \tValidation Loss: 2.332879\n",
      "Epoch: 6624 \tTraining Loss: 2.125108 \tValidation Loss: 2.332867\n",
      "Epoch: 6625 \tTraining Loss: 2.167133 \tValidation Loss: 2.333056\n",
      "Epoch: 6626 \tTraining Loss: 2.161525 \tValidation Loss: 2.333027\n",
      "Epoch: 6627 \tTraining Loss: 2.135910 \tValidation Loss: 2.332998\n",
      "Epoch: 6628 \tTraining Loss: 2.143524 \tValidation Loss: 2.332624\n",
      "Epoch: 6629 \tTraining Loss: 2.145620 \tValidation Loss: 2.332632\n",
      "Epoch: 6630 \tTraining Loss: 2.148242 \tValidation Loss: 2.332427\n",
      "Validation loss decreased (2.332454 --> 2.332427).  Saving model ...\n",
      "Epoch: 6631 \tTraining Loss: 2.114041 \tValidation Loss: 2.332395\n",
      "Validation loss decreased (2.332427 --> 2.332395).  Saving model ...\n",
      "Epoch: 6632 \tTraining Loss: 2.147388 \tValidation Loss: 2.332451\n",
      "Epoch: 6633 \tTraining Loss: 2.163404 \tValidation Loss: 2.332680\n",
      "Epoch: 6634 \tTraining Loss: 2.173318 \tValidation Loss: 2.332660\n",
      "Epoch: 6635 \tTraining Loss: 2.145262 \tValidation Loss: 2.332641\n",
      "Epoch: 6636 \tTraining Loss: 2.138969 \tValidation Loss: 2.332544\n",
      "Epoch: 6637 \tTraining Loss: 2.139185 \tValidation Loss: 2.332702\n",
      "Epoch: 6638 \tTraining Loss: 2.173898 \tValidation Loss: 2.332474\n",
      "Epoch: 6639 \tTraining Loss: 2.137377 \tValidation Loss: 2.332253\n",
      "Validation loss decreased (2.332395 --> 2.332253).  Saving model ...\n",
      "Epoch: 6640 \tTraining Loss: 2.114868 \tValidation Loss: 2.332180\n",
      "Validation loss decreased (2.332253 --> 2.332180).  Saving model ...\n",
      "Epoch: 6641 \tTraining Loss: 2.142156 \tValidation Loss: 2.332253\n",
      "Epoch: 6642 \tTraining Loss: 2.132190 \tValidation Loss: 2.332381\n",
      "Epoch: 6643 \tTraining Loss: 2.168136 \tValidation Loss: 2.332440\n",
      "Epoch: 6644 \tTraining Loss: 2.133079 \tValidation Loss: 2.332392\n",
      "Epoch: 6645 \tTraining Loss: 2.126959 \tValidation Loss: 2.332179\n",
      "Validation loss decreased (2.332180 --> 2.332179).  Saving model ...\n",
      "Epoch: 6646 \tTraining Loss: 2.124685 \tValidation Loss: 2.332349\n",
      "Epoch: 6647 \tTraining Loss: 2.143430 \tValidation Loss: 2.332129\n",
      "Validation loss decreased (2.332179 --> 2.332129).  Saving model ...\n",
      "Epoch: 6648 \tTraining Loss: 2.133234 \tValidation Loss: 2.332011\n",
      "Validation loss decreased (2.332129 --> 2.332011).  Saving model ...\n",
      "Epoch: 6649 \tTraining Loss: 2.143084 \tValidation Loss: 2.331911\n",
      "Validation loss decreased (2.332011 --> 2.331911).  Saving model ...\n",
      "Epoch: 6650 \tTraining Loss: 2.140559 \tValidation Loss: 2.332071\n",
      "Epoch: 6651 \tTraining Loss: 2.142992 \tValidation Loss: 2.332203\n",
      "Epoch: 6652 \tTraining Loss: 2.147361 \tValidation Loss: 2.331990\n",
      "Epoch: 6653 \tTraining Loss: 2.140680 \tValidation Loss: 2.332034\n",
      "Epoch: 6654 \tTraining Loss: 2.131914 \tValidation Loss: 2.331803\n",
      "Validation loss decreased (2.331911 --> 2.331803).  Saving model ...\n",
      "Epoch: 6655 \tTraining Loss: 2.119862 \tValidation Loss: 2.331573\n",
      "Validation loss decreased (2.331803 --> 2.331573).  Saving model ...\n",
      "Epoch: 6656 \tTraining Loss: 2.143987 \tValidation Loss: 2.331945\n",
      "Epoch: 6657 \tTraining Loss: 2.143369 \tValidation Loss: 2.331849\n",
      "Epoch: 6658 \tTraining Loss: 2.152537 \tValidation Loss: 2.331964\n",
      "Epoch: 6659 \tTraining Loss: 2.144231 \tValidation Loss: 2.332143\n",
      "Epoch: 6660 \tTraining Loss: 2.146111 \tValidation Loss: 2.332387\n",
      "Epoch: 6661 \tTraining Loss: 2.122991 \tValidation Loss: 2.332094\n",
      "Epoch: 6662 \tTraining Loss: 2.133913 \tValidation Loss: 2.332082\n",
      "Epoch: 6663 \tTraining Loss: 2.175451 \tValidation Loss: 2.332306\n",
      "Epoch: 6664 \tTraining Loss: 2.140014 \tValidation Loss: 2.332082\n",
      "Epoch: 6665 \tTraining Loss: 2.115330 \tValidation Loss: 2.331960\n",
      "Epoch: 6666 \tTraining Loss: 2.138945 \tValidation Loss: 2.332063\n",
      "Epoch: 6667 \tTraining Loss: 2.123038 \tValidation Loss: 2.331981\n",
      "Epoch: 6668 \tTraining Loss: 2.133660 \tValidation Loss: 2.332002\n",
      "Epoch: 6669 \tTraining Loss: 2.154323 \tValidation Loss: 2.331924\n",
      "Epoch: 6670 \tTraining Loss: 2.132807 \tValidation Loss: 2.331874\n",
      "Epoch: 6671 \tTraining Loss: 2.122798 \tValidation Loss: 2.331890\n",
      "Epoch: 6672 \tTraining Loss: 2.130717 \tValidation Loss: 2.331987\n",
      "Epoch: 6673 \tTraining Loss: 2.146339 \tValidation Loss: 2.332084\n",
      "Epoch: 6674 \tTraining Loss: 2.148146 \tValidation Loss: 2.331953\n",
      "Epoch: 6675 \tTraining Loss: 2.138148 \tValidation Loss: 2.331981\n",
      "Epoch: 6676 \tTraining Loss: 2.126398 \tValidation Loss: 2.331784\n",
      "Epoch: 6677 \tTraining Loss: 2.135189 \tValidation Loss: 2.331303\n",
      "Validation loss decreased (2.331573 --> 2.331303).  Saving model ...\n",
      "Epoch: 6678 \tTraining Loss: 2.155564 \tValidation Loss: 2.331519\n",
      "Epoch: 6679 \tTraining Loss: 2.145693 \tValidation Loss: 2.331692\n",
      "Epoch: 6680 \tTraining Loss: 2.120842 \tValidation Loss: 2.331725\n",
      "Epoch: 6681 \tTraining Loss: 2.128901 \tValidation Loss: 2.331736\n",
      "Epoch: 6682 \tTraining Loss: 2.165153 \tValidation Loss: 2.331945\n",
      "Epoch: 6683 \tTraining Loss: 2.152999 \tValidation Loss: 2.331959\n",
      "Epoch: 6684 \tTraining Loss: 2.130810 \tValidation Loss: 2.331956\n",
      "Epoch: 6685 \tTraining Loss: 2.165118 \tValidation Loss: 2.332168\n",
      "Epoch: 6686 \tTraining Loss: 2.138333 \tValidation Loss: 2.332145\n",
      "Epoch: 6687 \tTraining Loss: 2.141844 \tValidation Loss: 2.331922\n",
      "Epoch: 6688 \tTraining Loss: 2.127373 \tValidation Loss: 2.331909\n",
      "Epoch: 6689 \tTraining Loss: 2.124967 \tValidation Loss: 2.331968\n",
      "Epoch: 6690 \tTraining Loss: 2.134403 \tValidation Loss: 2.331678\n",
      "Epoch: 6691 \tTraining Loss: 2.156596 \tValidation Loss: 2.331811\n",
      "Epoch: 6692 \tTraining Loss: 2.147199 \tValidation Loss: 2.331956\n",
      "Epoch: 6693 \tTraining Loss: 2.138206 \tValidation Loss: 2.331871\n",
      "Epoch: 6694 \tTraining Loss: 2.138018 \tValidation Loss: 2.332150\n",
      "Epoch: 6695 \tTraining Loss: 2.135512 \tValidation Loss: 2.332038\n",
      "Epoch: 6696 \tTraining Loss: 2.154091 \tValidation Loss: 2.332185\n",
      "Epoch: 6697 \tTraining Loss: 2.129005 \tValidation Loss: 2.332052\n",
      "Epoch: 6698 \tTraining Loss: 2.123076 \tValidation Loss: 2.331937\n",
      "Epoch: 6699 \tTraining Loss: 2.144656 \tValidation Loss: 2.332051\n",
      "Epoch: 6700 \tTraining Loss: 2.142120 \tValidation Loss: 2.331685\n",
      "Epoch: 6701 \tTraining Loss: 2.156919 \tValidation Loss: 2.331826\n",
      "Epoch: 6702 \tTraining Loss: 2.148755 \tValidation Loss: 2.331831\n",
      "Epoch: 6703 \tTraining Loss: 2.140088 \tValidation Loss: 2.332109\n",
      "Epoch: 6704 \tTraining Loss: 2.147785 \tValidation Loss: 2.331888\n",
      "Epoch: 6705 \tTraining Loss: 2.142260 \tValidation Loss: 2.331851\n",
      "Epoch: 6706 \tTraining Loss: 2.144476 \tValidation Loss: 2.331633\n",
      "Epoch: 6707 \tTraining Loss: 2.149066 \tValidation Loss: 2.331445\n",
      "Epoch: 6708 \tTraining Loss: 2.131396 \tValidation Loss: 2.331368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6709 \tTraining Loss: 2.134377 \tValidation Loss: 2.331510\n",
      "Epoch: 6710 \tTraining Loss: 2.143283 \tValidation Loss: 2.331354\n",
      "Epoch: 6711 \tTraining Loss: 2.147705 \tValidation Loss: 2.331312\n",
      "Epoch: 6712 \tTraining Loss: 2.151798 \tValidation Loss: 2.331304\n",
      "Epoch: 6713 \tTraining Loss: 2.150618 \tValidation Loss: 2.331417\n",
      "Epoch: 6714 \tTraining Loss: 2.146292 \tValidation Loss: 2.331630\n",
      "Epoch: 6715 \tTraining Loss: 2.133834 \tValidation Loss: 2.331563\n",
      "Epoch: 6716 \tTraining Loss: 2.149536 \tValidation Loss: 2.331113\n",
      "Validation loss decreased (2.331303 --> 2.331113).  Saving model ...\n",
      "Epoch: 6717 \tTraining Loss: 2.167410 \tValidation Loss: 2.331001\n",
      "Validation loss decreased (2.331113 --> 2.331001).  Saving model ...\n",
      "Epoch: 6718 \tTraining Loss: 2.114880 \tValidation Loss: 2.330963\n",
      "Validation loss decreased (2.331001 --> 2.330963).  Saving model ...\n",
      "Epoch: 6719 \tTraining Loss: 2.170453 \tValidation Loss: 2.331101\n",
      "Epoch: 6720 \tTraining Loss: 2.142850 \tValidation Loss: 2.330888\n",
      "Validation loss decreased (2.330963 --> 2.330888).  Saving model ...\n",
      "Epoch: 6721 \tTraining Loss: 2.146535 \tValidation Loss: 2.330974\n",
      "Epoch: 6722 \tTraining Loss: 2.164973 \tValidation Loss: 2.330995\n",
      "Epoch: 6723 \tTraining Loss: 2.144257 \tValidation Loss: 2.330899\n",
      "Epoch: 6724 \tTraining Loss: 2.140843 \tValidation Loss: 2.330924\n",
      "Epoch: 6725 \tTraining Loss: 2.123610 \tValidation Loss: 2.330989\n",
      "Epoch: 6726 \tTraining Loss: 2.120953 \tValidation Loss: 2.330897\n",
      "Epoch: 6727 \tTraining Loss: 2.129995 \tValidation Loss: 2.330950\n",
      "Epoch: 6728 \tTraining Loss: 2.151503 \tValidation Loss: 2.331003\n",
      "Epoch: 6729 \tTraining Loss: 2.140255 \tValidation Loss: 2.330725\n",
      "Validation loss decreased (2.330888 --> 2.330725).  Saving model ...\n",
      "Epoch: 6730 \tTraining Loss: 2.139121 \tValidation Loss: 2.330620\n",
      "Validation loss decreased (2.330725 --> 2.330620).  Saving model ...\n",
      "Epoch: 6731 \tTraining Loss: 2.123201 \tValidation Loss: 2.330709\n",
      "Epoch: 6732 \tTraining Loss: 2.143715 \tValidation Loss: 2.330897\n",
      "Epoch: 6733 \tTraining Loss: 2.170827 \tValidation Loss: 2.330839\n",
      "Epoch: 6734 \tTraining Loss: 2.109828 \tValidation Loss: 2.330703\n",
      "Epoch: 6735 \tTraining Loss: 2.154866 \tValidation Loss: 2.330830\n",
      "Epoch: 6736 \tTraining Loss: 2.144958 \tValidation Loss: 2.331054\n",
      "Epoch: 6737 \tTraining Loss: 2.170193 \tValidation Loss: 2.330929\n",
      "Epoch: 6738 \tTraining Loss: 2.140675 \tValidation Loss: 2.330876\n",
      "Epoch: 6739 \tTraining Loss: 2.133269 \tValidation Loss: 2.331121\n",
      "Epoch: 6740 \tTraining Loss: 2.120256 \tValidation Loss: 2.330834\n",
      "Epoch: 6741 \tTraining Loss: 2.140267 \tValidation Loss: 2.331094\n",
      "Epoch: 6742 \tTraining Loss: 2.125563 \tValidation Loss: 2.330978\n",
      "Epoch: 6743 \tTraining Loss: 2.126771 \tValidation Loss: 2.331073\n",
      "Epoch: 6744 \tTraining Loss: 2.119919 \tValidation Loss: 2.331002\n",
      "Epoch: 6745 \tTraining Loss: 2.110248 \tValidation Loss: 2.330784\n",
      "Epoch: 6746 \tTraining Loss: 2.126446 \tValidation Loss: 2.330792\n",
      "Epoch: 6747 \tTraining Loss: 2.130907 \tValidation Loss: 2.330770\n",
      "Epoch: 6748 \tTraining Loss: 2.153402 \tValidation Loss: 2.330966\n",
      "Epoch: 6749 \tTraining Loss: 2.130967 \tValidation Loss: 2.331210\n",
      "Epoch: 6750 \tTraining Loss: 2.149486 \tValidation Loss: 2.331181\n",
      "Epoch: 6751 \tTraining Loss: 2.127710 \tValidation Loss: 2.331026\n",
      "Epoch: 6752 \tTraining Loss: 2.146666 \tValidation Loss: 2.330892\n",
      "Epoch: 6753 \tTraining Loss: 2.119789 \tValidation Loss: 2.330818\n",
      "Epoch: 6754 \tTraining Loss: 2.152645 \tValidation Loss: 2.331065\n",
      "Epoch: 6755 \tTraining Loss: 2.115943 \tValidation Loss: 2.330994\n",
      "Epoch: 6756 \tTraining Loss: 2.122111 \tValidation Loss: 2.330963\n",
      "Epoch: 6757 \tTraining Loss: 2.127658 \tValidation Loss: 2.330891\n",
      "Epoch: 6758 \tTraining Loss: 2.159296 \tValidation Loss: 2.330840\n",
      "Epoch: 6759 \tTraining Loss: 2.130948 \tValidation Loss: 2.331101\n",
      "Epoch: 6760 \tTraining Loss: 2.137068 \tValidation Loss: 2.331048\n",
      "Epoch: 6761 \tTraining Loss: 2.131823 \tValidation Loss: 2.330975\n",
      "Epoch: 6762 \tTraining Loss: 2.147013 \tValidation Loss: 2.330937\n",
      "Epoch: 6763 \tTraining Loss: 2.133684 \tValidation Loss: 2.330847\n",
      "Epoch: 6764 \tTraining Loss: 2.109286 \tValidation Loss: 2.330528\n",
      "Validation loss decreased (2.330620 --> 2.330528).  Saving model ...\n",
      "Epoch: 6765 \tTraining Loss: 2.117081 \tValidation Loss: 2.330751\n",
      "Epoch: 6766 \tTraining Loss: 2.129087 \tValidation Loss: 2.330780\n",
      "Epoch: 6767 \tTraining Loss: 2.151111 \tValidation Loss: 2.330801\n",
      "Epoch: 6768 \tTraining Loss: 2.133513 \tValidation Loss: 2.330810\n",
      "Epoch: 6769 \tTraining Loss: 2.116123 \tValidation Loss: 2.330635\n",
      "Epoch: 6770 \tTraining Loss: 2.137927 \tValidation Loss: 2.330785\n",
      "Epoch: 6771 \tTraining Loss: 2.119827 \tValidation Loss: 2.330890\n",
      "Epoch: 6772 \tTraining Loss: 2.126026 \tValidation Loss: 2.330849\n",
      "Epoch: 6773 \tTraining Loss: 2.117955 \tValidation Loss: 2.330669\n",
      "Epoch: 6774 \tTraining Loss: 2.145372 \tValidation Loss: 2.330649\n",
      "Epoch: 6775 \tTraining Loss: 2.169587 \tValidation Loss: 2.330720\n",
      "Epoch: 6776 \tTraining Loss: 2.148704 \tValidation Loss: 2.330597\n",
      "Epoch: 6777 \tTraining Loss: 2.164753 \tValidation Loss: 2.330589\n",
      "Epoch: 6778 \tTraining Loss: 2.101577 \tValidation Loss: 2.330477\n",
      "Validation loss decreased (2.330528 --> 2.330477).  Saving model ...\n",
      "Epoch: 6779 \tTraining Loss: 2.127749 \tValidation Loss: 2.330511\n",
      "Epoch: 6780 \tTraining Loss: 2.117787 \tValidation Loss: 2.330718\n",
      "Epoch: 6781 \tTraining Loss: 2.145082 \tValidation Loss: 2.330979\n",
      "Epoch: 6782 \tTraining Loss: 2.161463 \tValidation Loss: 2.331187\n",
      "Epoch: 6783 \tTraining Loss: 2.147050 \tValidation Loss: 2.331109\n",
      "Epoch: 6784 \tTraining Loss: 2.142533 \tValidation Loss: 2.331420\n",
      "Epoch: 6785 \tTraining Loss: 2.126865 \tValidation Loss: 2.331290\n",
      "Epoch: 6786 \tTraining Loss: 2.109492 \tValidation Loss: 2.331433\n",
      "Epoch: 6787 \tTraining Loss: 2.130099 \tValidation Loss: 2.331193\n",
      "Epoch: 6788 \tTraining Loss: 2.128603 \tValidation Loss: 2.331052\n",
      "Epoch: 6789 \tTraining Loss: 2.138540 \tValidation Loss: 2.331048\n",
      "Epoch: 6790 \tTraining Loss: 2.144880 \tValidation Loss: 2.331096\n",
      "Epoch: 6791 \tTraining Loss: 2.138676 \tValidation Loss: 2.330606\n",
      "Epoch: 6792 \tTraining Loss: 2.126864 \tValidation Loss: 2.330375\n",
      "Validation loss decreased (2.330477 --> 2.330375).  Saving model ...\n",
      "Epoch: 6793 \tTraining Loss: 2.142825 \tValidation Loss: 2.330133\n",
      "Validation loss decreased (2.330375 --> 2.330133).  Saving model ...\n",
      "Epoch: 6794 \tTraining Loss: 2.134331 \tValidation Loss: 2.330126\n",
      "Validation loss decreased (2.330133 --> 2.330126).  Saving model ...\n",
      "Epoch: 6795 \tTraining Loss: 2.155877 \tValidation Loss: 2.329969\n",
      "Validation loss decreased (2.330126 --> 2.329969).  Saving model ...\n",
      "Epoch: 6796 \tTraining Loss: 2.131966 \tValidation Loss: 2.329927\n",
      "Validation loss decreased (2.329969 --> 2.329927).  Saving model ...\n",
      "Epoch: 6797 \tTraining Loss: 2.130706 \tValidation Loss: 2.330014\n",
      "Epoch: 6798 \tTraining Loss: 2.165469 \tValidation Loss: 2.330217\n",
      "Epoch: 6799 \tTraining Loss: 2.122418 \tValidation Loss: 2.330297\n",
      "Epoch: 6800 \tTraining Loss: 2.136356 \tValidation Loss: 2.330300\n",
      "Epoch: 6801 \tTraining Loss: 2.144768 \tValidation Loss: 2.330118\n",
      "Epoch: 6802 \tTraining Loss: 2.151859 \tValidation Loss: 2.329782\n",
      "Validation loss decreased (2.329927 --> 2.329782).  Saving model ...\n",
      "Epoch: 6803 \tTraining Loss: 2.102946 \tValidation Loss: 2.329726\n",
      "Validation loss decreased (2.329782 --> 2.329726).  Saving model ...\n",
      "Epoch: 6804 \tTraining Loss: 2.151956 \tValidation Loss: 2.329636\n",
      "Validation loss decreased (2.329726 --> 2.329636).  Saving model ...\n",
      "Epoch: 6805 \tTraining Loss: 2.147808 \tValidation Loss: 2.329989\n",
      "Epoch: 6806 \tTraining Loss: 2.124180 \tValidation Loss: 2.329822\n",
      "Epoch: 6807 \tTraining Loss: 2.125205 \tValidation Loss: 2.329757\n",
      "Epoch: 6808 \tTraining Loss: 2.127146 \tValidation Loss: 2.329619\n",
      "Validation loss decreased (2.329636 --> 2.329619).  Saving model ...\n",
      "Epoch: 6809 \tTraining Loss: 2.137349 \tValidation Loss: 2.329641\n",
      "Epoch: 6810 \tTraining Loss: 2.128437 \tValidation Loss: 2.329902\n",
      "Epoch: 6811 \tTraining Loss: 2.142423 \tValidation Loss: 2.329813\n",
      "Epoch: 6812 \tTraining Loss: 2.122453 \tValidation Loss: 2.329941\n",
      "Epoch: 6813 \tTraining Loss: 2.116427 \tValidation Loss: 2.329863\n",
      "Epoch: 6814 \tTraining Loss: 2.124190 \tValidation Loss: 2.329674\n",
      "Epoch: 6815 \tTraining Loss: 2.130988 \tValidation Loss: 2.329744\n",
      "Epoch: 6816 \tTraining Loss: 2.134543 \tValidation Loss: 2.329662\n",
      "Epoch: 6817 \tTraining Loss: 2.132843 \tValidation Loss: 2.329812\n",
      "Epoch: 6818 \tTraining Loss: 2.154901 \tValidation Loss: 2.329727\n",
      "Epoch: 6819 \tTraining Loss: 2.123533 \tValidation Loss: 2.329576\n",
      "Validation loss decreased (2.329619 --> 2.329576).  Saving model ...\n",
      "Epoch: 6820 \tTraining Loss: 2.135410 \tValidation Loss: 2.329543\n",
      "Validation loss decreased (2.329576 --> 2.329543).  Saving model ...\n",
      "Epoch: 6821 \tTraining Loss: 2.128826 \tValidation Loss: 2.329485\n",
      "Validation loss decreased (2.329543 --> 2.329485).  Saving model ...\n",
      "Epoch: 6822 \tTraining Loss: 2.138947 \tValidation Loss: 2.329593\n",
      "Epoch: 6823 \tTraining Loss: 2.135054 \tValidation Loss: 2.329561\n",
      "Epoch: 6824 \tTraining Loss: 2.129559 \tValidation Loss: 2.329673\n",
      "Epoch: 6825 \tTraining Loss: 2.148880 \tValidation Loss: 2.329633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6826 \tTraining Loss: 2.106849 \tValidation Loss: 2.329581\n",
      "Epoch: 6827 \tTraining Loss: 2.175831 \tValidation Loss: 2.329666\n",
      "Epoch: 6828 \tTraining Loss: 2.133722 \tValidation Loss: 2.329675\n",
      "Epoch: 6829 \tTraining Loss: 2.143171 \tValidation Loss: 2.329688\n",
      "Epoch: 6830 \tTraining Loss: 2.150382 \tValidation Loss: 2.329737\n",
      "Epoch: 6831 \tTraining Loss: 2.151840 \tValidation Loss: 2.329749\n",
      "Epoch: 6832 \tTraining Loss: 2.114803 \tValidation Loss: 2.329878\n",
      "Epoch: 6833 \tTraining Loss: 2.129978 \tValidation Loss: 2.329879\n",
      "Epoch: 6834 \tTraining Loss: 2.120222 \tValidation Loss: 2.329942\n",
      "Epoch: 6835 \tTraining Loss: 2.137260 \tValidation Loss: 2.329977\n",
      "Epoch: 6836 \tTraining Loss: 2.137825 \tValidation Loss: 2.329854\n",
      "Epoch: 6837 \tTraining Loss: 2.122400 \tValidation Loss: 2.329871\n",
      "Epoch: 6838 \tTraining Loss: 2.118701 \tValidation Loss: 2.329642\n",
      "Epoch: 6839 \tTraining Loss: 2.140204 \tValidation Loss: 2.329577\n",
      "Epoch: 6840 \tTraining Loss: 2.128654 \tValidation Loss: 2.329570\n",
      "Epoch: 6841 \tTraining Loss: 2.125341 \tValidation Loss: 2.329431\n",
      "Validation loss decreased (2.329485 --> 2.329431).  Saving model ...\n",
      "Epoch: 6842 \tTraining Loss: 2.115606 \tValidation Loss: 2.329149\n",
      "Validation loss decreased (2.329431 --> 2.329149).  Saving model ...\n",
      "Epoch: 6843 \tTraining Loss: 2.122349 \tValidation Loss: 2.329041\n",
      "Validation loss decreased (2.329149 --> 2.329041).  Saving model ...\n",
      "Epoch: 6844 \tTraining Loss: 2.111708 \tValidation Loss: 2.329015\n",
      "Validation loss decreased (2.329041 --> 2.329015).  Saving model ...\n",
      "Epoch: 6845 \tTraining Loss: 2.138282 \tValidation Loss: 2.329017\n",
      "Epoch: 6846 \tTraining Loss: 2.130282 \tValidation Loss: 2.328854\n",
      "Validation loss decreased (2.329015 --> 2.328854).  Saving model ...\n",
      "Epoch: 6847 \tTraining Loss: 2.144381 \tValidation Loss: 2.328886\n",
      "Epoch: 6848 \tTraining Loss: 2.147337 \tValidation Loss: 2.329099\n",
      "Epoch: 6849 \tTraining Loss: 2.119632 \tValidation Loss: 2.329220\n",
      "Epoch: 6850 \tTraining Loss: 2.126611 \tValidation Loss: 2.329106\n",
      "Epoch: 6851 \tTraining Loss: 2.119811 \tValidation Loss: 2.328890\n",
      "Epoch: 6852 \tTraining Loss: 2.109908 \tValidation Loss: 2.328922\n",
      "Epoch: 6853 \tTraining Loss: 2.135993 \tValidation Loss: 2.328945\n",
      "Epoch: 6854 \tTraining Loss: 2.145293 \tValidation Loss: 2.328903\n",
      "Epoch: 6855 \tTraining Loss: 2.135793 \tValidation Loss: 2.328977\n",
      "Epoch: 6856 \tTraining Loss: 2.139877 \tValidation Loss: 2.328921\n",
      "Epoch: 6857 \tTraining Loss: 2.140208 \tValidation Loss: 2.329169\n",
      "Epoch: 6858 \tTraining Loss: 2.123401 \tValidation Loss: 2.329126\n",
      "Epoch: 6859 \tTraining Loss: 2.130931 \tValidation Loss: 2.329326\n",
      "Epoch: 6860 \tTraining Loss: 2.157772 \tValidation Loss: 2.329353\n",
      "Epoch: 6861 \tTraining Loss: 2.122318 \tValidation Loss: 2.329316\n",
      "Epoch: 6862 \tTraining Loss: 2.125183 \tValidation Loss: 2.328955\n",
      "Epoch: 6863 \tTraining Loss: 2.142539 \tValidation Loss: 2.329180\n",
      "Epoch: 6864 \tTraining Loss: 2.140026 \tValidation Loss: 2.329157\n",
      "Epoch: 6865 \tTraining Loss: 2.117127 \tValidation Loss: 2.329152\n",
      "Epoch: 6866 \tTraining Loss: 2.156539 \tValidation Loss: 2.329057\n",
      "Epoch: 6867 \tTraining Loss: 2.124058 \tValidation Loss: 2.329335\n",
      "Epoch: 6868 \tTraining Loss: 2.120319 \tValidation Loss: 2.329441\n",
      "Epoch: 6869 \tTraining Loss: 2.120145 \tValidation Loss: 2.329262\n",
      "Epoch: 6870 \tTraining Loss: 2.124065 \tValidation Loss: 2.329061\n",
      "Epoch: 6871 \tTraining Loss: 2.150219 \tValidation Loss: 2.329076\n",
      "Epoch: 6872 \tTraining Loss: 2.156583 \tValidation Loss: 2.329247\n",
      "Epoch: 6873 \tTraining Loss: 2.119191 \tValidation Loss: 2.328999\n",
      "Epoch: 6874 \tTraining Loss: 2.159413 \tValidation Loss: 2.329082\n",
      "Epoch: 6875 \tTraining Loss: 2.133770 \tValidation Loss: 2.328943\n",
      "Epoch: 6876 \tTraining Loss: 2.143135 \tValidation Loss: 2.329070\n",
      "Epoch: 6877 \tTraining Loss: 2.120150 \tValidation Loss: 2.328692\n",
      "Validation loss decreased (2.328854 --> 2.328692).  Saving model ...\n",
      "Epoch: 6878 \tTraining Loss: 2.127664 \tValidation Loss: 2.328972\n",
      "Epoch: 6879 \tTraining Loss: 2.118043 \tValidation Loss: 2.328734\n",
      "Epoch: 6880 \tTraining Loss: 2.170590 \tValidation Loss: 2.328912\n",
      "Epoch: 6881 \tTraining Loss: 2.124323 \tValidation Loss: 2.328690\n",
      "Validation loss decreased (2.328692 --> 2.328690).  Saving model ...\n",
      "Epoch: 6882 \tTraining Loss: 2.136204 \tValidation Loss: 2.328754\n",
      "Epoch: 6883 \tTraining Loss: 2.115400 \tValidation Loss: 2.328865\n",
      "Epoch: 6884 \tTraining Loss: 2.125917 \tValidation Loss: 2.328809\n",
      "Epoch: 6885 \tTraining Loss: 2.154384 \tValidation Loss: 2.329076\n",
      "Epoch: 6886 \tTraining Loss: 2.112309 \tValidation Loss: 2.329093\n",
      "Epoch: 6887 \tTraining Loss: 2.125191 \tValidation Loss: 2.328970\n",
      "Epoch: 6888 \tTraining Loss: 2.114110 \tValidation Loss: 2.328941\n",
      "Epoch: 6889 \tTraining Loss: 2.125392 \tValidation Loss: 2.328499\n",
      "Validation loss decreased (2.328690 --> 2.328499).  Saving model ...\n",
      "Epoch: 6890 \tTraining Loss: 2.132361 \tValidation Loss: 2.328571\n",
      "Epoch: 6891 \tTraining Loss: 2.135125 \tValidation Loss: 2.328562\n",
      "Epoch: 6892 \tTraining Loss: 2.105890 \tValidation Loss: 2.328475\n",
      "Validation loss decreased (2.328499 --> 2.328475).  Saving model ...\n",
      "Epoch: 6893 \tTraining Loss: 2.142121 \tValidation Loss: 2.328275\n",
      "Validation loss decreased (2.328475 --> 2.328275).  Saving model ...\n",
      "Epoch: 6894 \tTraining Loss: 2.124656 \tValidation Loss: 2.328196\n",
      "Validation loss decreased (2.328275 --> 2.328196).  Saving model ...\n",
      "Epoch: 6895 \tTraining Loss: 2.129623 \tValidation Loss: 2.328401\n",
      "Epoch: 6896 \tTraining Loss: 2.149898 \tValidation Loss: 2.328851\n",
      "Epoch: 6897 \tTraining Loss: 2.131574 \tValidation Loss: 2.329009\n",
      "Epoch: 6898 \tTraining Loss: 2.126162 \tValidation Loss: 2.328838\n",
      "Epoch: 6899 \tTraining Loss: 2.134538 \tValidation Loss: 2.328438\n",
      "Epoch: 6900 \tTraining Loss: 2.132220 \tValidation Loss: 2.328527\n",
      "Epoch: 6901 \tTraining Loss: 2.131677 \tValidation Loss: 2.328503\n",
      "Epoch: 6902 \tTraining Loss: 2.127065 \tValidation Loss: 2.328364\n",
      "Epoch: 6903 \tTraining Loss: 2.124796 \tValidation Loss: 2.328381\n",
      "Epoch: 6904 \tTraining Loss: 2.123096 \tValidation Loss: 2.328597\n",
      "Epoch: 6905 \tTraining Loss: 2.114968 \tValidation Loss: 2.328861\n",
      "Epoch: 6906 \tTraining Loss: 2.106596 \tValidation Loss: 2.328710\n",
      "Epoch: 6907 \tTraining Loss: 2.130412 \tValidation Loss: 2.328687\n",
      "Epoch: 6908 \tTraining Loss: 2.130835 \tValidation Loss: 2.328805\n",
      "Epoch: 6909 \tTraining Loss: 2.122320 \tValidation Loss: 2.328636\n",
      "Epoch: 6910 \tTraining Loss: 2.121746 \tValidation Loss: 2.328696\n",
      "Epoch: 6911 \tTraining Loss: 2.131726 \tValidation Loss: 2.328728\n",
      "Epoch: 6912 \tTraining Loss: 2.092895 \tValidation Loss: 2.328768\n",
      "Epoch: 6913 \tTraining Loss: 2.155300 \tValidation Loss: 2.328594\n",
      "Epoch: 6914 \tTraining Loss: 2.100400 \tValidation Loss: 2.328632\n",
      "Epoch: 6915 \tTraining Loss: 2.148519 \tValidation Loss: 2.328836\n",
      "Epoch: 6916 \tTraining Loss: 2.131702 \tValidation Loss: 2.328485\n",
      "Epoch: 6917 \tTraining Loss: 2.107704 \tValidation Loss: 2.328680\n",
      "Epoch: 6918 \tTraining Loss: 2.142105 \tValidation Loss: 2.328826\n",
      "Epoch: 6919 \tTraining Loss: 2.120284 \tValidation Loss: 2.328687\n",
      "Epoch: 6920 \tTraining Loss: 2.113740 \tValidation Loss: 2.328723\n",
      "Epoch: 6921 \tTraining Loss: 2.114580 \tValidation Loss: 2.328770\n",
      "Epoch: 6922 \tTraining Loss: 2.110052 \tValidation Loss: 2.328702\n",
      "Epoch: 6923 \tTraining Loss: 2.109583 \tValidation Loss: 2.328596\n",
      "Epoch: 6924 \tTraining Loss: 2.141063 \tValidation Loss: 2.328789\n",
      "Epoch: 6925 \tTraining Loss: 2.129450 \tValidation Loss: 2.328768\n",
      "Epoch: 6926 \tTraining Loss: 2.123837 \tValidation Loss: 2.328819\n",
      "Epoch: 6927 \tTraining Loss: 2.128864 \tValidation Loss: 2.328563\n",
      "Epoch: 6928 \tTraining Loss: 2.131677 \tValidation Loss: 2.328594\n",
      "Epoch: 6929 \tTraining Loss: 2.136841 \tValidation Loss: 2.328430\n",
      "Epoch: 6930 \tTraining Loss: 2.089872 \tValidation Loss: 2.328662\n",
      "Epoch: 6931 \tTraining Loss: 2.113187 \tValidation Loss: 2.328601\n",
      "Epoch: 6932 \tTraining Loss: 2.124943 \tValidation Loss: 2.328716\n",
      "Epoch: 6933 \tTraining Loss: 2.103907 \tValidation Loss: 2.328738\n",
      "Epoch: 6934 \tTraining Loss: 2.127402 \tValidation Loss: 2.328479\n",
      "Epoch: 6935 \tTraining Loss: 2.121315 \tValidation Loss: 2.328404\n",
      "Epoch: 6936 \tTraining Loss: 2.124799 \tValidation Loss: 2.328415\n",
      "Epoch: 6937 \tTraining Loss: 2.150881 \tValidation Loss: 2.328124\n",
      "Validation loss decreased (2.328196 --> 2.328124).  Saving model ...\n",
      "Epoch: 6938 \tTraining Loss: 2.116707 \tValidation Loss: 2.328242\n",
      "Epoch: 6939 \tTraining Loss: 2.135635 \tValidation Loss: 2.328143\n",
      "Epoch: 6940 \tTraining Loss: 2.097592 \tValidation Loss: 2.328038\n",
      "Validation loss decreased (2.328124 --> 2.328038).  Saving model ...\n",
      "Epoch: 6941 \tTraining Loss: 2.143224 \tValidation Loss: 2.328155\n",
      "Epoch: 6942 \tTraining Loss: 2.112349 \tValidation Loss: 2.328085\n",
      "Epoch: 6943 \tTraining Loss: 2.115205 \tValidation Loss: 2.328066\n",
      "Epoch: 6944 \tTraining Loss: 2.112365 \tValidation Loss: 2.327961\n",
      "Validation loss decreased (2.328038 --> 2.327961).  Saving model ...\n",
      "Epoch: 6945 \tTraining Loss: 2.118713 \tValidation Loss: 2.327677\n",
      "Validation loss decreased (2.327961 --> 2.327677).  Saving model ...\n",
      "Epoch: 6946 \tTraining Loss: 2.127123 \tValidation Loss: 2.327834\n",
      "Epoch: 6947 \tTraining Loss: 2.133437 \tValidation Loss: 2.327874\n",
      "Epoch: 6948 \tTraining Loss: 2.130225 \tValidation Loss: 2.327816\n",
      "Epoch: 6949 \tTraining Loss: 2.113263 \tValidation Loss: 2.327849\n",
      "Epoch: 6950 \tTraining Loss: 2.106557 \tValidation Loss: 2.327731\n",
      "Epoch: 6951 \tTraining Loss: 2.118023 \tValidation Loss: 2.327558\n",
      "Validation loss decreased (2.327677 --> 2.327558).  Saving model ...\n",
      "Epoch: 6952 \tTraining Loss: 2.117387 \tValidation Loss: 2.327715\n",
      "Epoch: 6953 \tTraining Loss: 2.134760 \tValidation Loss: 2.327914\n",
      "Epoch: 6954 \tTraining Loss: 2.130353 \tValidation Loss: 2.328175\n",
      "Epoch: 6955 \tTraining Loss: 2.132796 \tValidation Loss: 2.328187\n",
      "Epoch: 6956 \tTraining Loss: 2.135401 \tValidation Loss: 2.328137\n",
      "Epoch: 6957 \tTraining Loss: 2.138887 \tValidation Loss: 2.328016\n",
      "Epoch: 6958 \tTraining Loss: 2.118341 \tValidation Loss: 2.327665\n",
      "Epoch: 6959 \tTraining Loss: 2.113113 \tValidation Loss: 2.327806\n",
      "Epoch: 6960 \tTraining Loss: 2.131578 \tValidation Loss: 2.327805\n",
      "Epoch: 6961 \tTraining Loss: 2.115773 \tValidation Loss: 2.327574\n",
      "Epoch: 6962 \tTraining Loss: 2.104561 \tValidation Loss: 2.327610\n",
      "Epoch: 6963 \tTraining Loss: 2.127544 \tValidation Loss: 2.327723\n",
      "Epoch: 6964 \tTraining Loss: 2.136771 \tValidation Loss: 2.328036\n",
      "Epoch: 6965 \tTraining Loss: 2.136792 \tValidation Loss: 2.327810\n",
      "Epoch: 6966 \tTraining Loss: 2.118339 \tValidation Loss: 2.327989\n",
      "Epoch: 6967 \tTraining Loss: 2.120916 \tValidation Loss: 2.327774\n",
      "Epoch: 6968 \tTraining Loss: 2.128339 \tValidation Loss: 2.327847\n",
      "Epoch: 6969 \tTraining Loss: 2.116553 \tValidation Loss: 2.327755\n",
      "Epoch: 6970 \tTraining Loss: 2.119411 \tValidation Loss: 2.327611\n",
      "Epoch: 6971 \tTraining Loss: 2.112705 \tValidation Loss: 2.327896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6972 \tTraining Loss: 2.135196 \tValidation Loss: 2.327859\n",
      "Epoch: 6973 \tTraining Loss: 2.114211 \tValidation Loss: 2.327659\n",
      "Epoch: 6974 \tTraining Loss: 2.146964 \tValidation Loss: 2.327793\n",
      "Epoch: 6975 \tTraining Loss: 2.139037 \tValidation Loss: 2.328004\n",
      "Epoch: 6976 \tTraining Loss: 2.109808 \tValidation Loss: 2.327994\n",
      "Epoch: 6977 \tTraining Loss: 2.154788 \tValidation Loss: 2.328101\n",
      "Epoch: 6978 \tTraining Loss: 2.105525 \tValidation Loss: 2.328004\n",
      "Epoch: 6979 \tTraining Loss: 2.112074 \tValidation Loss: 2.328010\n",
      "Epoch: 6980 \tTraining Loss: 2.139765 \tValidation Loss: 2.327818\n",
      "Epoch: 6981 \tTraining Loss: 2.128531 \tValidation Loss: 2.327767\n",
      "Epoch: 6982 \tTraining Loss: 2.131776 \tValidation Loss: 2.327516\n",
      "Validation loss decreased (2.327558 --> 2.327516).  Saving model ...\n",
      "Epoch: 6983 \tTraining Loss: 2.123559 \tValidation Loss: 2.327438\n",
      "Validation loss decreased (2.327516 --> 2.327438).  Saving model ...\n",
      "Epoch: 6984 \tTraining Loss: 2.110048 \tValidation Loss: 2.327407\n",
      "Validation loss decreased (2.327438 --> 2.327407).  Saving model ...\n",
      "Epoch: 6985 \tTraining Loss: 2.130581 \tValidation Loss: 2.327361\n",
      "Validation loss decreased (2.327407 --> 2.327361).  Saving model ...\n",
      "Epoch: 6986 \tTraining Loss: 2.135952 \tValidation Loss: 2.327407\n",
      "Epoch: 6987 \tTraining Loss: 2.120964 \tValidation Loss: 2.327214\n",
      "Validation loss decreased (2.327361 --> 2.327214).  Saving model ...\n",
      "Epoch: 6988 \tTraining Loss: 2.122401 \tValidation Loss: 2.327510\n",
      "Epoch: 6989 \tTraining Loss: 2.137583 \tValidation Loss: 2.327464\n",
      "Epoch: 6990 \tTraining Loss: 2.102311 \tValidation Loss: 2.327572\n",
      "Epoch: 6991 \tTraining Loss: 2.130571 \tValidation Loss: 2.327585\n",
      "Epoch: 6992 \tTraining Loss: 2.097509 \tValidation Loss: 2.327466\n",
      "Epoch: 6993 \tTraining Loss: 2.088946 \tValidation Loss: 2.327290\n",
      "Epoch: 6994 \tTraining Loss: 2.142456 \tValidation Loss: 2.327225\n",
      "Epoch: 6995 \tTraining Loss: 2.128628 \tValidation Loss: 2.327358\n",
      "Epoch: 6996 \tTraining Loss: 2.137969 \tValidation Loss: 2.327325\n",
      "Epoch: 6997 \tTraining Loss: 2.124016 \tValidation Loss: 2.327280\n",
      "Epoch: 6998 \tTraining Loss: 2.136887 \tValidation Loss: 2.327500\n",
      "Epoch: 6999 \tTraining Loss: 2.115300 \tValidation Loss: 2.327641\n",
      "Epoch: 7000 \tTraining Loss: 2.123937 \tValidation Loss: 2.327804\n",
      "Epoch: 7001 \tTraining Loss: 2.130226 \tValidation Loss: 2.327718\n",
      "Epoch: 7002 \tTraining Loss: 2.100124 \tValidation Loss: 2.327722\n",
      "Epoch: 7003 \tTraining Loss: 2.130808 \tValidation Loss: 2.327967\n",
      "Epoch: 7004 \tTraining Loss: 2.127796 \tValidation Loss: 2.328130\n",
      "Epoch: 7005 \tTraining Loss: 2.107079 \tValidation Loss: 2.327779\n",
      "Epoch: 7006 \tTraining Loss: 2.112729 \tValidation Loss: 2.327784\n",
      "Epoch: 7007 \tTraining Loss: 2.114540 \tValidation Loss: 2.327688\n",
      "Epoch: 7008 \tTraining Loss: 2.118996 \tValidation Loss: 2.327794\n",
      "Epoch: 7009 \tTraining Loss: 2.136832 \tValidation Loss: 2.327550\n",
      "Epoch: 7010 \tTraining Loss: 2.096560 \tValidation Loss: 2.327343\n",
      "Epoch: 7011 \tTraining Loss: 2.127902 \tValidation Loss: 2.327048\n",
      "Validation loss decreased (2.327214 --> 2.327048).  Saving model ...\n",
      "Epoch: 7012 \tTraining Loss: 2.130631 \tValidation Loss: 2.327021\n",
      "Validation loss decreased (2.327048 --> 2.327021).  Saving model ...\n",
      "Epoch: 7013 \tTraining Loss: 2.116630 \tValidation Loss: 2.326734\n",
      "Validation loss decreased (2.327021 --> 2.326734).  Saving model ...\n",
      "Epoch: 7014 \tTraining Loss: 2.102758 \tValidation Loss: 2.326495\n",
      "Validation loss decreased (2.326734 --> 2.326495).  Saving model ...\n",
      "Epoch: 7015 \tTraining Loss: 2.120602 \tValidation Loss: 2.326421\n",
      "Validation loss decreased (2.326495 --> 2.326421).  Saving model ...\n",
      "Epoch: 7016 \tTraining Loss: 2.117011 \tValidation Loss: 2.326760\n",
      "Epoch: 7017 \tTraining Loss: 2.119181 \tValidation Loss: 2.326615\n",
      "Epoch: 7018 \tTraining Loss: 2.141835 \tValidation Loss: 2.326984\n",
      "Epoch: 7019 \tTraining Loss: 2.125447 \tValidation Loss: 2.327101\n",
      "Epoch: 7020 \tTraining Loss: 2.120984 \tValidation Loss: 2.327432\n",
      "Epoch: 7021 \tTraining Loss: 2.117160 \tValidation Loss: 2.327562\n",
      "Epoch: 7022 \tTraining Loss: 2.133936 \tValidation Loss: 2.327904\n",
      "Epoch: 7023 \tTraining Loss: 2.113708 \tValidation Loss: 2.327919\n",
      "Epoch: 7024 \tTraining Loss: 2.081905 \tValidation Loss: 2.327809\n",
      "Epoch: 7025 \tTraining Loss: 2.097208 \tValidation Loss: 2.327644\n",
      "Epoch: 7026 \tTraining Loss: 2.121486 \tValidation Loss: 2.327705\n",
      "Epoch: 7027 \tTraining Loss: 2.152661 \tValidation Loss: 2.327766\n",
      "Epoch: 7028 \tTraining Loss: 2.134418 \tValidation Loss: 2.327549\n",
      "Epoch: 7029 \tTraining Loss: 2.131031 \tValidation Loss: 2.327438\n",
      "Epoch: 7030 \tTraining Loss: 2.140691 \tValidation Loss: 2.327402\n",
      "Epoch: 7031 \tTraining Loss: 2.135984 \tValidation Loss: 2.327296\n",
      "Epoch: 7032 \tTraining Loss: 2.122080 \tValidation Loss: 2.327356\n",
      "Epoch: 7033 \tTraining Loss: 2.125502 \tValidation Loss: 2.327201\n",
      "Epoch: 7034 \tTraining Loss: 2.139793 \tValidation Loss: 2.327188\n",
      "Epoch: 7035 \tTraining Loss: 2.113941 \tValidation Loss: 2.327063\n",
      "Epoch: 7036 \tTraining Loss: 2.110637 \tValidation Loss: 2.327026\n",
      "Epoch: 7037 \tTraining Loss: 2.117864 \tValidation Loss: 2.327396\n",
      "Epoch: 7038 \tTraining Loss: 2.121137 \tValidation Loss: 2.327503\n",
      "Epoch: 7039 \tTraining Loss: 2.120448 \tValidation Loss: 2.327480\n",
      "Epoch: 7040 \tTraining Loss: 2.115273 \tValidation Loss: 2.327213\n",
      "Epoch: 7041 \tTraining Loss: 2.138793 \tValidation Loss: 2.327357\n",
      "Epoch: 7042 \tTraining Loss: 2.121985 \tValidation Loss: 2.327335\n",
      "Epoch: 7043 \tTraining Loss: 2.117633 \tValidation Loss: 2.327219\n",
      "Epoch: 7044 \tTraining Loss: 2.128880 \tValidation Loss: 2.327186\n",
      "Epoch: 7045 \tTraining Loss: 2.103077 \tValidation Loss: 2.327278\n",
      "Epoch: 7046 \tTraining Loss: 2.110481 \tValidation Loss: 2.327139\n",
      "Epoch: 7047 \tTraining Loss: 2.119548 \tValidation Loss: 2.327178\n",
      "Epoch: 7048 \tTraining Loss: 2.105720 \tValidation Loss: 2.327053\n",
      "Epoch: 7049 \tTraining Loss: 2.126834 \tValidation Loss: 2.326973\n",
      "Epoch: 7050 \tTraining Loss: 2.095510 \tValidation Loss: 2.326921\n",
      "Epoch: 7051 \tTraining Loss: 2.121715 \tValidation Loss: 2.327194\n",
      "Epoch: 7052 \tTraining Loss: 2.126625 \tValidation Loss: 2.327408\n",
      "Epoch: 7053 \tTraining Loss: 2.115098 \tValidation Loss: 2.327499\n",
      "Epoch: 7054 \tTraining Loss: 2.148066 \tValidation Loss: 2.327139\n",
      "Epoch: 7055 \tTraining Loss: 2.114146 \tValidation Loss: 2.327034\n",
      "Epoch: 7056 \tTraining Loss: 2.120507 \tValidation Loss: 2.326950\n",
      "Epoch: 7057 \tTraining Loss: 2.104006 \tValidation Loss: 2.326871\n",
      "Epoch: 7058 \tTraining Loss: 2.109760 \tValidation Loss: 2.326919\n",
      "Epoch: 7059 \tTraining Loss: 2.148446 \tValidation Loss: 2.327087\n",
      "Epoch: 7060 \tTraining Loss: 2.116944 \tValidation Loss: 2.327004\n",
      "Epoch: 7061 \tTraining Loss: 2.110010 \tValidation Loss: 2.327128\n",
      "Epoch: 7062 \tTraining Loss: 2.135736 \tValidation Loss: 2.326957\n",
      "Epoch: 7063 \tTraining Loss: 2.128048 \tValidation Loss: 2.327022\n",
      "Epoch: 7064 \tTraining Loss: 2.116924 \tValidation Loss: 2.326901\n",
      "Epoch: 7065 \tTraining Loss: 2.125128 \tValidation Loss: 2.326540\n",
      "Epoch: 7066 \tTraining Loss: 2.109017 \tValidation Loss: 2.326471\n",
      "Epoch: 7067 \tTraining Loss: 2.130351 \tValidation Loss: 2.326594\n",
      "Epoch: 7068 \tTraining Loss: 2.113818 \tValidation Loss: 2.326835\n",
      "Epoch: 7069 \tTraining Loss: 2.134263 \tValidation Loss: 2.327035\n",
      "Epoch: 7070 \tTraining Loss: 2.135083 \tValidation Loss: 2.327203\n",
      "Epoch: 7071 \tTraining Loss: 2.123260 \tValidation Loss: 2.326996\n",
      "Epoch: 7072 \tTraining Loss: 2.114627 \tValidation Loss: 2.326704\n",
      "Epoch: 7073 \tTraining Loss: 2.132377 \tValidation Loss: 2.326527\n",
      "Epoch: 7074 \tTraining Loss: 2.127163 \tValidation Loss: 2.326471\n",
      "Epoch: 7075 \tTraining Loss: 2.116633 \tValidation Loss: 2.326473\n",
      "Epoch: 7076 \tTraining Loss: 2.101017 \tValidation Loss: 2.326652\n",
      "Epoch: 7077 \tTraining Loss: 2.130328 \tValidation Loss: 2.326420\n",
      "Validation loss decreased (2.326421 --> 2.326420).  Saving model ...\n",
      "Epoch: 7078 \tTraining Loss: 2.123213 \tValidation Loss: 2.326320\n",
      "Validation loss decreased (2.326420 --> 2.326320).  Saving model ...\n",
      "Epoch: 7079 \tTraining Loss: 2.082300 \tValidation Loss: 2.326118\n",
      "Validation loss decreased (2.326320 --> 2.326118).  Saving model ...\n",
      "Epoch: 7080 \tTraining Loss: 2.112743 \tValidation Loss: 2.325913\n",
      "Validation loss decreased (2.326118 --> 2.325913).  Saving model ...\n",
      "Epoch: 7081 \tTraining Loss: 2.101354 \tValidation Loss: 2.326152\n",
      "Epoch: 7082 \tTraining Loss: 2.126083 \tValidation Loss: 2.326191\n",
      "Epoch: 7083 \tTraining Loss: 2.085012 \tValidation Loss: 2.325804\n",
      "Validation loss decreased (2.325913 --> 2.325804).  Saving model ...\n",
      "Epoch: 7084 \tTraining Loss: 2.120860 \tValidation Loss: 2.325919\n",
      "Epoch: 7085 \tTraining Loss: 2.126370 \tValidation Loss: 2.325621\n",
      "Validation loss decreased (2.325804 --> 2.325621).  Saving model ...\n",
      "Epoch: 7086 \tTraining Loss: 2.127867 \tValidation Loss: 2.325850\n",
      "Epoch: 7087 \tTraining Loss: 2.116656 \tValidation Loss: 2.326234\n",
      "Epoch: 7088 \tTraining Loss: 2.127484 \tValidation Loss: 2.326492\n",
      "Epoch: 7089 \tTraining Loss: 2.123961 \tValidation Loss: 2.326478\n",
      "Epoch: 7090 \tTraining Loss: 2.080011 \tValidation Loss: 2.326359\n",
      "Epoch: 7091 \tTraining Loss: 2.128218 \tValidation Loss: 2.326337\n",
      "Epoch: 7092 \tTraining Loss: 2.119113 \tValidation Loss: 2.326458\n",
      "Epoch: 7093 \tTraining Loss: 2.119292 \tValidation Loss: 2.326442\n",
      "Epoch: 7094 \tTraining Loss: 2.108317 \tValidation Loss: 2.326196\n",
      "Epoch: 7095 \tTraining Loss: 2.121312 \tValidation Loss: 2.326040\n",
      "Epoch: 7096 \tTraining Loss: 2.107032 \tValidation Loss: 2.325857\n",
      "Epoch: 7097 \tTraining Loss: 2.101820 \tValidation Loss: 2.325933\n",
      "Epoch: 7098 \tTraining Loss: 2.117609 \tValidation Loss: 2.325954\n",
      "Epoch: 7099 \tTraining Loss: 2.104696 \tValidation Loss: 2.325480\n",
      "Validation loss decreased (2.325621 --> 2.325480).  Saving model ...\n",
      "Epoch: 7100 \tTraining Loss: 2.104953 \tValidation Loss: 2.325716\n",
      "Epoch: 7101 \tTraining Loss: 2.081509 \tValidation Loss: 2.325621\n",
      "Epoch: 7102 \tTraining Loss: 2.107378 \tValidation Loss: 2.325695\n",
      "Epoch: 7103 \tTraining Loss: 2.100553 \tValidation Loss: 2.325781\n",
      "Epoch: 7104 \tTraining Loss: 2.111908 \tValidation Loss: 2.325838\n",
      "Epoch: 7105 \tTraining Loss: 2.120214 \tValidation Loss: 2.325861\n",
      "Epoch: 7106 \tTraining Loss: 2.108909 \tValidation Loss: 2.325729\n",
      "Epoch: 7107 \tTraining Loss: 2.127126 \tValidation Loss: 2.325747\n",
      "Epoch: 7108 \tTraining Loss: 2.106277 \tValidation Loss: 2.325465\n",
      "Validation loss decreased (2.325480 --> 2.325465).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7109 \tTraining Loss: 2.108989 \tValidation Loss: 2.325534\n",
      "Epoch: 7110 \tTraining Loss: 2.106190 \tValidation Loss: 2.325347\n",
      "Validation loss decreased (2.325465 --> 2.325347).  Saving model ...\n",
      "Epoch: 7111 \tTraining Loss: 2.112947 \tValidation Loss: 2.325279\n",
      "Validation loss decreased (2.325347 --> 2.325279).  Saving model ...\n",
      "Epoch: 7112 \tTraining Loss: 2.089244 \tValidation Loss: 2.325492\n",
      "Epoch: 7113 \tTraining Loss: 2.116664 \tValidation Loss: 2.325454\n",
      "Epoch: 7114 \tTraining Loss: 2.111263 \tValidation Loss: 2.325523\n",
      "Epoch: 7115 \tTraining Loss: 2.138301 \tValidation Loss: 2.325427\n",
      "Epoch: 7116 \tTraining Loss: 2.111623 \tValidation Loss: 2.325541\n",
      "Epoch: 7117 \tTraining Loss: 2.104427 \tValidation Loss: 2.325479\n",
      "Epoch: 7118 \tTraining Loss: 2.130626 \tValidation Loss: 2.325400\n",
      "Epoch: 7119 \tTraining Loss: 2.102033 \tValidation Loss: 2.325584\n",
      "Epoch: 7120 \tTraining Loss: 2.124299 \tValidation Loss: 2.325556\n",
      "Epoch: 7121 \tTraining Loss: 2.111902 \tValidation Loss: 2.325441\n",
      "Epoch: 7122 \tTraining Loss: 2.093614 \tValidation Loss: 2.325294\n",
      "Epoch: 7123 \tTraining Loss: 2.108978 \tValidation Loss: 2.325359\n",
      "Epoch: 7124 \tTraining Loss: 2.130667 \tValidation Loss: 2.325754\n",
      "Epoch: 7125 \tTraining Loss: 2.110983 \tValidation Loss: 2.325812\n",
      "Epoch: 7126 \tTraining Loss: 2.134291 \tValidation Loss: 2.325922\n",
      "Epoch: 7127 \tTraining Loss: 2.090461 \tValidation Loss: 2.325974\n",
      "Epoch: 7128 \tTraining Loss: 2.109602 \tValidation Loss: 2.325885\n",
      "Epoch: 7129 \tTraining Loss: 2.110280 \tValidation Loss: 2.325933\n",
      "Epoch: 7130 \tTraining Loss: 2.127234 \tValidation Loss: 2.325785\n",
      "Epoch: 7131 \tTraining Loss: 2.108813 \tValidation Loss: 2.325365\n",
      "Epoch: 7132 \tTraining Loss: 2.095998 \tValidation Loss: 2.325231\n",
      "Validation loss decreased (2.325279 --> 2.325231).  Saving model ...\n",
      "Epoch: 7133 \tTraining Loss: 2.113829 \tValidation Loss: 2.325369\n",
      "Epoch: 7134 \tTraining Loss: 2.097808 \tValidation Loss: 2.325341\n",
      "Epoch: 7135 \tTraining Loss: 2.116608 \tValidation Loss: 2.325444\n",
      "Epoch: 7136 \tTraining Loss: 2.106869 \tValidation Loss: 2.325480\n",
      "Epoch: 7137 \tTraining Loss: 2.112567 \tValidation Loss: 2.325471\n",
      "Epoch: 7138 \tTraining Loss: 2.106217 \tValidation Loss: 2.325470\n",
      "Epoch: 7139 \tTraining Loss: 2.123087 \tValidation Loss: 2.325489\n",
      "Epoch: 7140 \tTraining Loss: 2.112024 \tValidation Loss: 2.325236\n",
      "Epoch: 7141 \tTraining Loss: 2.104323 \tValidation Loss: 2.325356\n",
      "Epoch: 7142 \tTraining Loss: 2.129812 \tValidation Loss: 2.325417\n",
      "Epoch: 7143 \tTraining Loss: 2.112206 \tValidation Loss: 2.325521\n",
      "Epoch: 7144 \tTraining Loss: 2.088797 \tValidation Loss: 2.325305\n",
      "Epoch: 7145 \tTraining Loss: 2.122142 \tValidation Loss: 2.325531\n",
      "Epoch: 7146 \tTraining Loss: 2.113590 \tValidation Loss: 2.325325\n",
      "Epoch: 7147 \tTraining Loss: 2.138316 \tValidation Loss: 2.325524\n",
      "Epoch: 7148 \tTraining Loss: 2.107797 \tValidation Loss: 2.325741\n",
      "Epoch: 7149 \tTraining Loss: 2.139961 \tValidation Loss: 2.325692\n",
      "Epoch: 7150 \tTraining Loss: 2.113092 \tValidation Loss: 2.325724\n",
      "Epoch: 7151 \tTraining Loss: 2.130212 \tValidation Loss: 2.325647\n",
      "Epoch: 7152 \tTraining Loss: 2.105586 \tValidation Loss: 2.325484\n",
      "Epoch: 7153 \tTraining Loss: 2.117148 \tValidation Loss: 2.325677\n",
      "Epoch: 7154 \tTraining Loss: 2.102597 \tValidation Loss: 2.325670\n",
      "Epoch: 7155 \tTraining Loss: 2.105718 \tValidation Loss: 2.325392\n",
      "Epoch: 7156 \tTraining Loss: 2.092306 \tValidation Loss: 2.325254\n",
      "Epoch: 7157 \tTraining Loss: 2.105847 \tValidation Loss: 2.325171\n",
      "Validation loss decreased (2.325231 --> 2.325171).  Saving model ...\n",
      "Epoch: 7158 \tTraining Loss: 2.113736 \tValidation Loss: 2.325366\n",
      "Epoch: 7159 \tTraining Loss: 2.108340 \tValidation Loss: 2.324967\n",
      "Validation loss decreased (2.325171 --> 2.324967).  Saving model ...\n",
      "Epoch: 7160 \tTraining Loss: 2.100439 \tValidation Loss: 2.325254\n",
      "Epoch: 7161 \tTraining Loss: 2.123005 \tValidation Loss: 2.325478\n",
      "Epoch: 7162 \tTraining Loss: 2.117019 \tValidation Loss: 2.325819\n",
      "Epoch: 7163 \tTraining Loss: 2.113094 \tValidation Loss: 2.325848\n",
      "Epoch: 7164 \tTraining Loss: 2.112946 \tValidation Loss: 2.325849\n",
      "Epoch: 7165 \tTraining Loss: 2.125488 \tValidation Loss: 2.325862\n",
      "Epoch: 7166 \tTraining Loss: 2.135448 \tValidation Loss: 2.325967\n",
      "Epoch: 7167 \tTraining Loss: 2.116580 \tValidation Loss: 2.325954\n",
      "Epoch: 7168 \tTraining Loss: 2.110195 \tValidation Loss: 2.326138\n",
      "Epoch: 7169 \tTraining Loss: 2.107427 \tValidation Loss: 2.326451\n",
      "Epoch: 7170 \tTraining Loss: 2.122910 \tValidation Loss: 2.326470\n",
      "Epoch: 7171 \tTraining Loss: 2.124853 \tValidation Loss: 2.326227\n",
      "Epoch: 7172 \tTraining Loss: 2.117785 \tValidation Loss: 2.326321\n",
      "Epoch: 7173 \tTraining Loss: 2.099054 \tValidation Loss: 2.326022\n",
      "Epoch: 7174 \tTraining Loss: 2.091795 \tValidation Loss: 2.325817\n",
      "Epoch: 7175 \tTraining Loss: 2.126018 \tValidation Loss: 2.325576\n",
      "Epoch: 7176 \tTraining Loss: 2.101959 \tValidation Loss: 2.325903\n",
      "Epoch: 7177 \tTraining Loss: 2.114636 \tValidation Loss: 2.325630\n",
      "Epoch: 7178 \tTraining Loss: 2.104312 \tValidation Loss: 2.325549\n",
      "Epoch: 7179 \tTraining Loss: 2.113668 \tValidation Loss: 2.325140\n",
      "Epoch: 7180 \tTraining Loss: 2.127953 \tValidation Loss: 2.325441\n",
      "Epoch: 7181 \tTraining Loss: 2.104208 \tValidation Loss: 2.325138\n",
      "Epoch: 7182 \tTraining Loss: 2.127961 \tValidation Loss: 2.325114\n",
      "Epoch: 7183 \tTraining Loss: 2.112603 \tValidation Loss: 2.325238\n",
      "Epoch: 7184 \tTraining Loss: 2.103448 \tValidation Loss: 2.325254\n",
      "Epoch: 7185 \tTraining Loss: 2.102288 \tValidation Loss: 2.325066\n",
      "Epoch: 7186 \tTraining Loss: 2.085141 \tValidation Loss: 2.324669\n",
      "Validation loss decreased (2.324967 --> 2.324669).  Saving model ...\n",
      "Epoch: 7187 \tTraining Loss: 2.108503 \tValidation Loss: 2.324492\n",
      "Validation loss decreased (2.324669 --> 2.324492).  Saving model ...\n",
      "Epoch: 7188 \tTraining Loss: 2.109410 \tValidation Loss: 2.324327\n",
      "Validation loss decreased (2.324492 --> 2.324327).  Saving model ...\n",
      "Epoch: 7189 \tTraining Loss: 2.124698 \tValidation Loss: 2.324353\n",
      "Epoch: 7190 \tTraining Loss: 2.102008 \tValidation Loss: 2.324384\n",
      "Epoch: 7191 \tTraining Loss: 2.095865 \tValidation Loss: 2.324366\n",
      "Epoch: 7192 \tTraining Loss: 2.112285 \tValidation Loss: 2.324430\n",
      "Epoch: 7193 \tTraining Loss: 2.075388 \tValidation Loss: 2.324406\n",
      "Epoch: 7194 \tTraining Loss: 2.130829 \tValidation Loss: 2.324528\n",
      "Epoch: 7195 \tTraining Loss: 2.108068 \tValidation Loss: 2.324644\n",
      "Epoch: 7196 \tTraining Loss: 2.122278 \tValidation Loss: 2.324724\n",
      "Epoch: 7197 \tTraining Loss: 2.088312 \tValidation Loss: 2.324989\n",
      "Epoch: 7198 \tTraining Loss: 2.102501 \tValidation Loss: 2.324457\n",
      "Epoch: 7199 \tTraining Loss: 2.104648 \tValidation Loss: 2.324528\n",
      "Epoch: 7200 \tTraining Loss: 2.095061 \tValidation Loss: 2.324589\n",
      "Epoch: 7201 \tTraining Loss: 2.119747 \tValidation Loss: 2.324747\n",
      "Epoch: 7202 \tTraining Loss: 2.099153 \tValidation Loss: 2.324733\n",
      "Epoch: 7203 \tTraining Loss: 2.087816 \tValidation Loss: 2.324873\n",
      "Epoch: 7204 \tTraining Loss: 2.116706 \tValidation Loss: 2.324923\n",
      "Epoch: 7205 \tTraining Loss: 2.138771 \tValidation Loss: 2.324917\n",
      "Epoch: 7206 \tTraining Loss: 2.124876 \tValidation Loss: 2.324712\n",
      "Epoch: 7207 \tTraining Loss: 2.088756 \tValidation Loss: 2.324626\n",
      "Epoch: 7208 \tTraining Loss: 2.122332 \tValidation Loss: 2.324459\n",
      "Epoch: 7209 \tTraining Loss: 2.113927 \tValidation Loss: 2.324498\n",
      "Epoch: 7210 \tTraining Loss: 2.115846 \tValidation Loss: 2.324355\n",
      "Epoch: 7211 \tTraining Loss: 2.111063 \tValidation Loss: 2.324296\n",
      "Validation loss decreased (2.324327 --> 2.324296).  Saving model ...\n",
      "Epoch: 7212 \tTraining Loss: 2.107076 \tValidation Loss: 2.324215\n",
      "Validation loss decreased (2.324296 --> 2.324215).  Saving model ...\n",
      "Epoch: 7213 \tTraining Loss: 2.101012 \tValidation Loss: 2.324551\n",
      "Epoch: 7214 \tTraining Loss: 2.122529 \tValidation Loss: 2.324693\n",
      "Epoch: 7215 \tTraining Loss: 2.096876 \tValidation Loss: 2.324526\n",
      "Epoch: 7216 \tTraining Loss: 2.096992 \tValidation Loss: 2.324429\n",
      "Epoch: 7217 \tTraining Loss: 2.114336 \tValidation Loss: 2.324332\n",
      "Epoch: 7218 \tTraining Loss: 2.114923 \tValidation Loss: 2.324478\n",
      "Epoch: 7219 \tTraining Loss: 2.092497 \tValidation Loss: 2.324417\n",
      "Epoch: 7220 \tTraining Loss: 2.121084 \tValidation Loss: 2.324568\n",
      "Epoch: 7221 \tTraining Loss: 2.108744 \tValidation Loss: 2.324487\n",
      "Epoch: 7222 \tTraining Loss: 2.129360 \tValidation Loss: 2.324482\n",
      "Epoch: 7223 \tTraining Loss: 2.087393 \tValidation Loss: 2.324520\n",
      "Epoch: 7224 \tTraining Loss: 2.117600 \tValidation Loss: 2.324535\n",
      "Epoch: 7225 \tTraining Loss: 2.145509 \tValidation Loss: 2.325004\n",
      "Epoch: 7226 \tTraining Loss: 2.121783 \tValidation Loss: 2.324856\n",
      "Epoch: 7227 \tTraining Loss: 2.116118 \tValidation Loss: 2.324934\n",
      "Epoch: 7228 \tTraining Loss: 2.113731 \tValidation Loss: 2.324744\n",
      "Epoch: 7229 \tTraining Loss: 2.118719 \tValidation Loss: 2.324684\n",
      "Epoch: 7230 \tTraining Loss: 2.106710 \tValidation Loss: 2.324332\n",
      "Epoch: 7231 \tTraining Loss: 2.094564 \tValidation Loss: 2.324208\n",
      "Validation loss decreased (2.324215 --> 2.324208).  Saving model ...\n",
      "Epoch: 7232 \tTraining Loss: 2.108784 \tValidation Loss: 2.324105\n",
      "Validation loss decreased (2.324208 --> 2.324105).  Saving model ...\n",
      "Epoch: 7233 \tTraining Loss: 2.085351 \tValidation Loss: 2.323998\n",
      "Validation loss decreased (2.324105 --> 2.323998).  Saving model ...\n",
      "Epoch: 7234 \tTraining Loss: 2.120049 \tValidation Loss: 2.324208\n",
      "Epoch: 7235 \tTraining Loss: 2.110439 \tValidation Loss: 2.324236\n",
      "Epoch: 7236 \tTraining Loss: 2.122083 \tValidation Loss: 2.324511\n",
      "Epoch: 7237 \tTraining Loss: 2.114160 \tValidation Loss: 2.324415\n",
      "Epoch: 7238 \tTraining Loss: 2.103108 \tValidation Loss: 2.324581\n",
      "Epoch: 7239 \tTraining Loss: 2.148994 \tValidation Loss: 2.324645\n",
      "Epoch: 7240 \tTraining Loss: 2.098665 \tValidation Loss: 2.324448\n",
      "Epoch: 7241 \tTraining Loss: 2.115947 \tValidation Loss: 2.324647\n",
      "Epoch: 7242 \tTraining Loss: 2.070691 \tValidation Loss: 2.324695\n",
      "Epoch: 7243 \tTraining Loss: 2.122335 \tValidation Loss: 2.324566\n",
      "Epoch: 7244 \tTraining Loss: 2.119203 \tValidation Loss: 2.324583\n",
      "Epoch: 7245 \tTraining Loss: 2.095551 \tValidation Loss: 2.324527\n",
      "Epoch: 7246 \tTraining Loss: 2.116072 \tValidation Loss: 2.324493\n",
      "Epoch: 7247 \tTraining Loss: 2.093706 \tValidation Loss: 2.324465\n",
      "Epoch: 7248 \tTraining Loss: 2.110053 \tValidation Loss: 2.324366\n",
      "Epoch: 7249 \tTraining Loss: 2.092346 \tValidation Loss: 2.324542\n",
      "Epoch: 7250 \tTraining Loss: 2.123714 \tValidation Loss: 2.324783\n",
      "Epoch: 7251 \tTraining Loss: 2.109178 \tValidation Loss: 2.324834\n",
      "Epoch: 7252 \tTraining Loss: 2.124356 \tValidation Loss: 2.324802\n",
      "Epoch: 7253 \tTraining Loss: 2.114079 \tValidation Loss: 2.324573\n",
      "Epoch: 7254 \tTraining Loss: 2.114191 \tValidation Loss: 2.324604\n",
      "Epoch: 7255 \tTraining Loss: 2.103393 \tValidation Loss: 2.324655\n",
      "Epoch: 7256 \tTraining Loss: 2.103894 \tValidation Loss: 2.324771\n",
      "Epoch: 7257 \tTraining Loss: 2.105415 \tValidation Loss: 2.324946\n",
      "Epoch: 7258 \tTraining Loss: 2.108060 \tValidation Loss: 2.324869\n",
      "Epoch: 7259 \tTraining Loss: 2.097945 \tValidation Loss: 2.324985\n",
      "Epoch: 7260 \tTraining Loss: 2.069552 \tValidation Loss: 2.324856\n",
      "Epoch: 7261 \tTraining Loss: 2.132139 \tValidation Loss: 2.324760\n",
      "Epoch: 7262 \tTraining Loss: 2.093045 \tValidation Loss: 2.324781\n",
      "Epoch: 7263 \tTraining Loss: 2.117265 \tValidation Loss: 2.325036\n",
      "Epoch: 7264 \tTraining Loss: 2.107149 \tValidation Loss: 2.324641\n",
      "Epoch: 7265 \tTraining Loss: 2.116954 \tValidation Loss: 2.324881\n",
      "Epoch: 7266 \tTraining Loss: 2.092645 \tValidation Loss: 2.324672\n",
      "Epoch: 7267 \tTraining Loss: 2.115199 \tValidation Loss: 2.324764\n",
      "Epoch: 7268 \tTraining Loss: 2.097046 \tValidation Loss: 2.324708\n",
      "Epoch: 7269 \tTraining Loss: 2.100405 \tValidation Loss: 2.324713\n",
      "Epoch: 7270 \tTraining Loss: 2.091636 \tValidation Loss: 2.324572\n",
      "Epoch: 7271 \tTraining Loss: 2.101146 \tValidation Loss: 2.324417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7272 \tTraining Loss: 2.084476 \tValidation Loss: 2.324219\n",
      "Epoch: 7273 \tTraining Loss: 2.089912 \tValidation Loss: 2.323865\n",
      "Validation loss decreased (2.323998 --> 2.323865).  Saving model ...\n",
      "Epoch: 7274 \tTraining Loss: 2.101925 \tValidation Loss: 2.324021\n",
      "Epoch: 7275 \tTraining Loss: 2.101313 \tValidation Loss: 2.323857\n",
      "Validation loss decreased (2.323865 --> 2.323857).  Saving model ...\n",
      "Epoch: 7276 \tTraining Loss: 2.100348 \tValidation Loss: 2.323455\n",
      "Validation loss decreased (2.323857 --> 2.323455).  Saving model ...\n",
      "Epoch: 7277 \tTraining Loss: 2.087646 \tValidation Loss: 2.323754\n",
      "Epoch: 7278 \tTraining Loss: 2.104474 \tValidation Loss: 2.323513\n",
      "Epoch: 7279 \tTraining Loss: 2.086497 \tValidation Loss: 2.323768\n",
      "Epoch: 7280 \tTraining Loss: 2.099044 \tValidation Loss: 2.324017\n",
      "Epoch: 7281 \tTraining Loss: 2.098079 \tValidation Loss: 2.324008\n",
      "Epoch: 7282 \tTraining Loss: 2.090078 \tValidation Loss: 2.324126\n",
      "Epoch: 7283 \tTraining Loss: 2.090497 \tValidation Loss: 2.324039\n",
      "Epoch: 7284 \tTraining Loss: 2.099511 \tValidation Loss: 2.323997\n",
      "Epoch: 7285 \tTraining Loss: 2.104299 \tValidation Loss: 2.323965\n",
      "Epoch: 7286 \tTraining Loss: 2.108470 \tValidation Loss: 2.323920\n",
      "Epoch: 7287 \tTraining Loss: 2.123844 \tValidation Loss: 2.323596\n",
      "Epoch: 7288 \tTraining Loss: 2.102925 \tValidation Loss: 2.323644\n",
      "Epoch: 7289 \tTraining Loss: 2.111434 \tValidation Loss: 2.323620\n",
      "Epoch: 7290 \tTraining Loss: 2.140351 \tValidation Loss: 2.323552\n",
      "Epoch: 7291 \tTraining Loss: 2.100898 \tValidation Loss: 2.323956\n",
      "Epoch: 7292 \tTraining Loss: 2.102540 \tValidation Loss: 2.324200\n",
      "Epoch: 7293 \tTraining Loss: 2.121710 \tValidation Loss: 2.324274\n",
      "Epoch: 7294 \tTraining Loss: 2.119612 \tValidation Loss: 2.324169\n",
      "Epoch: 7295 \tTraining Loss: 2.116064 \tValidation Loss: 2.324059\n",
      "Epoch: 7296 \tTraining Loss: 2.109911 \tValidation Loss: 2.324122\n",
      "Epoch: 7297 \tTraining Loss: 2.120534 \tValidation Loss: 2.324085\n",
      "Epoch: 7298 \tTraining Loss: 2.103782 \tValidation Loss: 2.324018\n",
      "Epoch: 7299 \tTraining Loss: 2.084656 \tValidation Loss: 2.324244\n",
      "Epoch: 7300 \tTraining Loss: 2.119984 \tValidation Loss: 2.324230\n",
      "Epoch: 7301 \tTraining Loss: 2.092780 \tValidation Loss: 2.324216\n",
      "Epoch: 7302 \tTraining Loss: 2.101362 \tValidation Loss: 2.323952\n",
      "Epoch: 7303 \tTraining Loss: 2.098621 \tValidation Loss: 2.323935\n",
      "Epoch: 7304 \tTraining Loss: 2.128631 \tValidation Loss: 2.323736\n",
      "Epoch: 7305 \tTraining Loss: 2.092369 \tValidation Loss: 2.323314\n",
      "Validation loss decreased (2.323455 --> 2.323314).  Saving model ...\n",
      "Epoch: 7306 \tTraining Loss: 2.095031 \tValidation Loss: 2.323251\n",
      "Validation loss decreased (2.323314 --> 2.323251).  Saving model ...\n",
      "Epoch: 7307 \tTraining Loss: 2.092303 \tValidation Loss: 2.323487\n",
      "Epoch: 7308 \tTraining Loss: 2.093187 \tValidation Loss: 2.323256\n",
      "Epoch: 7309 \tTraining Loss: 2.087177 \tValidation Loss: 2.323180\n",
      "Validation loss decreased (2.323251 --> 2.323180).  Saving model ...\n",
      "Epoch: 7310 \tTraining Loss: 2.102249 \tValidation Loss: 2.323105\n",
      "Validation loss decreased (2.323180 --> 2.323105).  Saving model ...\n",
      "Epoch: 7311 \tTraining Loss: 2.099285 \tValidation Loss: 2.323244\n",
      "Epoch: 7312 \tTraining Loss: 2.120553 \tValidation Loss: 2.323397\n",
      "Epoch: 7313 \tTraining Loss: 2.089496 \tValidation Loss: 2.323426\n",
      "Epoch: 7314 \tTraining Loss: 2.093312 \tValidation Loss: 2.323676\n",
      "Epoch: 7315 \tTraining Loss: 2.099184 \tValidation Loss: 2.323862\n",
      "Epoch: 7316 \tTraining Loss: 2.101938 \tValidation Loss: 2.324068\n",
      "Epoch: 7317 \tTraining Loss: 2.089511 \tValidation Loss: 2.324060\n",
      "Epoch: 7318 \tTraining Loss: 2.128640 \tValidation Loss: 2.324213\n",
      "Epoch: 7319 \tTraining Loss: 2.112128 \tValidation Loss: 2.324411\n",
      "Epoch: 7320 \tTraining Loss: 2.109236 \tValidation Loss: 2.324165\n",
      "Epoch: 7321 \tTraining Loss: 2.106968 \tValidation Loss: 2.324008\n",
      "Epoch: 7322 \tTraining Loss: 2.117360 \tValidation Loss: 2.324030\n",
      "Epoch: 7323 \tTraining Loss: 2.125589 \tValidation Loss: 2.324248\n",
      "Epoch: 7324 \tTraining Loss: 2.071280 \tValidation Loss: 2.324261\n",
      "Epoch: 7325 \tTraining Loss: 2.106134 \tValidation Loss: 2.324208\n",
      "Epoch: 7326 \tTraining Loss: 2.115766 \tValidation Loss: 2.324388\n",
      "Epoch: 7327 \tTraining Loss: 2.100363 \tValidation Loss: 2.324360\n",
      "Epoch: 7328 \tTraining Loss: 2.098562 \tValidation Loss: 2.324209\n",
      "Epoch: 7329 \tTraining Loss: 2.111506 \tValidation Loss: 2.323752\n",
      "Epoch: 7330 \tTraining Loss: 2.103770 \tValidation Loss: 2.323759\n",
      "Epoch: 7331 \tTraining Loss: 2.101600 \tValidation Loss: 2.323681\n",
      "Epoch: 7332 \tTraining Loss: 2.101227 \tValidation Loss: 2.323553\n",
      "Epoch: 7333 \tTraining Loss: 2.106511 \tValidation Loss: 2.323836\n",
      "Epoch: 7334 \tTraining Loss: 2.100613 \tValidation Loss: 2.323728\n",
      "Epoch: 7335 \tTraining Loss: 2.101225 \tValidation Loss: 2.323752\n",
      "Epoch: 7336 \tTraining Loss: 2.118303 \tValidation Loss: 2.323686\n",
      "Epoch: 7337 \tTraining Loss: 2.092067 \tValidation Loss: 2.323697\n",
      "Epoch: 7338 \tTraining Loss: 2.132268 \tValidation Loss: 2.323371\n",
      "Epoch: 7339 \tTraining Loss: 2.105893 \tValidation Loss: 2.323720\n",
      "Epoch: 7340 \tTraining Loss: 2.117085 \tValidation Loss: 2.323742\n",
      "Epoch: 7341 \tTraining Loss: 2.092861 \tValidation Loss: 2.323647\n",
      "Epoch: 7342 \tTraining Loss: 2.098463 \tValidation Loss: 2.323429\n",
      "Epoch: 7343 \tTraining Loss: 2.100873 \tValidation Loss: 2.323351\n",
      "Epoch: 7344 \tTraining Loss: 2.132076 \tValidation Loss: 2.323348\n",
      "Epoch: 7345 \tTraining Loss: 2.120881 \tValidation Loss: 2.323478\n",
      "Epoch: 7346 \tTraining Loss: 2.108778 \tValidation Loss: 2.323491\n",
      "Epoch: 7347 \tTraining Loss: 2.101258 \tValidation Loss: 2.323227\n",
      "Epoch: 7348 \tTraining Loss: 2.103002 \tValidation Loss: 2.322931\n",
      "Validation loss decreased (2.323105 --> 2.322931).  Saving model ...\n",
      "Epoch: 7349 \tTraining Loss: 2.097407 \tValidation Loss: 2.322955\n",
      "Epoch: 7350 \tTraining Loss: 2.088367 \tValidation Loss: 2.322982\n",
      "Epoch: 7351 \tTraining Loss: 2.101905 \tValidation Loss: 2.322996\n",
      "Epoch: 7352 \tTraining Loss: 2.101256 \tValidation Loss: 2.323160\n",
      "Epoch: 7353 \tTraining Loss: 2.111525 \tValidation Loss: 2.323204\n",
      "Epoch: 7354 \tTraining Loss: 2.128214 \tValidation Loss: 2.323332\n",
      "Epoch: 7355 \tTraining Loss: 2.097156 \tValidation Loss: 2.323369\n",
      "Epoch: 7356 \tTraining Loss: 2.113963 \tValidation Loss: 2.323119\n",
      "Epoch: 7357 \tTraining Loss: 2.096921 \tValidation Loss: 2.323275\n",
      "Epoch: 7358 \tTraining Loss: 2.095925 \tValidation Loss: 2.323420\n",
      "Epoch: 7359 \tTraining Loss: 2.086025 \tValidation Loss: 2.323646\n",
      "Epoch: 7360 \tTraining Loss: 2.101793 \tValidation Loss: 2.323488\n",
      "Epoch: 7361 \tTraining Loss: 2.088775 \tValidation Loss: 2.323542\n",
      "Epoch: 7362 \tTraining Loss: 2.095702 \tValidation Loss: 2.323508\n",
      "Epoch: 7363 \tTraining Loss: 2.077527 \tValidation Loss: 2.323620\n",
      "Epoch: 7364 \tTraining Loss: 2.122767 \tValidation Loss: 2.323282\n",
      "Epoch: 7365 \tTraining Loss: 2.103623 \tValidation Loss: 2.323180\n",
      "Epoch: 7366 \tTraining Loss: 2.090621 \tValidation Loss: 2.323052\n",
      "Epoch: 7367 \tTraining Loss: 2.095356 \tValidation Loss: 2.323176\n",
      "Epoch: 7368 \tTraining Loss: 2.109883 \tValidation Loss: 2.323410\n",
      "Epoch: 7369 \tTraining Loss: 2.088449 \tValidation Loss: 2.323205\n",
      "Epoch: 7370 \tTraining Loss: 2.101232 \tValidation Loss: 2.323146\n",
      "Epoch: 7371 \tTraining Loss: 2.075258 \tValidation Loss: 2.323187\n",
      "Epoch: 7372 \tTraining Loss: 2.101171 \tValidation Loss: 2.323164\n",
      "Epoch: 7373 \tTraining Loss: 2.090074 \tValidation Loss: 2.323498\n",
      "Epoch: 7374 \tTraining Loss: 2.089453 \tValidation Loss: 2.323164\n",
      "Epoch: 7375 \tTraining Loss: 2.114095 \tValidation Loss: 2.322948\n",
      "Epoch: 7376 \tTraining Loss: 2.101372 \tValidation Loss: 2.322938\n",
      "Epoch: 7377 \tTraining Loss: 2.104374 \tValidation Loss: 2.322795\n",
      "Validation loss decreased (2.322931 --> 2.322795).  Saving model ...\n",
      "Epoch: 7378 \tTraining Loss: 2.115495 \tValidation Loss: 2.322976\n",
      "Epoch: 7379 \tTraining Loss: 2.079572 \tValidation Loss: 2.323022\n",
      "Epoch: 7380 \tTraining Loss: 2.098645 \tValidation Loss: 2.322958\n",
      "Epoch: 7381 \tTraining Loss: 2.116829 \tValidation Loss: 2.323147\n",
      "Epoch: 7382 \tTraining Loss: 2.085050 \tValidation Loss: 2.323072\n",
      "Epoch: 7383 \tTraining Loss: 2.103695 \tValidation Loss: 2.322940\n",
      "Epoch: 7384 \tTraining Loss: 2.110962 \tValidation Loss: 2.323284\n",
      "Epoch: 7385 \tTraining Loss: 2.099181 \tValidation Loss: 2.322969\n",
      "Epoch: 7386 \tTraining Loss: 2.118366 \tValidation Loss: 2.323062\n",
      "Epoch: 7387 \tTraining Loss: 2.090376 \tValidation Loss: 2.323199\n",
      "Epoch: 7388 \tTraining Loss: 2.096464 \tValidation Loss: 2.323465\n",
      "Epoch: 7389 \tTraining Loss: 2.101888 \tValidation Loss: 2.323449\n",
      "Epoch: 7390 \tTraining Loss: 2.083898 \tValidation Loss: 2.323319\n",
      "Epoch: 7391 \tTraining Loss: 2.096009 \tValidation Loss: 2.323281\n",
      "Epoch: 7392 \tTraining Loss: 2.115218 \tValidation Loss: 2.323056\n",
      "Epoch: 7393 \tTraining Loss: 2.105926 \tValidation Loss: 2.323148\n",
      "Epoch: 7394 \tTraining Loss: 2.114189 \tValidation Loss: 2.322624\n",
      "Validation loss decreased (2.322795 --> 2.322624).  Saving model ...\n",
      "Epoch: 7395 \tTraining Loss: 2.142205 \tValidation Loss: 2.322809\n",
      "Epoch: 7396 \tTraining Loss: 2.091910 \tValidation Loss: 2.323106\n",
      "Epoch: 7397 \tTraining Loss: 2.096219 \tValidation Loss: 2.323123\n",
      "Epoch: 7398 \tTraining Loss: 2.081500 \tValidation Loss: 2.322970\n",
      "Epoch: 7399 \tTraining Loss: 2.117639 \tValidation Loss: 2.323338\n",
      "Epoch: 7400 \tTraining Loss: 2.101113 \tValidation Loss: 2.323395\n",
      "Epoch: 7401 \tTraining Loss: 2.095344 \tValidation Loss: 2.323185\n",
      "Epoch: 7402 \tTraining Loss: 2.120846 \tValidation Loss: 2.323215\n",
      "Epoch: 7403 \tTraining Loss: 2.084598 \tValidation Loss: 2.322981\n",
      "Epoch: 7404 \tTraining Loss: 2.074653 \tValidation Loss: 2.322687\n",
      "Epoch: 7405 \tTraining Loss: 2.114177 \tValidation Loss: 2.322901\n",
      "Epoch: 7406 \tTraining Loss: 2.114050 \tValidation Loss: 2.322726\n",
      "Epoch: 7407 \tTraining Loss: 2.097618 \tValidation Loss: 2.323120\n",
      "Epoch: 7408 \tTraining Loss: 2.071587 \tValidation Loss: 2.322999\n",
      "Epoch: 7409 \tTraining Loss: 2.107173 \tValidation Loss: 2.322940\n",
      "Epoch: 7410 \tTraining Loss: 2.105507 \tValidation Loss: 2.322998\n",
      "Epoch: 7411 \tTraining Loss: 2.094572 \tValidation Loss: 2.323058\n",
      "Epoch: 7412 \tTraining Loss: 2.089589 \tValidation Loss: 2.322786\n",
      "Epoch: 7413 \tTraining Loss: 2.112253 \tValidation Loss: 2.322812\n",
      "Epoch: 7414 \tTraining Loss: 2.103058 \tValidation Loss: 2.322858\n",
      "Epoch: 7415 \tTraining Loss: 2.089291 \tValidation Loss: 2.322589\n",
      "Validation loss decreased (2.322624 --> 2.322589).  Saving model ...\n",
      "Epoch: 7416 \tTraining Loss: 2.091270 \tValidation Loss: 2.322508\n",
      "Validation loss decreased (2.322589 --> 2.322508).  Saving model ...\n",
      "Epoch: 7417 \tTraining Loss: 2.118547 \tValidation Loss: 2.322689\n",
      "Epoch: 7418 \tTraining Loss: 2.072242 \tValidation Loss: 2.322419\n",
      "Validation loss decreased (2.322508 --> 2.322419).  Saving model ...\n",
      "Epoch: 7419 \tTraining Loss: 2.112293 \tValidation Loss: 2.322435\n",
      "Epoch: 7420 \tTraining Loss: 2.076202 \tValidation Loss: 2.322433\n",
      "Epoch: 7421 \tTraining Loss: 2.105936 \tValidation Loss: 2.322783\n",
      "Epoch: 7422 \tTraining Loss: 2.130330 \tValidation Loss: 2.322883\n",
      "Epoch: 7423 \tTraining Loss: 2.080174 \tValidation Loss: 2.322734\n",
      "Epoch: 7424 \tTraining Loss: 2.106718 \tValidation Loss: 2.322581\n",
      "Epoch: 7425 \tTraining Loss: 2.099629 \tValidation Loss: 2.322796\n",
      "Epoch: 7426 \tTraining Loss: 2.098264 \tValidation Loss: 2.322649\n",
      "Epoch: 7427 \tTraining Loss: 2.101930 \tValidation Loss: 2.322178\n",
      "Validation loss decreased (2.322419 --> 2.322178).  Saving model ...\n",
      "Epoch: 7428 \tTraining Loss: 2.105757 \tValidation Loss: 2.322543\n",
      "Epoch: 7429 \tTraining Loss: 2.084387 \tValidation Loss: 2.322345\n",
      "Epoch: 7430 \tTraining Loss: 2.106280 \tValidation Loss: 2.322161\n",
      "Validation loss decreased (2.322178 --> 2.322161).  Saving model ...\n",
      "Epoch: 7431 \tTraining Loss: 2.094641 \tValidation Loss: 2.322237\n",
      "Epoch: 7432 \tTraining Loss: 2.106485 \tValidation Loss: 2.322577\n",
      "Epoch: 7433 \tTraining Loss: 2.094852 \tValidation Loss: 2.322775\n",
      "Epoch: 7434 \tTraining Loss: 2.098491 \tValidation Loss: 2.322491\n",
      "Epoch: 7435 \tTraining Loss: 2.110090 \tValidation Loss: 2.322420\n",
      "Epoch: 7436 \tTraining Loss: 2.063103 \tValidation Loss: 2.322322\n",
      "Epoch: 7437 \tTraining Loss: 2.089175 \tValidation Loss: 2.322386\n",
      "Epoch: 7438 \tTraining Loss: 2.089101 \tValidation Loss: 2.322520\n",
      "Epoch: 7439 \tTraining Loss: 2.106178 \tValidation Loss: 2.321946\n",
      "Validation loss decreased (2.322161 --> 2.321946).  Saving model ...\n",
      "Epoch: 7440 \tTraining Loss: 2.108320 \tValidation Loss: 2.321865\n",
      "Validation loss decreased (2.321946 --> 2.321865).  Saving model ...\n",
      "Epoch: 7441 \tTraining Loss: 2.091403 \tValidation Loss: 2.321727\n",
      "Validation loss decreased (2.321865 --> 2.321727).  Saving model ...\n",
      "Epoch: 7442 \tTraining Loss: 2.120677 \tValidation Loss: 2.321533\n",
      "Validation loss decreased (2.321727 --> 2.321533).  Saving model ...\n",
      "Epoch: 7443 \tTraining Loss: 2.114184 \tValidation Loss: 2.321342\n",
      "Validation loss decreased (2.321533 --> 2.321342).  Saving model ...\n",
      "Epoch: 7444 \tTraining Loss: 2.084377 \tValidation Loss: 2.321534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7445 \tTraining Loss: 2.109530 \tValidation Loss: 2.321565\n",
      "Epoch: 7446 \tTraining Loss: 2.100889 \tValidation Loss: 2.321696\n",
      "Epoch: 7447 \tTraining Loss: 2.101302 \tValidation Loss: 2.321684\n",
      "Epoch: 7448 \tTraining Loss: 2.115258 \tValidation Loss: 2.321807\n",
      "Epoch: 7449 \tTraining Loss: 2.100997 \tValidation Loss: 2.321753\n",
      "Epoch: 7450 \tTraining Loss: 2.067050 \tValidation Loss: 2.321689\n",
      "Epoch: 7451 \tTraining Loss: 2.090999 \tValidation Loss: 2.321851\n",
      "Epoch: 7452 \tTraining Loss: 2.087993 \tValidation Loss: 2.321934\n",
      "Epoch: 7453 \tTraining Loss: 2.115796 \tValidation Loss: 2.322278\n",
      "Epoch: 7454 \tTraining Loss: 2.085356 \tValidation Loss: 2.322405\n",
      "Epoch: 7455 \tTraining Loss: 2.102705 \tValidation Loss: 2.322235\n",
      "Epoch: 7456 \tTraining Loss: 2.071248 \tValidation Loss: 2.322060\n",
      "Epoch: 7457 \tTraining Loss: 2.104155 \tValidation Loss: 2.321800\n",
      "Epoch: 7458 \tTraining Loss: 2.104588 \tValidation Loss: 2.321964\n",
      "Epoch: 7459 \tTraining Loss: 2.080271 \tValidation Loss: 2.321673\n",
      "Epoch: 7460 \tTraining Loss: 2.096205 \tValidation Loss: 2.321733\n",
      "Epoch: 7461 \tTraining Loss: 2.075341 \tValidation Loss: 2.321891\n",
      "Epoch: 7462 \tTraining Loss: 2.077493 \tValidation Loss: 2.321679\n",
      "Epoch: 7463 \tTraining Loss: 2.089283 \tValidation Loss: 2.321439\n",
      "Epoch: 7464 \tTraining Loss: 2.105494 \tValidation Loss: 2.321606\n",
      "Epoch: 7465 \tTraining Loss: 2.095024 \tValidation Loss: 2.321774\n",
      "Epoch: 7466 \tTraining Loss: 2.090741 \tValidation Loss: 2.321780\n",
      "Epoch: 7467 \tTraining Loss: 2.113867 \tValidation Loss: 2.321977\n",
      "Epoch: 7468 \tTraining Loss: 2.082158 \tValidation Loss: 2.321884\n",
      "Epoch: 7469 \tTraining Loss: 2.079064 \tValidation Loss: 2.321909\n",
      "Epoch: 7470 \tTraining Loss: 2.087727 \tValidation Loss: 2.321650\n",
      "Epoch: 7471 \tTraining Loss: 2.094379 \tValidation Loss: 2.321645\n",
      "Epoch: 7472 \tTraining Loss: 2.091699 \tValidation Loss: 2.321607\n",
      "Epoch: 7473 \tTraining Loss: 2.079684 \tValidation Loss: 2.321726\n",
      "Epoch: 7474 \tTraining Loss: 2.077625 \tValidation Loss: 2.321878\n",
      "Epoch: 7475 \tTraining Loss: 2.087644 \tValidation Loss: 2.321922\n",
      "Epoch: 7476 \tTraining Loss: 2.090805 \tValidation Loss: 2.321781\n",
      "Epoch: 7477 \tTraining Loss: 2.112534 \tValidation Loss: 2.321865\n",
      "Epoch: 7478 \tTraining Loss: 2.075962 \tValidation Loss: 2.321897\n",
      "Epoch: 7479 \tTraining Loss: 2.082702 \tValidation Loss: 2.321788\n",
      "Epoch: 7480 \tTraining Loss: 2.092540 \tValidation Loss: 2.321774\n",
      "Epoch: 7481 \tTraining Loss: 2.097508 \tValidation Loss: 2.321611\n",
      "Epoch: 7482 \tTraining Loss: 2.105261 \tValidation Loss: 2.321607\n",
      "Epoch: 7483 \tTraining Loss: 2.109736 \tValidation Loss: 2.321568\n",
      "Epoch: 7484 \tTraining Loss: 2.075057 \tValidation Loss: 2.321815\n",
      "Epoch: 7485 \tTraining Loss: 2.109437 \tValidation Loss: 2.322058\n",
      "Epoch: 7486 \tTraining Loss: 2.074351 \tValidation Loss: 2.321912\n",
      "Epoch: 7487 \tTraining Loss: 2.105316 \tValidation Loss: 2.322148\n",
      "Epoch: 7488 \tTraining Loss: 2.088361 \tValidation Loss: 2.322096\n",
      "Epoch: 7489 \tTraining Loss: 2.087575 \tValidation Loss: 2.321972\n",
      "Epoch: 7490 \tTraining Loss: 2.092068 \tValidation Loss: 2.322139\n",
      "Epoch: 7491 \tTraining Loss: 2.106788 \tValidation Loss: 2.322368\n",
      "Epoch: 7492 \tTraining Loss: 2.105096 \tValidation Loss: 2.322647\n",
      "Epoch: 7493 \tTraining Loss: 2.081898 \tValidation Loss: 2.322527\n",
      "Epoch: 7494 \tTraining Loss: 2.089495 \tValidation Loss: 2.322211\n",
      "Epoch: 7495 \tTraining Loss: 2.086839 \tValidation Loss: 2.322170\n",
      "Epoch: 7496 \tTraining Loss: 2.099112 \tValidation Loss: 2.322231\n",
      "Epoch: 7497 \tTraining Loss: 2.073312 \tValidation Loss: 2.322264\n",
      "Epoch: 7498 \tTraining Loss: 2.082227 \tValidation Loss: 2.322089\n",
      "Epoch: 7499 \tTraining Loss: 2.100488 \tValidation Loss: 2.321875\n",
      "Epoch: 7500 \tTraining Loss: 2.099720 \tValidation Loss: 2.321625\n",
      "Epoch: 7501 \tTraining Loss: 2.092630 \tValidation Loss: 2.321451\n",
      "Epoch: 7502 \tTraining Loss: 2.086030 \tValidation Loss: 2.321409\n",
      "Epoch: 7503 \tTraining Loss: 2.078875 \tValidation Loss: 2.321521\n",
      "Epoch: 7504 \tTraining Loss: 2.081367 \tValidation Loss: 2.321275\n",
      "Validation loss decreased (2.321342 --> 2.321275).  Saving model ...\n",
      "Epoch: 7505 \tTraining Loss: 2.079854 \tValidation Loss: 2.321063\n",
      "Validation loss decreased (2.321275 --> 2.321063).  Saving model ...\n",
      "Epoch: 7506 \tTraining Loss: 2.100929 \tValidation Loss: 2.320985\n",
      "Validation loss decreased (2.321063 --> 2.320985).  Saving model ...\n",
      "Epoch: 7507 \tTraining Loss: 2.091846 \tValidation Loss: 2.320990\n",
      "Epoch: 7508 \tTraining Loss: 2.089728 \tValidation Loss: 2.321232\n",
      "Epoch: 7509 \tTraining Loss: 2.099354 \tValidation Loss: 2.321214\n",
      "Epoch: 7510 \tTraining Loss: 2.111799 \tValidation Loss: 2.321346\n",
      "Epoch: 7511 \tTraining Loss: 2.092479 \tValidation Loss: 2.321437\n",
      "Epoch: 7512 \tTraining Loss: 2.094904 \tValidation Loss: 2.321383\n",
      "Epoch: 7513 \tTraining Loss: 2.096366 \tValidation Loss: 2.321239\n",
      "Epoch: 7514 \tTraining Loss: 2.108765 \tValidation Loss: 2.320962\n",
      "Validation loss decreased (2.320985 --> 2.320962).  Saving model ...\n",
      "Epoch: 7515 \tTraining Loss: 2.101533 \tValidation Loss: 2.321316\n",
      "Epoch: 7516 \tTraining Loss: 2.113848 \tValidation Loss: 2.321160\n",
      "Epoch: 7517 \tTraining Loss: 2.069204 \tValidation Loss: 2.321138\n",
      "Epoch: 7518 \tTraining Loss: 2.117224 \tValidation Loss: 2.321163\n",
      "Epoch: 7519 \tTraining Loss: 2.111438 \tValidation Loss: 2.321264\n",
      "Epoch: 7520 \tTraining Loss: 2.081043 \tValidation Loss: 2.321246\n",
      "Epoch: 7521 \tTraining Loss: 2.086940 \tValidation Loss: 2.321331\n",
      "Epoch: 7522 \tTraining Loss: 2.080315 \tValidation Loss: 2.321233\n",
      "Epoch: 7523 \tTraining Loss: 2.061997 \tValidation Loss: 2.321178\n",
      "Epoch: 7524 \tTraining Loss: 2.108467 \tValidation Loss: 2.320873\n",
      "Validation loss decreased (2.320962 --> 2.320873).  Saving model ...\n",
      "Epoch: 7525 \tTraining Loss: 2.104684 \tValidation Loss: 2.321071\n",
      "Epoch: 7526 \tTraining Loss: 2.065700 \tValidation Loss: 2.320907\n",
      "Epoch: 7527 \tTraining Loss: 2.116892 \tValidation Loss: 2.321227\n",
      "Epoch: 7528 \tTraining Loss: 2.106336 \tValidation Loss: 2.321260\n",
      "Epoch: 7529 \tTraining Loss: 2.084779 \tValidation Loss: 2.321166\n",
      "Epoch: 7530 \tTraining Loss: 2.076960 \tValidation Loss: 2.321177\n",
      "Epoch: 7531 \tTraining Loss: 2.092824 \tValidation Loss: 2.321110\n",
      "Epoch: 7532 \tTraining Loss: 2.062993 \tValidation Loss: 2.320656\n",
      "Validation loss decreased (2.320873 --> 2.320656).  Saving model ...\n",
      "Epoch: 7533 \tTraining Loss: 2.085506 \tValidation Loss: 2.320813\n",
      "Epoch: 7534 \tTraining Loss: 2.081342 \tValidation Loss: 2.321004\n",
      "Epoch: 7535 \tTraining Loss: 2.105191 \tValidation Loss: 2.320954\n",
      "Epoch: 7536 \tTraining Loss: 2.088690 \tValidation Loss: 2.320683\n",
      "Epoch: 7537 \tTraining Loss: 2.112113 \tValidation Loss: 2.320854\n",
      "Epoch: 7538 \tTraining Loss: 2.112403 \tValidation Loss: 2.321058\n",
      "Epoch: 7539 \tTraining Loss: 2.102692 \tValidation Loss: 2.321036\n",
      "Epoch: 7540 \tTraining Loss: 2.105791 \tValidation Loss: 2.320899\n",
      "Epoch: 7541 \tTraining Loss: 2.069182 \tValidation Loss: 2.320626\n",
      "Validation loss decreased (2.320656 --> 2.320626).  Saving model ...\n",
      "Epoch: 7542 \tTraining Loss: 2.120326 \tValidation Loss: 2.321004\n",
      "Epoch: 7543 \tTraining Loss: 2.101028 \tValidation Loss: 2.320807\n",
      "Epoch: 7544 \tTraining Loss: 2.092098 \tValidation Loss: 2.320713\n",
      "Epoch: 7545 \tTraining Loss: 2.107001 \tValidation Loss: 2.320732\n",
      "Epoch: 7546 \tTraining Loss: 2.091043 \tValidation Loss: 2.320486\n",
      "Validation loss decreased (2.320626 --> 2.320486).  Saving model ...\n",
      "Epoch: 7547 \tTraining Loss: 2.098259 \tValidation Loss: 2.320479\n",
      "Validation loss decreased (2.320486 --> 2.320479).  Saving model ...\n",
      "Epoch: 7548 \tTraining Loss: 2.093325 \tValidation Loss: 2.320377\n",
      "Validation loss decreased (2.320479 --> 2.320377).  Saving model ...\n",
      "Epoch: 7549 \tTraining Loss: 2.098947 \tValidation Loss: 2.320694\n",
      "Epoch: 7550 \tTraining Loss: 2.092516 \tValidation Loss: 2.320727\n",
      "Epoch: 7551 \tTraining Loss: 2.110178 \tValidation Loss: 2.320488\n",
      "Epoch: 7552 \tTraining Loss: 2.108195 \tValidation Loss: 2.320286\n",
      "Validation loss decreased (2.320377 --> 2.320286).  Saving model ...\n",
      "Epoch: 7553 \tTraining Loss: 2.077481 \tValidation Loss: 2.320474\n",
      "Epoch: 7554 \tTraining Loss: 2.106047 \tValidation Loss: 2.320440\n",
      "Epoch: 7555 \tTraining Loss: 2.061255 \tValidation Loss: 2.320551\n",
      "Epoch: 7556 \tTraining Loss: 2.090672 \tValidation Loss: 2.320466\n",
      "Epoch: 7557 \tTraining Loss: 2.090617 \tValidation Loss: 2.320686\n",
      "Epoch: 7558 \tTraining Loss: 2.102818 \tValidation Loss: 2.320849\n",
      "Epoch: 7559 \tTraining Loss: 2.112179 \tValidation Loss: 2.320989\n",
      "Epoch: 7560 \tTraining Loss: 2.096415 \tValidation Loss: 2.320508\n",
      "Epoch: 7561 \tTraining Loss: 2.100440 \tValidation Loss: 2.320446\n",
      "Epoch: 7562 \tTraining Loss: 2.098402 \tValidation Loss: 2.320407\n",
      "Epoch: 7563 \tTraining Loss: 2.109286 \tValidation Loss: 2.320640\n",
      "Epoch: 7564 \tTraining Loss: 2.092203 \tValidation Loss: 2.320817\n",
      "Epoch: 7565 \tTraining Loss: 2.088576 \tValidation Loss: 2.320656\n",
      "Epoch: 7566 \tTraining Loss: 2.084419 \tValidation Loss: 2.320403\n",
      "Epoch: 7567 \tTraining Loss: 2.090651 \tValidation Loss: 2.320610\n",
      "Epoch: 7568 \tTraining Loss: 2.070493 \tValidation Loss: 2.320259\n",
      "Validation loss decreased (2.320286 --> 2.320259).  Saving model ...\n",
      "Epoch: 7569 \tTraining Loss: 2.111930 \tValidation Loss: 2.320230\n",
      "Validation loss decreased (2.320259 --> 2.320230).  Saving model ...\n",
      "Epoch: 7570 \tTraining Loss: 2.082625 \tValidation Loss: 2.320187\n",
      "Validation loss decreased (2.320230 --> 2.320187).  Saving model ...\n",
      "Epoch: 7571 \tTraining Loss: 2.107268 \tValidation Loss: 2.320135\n",
      "Validation loss decreased (2.320187 --> 2.320135).  Saving model ...\n",
      "Epoch: 7572 \tTraining Loss: 2.094900 \tValidation Loss: 2.320131\n",
      "Validation loss decreased (2.320135 --> 2.320131).  Saving model ...\n",
      "Epoch: 7573 \tTraining Loss: 2.093156 \tValidation Loss: 2.319978\n",
      "Validation loss decreased (2.320131 --> 2.319978).  Saving model ...\n",
      "Epoch: 7574 \tTraining Loss: 2.081483 \tValidation Loss: 2.320035\n",
      "Epoch: 7575 \tTraining Loss: 2.106137 \tValidation Loss: 2.320258\n",
      "Epoch: 7576 \tTraining Loss: 2.082733 \tValidation Loss: 2.320068\n",
      "Epoch: 7577 \tTraining Loss: 2.122730 \tValidation Loss: 2.320040\n",
      "Epoch: 7578 \tTraining Loss: 2.092014 \tValidation Loss: 2.319935\n",
      "Validation loss decreased (2.319978 --> 2.319935).  Saving model ...\n",
      "Epoch: 7579 \tTraining Loss: 2.092409 \tValidation Loss: 2.319855\n",
      "Validation loss decreased (2.319935 --> 2.319855).  Saving model ...\n",
      "Epoch: 7580 \tTraining Loss: 2.082848 \tValidation Loss: 2.319517\n",
      "Validation loss decreased (2.319855 --> 2.319517).  Saving model ...\n",
      "Epoch: 7581 \tTraining Loss: 2.057419 \tValidation Loss: 2.319653\n",
      "Epoch: 7582 \tTraining Loss: 2.095360 \tValidation Loss: 2.319607\n",
      "Epoch: 7583 \tTraining Loss: 2.082428 \tValidation Loss: 2.319251\n",
      "Validation loss decreased (2.319517 --> 2.319251).  Saving model ...\n",
      "Epoch: 7584 \tTraining Loss: 2.073171 \tValidation Loss: 2.319788\n",
      "Epoch: 7585 \tTraining Loss: 2.093483 \tValidation Loss: 2.319391\n",
      "Epoch: 7586 \tTraining Loss: 2.119421 \tValidation Loss: 2.319852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7587 \tTraining Loss: 2.098345 \tValidation Loss: 2.319816\n",
      "Epoch: 7588 \tTraining Loss: 2.080680 \tValidation Loss: 2.319852\n",
      "Epoch: 7589 \tTraining Loss: 2.093376 \tValidation Loss: 2.320189\n",
      "Epoch: 7590 \tTraining Loss: 2.081285 \tValidation Loss: 2.320434\n",
      "Epoch: 7591 \tTraining Loss: 2.065387 \tValidation Loss: 2.320395\n",
      "Epoch: 7592 \tTraining Loss: 2.074769 \tValidation Loss: 2.320257\n",
      "Epoch: 7593 \tTraining Loss: 2.094343 \tValidation Loss: 2.320415\n",
      "Epoch: 7594 \tTraining Loss: 2.087009 \tValidation Loss: 2.320369\n",
      "Epoch: 7595 \tTraining Loss: 2.055748 \tValidation Loss: 2.320551\n",
      "Epoch: 7596 \tTraining Loss: 2.070664 \tValidation Loss: 2.320177\n",
      "Epoch: 7597 \tTraining Loss: 2.055974 \tValidation Loss: 2.319971\n",
      "Epoch: 7598 \tTraining Loss: 2.107375 \tValidation Loss: 2.320003\n",
      "Epoch: 7599 \tTraining Loss: 2.079010 \tValidation Loss: 2.320096\n",
      "Epoch: 7600 \tTraining Loss: 2.072214 \tValidation Loss: 2.320160\n",
      "Epoch: 7601 \tTraining Loss: 2.100913 \tValidation Loss: 2.320112\n",
      "Epoch: 7602 \tTraining Loss: 2.108325 \tValidation Loss: 2.320026\n",
      "Epoch: 7603 \tTraining Loss: 2.100390 \tValidation Loss: 2.319849\n",
      "Epoch: 7604 \tTraining Loss: 2.098626 \tValidation Loss: 2.319769\n",
      "Epoch: 7605 \tTraining Loss: 2.069264 \tValidation Loss: 2.319690\n",
      "Epoch: 7606 \tTraining Loss: 2.088964 \tValidation Loss: 2.319738\n",
      "Epoch: 7607 \tTraining Loss: 2.091812 \tValidation Loss: 2.319856\n",
      "Epoch: 7608 \tTraining Loss: 2.093404 \tValidation Loss: 2.319550\n",
      "Epoch: 7609 \tTraining Loss: 2.093251 \tValidation Loss: 2.319949\n",
      "Epoch: 7610 \tTraining Loss: 2.103314 \tValidation Loss: 2.320282\n",
      "Epoch: 7611 \tTraining Loss: 2.098631 \tValidation Loss: 2.320571\n",
      "Epoch: 7612 \tTraining Loss: 2.080389 \tValidation Loss: 2.320485\n",
      "Epoch: 7613 \tTraining Loss: 2.085613 \tValidation Loss: 2.320034\n",
      "Epoch: 7614 \tTraining Loss: 2.108853 \tValidation Loss: 2.319929\n",
      "Epoch: 7615 \tTraining Loss: 2.100249 \tValidation Loss: 2.320050\n",
      "Epoch: 7616 \tTraining Loss: 2.089652 \tValidation Loss: 2.320382\n",
      "Epoch: 7617 \tTraining Loss: 2.099796 \tValidation Loss: 2.320301\n",
      "Epoch: 7618 \tTraining Loss: 2.063197 \tValidation Loss: 2.320194\n",
      "Epoch: 7619 \tTraining Loss: 2.078583 \tValidation Loss: 2.320438\n",
      "Epoch: 7620 \tTraining Loss: 2.075364 \tValidation Loss: 2.320487\n",
      "Epoch: 7621 \tTraining Loss: 2.088986 \tValidation Loss: 2.320246\n",
      "Epoch: 7622 \tTraining Loss: 2.099162 \tValidation Loss: 2.320360\n",
      "Epoch: 7623 \tTraining Loss: 2.101986 \tValidation Loss: 2.320292\n",
      "Epoch: 7624 \tTraining Loss: 2.098126 \tValidation Loss: 2.320288\n",
      "Epoch: 7625 \tTraining Loss: 2.075159 \tValidation Loss: 2.320121\n",
      "Epoch: 7626 \tTraining Loss: 2.077803 \tValidation Loss: 2.320197\n",
      "Epoch: 7627 \tTraining Loss: 2.091198 \tValidation Loss: 2.320229\n",
      "Epoch: 7628 \tTraining Loss: 2.073613 \tValidation Loss: 2.320160\n",
      "Epoch: 7629 \tTraining Loss: 2.080228 \tValidation Loss: 2.320066\n",
      "Epoch: 7630 \tTraining Loss: 2.058612 \tValidation Loss: 2.319970\n",
      "Epoch: 7631 \tTraining Loss: 2.075024 \tValidation Loss: 2.319668\n",
      "Epoch: 7632 \tTraining Loss: 2.076827 \tValidation Loss: 2.319798\n",
      "Epoch: 7633 \tTraining Loss: 2.081261 \tValidation Loss: 2.319927\n",
      "Epoch: 7634 \tTraining Loss: 2.082311 \tValidation Loss: 2.319951\n",
      "Epoch: 7635 \tTraining Loss: 2.050414 \tValidation Loss: 2.320082\n",
      "Epoch: 7636 \tTraining Loss: 2.077494 \tValidation Loss: 2.320203\n",
      "Epoch: 7637 \tTraining Loss: 2.067050 \tValidation Loss: 2.320021\n",
      "Epoch: 7638 \tTraining Loss: 2.096960 \tValidation Loss: 2.320189\n",
      "Epoch: 7639 \tTraining Loss: 2.060851 \tValidation Loss: 2.320093\n",
      "Epoch: 7640 \tTraining Loss: 2.083111 \tValidation Loss: 2.319892\n",
      "Epoch: 7641 \tTraining Loss: 2.080906 \tValidation Loss: 2.320013\n",
      "Epoch: 7642 \tTraining Loss: 2.096794 \tValidation Loss: 2.320053\n",
      "Epoch: 7643 \tTraining Loss: 2.076615 \tValidation Loss: 2.320040\n",
      "Epoch: 7644 \tTraining Loss: 2.083612 \tValidation Loss: 2.320079\n",
      "Epoch: 7645 \tTraining Loss: 2.080502 \tValidation Loss: 2.319827\n",
      "Epoch: 7646 \tTraining Loss: 2.073840 \tValidation Loss: 2.319814\n",
      "Epoch: 7647 \tTraining Loss: 2.084527 \tValidation Loss: 2.319872\n",
      "Epoch: 7648 \tTraining Loss: 2.072517 \tValidation Loss: 2.320038\n",
      "Epoch: 7649 \tTraining Loss: 2.101763 \tValidation Loss: 2.320128\n",
      "Epoch: 7650 \tTraining Loss: 2.076429 \tValidation Loss: 2.320114\n",
      "Epoch: 7651 \tTraining Loss: 2.085988 \tValidation Loss: 2.320308\n",
      "Epoch: 7652 \tTraining Loss: 2.059387 \tValidation Loss: 2.319903\n",
      "Epoch: 7653 \tTraining Loss: 2.106983 \tValidation Loss: 2.319856\n",
      "Epoch: 7654 \tTraining Loss: 2.105624 \tValidation Loss: 2.319870\n",
      "Epoch: 7655 \tTraining Loss: 2.082086 \tValidation Loss: 2.319826\n",
      "Epoch: 7656 \tTraining Loss: 2.065585 \tValidation Loss: 2.319712\n",
      "Epoch: 7657 \tTraining Loss: 2.065789 \tValidation Loss: 2.319389\n",
      "Epoch: 7658 \tTraining Loss: 2.069891 \tValidation Loss: 2.319668\n",
      "Epoch: 7659 \tTraining Loss: 2.101205 \tValidation Loss: 2.320006\n",
      "Epoch: 7660 \tTraining Loss: 2.089120 \tValidation Loss: 2.319764\n",
      "Epoch: 7661 \tTraining Loss: 2.096978 \tValidation Loss: 2.320042\n",
      "Epoch: 7662 \tTraining Loss: 2.107275 \tValidation Loss: 2.320101\n",
      "Epoch: 7663 \tTraining Loss: 2.055712 \tValidation Loss: 2.320019\n",
      "Epoch: 7664 \tTraining Loss: 2.093122 \tValidation Loss: 2.319952\n",
      "Epoch: 7665 \tTraining Loss: 2.106761 \tValidation Loss: 2.319873\n",
      "Epoch: 7666 \tTraining Loss: 2.070639 \tValidation Loss: 2.319669\n",
      "Epoch: 7667 \tTraining Loss: 2.100777 \tValidation Loss: 2.319757\n",
      "Epoch: 7668 \tTraining Loss: 2.097630 \tValidation Loss: 2.319390\n",
      "Epoch: 7669 \tTraining Loss: 2.087530 \tValidation Loss: 2.319557\n",
      "Epoch: 7670 \tTraining Loss: 2.093002 \tValidation Loss: 2.319652\n",
      "Epoch: 7671 \tTraining Loss: 2.099011 \tValidation Loss: 2.319757\n",
      "Epoch: 7672 \tTraining Loss: 2.106596 \tValidation Loss: 2.319775\n",
      "Epoch: 7673 \tTraining Loss: 2.084753 \tValidation Loss: 2.319882\n",
      "Epoch: 7674 \tTraining Loss: 2.070412 \tValidation Loss: 2.319613\n",
      "Epoch: 7675 \tTraining Loss: 2.089692 \tValidation Loss: 2.319930\n",
      "Epoch: 7676 \tTraining Loss: 2.070982 \tValidation Loss: 2.320053\n",
      "Epoch: 7677 \tTraining Loss: 2.066488 \tValidation Loss: 2.319895\n",
      "Epoch: 7678 \tTraining Loss: 2.069907 \tValidation Loss: 2.320003\n",
      "Epoch: 7679 \tTraining Loss: 2.082195 \tValidation Loss: 2.319901\n",
      "Epoch: 7680 \tTraining Loss: 2.067941 \tValidation Loss: 2.319871\n",
      "Epoch: 7681 \tTraining Loss: 2.045952 \tValidation Loss: 2.319836\n",
      "Epoch: 7682 \tTraining Loss: 2.100633 \tValidation Loss: 2.319944\n",
      "Epoch: 7683 \tTraining Loss: 2.070589 \tValidation Loss: 2.319603\n",
      "Epoch: 7684 \tTraining Loss: 2.080761 \tValidation Loss: 2.319302\n",
      "Epoch: 7685 \tTraining Loss: 2.069047 \tValidation Loss: 2.319479\n",
      "Epoch: 7686 \tTraining Loss: 2.096298 \tValidation Loss: 2.319149\n",
      "Validation loss decreased (2.319251 --> 2.319149).  Saving model ...\n",
      "Epoch: 7687 \tTraining Loss: 2.056044 \tValidation Loss: 2.319523\n",
      "Epoch: 7688 \tTraining Loss: 2.073605 \tValidation Loss: 2.319705\n",
      "Epoch: 7689 \tTraining Loss: 2.077754 \tValidation Loss: 2.319585\n",
      "Epoch: 7690 \tTraining Loss: 2.087314 \tValidation Loss: 2.319587\n",
      "Epoch: 7691 \tTraining Loss: 2.077720 \tValidation Loss: 2.319242\n",
      "Epoch: 7692 \tTraining Loss: 2.098539 \tValidation Loss: 2.319126\n",
      "Validation loss decreased (2.319149 --> 2.319126).  Saving model ...\n",
      "Epoch: 7693 \tTraining Loss: 2.103740 \tValidation Loss: 2.319368\n",
      "Epoch: 7694 \tTraining Loss: 2.116381 \tValidation Loss: 2.319194\n",
      "Epoch: 7695 \tTraining Loss: 2.074951 \tValidation Loss: 2.319281\n",
      "Epoch: 7696 \tTraining Loss: 2.069705 \tValidation Loss: 2.319040\n",
      "Validation loss decreased (2.319126 --> 2.319040).  Saving model ...\n",
      "Epoch: 7697 \tTraining Loss: 2.081063 \tValidation Loss: 2.319259\n",
      "Epoch: 7698 \tTraining Loss: 2.089215 \tValidation Loss: 2.319343\n",
      "Epoch: 7699 \tTraining Loss: 2.076819 \tValidation Loss: 2.319643\n",
      "Epoch: 7700 \tTraining Loss: 2.083700 \tValidation Loss: 2.319543\n",
      "Epoch: 7701 \tTraining Loss: 2.093609 \tValidation Loss: 2.319393\n",
      "Epoch: 7702 \tTraining Loss: 2.085271 \tValidation Loss: 2.319340\n",
      "Epoch: 7703 \tTraining Loss: 2.072798 \tValidation Loss: 2.319423\n",
      "Epoch: 7704 \tTraining Loss: 2.077081 \tValidation Loss: 2.319155\n",
      "Epoch: 7705 \tTraining Loss: 2.067708 \tValidation Loss: 2.319215\n",
      "Epoch: 7706 \tTraining Loss: 2.077558 \tValidation Loss: 2.319162\n",
      "Epoch: 7707 \tTraining Loss: 2.078519 \tValidation Loss: 2.319221\n",
      "Epoch: 7708 \tTraining Loss: 2.070396 \tValidation Loss: 2.319210\n",
      "Epoch: 7709 \tTraining Loss: 2.075924 \tValidation Loss: 2.318827\n",
      "Validation loss decreased (2.319040 --> 2.318827).  Saving model ...\n",
      "Epoch: 7710 \tTraining Loss: 2.071048 \tValidation Loss: 2.318593\n",
      "Validation loss decreased (2.318827 --> 2.318593).  Saving model ...\n",
      "Epoch: 7711 \tTraining Loss: 2.066180 \tValidation Loss: 2.318839\n",
      "Epoch: 7712 \tTraining Loss: 2.078853 \tValidation Loss: 2.318934\n",
      "Epoch: 7713 \tTraining Loss: 2.079119 \tValidation Loss: 2.318729\n",
      "Epoch: 7714 \tTraining Loss: 2.082915 \tValidation Loss: 2.318490\n",
      "Validation loss decreased (2.318593 --> 2.318490).  Saving model ...\n",
      "Epoch: 7715 \tTraining Loss: 2.063394 \tValidation Loss: 2.318695\n",
      "Epoch: 7716 \tTraining Loss: 2.053852 \tValidation Loss: 2.318642\n",
      "Epoch: 7717 \tTraining Loss: 2.055705 \tValidation Loss: 2.318707\n",
      "Epoch: 7718 \tTraining Loss: 2.120948 \tValidation Loss: 2.319190\n",
      "Epoch: 7719 \tTraining Loss: 2.114544 \tValidation Loss: 2.319015\n",
      "Epoch: 7720 \tTraining Loss: 2.047388 \tValidation Loss: 2.319287\n",
      "Epoch: 7721 \tTraining Loss: 2.081960 \tValidation Loss: 2.319307\n",
      "Epoch: 7722 \tTraining Loss: 2.083991 \tValidation Loss: 2.319392\n",
      "Epoch: 7723 \tTraining Loss: 2.066992 \tValidation Loss: 2.319563\n",
      "Epoch: 7724 \tTraining Loss: 2.076103 \tValidation Loss: 2.319799\n",
      "Epoch: 7725 \tTraining Loss: 2.072099 \tValidation Loss: 2.320189\n",
      "Epoch: 7726 \tTraining Loss: 2.073235 \tValidation Loss: 2.319922\n",
      "Epoch: 7727 \tTraining Loss: 2.055983 \tValidation Loss: 2.319772\n",
      "Epoch: 7728 \tTraining Loss: 2.074665 \tValidation Loss: 2.320026\n",
      "Epoch: 7729 \tTraining Loss: 2.057409 \tValidation Loss: 2.320287\n",
      "Epoch: 7730 \tTraining Loss: 2.081729 \tValidation Loss: 2.320313\n",
      "Epoch: 7731 \tTraining Loss: 2.084090 \tValidation Loss: 2.320166\n",
      "Epoch: 7732 \tTraining Loss: 2.064864 \tValidation Loss: 2.319868\n",
      "Epoch: 7733 \tTraining Loss: 2.090845 \tValidation Loss: 2.319272\n",
      "Epoch: 7734 \tTraining Loss: 2.068252 \tValidation Loss: 2.319723\n",
      "Epoch: 7735 \tTraining Loss: 2.082607 \tValidation Loss: 2.319744\n",
      "Epoch: 7736 \tTraining Loss: 2.083184 \tValidation Loss: 2.319507\n",
      "Epoch: 7737 \tTraining Loss: 2.081662 \tValidation Loss: 2.319734\n",
      "Epoch: 7738 \tTraining Loss: 2.074141 \tValidation Loss: 2.319215\n",
      "Epoch: 7739 \tTraining Loss: 2.070679 \tValidation Loss: 2.319324\n",
      "Epoch: 7740 \tTraining Loss: 2.084080 \tValidation Loss: 2.319126\n",
      "Epoch: 7741 \tTraining Loss: 2.085732 \tValidation Loss: 2.319196\n",
      "Epoch: 7742 \tTraining Loss: 2.077832 \tValidation Loss: 2.319103\n",
      "Epoch: 7743 \tTraining Loss: 2.091652 \tValidation Loss: 2.319206\n",
      "Epoch: 7744 \tTraining Loss: 2.073929 \tValidation Loss: 2.319154\n",
      "Epoch: 7745 \tTraining Loss: 2.093187 \tValidation Loss: 2.318843\n",
      "Epoch: 7746 \tTraining Loss: 2.080684 \tValidation Loss: 2.318608\n",
      "Epoch: 7747 \tTraining Loss: 2.060570 \tValidation Loss: 2.318493\n",
      "Epoch: 7748 \tTraining Loss: 2.070547 \tValidation Loss: 2.318540\n",
      "Epoch: 7749 \tTraining Loss: 2.094067 \tValidation Loss: 2.319023\n",
      "Epoch: 7750 \tTraining Loss: 2.087981 \tValidation Loss: 2.319319\n",
      "Epoch: 7751 \tTraining Loss: 2.049114 \tValidation Loss: 2.319510\n",
      "Epoch: 7752 \tTraining Loss: 2.071222 \tValidation Loss: 2.319443\n",
      "Epoch: 7753 \tTraining Loss: 2.070359 \tValidation Loss: 2.319589\n",
      "Epoch: 7754 \tTraining Loss: 2.070314 \tValidation Loss: 2.319519\n",
      "Epoch: 7755 \tTraining Loss: 2.080092 \tValidation Loss: 2.319818\n",
      "Epoch: 7756 \tTraining Loss: 2.063690 \tValidation Loss: 2.319625\n",
      "Epoch: 7757 \tTraining Loss: 2.067029 \tValidation Loss: 2.319461\n",
      "Epoch: 7758 \tTraining Loss: 2.072457 \tValidation Loss: 2.319695\n",
      "Epoch: 7759 \tTraining Loss: 2.099287 \tValidation Loss: 2.319412\n",
      "Epoch: 7760 \tTraining Loss: 2.105439 \tValidation Loss: 2.319770\n",
      "Epoch: 7761 \tTraining Loss: 2.076670 \tValidation Loss: 2.319595\n",
      "Epoch: 7762 \tTraining Loss: 2.072809 \tValidation Loss: 2.319614\n",
      "Epoch: 7763 \tTraining Loss: 2.047042 \tValidation Loss: 2.319439\n",
      "Epoch: 7764 \tTraining Loss: 2.060615 \tValidation Loss: 2.319174\n",
      "Epoch: 7765 \tTraining Loss: 2.106819 \tValidation Loss: 2.319216\n",
      "Epoch: 7766 \tTraining Loss: 2.077986 \tValidation Loss: 2.319202\n",
      "Epoch: 7767 \tTraining Loss: 2.075122 \tValidation Loss: 2.319380\n",
      "Epoch: 7768 \tTraining Loss: 2.060366 \tValidation Loss: 2.318887\n",
      "Epoch: 7769 \tTraining Loss: 2.070542 \tValidation Loss: 2.318673\n",
      "Epoch: 7770 \tTraining Loss: 2.073489 \tValidation Loss: 2.318806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7771 \tTraining Loss: 2.110129 \tValidation Loss: 2.318879\n",
      "Epoch: 7772 \tTraining Loss: 2.076978 \tValidation Loss: 2.318666\n",
      "Epoch: 7773 \tTraining Loss: 2.067138 \tValidation Loss: 2.318813\n",
      "Epoch: 7774 \tTraining Loss: 2.087913 \tValidation Loss: 2.318925\n",
      "Epoch: 7775 \tTraining Loss: 2.056879 \tValidation Loss: 2.318934\n",
      "Epoch: 7776 \tTraining Loss: 2.105537 \tValidation Loss: 2.318843\n",
      "Epoch: 7777 \tTraining Loss: 2.089855 \tValidation Loss: 2.318835\n",
      "Epoch: 7778 \tTraining Loss: 2.076973 \tValidation Loss: 2.318922\n",
      "Epoch: 7779 \tTraining Loss: 2.058365 \tValidation Loss: 2.318458\n",
      "Validation loss decreased (2.318490 --> 2.318458).  Saving model ...\n",
      "Epoch: 7780 \tTraining Loss: 2.061071 \tValidation Loss: 2.318328\n",
      "Validation loss decreased (2.318458 --> 2.318328).  Saving model ...\n",
      "Epoch: 7781 \tTraining Loss: 2.104065 \tValidation Loss: 2.318634\n",
      "Epoch: 7782 \tTraining Loss: 2.071042 \tValidation Loss: 2.318518\n",
      "Epoch: 7783 \tTraining Loss: 2.065964 \tValidation Loss: 2.318393\n",
      "Epoch: 7784 \tTraining Loss: 2.072553 \tValidation Loss: 2.318590\n",
      "Epoch: 7785 \tTraining Loss: 2.042720 \tValidation Loss: 2.318235\n",
      "Validation loss decreased (2.318328 --> 2.318235).  Saving model ...\n",
      "Epoch: 7786 \tTraining Loss: 2.043508 \tValidation Loss: 2.317900\n",
      "Validation loss decreased (2.318235 --> 2.317900).  Saving model ...\n",
      "Epoch: 7787 \tTraining Loss: 2.066997 \tValidation Loss: 2.317832\n",
      "Validation loss decreased (2.317900 --> 2.317832).  Saving model ...\n",
      "Epoch: 7788 \tTraining Loss: 2.097563 \tValidation Loss: 2.317864\n",
      "Epoch: 7789 \tTraining Loss: 2.089815 \tValidation Loss: 2.317656\n",
      "Validation loss decreased (2.317832 --> 2.317656).  Saving model ...\n",
      "Epoch: 7790 \tTraining Loss: 2.058592 \tValidation Loss: 2.317540\n",
      "Validation loss decreased (2.317656 --> 2.317540).  Saving model ...\n",
      "Epoch: 7791 \tTraining Loss: 2.085180 \tValidation Loss: 2.317410\n",
      "Validation loss decreased (2.317540 --> 2.317410).  Saving model ...\n",
      "Epoch: 7792 \tTraining Loss: 2.091456 \tValidation Loss: 2.317368\n",
      "Validation loss decreased (2.317410 --> 2.317368).  Saving model ...\n",
      "Epoch: 7793 \tTraining Loss: 2.060732 \tValidation Loss: 2.317463\n",
      "Epoch: 7794 \tTraining Loss: 2.068888 \tValidation Loss: 2.317322\n",
      "Validation loss decreased (2.317368 --> 2.317322).  Saving model ...\n",
      "Epoch: 7795 \tTraining Loss: 2.088626 \tValidation Loss: 2.317899\n",
      "Epoch: 7796 \tTraining Loss: 2.086704 \tValidation Loss: 2.317864\n",
      "Epoch: 7797 \tTraining Loss: 2.084906 \tValidation Loss: 2.317801\n",
      "Epoch: 7798 \tTraining Loss: 2.071734 \tValidation Loss: 2.317721\n",
      "Epoch: 7799 \tTraining Loss: 2.059631 \tValidation Loss: 2.317820\n",
      "Epoch: 7800 \tTraining Loss: 2.082186 \tValidation Loss: 2.318049\n",
      "Epoch: 7801 \tTraining Loss: 2.090966 \tValidation Loss: 2.318391\n",
      "Epoch: 7802 \tTraining Loss: 2.076989 \tValidation Loss: 2.317868\n",
      "Epoch: 7803 \tTraining Loss: 2.074415 \tValidation Loss: 2.317878\n",
      "Epoch: 7804 \tTraining Loss: 2.093667 \tValidation Loss: 2.317934\n",
      "Epoch: 7805 \tTraining Loss: 2.079328 \tValidation Loss: 2.317726\n",
      "Epoch: 7806 \tTraining Loss: 2.064402 \tValidation Loss: 2.317867\n",
      "Epoch: 7807 \tTraining Loss: 2.091918 \tValidation Loss: 2.317659\n",
      "Epoch: 7808 \tTraining Loss: 2.099212 \tValidation Loss: 2.318081\n",
      "Epoch: 7809 \tTraining Loss: 2.097086 \tValidation Loss: 2.318154\n",
      "Epoch: 7810 \tTraining Loss: 2.093084 \tValidation Loss: 2.318077\n",
      "Epoch: 7811 \tTraining Loss: 2.091450 \tValidation Loss: 2.317980\n",
      "Epoch: 7812 \tTraining Loss: 2.065187 \tValidation Loss: 2.318107\n",
      "Epoch: 7813 \tTraining Loss: 2.057188 \tValidation Loss: 2.317893\n",
      "Epoch: 7814 \tTraining Loss: 2.083545 \tValidation Loss: 2.317973\n",
      "Epoch: 7815 \tTraining Loss: 2.074239 \tValidation Loss: 2.317819\n",
      "Epoch: 7816 \tTraining Loss: 2.069242 \tValidation Loss: 2.317781\n",
      "Epoch: 7817 \tTraining Loss: 2.066412 \tValidation Loss: 2.317790\n",
      "Epoch: 7818 \tTraining Loss: 2.052036 \tValidation Loss: 2.317700\n",
      "Epoch: 7819 \tTraining Loss: 2.050025 \tValidation Loss: 2.317752\n",
      "Epoch: 7820 \tTraining Loss: 2.066885 \tValidation Loss: 2.317750\n",
      "Epoch: 7821 \tTraining Loss: 2.096757 \tValidation Loss: 2.317896\n",
      "Epoch: 7822 \tTraining Loss: 2.093268 \tValidation Loss: 2.317628\n",
      "Epoch: 7823 \tTraining Loss: 2.039418 \tValidation Loss: 2.317545\n",
      "Epoch: 7824 \tTraining Loss: 2.056848 \tValidation Loss: 2.317471\n",
      "Epoch: 7825 \tTraining Loss: 2.092623 \tValidation Loss: 2.317712\n",
      "Epoch: 7826 \tTraining Loss: 2.096209 \tValidation Loss: 2.317906\n",
      "Epoch: 7827 \tTraining Loss: 2.059499 \tValidation Loss: 2.318158\n",
      "Epoch: 7828 \tTraining Loss: 2.069317 \tValidation Loss: 2.318369\n",
      "Epoch: 7829 \tTraining Loss: 2.095940 \tValidation Loss: 2.318220\n",
      "Epoch: 7830 \tTraining Loss: 2.052046 \tValidation Loss: 2.318294\n",
      "Epoch: 7831 \tTraining Loss: 2.061579 \tValidation Loss: 2.318260\n",
      "Epoch: 7832 \tTraining Loss: 2.092790 \tValidation Loss: 2.318133\n",
      "Epoch: 7833 \tTraining Loss: 2.085634 \tValidation Loss: 2.318118\n",
      "Epoch: 7834 \tTraining Loss: 2.044795 \tValidation Loss: 2.318215\n",
      "Epoch: 7835 \tTraining Loss: 2.078718 \tValidation Loss: 2.318097\n",
      "Epoch: 7836 \tTraining Loss: 2.069807 \tValidation Loss: 2.318309\n",
      "Epoch: 7837 \tTraining Loss: 2.049443 \tValidation Loss: 2.318367\n",
      "Epoch: 7838 \tTraining Loss: 2.075081 \tValidation Loss: 2.318375\n",
      "Epoch: 7839 \tTraining Loss: 2.063518 \tValidation Loss: 2.318417\n",
      "Epoch: 7840 \tTraining Loss: 2.064433 \tValidation Loss: 2.318413\n",
      "Epoch: 7841 \tTraining Loss: 2.074435 \tValidation Loss: 2.318421\n",
      "Epoch: 7842 \tTraining Loss: 2.071154 \tValidation Loss: 2.318358\n",
      "Epoch: 7843 \tTraining Loss: 2.070724 \tValidation Loss: 2.318132\n",
      "Epoch: 7844 \tTraining Loss: 2.077030 \tValidation Loss: 2.318316\n",
      "Epoch: 7845 \tTraining Loss: 2.067791 \tValidation Loss: 2.318373\n",
      "Epoch: 7846 \tTraining Loss: 2.099625 \tValidation Loss: 2.318109\n",
      "Epoch: 7847 \tTraining Loss: 2.064446 \tValidation Loss: 2.317985\n",
      "Epoch: 7848 \tTraining Loss: 2.077342 \tValidation Loss: 2.318288\n",
      "Epoch: 7849 \tTraining Loss: 2.076747 \tValidation Loss: 2.318277\n",
      "Epoch: 7850 \tTraining Loss: 2.057911 \tValidation Loss: 2.318553\n",
      "Epoch: 7851 \tTraining Loss: 2.053800 \tValidation Loss: 2.318637\n",
      "Epoch: 7852 \tTraining Loss: 2.048049 \tValidation Loss: 2.318339\n",
      "Epoch: 7853 \tTraining Loss: 2.092664 \tValidation Loss: 2.318585\n",
      "Epoch: 7854 \tTraining Loss: 2.087938 \tValidation Loss: 2.318603\n",
      "Epoch: 7855 \tTraining Loss: 2.076791 \tValidation Loss: 2.318982\n",
      "Epoch: 7856 \tTraining Loss: 2.062405 \tValidation Loss: 2.318929\n",
      "Epoch: 7857 \tTraining Loss: 2.069818 \tValidation Loss: 2.318788\n",
      "Epoch: 7858 \tTraining Loss: 2.077732 \tValidation Loss: 2.318739\n",
      "Epoch: 7859 \tTraining Loss: 2.082519 \tValidation Loss: 2.318774\n",
      "Epoch: 7860 \tTraining Loss: 2.078696 \tValidation Loss: 2.319045\n",
      "Epoch: 7861 \tTraining Loss: 2.072862 \tValidation Loss: 2.318825\n",
      "Epoch: 7862 \tTraining Loss: 2.076419 \tValidation Loss: 2.318834\n",
      "Epoch: 7863 \tTraining Loss: 2.058260 \tValidation Loss: 2.318785\n",
      "Epoch: 7864 \tTraining Loss: 2.098287 \tValidation Loss: 2.318607\n",
      "Epoch: 7865 \tTraining Loss: 2.055314 \tValidation Loss: 2.319004\n",
      "Epoch: 7866 \tTraining Loss: 2.060529 \tValidation Loss: 2.319012\n",
      "Epoch: 7867 \tTraining Loss: 2.093326 \tValidation Loss: 2.318926\n",
      "Epoch: 7868 \tTraining Loss: 2.052968 \tValidation Loss: 2.318459\n",
      "Epoch: 7869 \tTraining Loss: 2.085041 \tValidation Loss: 2.318696\n",
      "Epoch: 7870 \tTraining Loss: 2.075176 \tValidation Loss: 2.318414\n",
      "Epoch: 7871 \tTraining Loss: 2.033386 \tValidation Loss: 2.318201\n",
      "Epoch: 7872 \tTraining Loss: 2.087637 \tValidation Loss: 2.318321\n",
      "Epoch: 7873 \tTraining Loss: 2.064299 \tValidation Loss: 2.318652\n",
      "Epoch: 7874 \tTraining Loss: 2.073438 \tValidation Loss: 2.318774\n",
      "Epoch: 7875 \tTraining Loss: 2.063130 \tValidation Loss: 2.319123\n",
      "Epoch: 7876 \tTraining Loss: 2.058945 \tValidation Loss: 2.319434\n",
      "Epoch: 7877 \tTraining Loss: 2.066886 \tValidation Loss: 2.319351\n",
      "Epoch: 7878 \tTraining Loss: 2.082080 \tValidation Loss: 2.319044\n",
      "Epoch: 7879 \tTraining Loss: 2.083966 \tValidation Loss: 2.319264\n",
      "Epoch: 7880 \tTraining Loss: 2.080788 \tValidation Loss: 2.318967\n",
      "Epoch: 7881 \tTraining Loss: 2.068477 \tValidation Loss: 2.318959\n",
      "Epoch: 7882 \tTraining Loss: 2.097684 \tValidation Loss: 2.318595\n",
      "Epoch: 7883 \tTraining Loss: 2.101224 \tValidation Loss: 2.318871\n",
      "Epoch: 7884 \tTraining Loss: 2.069864 \tValidation Loss: 2.319118\n",
      "Epoch: 7885 \tTraining Loss: 2.063592 \tValidation Loss: 2.318989\n",
      "Epoch: 7886 \tTraining Loss: 2.056572 \tValidation Loss: 2.318710\n",
      "Epoch: 7887 \tTraining Loss: 2.055481 \tValidation Loss: 2.318651\n",
      "Epoch: 7888 \tTraining Loss: 2.081405 \tValidation Loss: 2.318501\n",
      "Epoch: 7889 \tTraining Loss: 2.078225 \tValidation Loss: 2.318743\n",
      "Epoch: 7890 \tTraining Loss: 2.064842 \tValidation Loss: 2.318651\n",
      "Epoch: 7891 \tTraining Loss: 2.071160 \tValidation Loss: 2.318791\n",
      "Epoch: 7892 \tTraining Loss: 2.071886 \tValidation Loss: 2.318671\n",
      "Epoch: 7893 \tTraining Loss: 2.077573 \tValidation Loss: 2.318460\n",
      "Epoch: 7894 \tTraining Loss: 2.068683 \tValidation Loss: 2.318149\n",
      "Epoch: 7895 \tTraining Loss: 2.089733 \tValidation Loss: 2.317921\n",
      "Epoch: 7896 \tTraining Loss: 2.078228 \tValidation Loss: 2.317536\n",
      "Epoch: 7897 \tTraining Loss: 2.079302 \tValidation Loss: 2.317269\n",
      "Validation loss decreased (2.317322 --> 2.317269).  Saving model ...\n",
      "Epoch: 7898 \tTraining Loss: 2.087698 \tValidation Loss: 2.317251\n",
      "Validation loss decreased (2.317269 --> 2.317251).  Saving model ...\n",
      "Epoch: 7899 \tTraining Loss: 2.083193 \tValidation Loss: 2.317454\n",
      "Epoch: 7900 \tTraining Loss: 2.073854 \tValidation Loss: 2.317248\n",
      "Validation loss decreased (2.317251 --> 2.317248).  Saving model ...\n",
      "Epoch: 7901 \tTraining Loss: 2.064512 \tValidation Loss: 2.317608\n",
      "Epoch: 7902 \tTraining Loss: 2.060521 \tValidation Loss: 2.317434\n",
      "Epoch: 7903 \tTraining Loss: 2.084416 \tValidation Loss: 2.317291\n",
      "Epoch: 7904 \tTraining Loss: 2.077539 \tValidation Loss: 2.317214\n",
      "Validation loss decreased (2.317248 --> 2.317214).  Saving model ...\n",
      "Epoch: 7905 \tTraining Loss: 2.060346 \tValidation Loss: 2.317170\n",
      "Validation loss decreased (2.317214 --> 2.317170).  Saving model ...\n",
      "Epoch: 7906 \tTraining Loss: 2.064606 \tValidation Loss: 2.317342\n",
      "Epoch: 7907 \tTraining Loss: 2.039053 \tValidation Loss: 2.317213\n",
      "Epoch: 7908 \tTraining Loss: 2.068656 \tValidation Loss: 2.317094\n",
      "Validation loss decreased (2.317170 --> 2.317094).  Saving model ...\n",
      "Epoch: 7909 \tTraining Loss: 2.054028 \tValidation Loss: 2.317100\n",
      "Epoch: 7910 \tTraining Loss: 2.069730 \tValidation Loss: 2.317296\n",
      "Epoch: 7911 \tTraining Loss: 2.052900 \tValidation Loss: 2.316967\n",
      "Validation loss decreased (2.317094 --> 2.316967).  Saving model ...\n",
      "Epoch: 7912 \tTraining Loss: 2.071422 \tValidation Loss: 2.317195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7913 \tTraining Loss: 2.086805 \tValidation Loss: 2.317638\n",
      "Epoch: 7914 \tTraining Loss: 2.100350 \tValidation Loss: 2.317930\n",
      "Epoch: 7915 \tTraining Loss: 2.056846 \tValidation Loss: 2.317952\n",
      "Epoch: 7916 \tTraining Loss: 2.096434 \tValidation Loss: 2.318021\n",
      "Epoch: 7917 \tTraining Loss: 2.068607 \tValidation Loss: 2.317888\n",
      "Epoch: 7918 \tTraining Loss: 2.063206 \tValidation Loss: 2.317603\n",
      "Epoch: 7919 \tTraining Loss: 2.078580 \tValidation Loss: 2.317597\n",
      "Epoch: 7920 \tTraining Loss: 2.061219 \tValidation Loss: 2.317702\n",
      "Epoch: 7921 \tTraining Loss: 2.083279 \tValidation Loss: 2.317492\n",
      "Epoch: 7922 \tTraining Loss: 2.067248 \tValidation Loss: 2.317226\n",
      "Epoch: 7923 \tTraining Loss: 2.088334 \tValidation Loss: 2.317501\n",
      "Epoch: 7924 \tTraining Loss: 2.097717 \tValidation Loss: 2.317831\n",
      "Epoch: 7925 \tTraining Loss: 2.060136 \tValidation Loss: 2.317330\n",
      "Epoch: 7926 \tTraining Loss: 2.059443 \tValidation Loss: 2.317478\n",
      "Epoch: 7927 \tTraining Loss: 2.074769 \tValidation Loss: 2.317215\n",
      "Epoch: 7928 \tTraining Loss: 2.070693 \tValidation Loss: 2.317405\n",
      "Epoch: 7929 \tTraining Loss: 2.059473 \tValidation Loss: 2.317408\n",
      "Epoch: 7930 \tTraining Loss: 2.087514 \tValidation Loss: 2.317562\n",
      "Epoch: 7931 \tTraining Loss: 2.086176 \tValidation Loss: 2.317409\n",
      "Epoch: 7932 \tTraining Loss: 2.077156 \tValidation Loss: 2.317357\n",
      "Epoch: 7933 \tTraining Loss: 2.057283 \tValidation Loss: 2.317182\n",
      "Epoch: 7934 \tTraining Loss: 2.052422 \tValidation Loss: 2.317074\n",
      "Epoch: 7935 \tTraining Loss: 2.081291 \tValidation Loss: 2.317127\n",
      "Epoch: 7936 \tTraining Loss: 2.072077 \tValidation Loss: 2.316959\n",
      "Validation loss decreased (2.316967 --> 2.316959).  Saving model ...\n",
      "Epoch: 7937 \tTraining Loss: 2.059596 \tValidation Loss: 2.316939\n",
      "Validation loss decreased (2.316959 --> 2.316939).  Saving model ...\n",
      "Epoch: 7938 \tTraining Loss: 2.047890 \tValidation Loss: 2.316446\n",
      "Validation loss decreased (2.316939 --> 2.316446).  Saving model ...\n",
      "Epoch: 7939 \tTraining Loss: 2.074132 \tValidation Loss: 2.316573\n",
      "Epoch: 7940 \tTraining Loss: 2.050928 \tValidation Loss: 2.316198\n",
      "Validation loss decreased (2.316446 --> 2.316198).  Saving model ...\n",
      "Epoch: 7941 \tTraining Loss: 2.069770 \tValidation Loss: 2.316651\n",
      "Epoch: 7942 \tTraining Loss: 2.084664 \tValidation Loss: 2.316938\n",
      "Epoch: 7943 \tTraining Loss: 2.068007 \tValidation Loss: 2.317175\n",
      "Epoch: 7944 \tTraining Loss: 2.051677 \tValidation Loss: 2.317060\n",
      "Epoch: 7945 \tTraining Loss: 2.078976 \tValidation Loss: 2.316740\n",
      "Epoch: 7946 \tTraining Loss: 2.087174 \tValidation Loss: 2.316759\n",
      "Epoch: 7947 \tTraining Loss: 2.052608 \tValidation Loss: 2.316339\n",
      "Epoch: 7948 \tTraining Loss: 2.064277 \tValidation Loss: 2.316506\n",
      "Epoch: 7949 \tTraining Loss: 2.084408 \tValidation Loss: 2.316692\n",
      "Epoch: 7950 \tTraining Loss: 2.085938 \tValidation Loss: 2.316687\n",
      "Epoch: 7951 \tTraining Loss: 2.101729 \tValidation Loss: 2.316825\n",
      "Epoch: 7952 \tTraining Loss: 2.064480 \tValidation Loss: 2.316522\n",
      "Epoch: 7953 \tTraining Loss: 2.071337 \tValidation Loss: 2.316560\n",
      "Epoch: 7954 \tTraining Loss: 2.061550 \tValidation Loss: 2.316766\n",
      "Epoch: 7955 \tTraining Loss: 2.061692 \tValidation Loss: 2.316603\n",
      "Epoch: 7956 \tTraining Loss: 2.059940 \tValidation Loss: 2.316548\n",
      "Epoch: 7957 \tTraining Loss: 2.084602 \tValidation Loss: 2.316909\n",
      "Epoch: 7958 \tTraining Loss: 2.066613 \tValidation Loss: 2.316830\n",
      "Epoch: 7959 \tTraining Loss: 2.082086 \tValidation Loss: 2.317065\n",
      "Epoch: 7960 \tTraining Loss: 2.060493 \tValidation Loss: 2.316689\n",
      "Epoch: 7961 \tTraining Loss: 2.077543 \tValidation Loss: 2.316977\n",
      "Epoch: 7962 \tTraining Loss: 2.059758 \tValidation Loss: 2.317004\n",
      "Epoch: 7963 \tTraining Loss: 2.046468 \tValidation Loss: 2.317337\n",
      "Epoch: 7964 \tTraining Loss: 2.086694 \tValidation Loss: 2.317554\n",
      "Epoch: 7965 \tTraining Loss: 2.059999 \tValidation Loss: 2.317747\n",
      "Epoch: 7966 \tTraining Loss: 2.070857 \tValidation Loss: 2.317620\n",
      "Epoch: 7967 \tTraining Loss: 2.043520 \tValidation Loss: 2.317418\n",
      "Epoch: 7968 \tTraining Loss: 2.069050 \tValidation Loss: 2.317190\n",
      "Epoch: 7969 \tTraining Loss: 2.062056 \tValidation Loss: 2.316749\n",
      "Epoch: 7970 \tTraining Loss: 2.063941 \tValidation Loss: 2.316787\n",
      "Epoch: 7971 \tTraining Loss: 2.056604 \tValidation Loss: 2.316816\n",
      "Epoch: 7972 \tTraining Loss: 2.070720 \tValidation Loss: 2.317015\n",
      "Epoch: 7973 \tTraining Loss: 2.061049 \tValidation Loss: 2.316931\n",
      "Epoch: 7974 \tTraining Loss: 2.057987 \tValidation Loss: 2.317275\n",
      "Epoch: 7975 \tTraining Loss: 2.073053 \tValidation Loss: 2.317309\n",
      "Epoch: 7976 \tTraining Loss: 2.088083 \tValidation Loss: 2.317372\n",
      "Epoch: 7977 \tTraining Loss: 2.068871 \tValidation Loss: 2.317166\n",
      "Epoch: 7978 \tTraining Loss: 2.081176 \tValidation Loss: 2.317251\n",
      "Epoch: 7979 \tTraining Loss: 2.077095 \tValidation Loss: 2.317242\n",
      "Epoch: 7980 \tTraining Loss: 2.061099 \tValidation Loss: 2.317181\n",
      "Epoch: 7981 \tTraining Loss: 2.041460 \tValidation Loss: 2.317400\n",
      "Epoch: 7982 \tTraining Loss: 2.040786 \tValidation Loss: 2.317405\n",
      "Epoch: 7983 \tTraining Loss: 2.086201 \tValidation Loss: 2.317073\n",
      "Epoch: 7984 \tTraining Loss: 2.063445 \tValidation Loss: 2.316572\n",
      "Epoch: 7985 \tTraining Loss: 2.070437 \tValidation Loss: 2.316512\n",
      "Epoch: 7986 \tTraining Loss: 2.063240 \tValidation Loss: 2.316436\n",
      "Epoch: 7987 \tTraining Loss: 2.061085 \tValidation Loss: 2.316426\n",
      "Epoch: 7988 \tTraining Loss: 2.056456 \tValidation Loss: 2.316495\n",
      "Epoch: 7989 \tTraining Loss: 2.071405 \tValidation Loss: 2.316776\n",
      "Epoch: 7990 \tTraining Loss: 2.061424 \tValidation Loss: 2.316629\n",
      "Epoch: 7991 \tTraining Loss: 2.077820 \tValidation Loss: 2.317003\n",
      "Epoch: 7992 \tTraining Loss: 2.077305 \tValidation Loss: 2.316930\n",
      "Epoch: 7993 \tTraining Loss: 2.074564 \tValidation Loss: 2.317217\n",
      "Epoch: 7994 \tTraining Loss: 2.076417 \tValidation Loss: 2.317244\n",
      "Epoch: 7995 \tTraining Loss: 2.076150 \tValidation Loss: 2.317117\n",
      "Epoch: 7996 \tTraining Loss: 2.066529 \tValidation Loss: 2.316826\n",
      "Epoch: 7997 \tTraining Loss: 2.084042 \tValidation Loss: 2.316904\n",
      "Epoch: 7998 \tTraining Loss: 2.049537 \tValidation Loss: 2.316766\n",
      "Epoch: 7999 \tTraining Loss: 2.073831 \tValidation Loss: 2.317235\n",
      "Epoch: 8000 \tTraining Loss: 2.046196 \tValidation Loss: 2.317149\n",
      "Epoch: 8001 \tTraining Loss: 2.075062 \tValidation Loss: 2.317184\n",
      "Epoch: 8002 \tTraining Loss: 2.069102 \tValidation Loss: 2.317167\n",
      "Epoch: 8003 \tTraining Loss: 2.046685 \tValidation Loss: 2.317230\n",
      "Epoch: 8004 \tTraining Loss: 2.074862 \tValidation Loss: 2.317037\n",
      "Epoch: 8005 \tTraining Loss: 2.066234 \tValidation Loss: 2.316864\n",
      "Epoch: 8006 \tTraining Loss: 2.063455 \tValidation Loss: 2.317156\n",
      "Epoch: 8007 \tTraining Loss: 2.036414 \tValidation Loss: 2.316870\n",
      "Epoch: 8008 \tTraining Loss: 2.067037 \tValidation Loss: 2.316900\n",
      "Epoch: 8009 \tTraining Loss: 2.104187 \tValidation Loss: 2.317033\n",
      "Epoch: 8010 \tTraining Loss: 2.071333 \tValidation Loss: 2.317077\n",
      "Epoch: 8011 \tTraining Loss: 2.055914 \tValidation Loss: 2.317063\n",
      "Epoch: 8012 \tTraining Loss: 2.078341 \tValidation Loss: 2.317023\n",
      "Epoch: 8013 \tTraining Loss: 2.090694 \tValidation Loss: 2.317076\n",
      "Epoch: 8014 \tTraining Loss: 2.062382 \tValidation Loss: 2.316841\n",
      "Epoch: 8015 \tTraining Loss: 2.054984 \tValidation Loss: 2.316674\n",
      "Epoch: 8016 \tTraining Loss: 2.053959 \tValidation Loss: 2.316836\n",
      "Epoch: 8017 \tTraining Loss: 2.054139 \tValidation Loss: 2.316659\n",
      "Epoch: 8018 \tTraining Loss: 2.065925 \tValidation Loss: 2.316669\n",
      "Epoch: 8019 \tTraining Loss: 2.095755 \tValidation Loss: 2.316641\n",
      "Epoch: 8020 \tTraining Loss: 2.065572 \tValidation Loss: 2.316631\n",
      "Epoch: 8021 \tTraining Loss: 2.029558 \tValidation Loss: 2.316850\n",
      "Epoch: 8022 \tTraining Loss: 2.071618 \tValidation Loss: 2.316587\n",
      "Epoch: 8023 \tTraining Loss: 2.045340 \tValidation Loss: 2.316366\n",
      "Epoch: 8024 \tTraining Loss: 2.085310 \tValidation Loss: 2.316782\n",
      "Epoch: 8025 \tTraining Loss: 2.063159 \tValidation Loss: 2.316899\n",
      "Epoch: 8026 \tTraining Loss: 2.082979 \tValidation Loss: 2.316889\n",
      "Epoch: 8027 \tTraining Loss: 2.045981 \tValidation Loss: 2.316820\n",
      "Epoch: 8028 \tTraining Loss: 2.069094 \tValidation Loss: 2.316792\n",
      "Epoch: 8029 \tTraining Loss: 2.058800 \tValidation Loss: 2.316941\n",
      "Epoch: 8030 \tTraining Loss: 2.086452 \tValidation Loss: 2.317287\n",
      "Epoch: 8031 \tTraining Loss: 2.066598 \tValidation Loss: 2.317198\n",
      "Epoch: 8032 \tTraining Loss: 2.060299 \tValidation Loss: 2.317078\n",
      "Epoch: 8033 \tTraining Loss: 2.048391 \tValidation Loss: 2.316868\n",
      "Epoch: 8034 \tTraining Loss: 2.057766 \tValidation Loss: 2.316940\n",
      "Epoch: 8035 \tTraining Loss: 2.049172 \tValidation Loss: 2.316467\n",
      "Epoch: 8036 \tTraining Loss: 2.037332 \tValidation Loss: 2.316485\n",
      "Epoch: 8037 \tTraining Loss: 2.052590 \tValidation Loss: 2.316400\n",
      "Epoch: 8038 \tTraining Loss: 2.065789 \tValidation Loss: 2.316461\n",
      "Epoch: 8039 \tTraining Loss: 2.083032 \tValidation Loss: 2.316684\n",
      "Epoch: 8040 \tTraining Loss: 2.077050 \tValidation Loss: 2.316646\n",
      "Epoch: 8041 \tTraining Loss: 2.079540 \tValidation Loss: 2.317078\n",
      "Epoch: 8042 \tTraining Loss: 2.069739 \tValidation Loss: 2.316892\n",
      "Epoch: 8043 \tTraining Loss: 2.071316 \tValidation Loss: 2.317151\n",
      "Epoch: 8044 \tTraining Loss: 2.070943 \tValidation Loss: 2.317447\n",
      "Epoch: 8045 \tTraining Loss: 2.061712 \tValidation Loss: 2.317315\n",
      "Epoch: 8046 \tTraining Loss: 2.042237 \tValidation Loss: 2.317078\n",
      "Epoch: 8047 \tTraining Loss: 2.049833 \tValidation Loss: 2.317204\n",
      "Epoch: 8048 \tTraining Loss: 2.063219 \tValidation Loss: 2.316965\n",
      "Epoch: 8049 \tTraining Loss: 2.073239 \tValidation Loss: 2.317064\n",
      "Epoch: 8050 \tTraining Loss: 2.057707 \tValidation Loss: 2.316887\n",
      "Epoch: 8051 \tTraining Loss: 2.061260 \tValidation Loss: 2.316630\n",
      "Epoch: 8052 \tTraining Loss: 2.033360 \tValidation Loss: 2.316451\n",
      "Epoch: 8053 \tTraining Loss: 2.054955 \tValidation Loss: 2.316329\n",
      "Epoch: 8054 \tTraining Loss: 2.082847 \tValidation Loss: 2.316208\n",
      "Epoch: 8055 \tTraining Loss: 2.073970 \tValidation Loss: 2.316397\n",
      "Epoch: 8056 \tTraining Loss: 2.051448 \tValidation Loss: 2.316584\n",
      "Epoch: 8057 \tTraining Loss: 2.044535 \tValidation Loss: 2.316589\n",
      "Epoch: 8058 \tTraining Loss: 2.055381 \tValidation Loss: 2.316614\n",
      "Epoch: 8059 \tTraining Loss: 2.088340 \tValidation Loss: 2.316695\n",
      "Epoch: 8060 \tTraining Loss: 2.038363 \tValidation Loss: 2.316603\n",
      "Epoch: 8061 \tTraining Loss: 2.065247 \tValidation Loss: 2.316475\n",
      "Epoch: 8062 \tTraining Loss: 2.071701 \tValidation Loss: 2.316375\n",
      "Epoch: 8063 \tTraining Loss: 2.063448 \tValidation Loss: 2.316541\n",
      "Epoch: 8064 \tTraining Loss: 2.051400 \tValidation Loss: 2.316550\n",
      "Epoch: 8065 \tTraining Loss: 2.048082 \tValidation Loss: 2.316488\n",
      "Epoch: 8066 \tTraining Loss: 2.069410 \tValidation Loss: 2.316236\n",
      "Epoch: 8067 \tTraining Loss: 2.046434 \tValidation Loss: 2.316316\n",
      "Epoch: 8068 \tTraining Loss: 2.053213 \tValidation Loss: 2.316056\n",
      "Validation loss decreased (2.316198 --> 2.316056).  Saving model ...\n",
      "Epoch: 8069 \tTraining Loss: 2.050087 \tValidation Loss: 2.316211\n",
      "Epoch: 8070 \tTraining Loss: 2.058602 \tValidation Loss: 2.316116\n",
      "Epoch: 8071 \tTraining Loss: 2.054355 \tValidation Loss: 2.316114\n",
      "Epoch: 8072 \tTraining Loss: 2.080797 \tValidation Loss: 2.315935\n",
      "Validation loss decreased (2.316056 --> 2.315935).  Saving model ...\n",
      "Epoch: 8073 \tTraining Loss: 2.054835 \tValidation Loss: 2.315859\n",
      "Validation loss decreased (2.315935 --> 2.315859).  Saving model ...\n",
      "Epoch: 8074 \tTraining Loss: 2.038515 \tValidation Loss: 2.315740\n",
      "Validation loss decreased (2.315859 --> 2.315740).  Saving model ...\n",
      "Epoch: 8075 \tTraining Loss: 2.056767 \tValidation Loss: 2.315872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8076 \tTraining Loss: 2.107793 \tValidation Loss: 2.316181\n",
      "Epoch: 8077 \tTraining Loss: 2.062349 \tValidation Loss: 2.315892\n",
      "Epoch: 8078 \tTraining Loss: 2.057966 \tValidation Loss: 2.316021\n",
      "Epoch: 8079 \tTraining Loss: 2.067752 \tValidation Loss: 2.316246\n",
      "Epoch: 8080 \tTraining Loss: 2.062782 \tValidation Loss: 2.316322\n",
      "Epoch: 8081 \tTraining Loss: 2.062329 \tValidation Loss: 2.315967\n",
      "Epoch: 8082 \tTraining Loss: 2.062551 \tValidation Loss: 2.315675\n",
      "Validation loss decreased (2.315740 --> 2.315675).  Saving model ...\n",
      "Epoch: 8083 \tTraining Loss: 2.063784 \tValidation Loss: 2.315662\n",
      "Validation loss decreased (2.315675 --> 2.315662).  Saving model ...\n",
      "Epoch: 8084 \tTraining Loss: 2.068191 \tValidation Loss: 2.315883\n",
      "Epoch: 8085 \tTraining Loss: 2.048947 \tValidation Loss: 2.316097\n",
      "Epoch: 8086 \tTraining Loss: 2.050586 \tValidation Loss: 2.315972\n",
      "Epoch: 8087 \tTraining Loss: 2.076513 \tValidation Loss: 2.316166\n",
      "Epoch: 8088 \tTraining Loss: 2.061209 \tValidation Loss: 2.315952\n",
      "Epoch: 8089 \tTraining Loss: 2.030062 \tValidation Loss: 2.315854\n",
      "Epoch: 8090 \tTraining Loss: 2.052609 \tValidation Loss: 2.316058\n",
      "Epoch: 8091 \tTraining Loss: 2.075627 \tValidation Loss: 2.316001\n",
      "Epoch: 8092 \tTraining Loss: 2.064005 \tValidation Loss: 2.315763\n",
      "Epoch: 8093 \tTraining Loss: 2.039949 \tValidation Loss: 2.315892\n",
      "Epoch: 8094 \tTraining Loss: 2.066921 \tValidation Loss: 2.315976\n",
      "Epoch: 8095 \tTraining Loss: 2.084892 \tValidation Loss: 2.315913\n",
      "Epoch: 8096 \tTraining Loss: 2.041450 \tValidation Loss: 2.315829\n",
      "Epoch: 8097 \tTraining Loss: 2.089939 \tValidation Loss: 2.315905\n",
      "Epoch: 8098 \tTraining Loss: 2.089341 \tValidation Loss: 2.315884\n",
      "Epoch: 8099 \tTraining Loss: 2.055146 \tValidation Loss: 2.315646\n",
      "Validation loss decreased (2.315662 --> 2.315646).  Saving model ...\n",
      "Epoch: 8100 \tTraining Loss: 2.059639 \tValidation Loss: 2.315477\n",
      "Validation loss decreased (2.315646 --> 2.315477).  Saving model ...\n",
      "Epoch: 8101 \tTraining Loss: 2.047297 \tValidation Loss: 2.315441\n",
      "Validation loss decreased (2.315477 --> 2.315441).  Saving model ...\n",
      "Epoch: 8102 \tTraining Loss: 2.060850 \tValidation Loss: 2.315136\n",
      "Validation loss decreased (2.315441 --> 2.315136).  Saving model ...\n",
      "Epoch: 8103 \tTraining Loss: 2.090465 \tValidation Loss: 2.315315\n",
      "Epoch: 8104 \tTraining Loss: 2.054687 \tValidation Loss: 2.315419\n",
      "Epoch: 8105 \tTraining Loss: 2.096039 \tValidation Loss: 2.315244\n",
      "Epoch: 8106 \tTraining Loss: 2.023647 \tValidation Loss: 2.315269\n",
      "Epoch: 8107 \tTraining Loss: 2.060969 \tValidation Loss: 2.315519\n",
      "Epoch: 8108 \tTraining Loss: 2.039486 \tValidation Loss: 2.315201\n",
      "Epoch: 8109 \tTraining Loss: 2.059113 \tValidation Loss: 2.315265\n",
      "Epoch: 8110 \tTraining Loss: 2.080680 \tValidation Loss: 2.315341\n",
      "Epoch: 8111 \tTraining Loss: 2.078534 \tValidation Loss: 2.315682\n",
      "Epoch: 8112 \tTraining Loss: 2.034476 \tValidation Loss: 2.315538\n",
      "Epoch: 8113 \tTraining Loss: 2.067453 \tValidation Loss: 2.315382\n",
      "Epoch: 8114 \tTraining Loss: 2.049581 \tValidation Loss: 2.315423\n",
      "Epoch: 8115 \tTraining Loss: 2.040503 \tValidation Loss: 2.315274\n",
      "Epoch: 8116 \tTraining Loss: 2.037521 \tValidation Loss: 2.315501\n",
      "Epoch: 8117 \tTraining Loss: 2.070562 \tValidation Loss: 2.315463\n",
      "Epoch: 8118 \tTraining Loss: 2.048983 \tValidation Loss: 2.315110\n",
      "Validation loss decreased (2.315136 --> 2.315110).  Saving model ...\n",
      "Epoch: 8119 \tTraining Loss: 2.049899 \tValidation Loss: 2.315239\n",
      "Epoch: 8120 \tTraining Loss: 2.062321 \tValidation Loss: 2.315156\n",
      "Epoch: 8121 \tTraining Loss: 2.042302 \tValidation Loss: 2.315112\n",
      "Epoch: 8122 \tTraining Loss: 2.060988 \tValidation Loss: 2.315175\n",
      "Epoch: 8123 \tTraining Loss: 2.040774 \tValidation Loss: 2.314898\n",
      "Validation loss decreased (2.315110 --> 2.314898).  Saving model ...\n",
      "Epoch: 8124 \tTraining Loss: 2.049606 \tValidation Loss: 2.315058\n",
      "Epoch: 8125 \tTraining Loss: 2.055642 \tValidation Loss: 2.315220\n",
      "Epoch: 8126 \tTraining Loss: 2.047480 \tValidation Loss: 2.315798\n",
      "Epoch: 8127 \tTraining Loss: 2.070157 \tValidation Loss: 2.315963\n",
      "Epoch: 8128 \tTraining Loss: 2.068272 \tValidation Loss: 2.315749\n",
      "Epoch: 8129 \tTraining Loss: 2.060705 \tValidation Loss: 2.315664\n",
      "Epoch: 8130 \tTraining Loss: 2.075068 \tValidation Loss: 2.315573\n",
      "Epoch: 8131 \tTraining Loss: 2.043426 \tValidation Loss: 2.315691\n",
      "Epoch: 8132 \tTraining Loss: 2.051864 \tValidation Loss: 2.315872\n",
      "Epoch: 8133 \tTraining Loss: 2.057626 \tValidation Loss: 2.315879\n",
      "Epoch: 8134 \tTraining Loss: 2.051717 \tValidation Loss: 2.315773\n",
      "Epoch: 8135 \tTraining Loss: 2.056868 \tValidation Loss: 2.315822\n",
      "Epoch: 8136 \tTraining Loss: 2.034936 \tValidation Loss: 2.315716\n",
      "Epoch: 8137 \tTraining Loss: 2.060180 \tValidation Loss: 2.315560\n",
      "Epoch: 8138 \tTraining Loss: 2.079795 \tValidation Loss: 2.315586\n",
      "Epoch: 8139 \tTraining Loss: 2.035220 \tValidation Loss: 2.315983\n",
      "Epoch: 8140 \tTraining Loss: 2.039305 \tValidation Loss: 2.315860\n",
      "Epoch: 8141 \tTraining Loss: 2.073972 \tValidation Loss: 2.315930\n",
      "Epoch: 8142 \tTraining Loss: 2.053406 \tValidation Loss: 2.315804\n",
      "Epoch: 8143 \tTraining Loss: 2.029692 \tValidation Loss: 2.315357\n",
      "Epoch: 8144 \tTraining Loss: 2.070906 \tValidation Loss: 2.315315\n",
      "Epoch: 8145 \tTraining Loss: 2.061300 \tValidation Loss: 2.315008\n",
      "Epoch: 8146 \tTraining Loss: 2.033096 \tValidation Loss: 2.315354\n",
      "Epoch: 8147 \tTraining Loss: 2.073116 \tValidation Loss: 2.315527\n",
      "Epoch: 8148 \tTraining Loss: 2.040679 \tValidation Loss: 2.315327\n",
      "Epoch: 8149 \tTraining Loss: 2.069989 \tValidation Loss: 2.315345\n",
      "Epoch: 8150 \tTraining Loss: 2.064976 \tValidation Loss: 2.315330\n",
      "Epoch: 8151 \tTraining Loss: 2.066266 \tValidation Loss: 2.315421\n",
      "Epoch: 8152 \tTraining Loss: 2.036374 \tValidation Loss: 2.315408\n",
      "Epoch: 8153 \tTraining Loss: 2.066365 \tValidation Loss: 2.315682\n",
      "Epoch: 8154 \tTraining Loss: 2.038383 \tValidation Loss: 2.315647\n",
      "Epoch: 8155 \tTraining Loss: 2.077923 \tValidation Loss: 2.315637\n",
      "Epoch: 8156 \tTraining Loss: 2.046055 \tValidation Loss: 2.315629\n",
      "Epoch: 8157 \tTraining Loss: 2.080253 \tValidation Loss: 2.315862\n",
      "Epoch: 8158 \tTraining Loss: 2.039417 \tValidation Loss: 2.316319\n",
      "Epoch: 8159 \tTraining Loss: 2.020882 \tValidation Loss: 2.316321\n",
      "Epoch: 8160 \tTraining Loss: 2.039809 \tValidation Loss: 2.315907\n",
      "Epoch: 8161 \tTraining Loss: 2.046407 \tValidation Loss: 2.315528\n",
      "Epoch: 8162 \tTraining Loss: 2.062235 \tValidation Loss: 2.315634\n",
      "Epoch: 8163 \tTraining Loss: 2.047691 \tValidation Loss: 2.315465\n",
      "Epoch: 8164 \tTraining Loss: 2.043522 \tValidation Loss: 2.315570\n",
      "Epoch: 8165 \tTraining Loss: 2.046764 \tValidation Loss: 2.315394\n",
      "Epoch: 8166 \tTraining Loss: 2.053037 \tValidation Loss: 2.315056\n",
      "Epoch: 8167 \tTraining Loss: 2.075751 \tValidation Loss: 2.315156\n",
      "Epoch: 8168 \tTraining Loss: 2.042787 \tValidation Loss: 2.314789\n",
      "Validation loss decreased (2.314898 --> 2.314789).  Saving model ...\n",
      "Epoch: 8169 \tTraining Loss: 2.040718 \tValidation Loss: 2.314713\n",
      "Validation loss decreased (2.314789 --> 2.314713).  Saving model ...\n",
      "Epoch: 8170 \tTraining Loss: 2.080224 \tValidation Loss: 2.314751\n",
      "Epoch: 8171 \tTraining Loss: 2.040288 \tValidation Loss: 2.314870\n",
      "Epoch: 8172 \tTraining Loss: 2.047392 \tValidation Loss: 2.314740\n",
      "Epoch: 8173 \tTraining Loss: 2.079893 \tValidation Loss: 2.314885\n",
      "Epoch: 8174 \tTraining Loss: 2.043739 \tValidation Loss: 2.315211\n",
      "Epoch: 8175 \tTraining Loss: 2.069063 \tValidation Loss: 2.314960\n",
      "Epoch: 8176 \tTraining Loss: 2.071044 \tValidation Loss: 2.314847\n",
      "Epoch: 8177 \tTraining Loss: 2.073202 \tValidation Loss: 2.314714\n",
      "Epoch: 8178 \tTraining Loss: 2.053837 \tValidation Loss: 2.314502\n",
      "Validation loss decreased (2.314713 --> 2.314502).  Saving model ...\n",
      "Epoch: 8179 \tTraining Loss: 2.041277 \tValidation Loss: 2.314250\n",
      "Validation loss decreased (2.314502 --> 2.314250).  Saving model ...\n",
      "Epoch: 8180 \tTraining Loss: 2.041157 \tValidation Loss: 2.314430\n",
      "Epoch: 8181 \tTraining Loss: 2.044500 \tValidation Loss: 2.314520\n",
      "Epoch: 8182 \tTraining Loss: 2.055739 \tValidation Loss: 2.314352\n",
      "Epoch: 8183 \tTraining Loss: 2.101784 \tValidation Loss: 2.314351\n",
      "Epoch: 8184 \tTraining Loss: 2.073607 \tValidation Loss: 2.314482\n",
      "Epoch: 8185 \tTraining Loss: 2.056587 \tValidation Loss: 2.314518\n",
      "Epoch: 8186 \tTraining Loss: 2.052944 \tValidation Loss: 2.314948\n",
      "Epoch: 8187 \tTraining Loss: 2.031038 \tValidation Loss: 2.314919\n",
      "Epoch: 8188 \tTraining Loss: 2.066296 \tValidation Loss: 2.315255\n",
      "Epoch: 8189 \tTraining Loss: 2.070335 \tValidation Loss: 2.315301\n",
      "Epoch: 8190 \tTraining Loss: 2.053006 \tValidation Loss: 2.314977\n",
      "Epoch: 8191 \tTraining Loss: 2.055449 \tValidation Loss: 2.314944\n",
      "Epoch: 8192 \tTraining Loss: 2.044138 \tValidation Loss: 2.315006\n",
      "Epoch: 8193 \tTraining Loss: 2.060717 \tValidation Loss: 2.314860\n",
      "Epoch: 8194 \tTraining Loss: 2.047240 \tValidation Loss: 2.314619\n",
      "Epoch: 8195 \tTraining Loss: 2.044475 \tValidation Loss: 2.314725\n",
      "Epoch: 8196 \tTraining Loss: 2.071983 \tValidation Loss: 2.314621\n",
      "Epoch: 8197 \tTraining Loss: 2.021517 \tValidation Loss: 2.314857\n",
      "Epoch: 8198 \tTraining Loss: 2.042269 \tValidation Loss: 2.314921\n",
      "Epoch: 8199 \tTraining Loss: 2.055766 \tValidation Loss: 2.314633\n",
      "Epoch: 8200 \tTraining Loss: 2.057804 \tValidation Loss: 2.314841\n",
      "Epoch: 8201 \tTraining Loss: 2.067940 \tValidation Loss: 2.314806\n",
      "Epoch: 8202 \tTraining Loss: 2.044190 \tValidation Loss: 2.314635\n",
      "Epoch: 8203 \tTraining Loss: 2.052485 \tValidation Loss: 2.314514\n",
      "Epoch: 8204 \tTraining Loss: 2.069699 \tValidation Loss: 2.314519\n",
      "Epoch: 8205 \tTraining Loss: 2.031581 \tValidation Loss: 2.314333\n",
      "Epoch: 8206 \tTraining Loss: 2.063673 \tValidation Loss: 2.314260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8207 \tTraining Loss: 2.056652 \tValidation Loss: 2.314417\n",
      "Epoch: 8208 \tTraining Loss: 2.050896 \tValidation Loss: 2.314342\n",
      "Epoch: 8209 \tTraining Loss: 2.047585 \tValidation Loss: 2.314720\n",
      "Epoch: 8210 \tTraining Loss: 2.048580 \tValidation Loss: 2.314743\n",
      "Epoch: 8211 \tTraining Loss: 2.064603 \tValidation Loss: 2.315209\n",
      "Epoch: 8212 \tTraining Loss: 2.049249 \tValidation Loss: 2.315235\n",
      "Epoch: 8213 \tTraining Loss: 2.045174 \tValidation Loss: 2.315217\n",
      "Epoch: 8214 \tTraining Loss: 2.069672 \tValidation Loss: 2.315187\n",
      "Epoch: 8215 \tTraining Loss: 2.050988 \tValidation Loss: 2.315135\n",
      "Epoch: 8216 \tTraining Loss: 2.084122 \tValidation Loss: 2.315184\n",
      "Epoch: 8217 \tTraining Loss: 2.059756 \tValidation Loss: 2.314791\n",
      "Epoch: 8218 \tTraining Loss: 2.051796 \tValidation Loss: 2.314825\n",
      "Epoch: 8219 \tTraining Loss: 2.042121 \tValidation Loss: 2.314601\n",
      "Epoch: 8220 \tTraining Loss: 2.040617 \tValidation Loss: 2.314629\n",
      "Epoch: 8221 \tTraining Loss: 2.043039 \tValidation Loss: 2.314405\n",
      "Epoch: 8222 \tTraining Loss: 2.040220 \tValidation Loss: 2.314680\n",
      "Epoch: 8223 \tTraining Loss: 2.060210 \tValidation Loss: 2.314876\n",
      "Epoch: 8224 \tTraining Loss: 2.036132 \tValidation Loss: 2.315038\n",
      "Epoch: 8225 \tTraining Loss: 2.074137 \tValidation Loss: 2.315121\n",
      "Epoch: 8226 \tTraining Loss: 2.048925 \tValidation Loss: 2.314904\n",
      "Epoch: 8227 \tTraining Loss: 2.036041 \tValidation Loss: 2.314858\n",
      "Epoch: 8228 \tTraining Loss: 2.032223 \tValidation Loss: 2.314730\n",
      "Epoch: 8229 \tTraining Loss: 2.022559 \tValidation Loss: 2.314669\n",
      "Epoch: 8230 \tTraining Loss: 2.054455 \tValidation Loss: 2.314759\n",
      "Epoch: 8231 \tTraining Loss: 2.037101 \tValidation Loss: 2.314919\n",
      "Epoch: 8232 \tTraining Loss: 2.040859 \tValidation Loss: 2.314815\n",
      "Epoch: 8233 \tTraining Loss: 2.037775 \tValidation Loss: 2.314696\n",
      "Epoch: 8234 \tTraining Loss: 2.044406 \tValidation Loss: 2.314858\n",
      "Epoch: 8235 \tTraining Loss: 2.053942 \tValidation Loss: 2.315048\n",
      "Epoch: 8236 \tTraining Loss: 2.049194 \tValidation Loss: 2.314920\n",
      "Epoch: 8237 \tTraining Loss: 2.071167 \tValidation Loss: 2.315215\n",
      "Epoch: 8238 \tTraining Loss: 2.054874 \tValidation Loss: 2.315146\n",
      "Epoch: 8239 \tTraining Loss: 2.056159 \tValidation Loss: 2.314985\n",
      "Epoch: 8240 \tTraining Loss: 2.059998 \tValidation Loss: 2.315159\n",
      "Epoch: 8241 \tTraining Loss: 2.070345 \tValidation Loss: 2.315191\n",
      "Epoch: 8242 \tTraining Loss: 2.052033 \tValidation Loss: 2.315382\n",
      "Epoch: 8243 \tTraining Loss: 2.055152 \tValidation Loss: 2.315417\n",
      "Epoch: 8244 \tTraining Loss: 2.036845 \tValidation Loss: 2.315250\n",
      "Epoch: 8245 \tTraining Loss: 2.042002 \tValidation Loss: 2.314889\n",
      "Epoch: 8246 \tTraining Loss: 2.057073 \tValidation Loss: 2.314607\n",
      "Epoch: 8247 \tTraining Loss: 2.044904 \tValidation Loss: 2.314489\n",
      "Epoch: 8248 \tTraining Loss: 2.030959 \tValidation Loss: 2.314633\n",
      "Epoch: 8249 \tTraining Loss: 2.050395 \tValidation Loss: 2.314851\n",
      "Epoch: 8250 \tTraining Loss: 2.047195 \tValidation Loss: 2.314377\n",
      "Epoch: 8251 \tTraining Loss: 2.034728 \tValidation Loss: 2.314212\n",
      "Validation loss decreased (2.314250 --> 2.314212).  Saving model ...\n",
      "Epoch: 8252 \tTraining Loss: 2.043326 \tValidation Loss: 2.314346\n",
      "Epoch: 8253 \tTraining Loss: 2.016093 \tValidation Loss: 2.314430\n",
      "Epoch: 8254 \tTraining Loss: 2.077352 \tValidation Loss: 2.314656\n",
      "Epoch: 8255 \tTraining Loss: 2.046148 \tValidation Loss: 2.314220\n",
      "Epoch: 8256 \tTraining Loss: 2.051909 \tValidation Loss: 2.314037\n",
      "Validation loss decreased (2.314212 --> 2.314037).  Saving model ...\n",
      "Epoch: 8257 \tTraining Loss: 2.071579 \tValidation Loss: 2.314473\n",
      "Epoch: 8258 \tTraining Loss: 2.056327 \tValidation Loss: 2.314791\n",
      "Epoch: 8259 \tTraining Loss: 2.078950 \tValidation Loss: 2.314740\n",
      "Epoch: 8260 \tTraining Loss: 2.049886 \tValidation Loss: 2.315150\n",
      "Epoch: 8261 \tTraining Loss: 2.061323 \tValidation Loss: 2.315016\n",
      "Epoch: 8262 \tTraining Loss: 2.037493 \tValidation Loss: 2.315225\n",
      "Epoch: 8263 \tTraining Loss: 2.040773 \tValidation Loss: 2.315379\n",
      "Epoch: 8264 \tTraining Loss: 2.048925 \tValidation Loss: 2.315244\n",
      "Epoch: 8265 \tTraining Loss: 2.050839 \tValidation Loss: 2.315184\n",
      "Epoch: 8266 \tTraining Loss: 2.039704 \tValidation Loss: 2.315182\n",
      "Epoch: 8267 \tTraining Loss: 2.055256 \tValidation Loss: 2.315164\n",
      "Epoch: 8268 \tTraining Loss: 2.037736 \tValidation Loss: 2.315229\n",
      "Epoch: 8269 \tTraining Loss: 2.037260 \tValidation Loss: 2.315189\n",
      "Epoch: 8270 \tTraining Loss: 2.032465 \tValidation Loss: 2.315167\n",
      "Epoch: 8271 \tTraining Loss: 2.050597 \tValidation Loss: 2.315073\n",
      "Epoch: 8272 \tTraining Loss: 2.029534 \tValidation Loss: 2.314907\n",
      "Epoch: 8273 \tTraining Loss: 2.045111 \tValidation Loss: 2.314732\n",
      "Epoch: 8274 \tTraining Loss: 2.061800 \tValidation Loss: 2.314464\n",
      "Epoch: 8275 \tTraining Loss: 2.067314 \tValidation Loss: 2.314455\n",
      "Epoch: 8276 \tTraining Loss: 2.058140 \tValidation Loss: 2.314751\n",
      "Epoch: 8277 \tTraining Loss: 2.034505 \tValidation Loss: 2.314770\n",
      "Epoch: 8278 \tTraining Loss: 2.052609 \tValidation Loss: 2.314313\n",
      "Epoch: 8279 \tTraining Loss: 2.048528 \tValidation Loss: 2.314267\n",
      "Epoch: 8280 \tTraining Loss: 2.040938 \tValidation Loss: 2.313968\n",
      "Validation loss decreased (2.314037 --> 2.313968).  Saving model ...\n",
      "Epoch: 8281 \tTraining Loss: 2.035866 \tValidation Loss: 2.313784\n",
      "Validation loss decreased (2.313968 --> 2.313784).  Saving model ...\n",
      "Epoch: 8282 \tTraining Loss: 2.061528 \tValidation Loss: 2.313801\n",
      "Epoch: 8283 \tTraining Loss: 2.028328 \tValidation Loss: 2.313759\n",
      "Validation loss decreased (2.313784 --> 2.313759).  Saving model ...\n",
      "Epoch: 8284 \tTraining Loss: 2.062433 \tValidation Loss: 2.313799\n",
      "Epoch: 8285 \tTraining Loss: 2.042308 \tValidation Loss: 2.313690\n",
      "Validation loss decreased (2.313759 --> 2.313690).  Saving model ...\n",
      "Epoch: 8286 \tTraining Loss: 2.065357 \tValidation Loss: 2.314044\n",
      "Epoch: 8287 \tTraining Loss: 2.052848 \tValidation Loss: 2.313730\n",
      "Epoch: 8288 \tTraining Loss: 2.047915 \tValidation Loss: 2.313901\n",
      "Epoch: 8289 \tTraining Loss: 2.061670 \tValidation Loss: 2.314369\n",
      "Epoch: 8290 \tTraining Loss: 2.049498 \tValidation Loss: 2.313968\n",
      "Epoch: 8291 \tTraining Loss: 2.029657 \tValidation Loss: 2.313943\n",
      "Epoch: 8292 \tTraining Loss: 2.040945 \tValidation Loss: 2.313834\n",
      "Epoch: 8293 \tTraining Loss: 2.093826 \tValidation Loss: 2.313790\n",
      "Epoch: 8294 \tTraining Loss: 2.056812 \tValidation Loss: 2.314009\n",
      "Epoch: 8295 \tTraining Loss: 2.078243 \tValidation Loss: 2.313849\n",
      "Epoch: 8296 \tTraining Loss: 2.054485 \tValidation Loss: 2.314030\n",
      "Epoch: 8297 \tTraining Loss: 2.015420 \tValidation Loss: 2.314213\n",
      "Epoch: 8298 \tTraining Loss: 2.072319 \tValidation Loss: 2.314078\n",
      "Epoch: 8299 \tTraining Loss: 2.039208 \tValidation Loss: 2.314548\n",
      "Epoch: 8300 \tTraining Loss: 2.061628 \tValidation Loss: 2.314775\n",
      "Epoch: 8301 \tTraining Loss: 2.038060 \tValidation Loss: 2.314951\n",
      "Epoch: 8302 \tTraining Loss: 2.051243 \tValidation Loss: 2.314739\n",
      "Epoch: 8303 \tTraining Loss: 2.056019 \tValidation Loss: 2.314650\n",
      "Epoch: 8304 \tTraining Loss: 2.007401 \tValidation Loss: 2.314882\n",
      "Epoch: 8305 \tTraining Loss: 2.017966 \tValidation Loss: 2.314595\n",
      "Epoch: 8306 \tTraining Loss: 2.053163 \tValidation Loss: 2.314267\n",
      "Epoch: 8307 \tTraining Loss: 2.036644 \tValidation Loss: 2.314101\n",
      "Epoch: 8308 \tTraining Loss: 2.037286 \tValidation Loss: 2.314228\n",
      "Epoch: 8309 \tTraining Loss: 2.043710 \tValidation Loss: 2.314047\n",
      "Epoch: 8310 \tTraining Loss: 2.060061 \tValidation Loss: 2.313760\n",
      "Epoch: 8311 \tTraining Loss: 2.079043 \tValidation Loss: 2.313912\n",
      "Epoch: 8312 \tTraining Loss: 2.051181 \tValidation Loss: 2.313778\n",
      "Epoch: 8313 \tTraining Loss: 2.065645 \tValidation Loss: 2.314252\n",
      "Epoch: 8314 \tTraining Loss: 2.037251 \tValidation Loss: 2.314443\n",
      "Epoch: 8315 \tTraining Loss: 2.068503 \tValidation Loss: 2.314517\n",
      "Epoch: 8316 \tTraining Loss: 2.039249 \tValidation Loss: 2.314590\n",
      "Epoch: 8317 \tTraining Loss: 2.056994 \tValidation Loss: 2.314384\n",
      "Epoch: 8318 \tTraining Loss: 2.049387 \tValidation Loss: 2.314025\n",
      "Epoch: 8319 \tTraining Loss: 2.045774 \tValidation Loss: 2.313723\n",
      "Epoch: 8320 \tTraining Loss: 2.034446 \tValidation Loss: 2.313714\n",
      "Epoch: 8321 \tTraining Loss: 2.039142 \tValidation Loss: 2.313272\n",
      "Validation loss decreased (2.313690 --> 2.313272).  Saving model ...\n",
      "Epoch: 8322 \tTraining Loss: 2.021857 \tValidation Loss: 2.313247\n",
      "Validation loss decreased (2.313272 --> 2.313247).  Saving model ...\n",
      "Epoch: 8323 \tTraining Loss: 2.039732 \tValidation Loss: 2.313421\n",
      "Epoch: 8324 \tTraining Loss: 2.032609 \tValidation Loss: 2.313367\n",
      "Epoch: 8325 \tTraining Loss: 2.066071 \tValidation Loss: 2.313380\n",
      "Epoch: 8326 \tTraining Loss: 2.046659 \tValidation Loss: 2.313419\n",
      "Epoch: 8327 \tTraining Loss: 2.031115 \tValidation Loss: 2.313415\n",
      "Epoch: 8328 \tTraining Loss: 2.062953 \tValidation Loss: 2.313355\n",
      "Epoch: 8329 \tTraining Loss: 2.040287 \tValidation Loss: 2.313397\n",
      "Epoch: 8330 \tTraining Loss: 2.068030 \tValidation Loss: 2.313326\n",
      "Epoch: 8331 \tTraining Loss: 2.056355 \tValidation Loss: 2.313339\n",
      "Epoch: 8332 \tTraining Loss: 2.036894 \tValidation Loss: 2.313373\n",
      "Epoch: 8333 \tTraining Loss: 2.037599 \tValidation Loss: 2.313827\n",
      "Epoch: 8334 \tTraining Loss: 2.031268 \tValidation Loss: 2.313962\n",
      "Epoch: 8335 \tTraining Loss: 2.080484 \tValidation Loss: 2.313933\n",
      "Epoch: 8336 \tTraining Loss: 2.034200 \tValidation Loss: 2.313779\n",
      "Epoch: 8337 \tTraining Loss: 2.037064 \tValidation Loss: 2.314097\n",
      "Epoch: 8338 \tTraining Loss: 2.018629 \tValidation Loss: 2.314357\n",
      "Epoch: 8339 \tTraining Loss: 2.023717 \tValidation Loss: 2.314480\n",
      "Epoch: 8340 \tTraining Loss: 2.065130 \tValidation Loss: 2.314797\n",
      "Epoch: 8341 \tTraining Loss: 2.059105 \tValidation Loss: 2.314670\n",
      "Epoch: 8342 \tTraining Loss: 2.054667 \tValidation Loss: 2.314696\n",
      "Epoch: 8343 \tTraining Loss: 2.046303 \tValidation Loss: 2.314799\n",
      "Epoch: 8344 \tTraining Loss: 2.065298 \tValidation Loss: 2.314633\n",
      "Epoch: 8345 \tTraining Loss: 2.056659 \tValidation Loss: 2.314840\n",
      "Epoch: 8346 \tTraining Loss: 2.055766 \tValidation Loss: 2.314912\n",
      "Epoch: 8347 \tTraining Loss: 2.065240 \tValidation Loss: 2.315043\n",
      "Epoch: 8348 \tTraining Loss: 2.029095 \tValidation Loss: 2.315129\n",
      "Epoch: 8349 \tTraining Loss: 2.052609 \tValidation Loss: 2.315063\n",
      "Epoch: 8350 \tTraining Loss: 2.039438 \tValidation Loss: 2.314928\n",
      "Epoch: 8351 \tTraining Loss: 2.036114 \tValidation Loss: 2.314956\n",
      "Epoch: 8352 \tTraining Loss: 2.047721 \tValidation Loss: 2.314755\n",
      "Epoch: 8353 \tTraining Loss: 2.055847 \tValidation Loss: 2.314371\n",
      "Epoch: 8354 \tTraining Loss: 2.055369 \tValidation Loss: 2.314237\n",
      "Epoch: 8355 \tTraining Loss: 2.034985 \tValidation Loss: 2.313963\n",
      "Epoch: 8356 \tTraining Loss: 2.042032 \tValidation Loss: 2.313860\n",
      "Epoch: 8357 \tTraining Loss: 2.040135 \tValidation Loss: 2.314205\n",
      "Epoch: 8358 \tTraining Loss: 2.047098 \tValidation Loss: 2.314012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8359 \tTraining Loss: 2.028596 \tValidation Loss: 2.313868\n",
      "Epoch: 8360 \tTraining Loss: 2.038165 \tValidation Loss: 2.314211\n",
      "Epoch: 8361 \tTraining Loss: 2.029944 \tValidation Loss: 2.313997\n",
      "Epoch: 8362 \tTraining Loss: 2.038059 \tValidation Loss: 2.313865\n",
      "Epoch: 8363 \tTraining Loss: 2.031965 \tValidation Loss: 2.313568\n",
      "Epoch: 8364 \tTraining Loss: 2.060894 \tValidation Loss: 2.313456\n",
      "Epoch: 8365 \tTraining Loss: 2.057566 \tValidation Loss: 2.313527\n",
      "Epoch: 8366 \tTraining Loss: 2.037373 \tValidation Loss: 2.313498\n",
      "Epoch: 8367 \tTraining Loss: 2.036773 \tValidation Loss: 2.313554\n",
      "Epoch: 8368 \tTraining Loss: 2.047630 \tValidation Loss: 2.313538\n",
      "Epoch: 8369 \tTraining Loss: 2.031890 \tValidation Loss: 2.313867\n",
      "Epoch: 8370 \tTraining Loss: 2.059881 \tValidation Loss: 2.313583\n",
      "Epoch: 8371 \tTraining Loss: 2.031783 \tValidation Loss: 2.313514\n",
      "Epoch: 8372 \tTraining Loss: 2.050554 \tValidation Loss: 2.313306\n",
      "Epoch: 8373 \tTraining Loss: 2.019654 \tValidation Loss: 2.313221\n",
      "Validation loss decreased (2.313247 --> 2.313221).  Saving model ...\n",
      "Epoch: 8374 \tTraining Loss: 2.056036 \tValidation Loss: 2.312916\n",
      "Validation loss decreased (2.313221 --> 2.312916).  Saving model ...\n",
      "Epoch: 8375 \tTraining Loss: 2.053704 \tValidation Loss: 2.313377\n",
      "Epoch: 8376 \tTraining Loss: 2.025023 \tValidation Loss: 2.313136\n",
      "Epoch: 8377 \tTraining Loss: 2.048026 \tValidation Loss: 2.313253\n",
      "Epoch: 8378 \tTraining Loss: 2.026893 \tValidation Loss: 2.313442\n",
      "Epoch: 8379 \tTraining Loss: 2.068395 \tValidation Loss: 2.313545\n",
      "Epoch: 8380 \tTraining Loss: 2.033592 \tValidation Loss: 2.313357\n",
      "Epoch: 8381 \tTraining Loss: 2.041874 \tValidation Loss: 2.313301\n",
      "Epoch: 8382 \tTraining Loss: 2.068677 \tValidation Loss: 2.313180\n",
      "Epoch: 8383 \tTraining Loss: 2.022831 \tValidation Loss: 2.313365\n",
      "Epoch: 8384 \tTraining Loss: 2.037775 \tValidation Loss: 2.313193\n",
      "Epoch: 8385 \tTraining Loss: 2.058329 \tValidation Loss: 2.313596\n",
      "Epoch: 8386 \tTraining Loss: 2.056423 \tValidation Loss: 2.313995\n",
      "Epoch: 8387 \tTraining Loss: 2.051893 \tValidation Loss: 2.313941\n",
      "Epoch: 8388 \tTraining Loss: 2.021320 \tValidation Loss: 2.314183\n",
      "Epoch: 8389 \tTraining Loss: 2.025726 \tValidation Loss: 2.314019\n",
      "Epoch: 8390 \tTraining Loss: 2.038826 \tValidation Loss: 2.313779\n",
      "Epoch: 8391 \tTraining Loss: 2.045659 \tValidation Loss: 2.313450\n",
      "Epoch: 8392 \tTraining Loss: 2.066338 \tValidation Loss: 2.313569\n",
      "Epoch: 8393 \tTraining Loss: 2.035653 \tValidation Loss: 2.313316\n",
      "Epoch: 8394 \tTraining Loss: 2.045924 \tValidation Loss: 2.313364\n",
      "Epoch: 8395 \tTraining Loss: 2.051459 \tValidation Loss: 2.313309\n",
      "Epoch: 8396 \tTraining Loss: 2.018759 \tValidation Loss: 2.313174\n",
      "Epoch: 8397 \tTraining Loss: 2.036635 \tValidation Loss: 2.312922\n",
      "Epoch: 8398 \tTraining Loss: 2.053489 \tValidation Loss: 2.312797\n",
      "Validation loss decreased (2.312916 --> 2.312797).  Saving model ...\n",
      "Epoch: 8399 \tTraining Loss: 2.042125 \tValidation Loss: 2.312592\n",
      "Validation loss decreased (2.312797 --> 2.312592).  Saving model ...\n",
      "Epoch: 8400 \tTraining Loss: 2.065488 \tValidation Loss: 2.312595\n",
      "Epoch: 8401 \tTraining Loss: 2.053135 \tValidation Loss: 2.312915\n",
      "Epoch: 8402 \tTraining Loss: 2.037376 \tValidation Loss: 2.312890\n",
      "Epoch: 8403 \tTraining Loss: 2.042211 \tValidation Loss: 2.312957\n",
      "Epoch: 8404 \tTraining Loss: 2.025333 \tValidation Loss: 2.312839\n",
      "Epoch: 8405 \tTraining Loss: 2.047277 \tValidation Loss: 2.312830\n",
      "Epoch: 8406 \tTraining Loss: 2.046326 \tValidation Loss: 2.313056\n",
      "Epoch: 8407 \tTraining Loss: 2.032487 \tValidation Loss: 2.313120\n",
      "Epoch: 8408 \tTraining Loss: 2.076775 \tValidation Loss: 2.313221\n",
      "Epoch: 8409 \tTraining Loss: 2.019056 \tValidation Loss: 2.313464\n",
      "Epoch: 8410 \tTraining Loss: 2.039992 \tValidation Loss: 2.313205\n",
      "Epoch: 8411 \tTraining Loss: 2.036150 \tValidation Loss: 2.313097\n",
      "Epoch: 8412 \tTraining Loss: 2.058686 \tValidation Loss: 2.313014\n",
      "Epoch: 8413 \tTraining Loss: 2.030804 \tValidation Loss: 2.312950\n",
      "Epoch: 8414 \tTraining Loss: 2.019212 \tValidation Loss: 2.312787\n",
      "Epoch: 8415 \tTraining Loss: 2.044181 \tValidation Loss: 2.312652\n",
      "Epoch: 8416 \tTraining Loss: 2.070734 \tValidation Loss: 2.312659\n",
      "Epoch: 8417 \tTraining Loss: 2.028557 \tValidation Loss: 2.312934\n",
      "Epoch: 8418 \tTraining Loss: 2.056165 \tValidation Loss: 2.312898\n",
      "Epoch: 8419 \tTraining Loss: 2.036267 \tValidation Loss: 2.313448\n",
      "Epoch: 8420 \tTraining Loss: 2.039855 \tValidation Loss: 2.313288\n",
      "Epoch: 8421 \tTraining Loss: 2.064533 \tValidation Loss: 2.313254\n",
      "Epoch: 8422 \tTraining Loss: 2.046613 \tValidation Loss: 2.312859\n",
      "Epoch: 8423 \tTraining Loss: 2.024545 \tValidation Loss: 2.312819\n",
      "Epoch: 8424 \tTraining Loss: 2.029519 \tValidation Loss: 2.313010\n",
      "Epoch: 8425 \tTraining Loss: 2.034211 \tValidation Loss: 2.312871\n",
      "Epoch: 8426 \tTraining Loss: 2.047611 \tValidation Loss: 2.312820\n",
      "Epoch: 8427 \tTraining Loss: 2.040764 \tValidation Loss: 2.312673\n",
      "Epoch: 8428 \tTraining Loss: 2.021411 \tValidation Loss: 2.312854\n",
      "Epoch: 8429 \tTraining Loss: 2.057598 \tValidation Loss: 2.312782\n",
      "Epoch: 8430 \tTraining Loss: 2.041572 \tValidation Loss: 2.312816\n",
      "Epoch: 8431 \tTraining Loss: 2.040522 \tValidation Loss: 2.312690\n",
      "Epoch: 8432 \tTraining Loss: 2.044261 \tValidation Loss: 2.312798\n",
      "Epoch: 8433 \tTraining Loss: 2.026935 \tValidation Loss: 2.312906\n",
      "Epoch: 8434 \tTraining Loss: 2.043106 \tValidation Loss: 2.312967\n",
      "Epoch: 8435 \tTraining Loss: 2.024526 \tValidation Loss: 2.312807\n",
      "Epoch: 8436 \tTraining Loss: 2.050279 \tValidation Loss: 2.312677\n",
      "Epoch: 8437 \tTraining Loss: 2.031328 \tValidation Loss: 2.312774\n",
      "Epoch: 8438 \tTraining Loss: 2.026228 \tValidation Loss: 2.312997\n",
      "Epoch: 8439 \tTraining Loss: 2.010929 \tValidation Loss: 2.312842\n",
      "Epoch: 8440 \tTraining Loss: 2.027714 \tValidation Loss: 2.313109\n",
      "Epoch: 8441 \tTraining Loss: 2.013755 \tValidation Loss: 2.313327\n",
      "Epoch: 8442 \tTraining Loss: 2.044545 \tValidation Loss: 2.313338\n",
      "Epoch: 8443 \tTraining Loss: 2.059914 \tValidation Loss: 2.313065\n",
      "Epoch: 8444 \tTraining Loss: 2.050296 \tValidation Loss: 2.313214\n",
      "Epoch: 8445 \tTraining Loss: 2.049855 \tValidation Loss: 2.313325\n",
      "Epoch: 8446 \tTraining Loss: 2.050652 \tValidation Loss: 2.313076\n",
      "Epoch: 8447 \tTraining Loss: 2.047967 \tValidation Loss: 2.313057\n",
      "Epoch: 8448 \tTraining Loss: 2.050118 \tValidation Loss: 2.313158\n",
      "Epoch: 8449 \tTraining Loss: 2.035079 \tValidation Loss: 2.313236\n",
      "Epoch: 8450 \tTraining Loss: 2.029840 \tValidation Loss: 2.313329\n",
      "Epoch: 8451 \tTraining Loss: 2.037846 \tValidation Loss: 2.313254\n",
      "Epoch: 8452 \tTraining Loss: 2.050844 \tValidation Loss: 2.313534\n",
      "Epoch: 8453 \tTraining Loss: 2.057276 \tValidation Loss: 2.313405\n",
      "Epoch: 8454 \tTraining Loss: 2.041155 \tValidation Loss: 2.313237\n",
      "Epoch: 8455 \tTraining Loss: 2.038154 \tValidation Loss: 2.313407\n",
      "Epoch: 8456 \tTraining Loss: 2.041861 \tValidation Loss: 2.313390\n",
      "Epoch: 8457 \tTraining Loss: 2.020908 \tValidation Loss: 2.313268\n",
      "Epoch: 8458 \tTraining Loss: 2.045569 \tValidation Loss: 2.313181\n",
      "Epoch: 8459 \tTraining Loss: 2.025487 \tValidation Loss: 2.312988\n",
      "Epoch: 8460 \tTraining Loss: 2.053594 \tValidation Loss: 2.312845\n",
      "Epoch: 8461 \tTraining Loss: 2.030238 \tValidation Loss: 2.313130\n",
      "Epoch: 8462 \tTraining Loss: 2.012987 \tValidation Loss: 2.313198\n",
      "Epoch: 8463 \tTraining Loss: 2.014789 \tValidation Loss: 2.313080\n",
      "Epoch: 8464 \tTraining Loss: 2.053208 \tValidation Loss: 2.313318\n",
      "Epoch: 8465 \tTraining Loss: 2.017006 \tValidation Loss: 2.313303\n",
      "Epoch: 8466 \tTraining Loss: 2.035116 \tValidation Loss: 2.313442\n",
      "Epoch: 8467 \tTraining Loss: 2.032389 \tValidation Loss: 2.313357\n",
      "Epoch: 8468 \tTraining Loss: 2.040151 \tValidation Loss: 2.313642\n",
      "Epoch: 8469 \tTraining Loss: 2.047427 \tValidation Loss: 2.313545\n",
      "Epoch: 8470 \tTraining Loss: 2.051484 \tValidation Loss: 2.313076\n",
      "Epoch: 8471 \tTraining Loss: 2.028502 \tValidation Loss: 2.313099\n",
      "Epoch: 8472 \tTraining Loss: 2.041684 \tValidation Loss: 2.312999\n",
      "Epoch: 8473 \tTraining Loss: 2.044525 \tValidation Loss: 2.313343\n",
      "Epoch: 8474 \tTraining Loss: 2.022931 \tValidation Loss: 2.313147\n",
      "Epoch: 8475 \tTraining Loss: 2.047995 \tValidation Loss: 2.313445\n",
      "Epoch: 8476 \tTraining Loss: 2.043340 \tValidation Loss: 2.313103\n",
      "Epoch: 8477 \tTraining Loss: 2.008769 \tValidation Loss: 2.313395\n",
      "Epoch: 8478 \tTraining Loss: 2.040322 \tValidation Loss: 2.313271\n",
      "Epoch: 8479 \tTraining Loss: 2.022929 \tValidation Loss: 2.313414\n",
      "Epoch: 8480 \tTraining Loss: 2.024122 \tValidation Loss: 2.313052\n",
      "Epoch: 8481 \tTraining Loss: 2.040510 \tValidation Loss: 2.312809\n",
      "Epoch: 8482 \tTraining Loss: 2.039284 \tValidation Loss: 2.313114\n",
      "Epoch: 8483 \tTraining Loss: 2.021617 \tValidation Loss: 2.313522\n",
      "Epoch: 8484 \tTraining Loss: 2.003915 \tValidation Loss: 2.313357\n",
      "Epoch: 8485 \tTraining Loss: 2.011660 \tValidation Loss: 2.313371\n",
      "Epoch: 8486 \tTraining Loss: 2.016165 \tValidation Loss: 2.313165\n",
      "Epoch: 8487 \tTraining Loss: 2.044973 \tValidation Loss: 2.313591\n",
      "Epoch: 8488 \tTraining Loss: 2.038413 \tValidation Loss: 2.313300\n",
      "Epoch: 8489 \tTraining Loss: 2.028049 \tValidation Loss: 2.313473\n",
      "Epoch: 8490 \tTraining Loss: 2.048383 \tValidation Loss: 2.313531\n",
      "Epoch: 8491 \tTraining Loss: 2.078648 \tValidation Loss: 2.313511\n",
      "Epoch: 8492 \tTraining Loss: 2.045016 \tValidation Loss: 2.313555\n",
      "Epoch: 8493 \tTraining Loss: 2.040193 \tValidation Loss: 2.313837\n",
      "Epoch: 8494 \tTraining Loss: 2.026390 \tValidation Loss: 2.313730\n",
      "Epoch: 8495 \tTraining Loss: 2.039715 \tValidation Loss: 2.312898\n",
      "Epoch: 8496 \tTraining Loss: 2.017459 \tValidation Loss: 2.312706\n",
      "Epoch: 8497 \tTraining Loss: 2.066534 \tValidation Loss: 2.312712\n",
      "Epoch: 8498 \tTraining Loss: 2.027255 \tValidation Loss: 2.312702\n",
      "Epoch: 8499 \tTraining Loss: 2.029832 \tValidation Loss: 2.312732\n",
      "Epoch: 8500 \tTraining Loss: 2.051564 \tValidation Loss: 2.312723\n",
      "Epoch: 8501 \tTraining Loss: 2.036458 \tValidation Loss: 2.312841\n",
      "Epoch: 8502 \tTraining Loss: 2.004650 \tValidation Loss: 2.312868\n",
      "Epoch: 8503 \tTraining Loss: 2.045500 \tValidation Loss: 2.312987\n",
      "Epoch: 8504 \tTraining Loss: 2.035146 \tValidation Loss: 2.312877\n",
      "Epoch: 8505 \tTraining Loss: 2.069785 \tValidation Loss: 2.312861\n",
      "Epoch: 8506 \tTraining Loss: 2.040472 \tValidation Loss: 2.312904\n",
      "Epoch: 8507 \tTraining Loss: 2.034414 \tValidation Loss: 2.312630\n",
      "Epoch: 8508 \tTraining Loss: 2.058946 \tValidation Loss: 2.312494\n",
      "Validation loss decreased (2.312592 --> 2.312494).  Saving model ...\n",
      "Epoch: 8509 \tTraining Loss: 2.030370 \tValidation Loss: 2.312721\n",
      "Epoch: 8510 \tTraining Loss: 2.035187 \tValidation Loss: 2.312616\n",
      "Epoch: 8511 \tTraining Loss: 2.039848 \tValidation Loss: 2.312558\n",
      "Epoch: 8512 \tTraining Loss: 2.040443 \tValidation Loss: 2.312835\n",
      "Epoch: 8513 \tTraining Loss: 2.046187 \tValidation Loss: 2.312854\n",
      "Epoch: 8514 \tTraining Loss: 2.016061 \tValidation Loss: 2.312983\n",
      "Epoch: 8515 \tTraining Loss: 2.020134 \tValidation Loss: 2.312788\n",
      "Epoch: 8516 \tTraining Loss: 2.027606 \tValidation Loss: 2.312547\n",
      "Epoch: 8517 \tTraining Loss: 2.049376 \tValidation Loss: 2.312543\n",
      "Epoch: 8518 \tTraining Loss: 2.060058 \tValidation Loss: 2.312579\n",
      "Epoch: 8519 \tTraining Loss: 2.017665 \tValidation Loss: 2.312491\n",
      "Validation loss decreased (2.312494 --> 2.312491).  Saving model ...\n",
      "Epoch: 8520 \tTraining Loss: 2.038800 \tValidation Loss: 2.312337\n",
      "Validation loss decreased (2.312491 --> 2.312337).  Saving model ...\n",
      "Epoch: 8521 \tTraining Loss: 2.028901 \tValidation Loss: 2.312523\n",
      "Epoch: 8522 \tTraining Loss: 2.036021 \tValidation Loss: 2.312711\n",
      "Epoch: 8523 \tTraining Loss: 2.024627 \tValidation Loss: 2.312896\n",
      "Epoch: 8524 \tTraining Loss: 2.031137 \tValidation Loss: 2.312829\n",
      "Epoch: 8525 \tTraining Loss: 2.037549 \tValidation Loss: 2.312976\n",
      "Epoch: 8526 \tTraining Loss: 2.015641 \tValidation Loss: 2.313000\n",
      "Epoch: 8527 \tTraining Loss: 2.038776 \tValidation Loss: 2.313175\n",
      "Epoch: 8528 \tTraining Loss: 2.039043 \tValidation Loss: 2.313022\n",
      "Epoch: 8529 \tTraining Loss: 2.051858 \tValidation Loss: 2.312674\n",
      "Epoch: 8530 \tTraining Loss: 2.014840 \tValidation Loss: 2.312626\n",
      "Epoch: 8531 \tTraining Loss: 2.041114 \tValidation Loss: 2.312597\n",
      "Epoch: 8532 \tTraining Loss: 2.047786 \tValidation Loss: 2.312836\n",
      "Epoch: 8533 \tTraining Loss: 2.068489 \tValidation Loss: 2.312899\n",
      "Epoch: 8534 \tTraining Loss: 2.034732 \tValidation Loss: 2.313219\n",
      "Epoch: 8535 \tTraining Loss: 2.025878 \tValidation Loss: 2.313323\n",
      "Epoch: 8536 \tTraining Loss: 2.061480 \tValidation Loss: 2.313309\n",
      "Epoch: 8537 \tTraining Loss: 2.028803 \tValidation Loss: 2.313761\n",
      "Epoch: 8538 \tTraining Loss: 2.026828 \tValidation Loss: 2.313306\n",
      "Epoch: 8539 \tTraining Loss: 2.021482 \tValidation Loss: 2.313428\n",
      "Epoch: 8540 \tTraining Loss: 2.020961 \tValidation Loss: 2.313420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8541 \tTraining Loss: 2.036150 \tValidation Loss: 2.313394\n",
      "Epoch: 8542 \tTraining Loss: 2.044666 \tValidation Loss: 2.313799\n",
      "Epoch: 8543 \tTraining Loss: 2.048076 \tValidation Loss: 2.313774\n",
      "Epoch: 8544 \tTraining Loss: 2.050049 \tValidation Loss: 2.314042\n",
      "Epoch: 8545 \tTraining Loss: 2.046753 \tValidation Loss: 2.314057\n",
      "Epoch: 8546 \tTraining Loss: 1.987764 \tValidation Loss: 2.313771\n",
      "Epoch: 8547 \tTraining Loss: 2.045835 \tValidation Loss: 2.313631\n",
      "Epoch: 8548 \tTraining Loss: 2.023042 \tValidation Loss: 2.313357\n",
      "Epoch: 8549 \tTraining Loss: 2.057378 \tValidation Loss: 2.313684\n",
      "Epoch: 8550 \tTraining Loss: 2.023218 \tValidation Loss: 2.313760\n",
      "Epoch: 8551 \tTraining Loss: 2.044397 \tValidation Loss: 2.313215\n",
      "Epoch: 8552 \tTraining Loss: 2.019051 \tValidation Loss: 2.313316\n",
      "Epoch: 8553 \tTraining Loss: 2.030871 \tValidation Loss: 2.313529\n",
      "Epoch: 8554 \tTraining Loss: 2.024703 \tValidation Loss: 2.313457\n",
      "Epoch: 8555 \tTraining Loss: 2.027128 \tValidation Loss: 2.313401\n",
      "Epoch: 8556 \tTraining Loss: 2.047919 \tValidation Loss: 2.313741\n",
      "Epoch: 8557 \tTraining Loss: 2.023860 \tValidation Loss: 2.313573\n",
      "Epoch: 8558 \tTraining Loss: 2.048582 \tValidation Loss: 2.313514\n",
      "Epoch: 8559 \tTraining Loss: 2.025523 \tValidation Loss: 2.313564\n",
      "Epoch: 8560 \tTraining Loss: 2.045545 \tValidation Loss: 2.313619\n",
      "Epoch: 8561 \tTraining Loss: 2.057556 \tValidation Loss: 2.313386\n",
      "Epoch: 8562 \tTraining Loss: 2.032406 \tValidation Loss: 2.313317\n",
      "Epoch: 8563 \tTraining Loss: 2.014539 \tValidation Loss: 2.313215\n",
      "Epoch: 8564 \tTraining Loss: 2.061377 \tValidation Loss: 2.313080\n",
      "Epoch: 8565 \tTraining Loss: 1.990559 \tValidation Loss: 2.313171\n",
      "Epoch: 8566 \tTraining Loss: 2.061165 \tValidation Loss: 2.313302\n",
      "Epoch: 8567 \tTraining Loss: 2.010426 \tValidation Loss: 2.312850\n",
      "Epoch: 8568 \tTraining Loss: 2.032090 \tValidation Loss: 2.312728\n",
      "Epoch: 8569 \tTraining Loss: 2.024424 \tValidation Loss: 2.313442\n",
      "Epoch: 8570 \tTraining Loss: 2.049204 \tValidation Loss: 2.313076\n",
      "Epoch: 8571 \tTraining Loss: 2.011005 \tValidation Loss: 2.313228\n",
      "Epoch: 8572 \tTraining Loss: 2.027851 \tValidation Loss: 2.313272\n",
      "Epoch: 8573 \tTraining Loss: 2.029331 \tValidation Loss: 2.313092\n",
      "Epoch: 8574 \tTraining Loss: 2.018786 \tValidation Loss: 2.312970\n",
      "Epoch: 8575 \tTraining Loss: 2.030532 \tValidation Loss: 2.312950\n",
      "Epoch: 8576 \tTraining Loss: 2.045789 \tValidation Loss: 2.312768\n",
      "Epoch: 8577 \tTraining Loss: 2.032675 \tValidation Loss: 2.312827\n",
      "Epoch: 8578 \tTraining Loss: 2.013504 \tValidation Loss: 2.312490\n",
      "Epoch: 8579 \tTraining Loss: 2.034076 \tValidation Loss: 2.312599\n",
      "Epoch: 8580 \tTraining Loss: 2.035990 \tValidation Loss: 2.312565\n",
      "Epoch: 8581 \tTraining Loss: 2.035350 \tValidation Loss: 2.312633\n",
      "Epoch: 8582 \tTraining Loss: 2.026533 \tValidation Loss: 2.312461\n",
      "Epoch: 8583 \tTraining Loss: 2.017631 \tValidation Loss: 2.312527\n",
      "Epoch: 8584 \tTraining Loss: 2.045273 \tValidation Loss: 2.312001\n",
      "Validation loss decreased (2.312337 --> 2.312001).  Saving model ...\n",
      "Epoch: 8585 \tTraining Loss: 2.035590 \tValidation Loss: 2.312104\n",
      "Epoch: 8586 \tTraining Loss: 2.024726 \tValidation Loss: 2.312211\n",
      "Epoch: 8587 \tTraining Loss: 2.041440 \tValidation Loss: 2.312036\n",
      "Epoch: 8588 \tTraining Loss: 2.025045 \tValidation Loss: 2.311930\n",
      "Validation loss decreased (2.312001 --> 2.311930).  Saving model ...\n",
      "Epoch: 8589 \tTraining Loss: 2.023685 \tValidation Loss: 2.311815\n",
      "Validation loss decreased (2.311930 --> 2.311815).  Saving model ...\n",
      "Epoch: 8590 \tTraining Loss: 2.032840 \tValidation Loss: 2.311758\n",
      "Validation loss decreased (2.311815 --> 2.311758).  Saving model ...\n",
      "Epoch: 8591 \tTraining Loss: 2.021620 \tValidation Loss: 2.311831\n",
      "Epoch: 8592 \tTraining Loss: 2.023421 \tValidation Loss: 2.312018\n",
      "Epoch: 8593 \tTraining Loss: 2.024203 \tValidation Loss: 2.311996\n",
      "Epoch: 8594 \tTraining Loss: 2.049308 \tValidation Loss: 2.312228\n",
      "Epoch: 8595 \tTraining Loss: 2.027115 \tValidation Loss: 2.312095\n",
      "Epoch: 8596 \tTraining Loss: 2.048668 \tValidation Loss: 2.311909\n",
      "Epoch: 8597 \tTraining Loss: 2.041077 \tValidation Loss: 2.311801\n",
      "Epoch: 8598 \tTraining Loss: 2.032415 \tValidation Loss: 2.311872\n",
      "Epoch: 8599 \tTraining Loss: 2.024962 \tValidation Loss: 2.311768\n",
      "Epoch: 8600 \tTraining Loss: 2.024497 \tValidation Loss: 2.312032\n",
      "Epoch: 8601 \tTraining Loss: 2.018121 \tValidation Loss: 2.312049\n",
      "Epoch: 8602 \tTraining Loss: 2.043898 \tValidation Loss: 2.312200\n",
      "Epoch: 8603 \tTraining Loss: 2.023234 \tValidation Loss: 2.312337\n",
      "Epoch: 8604 \tTraining Loss: 2.009361 \tValidation Loss: 2.312551\n",
      "Epoch: 8605 \tTraining Loss: 2.050226 \tValidation Loss: 2.312011\n",
      "Epoch: 8606 \tTraining Loss: 2.038164 \tValidation Loss: 2.311846\n",
      "Epoch: 8607 \tTraining Loss: 2.030740 \tValidation Loss: 2.312203\n",
      "Epoch: 8608 \tTraining Loss: 2.035135 \tValidation Loss: 2.312389\n",
      "Epoch: 8609 \tTraining Loss: 2.024966 \tValidation Loss: 2.312474\n",
      "Epoch: 8610 \tTraining Loss: 2.024903 \tValidation Loss: 2.312491\n",
      "Epoch: 8611 \tTraining Loss: 2.044459 \tValidation Loss: 2.312481\n",
      "Epoch: 8612 \tTraining Loss: 2.040017 \tValidation Loss: 2.312343\n",
      "Epoch: 8613 \tTraining Loss: 2.002490 \tValidation Loss: 2.311804\n",
      "Epoch: 8614 \tTraining Loss: 2.038397 \tValidation Loss: 2.311829\n",
      "Epoch: 8615 \tTraining Loss: 2.046316 \tValidation Loss: 2.311773\n",
      "Epoch: 8616 \tTraining Loss: 2.064000 \tValidation Loss: 2.312018\n",
      "Epoch: 8617 \tTraining Loss: 2.012117 \tValidation Loss: 2.311378\n",
      "Validation loss decreased (2.311758 --> 2.311378).  Saving model ...\n",
      "Epoch: 8618 \tTraining Loss: 2.026321 \tValidation Loss: 2.311965\n",
      "Epoch: 8619 \tTraining Loss: 2.047843 \tValidation Loss: 2.312006\n",
      "Epoch: 8620 \tTraining Loss: 2.023943 \tValidation Loss: 2.311973\n",
      "Epoch: 8621 \tTraining Loss: 2.064104 \tValidation Loss: 2.311982\n",
      "Epoch: 8622 \tTraining Loss: 2.028110 \tValidation Loss: 2.311786\n",
      "Epoch: 8623 \tTraining Loss: 2.049380 \tValidation Loss: 2.311480\n",
      "Epoch: 8624 \tTraining Loss: 2.042717 \tValidation Loss: 2.311846\n",
      "Epoch: 8625 \tTraining Loss: 1.997147 \tValidation Loss: 2.312043\n",
      "Epoch: 8626 \tTraining Loss: 2.031546 \tValidation Loss: 2.311891\n",
      "Epoch: 8627 \tTraining Loss: 2.024114 \tValidation Loss: 2.311804\n",
      "Epoch: 8628 \tTraining Loss: 2.023535 \tValidation Loss: 2.311616\n",
      "Epoch: 8629 \tTraining Loss: 2.057669 \tValidation Loss: 2.311966\n",
      "Epoch: 8630 \tTraining Loss: 2.014963 \tValidation Loss: 2.312002\n",
      "Epoch: 8631 \tTraining Loss: 2.026027 \tValidation Loss: 2.312349\n",
      "Epoch: 8632 \tTraining Loss: 2.045592 \tValidation Loss: 2.312126\n",
      "Epoch: 8633 \tTraining Loss: 2.045092 \tValidation Loss: 2.311921\n",
      "Epoch: 8634 \tTraining Loss: 2.034993 \tValidation Loss: 2.311891\n",
      "Epoch: 8635 \tTraining Loss: 2.024008 \tValidation Loss: 2.311695\n",
      "Epoch: 8636 \tTraining Loss: 2.053318 \tValidation Loss: 2.311667\n",
      "Epoch: 8637 \tTraining Loss: 2.031367 \tValidation Loss: 2.311876\n",
      "Epoch: 8638 \tTraining Loss: 2.014460 \tValidation Loss: 2.311785\n",
      "Epoch: 8639 \tTraining Loss: 2.024186 \tValidation Loss: 2.311799\n",
      "Epoch: 8640 \tTraining Loss: 2.049046 \tValidation Loss: 2.311700\n",
      "Epoch: 8641 \tTraining Loss: 2.013015 \tValidation Loss: 2.311615\n",
      "Epoch: 8642 \tTraining Loss: 2.043880 \tValidation Loss: 2.311665\n",
      "Epoch: 8643 \tTraining Loss: 2.011903 \tValidation Loss: 2.311755\n",
      "Epoch: 8644 \tTraining Loss: 2.028521 \tValidation Loss: 2.311663\n",
      "Epoch: 8645 \tTraining Loss: 2.041546 \tValidation Loss: 2.311620\n",
      "Epoch: 8646 \tTraining Loss: 2.017974 \tValidation Loss: 2.312338\n",
      "Epoch: 8647 \tTraining Loss: 2.036111 \tValidation Loss: 2.312060\n",
      "Epoch: 8648 \tTraining Loss: 2.028667 \tValidation Loss: 2.312251\n",
      "Epoch: 8649 \tTraining Loss: 2.016838 \tValidation Loss: 2.312292\n",
      "Epoch: 8650 \tTraining Loss: 2.008184 \tValidation Loss: 2.312090\n",
      "Epoch: 8651 \tTraining Loss: 2.059028 \tValidation Loss: 2.312140\n",
      "Epoch: 8652 \tTraining Loss: 2.055215 \tValidation Loss: 2.312129\n",
      "Epoch: 8653 \tTraining Loss: 2.032093 \tValidation Loss: 2.312311\n",
      "Epoch: 8654 \tTraining Loss: 2.031572 \tValidation Loss: 2.312503\n",
      "Epoch: 8655 \tTraining Loss: 2.031715 \tValidation Loss: 2.312744\n",
      "Epoch: 8656 \tTraining Loss: 2.050056 \tValidation Loss: 2.312732\n",
      "Epoch: 8657 \tTraining Loss: 2.050472 \tValidation Loss: 2.312542\n",
      "Epoch: 8658 \tTraining Loss: 2.024117 \tValidation Loss: 2.312626\n",
      "Epoch: 8659 \tTraining Loss: 2.032266 \tValidation Loss: 2.312547\n",
      "Epoch: 8660 \tTraining Loss: 2.014123 \tValidation Loss: 2.312310\n",
      "Epoch: 8661 \tTraining Loss: 2.013048 \tValidation Loss: 2.312522\n",
      "Epoch: 8662 \tTraining Loss: 2.045044 \tValidation Loss: 2.312102\n",
      "Epoch: 8663 \tTraining Loss: 2.006897 \tValidation Loss: 2.312181\n",
      "Epoch: 8664 \tTraining Loss: 2.015209 \tValidation Loss: 2.312150\n",
      "Epoch: 8665 \tTraining Loss: 2.006170 \tValidation Loss: 2.311930\n",
      "Epoch: 8666 \tTraining Loss: 2.032187 \tValidation Loss: 2.311576\n",
      "Epoch: 8667 \tTraining Loss: 2.005166 \tValidation Loss: 2.311568\n",
      "Epoch: 8668 \tTraining Loss: 2.024481 \tValidation Loss: 2.311531\n",
      "Epoch: 8669 \tTraining Loss: 2.019307 \tValidation Loss: 2.311458\n",
      "Epoch: 8670 \tTraining Loss: 2.026559 \tValidation Loss: 2.311853\n",
      "Epoch: 8671 \tTraining Loss: 2.004597 \tValidation Loss: 2.311678\n",
      "Epoch: 8672 \tTraining Loss: 2.037471 \tValidation Loss: 2.311866\n",
      "Epoch: 8673 \tTraining Loss: 2.020118 \tValidation Loss: 2.311627\n",
      "Epoch: 8674 \tTraining Loss: 2.011958 \tValidation Loss: 2.311419\n",
      "Epoch: 8675 \tTraining Loss: 2.034399 \tValidation Loss: 2.311590\n",
      "Epoch: 8676 \tTraining Loss: 2.031690 \tValidation Loss: 2.311749\n",
      "Epoch: 8677 \tTraining Loss: 2.033775 \tValidation Loss: 2.311285\n",
      "Validation loss decreased (2.311378 --> 2.311285).  Saving model ...\n",
      "Epoch: 8678 \tTraining Loss: 2.003695 \tValidation Loss: 2.311317\n",
      "Epoch: 8679 \tTraining Loss: 2.048499 \tValidation Loss: 2.311645\n",
      "Epoch: 8680 \tTraining Loss: 2.022815 \tValidation Loss: 2.311687\n",
      "Epoch: 8681 \tTraining Loss: 2.013226 \tValidation Loss: 2.311966\n",
      "Epoch: 8682 \tTraining Loss: 2.058150 \tValidation Loss: 2.311801\n",
      "Epoch: 8683 \tTraining Loss: 2.015857 \tValidation Loss: 2.312007\n",
      "Epoch: 8684 \tTraining Loss: 2.022665 \tValidation Loss: 2.312832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8685 \tTraining Loss: 2.031489 \tValidation Loss: 2.312530\n",
      "Epoch: 8686 \tTraining Loss: 2.033060 \tValidation Loss: 2.312291\n",
      "Epoch: 8687 \tTraining Loss: 2.034225 \tValidation Loss: 2.312279\n",
      "Epoch: 8688 \tTraining Loss: 2.028652 \tValidation Loss: 2.312340\n",
      "Epoch: 8689 \tTraining Loss: 2.025645 \tValidation Loss: 2.311949\n",
      "Epoch: 8690 \tTraining Loss: 2.029794 \tValidation Loss: 2.311923\n",
      "Epoch: 8691 \tTraining Loss: 2.041884 \tValidation Loss: 2.311757\n",
      "Epoch: 8692 \tTraining Loss: 2.033296 \tValidation Loss: 2.311975\n",
      "Epoch: 8693 \tTraining Loss: 2.035773 \tValidation Loss: 2.311991\n",
      "Epoch: 8694 \tTraining Loss: 2.002816 \tValidation Loss: 2.312131\n",
      "Epoch: 8695 \tTraining Loss: 2.032734 \tValidation Loss: 2.312185\n",
      "Epoch: 8696 \tTraining Loss: 2.033389 \tValidation Loss: 2.312195\n",
      "Epoch: 8697 \tTraining Loss: 2.042926 \tValidation Loss: 2.312244\n",
      "Epoch: 8698 \tTraining Loss: 2.050826 \tValidation Loss: 2.312086\n",
      "Epoch: 8699 \tTraining Loss: 2.015094 \tValidation Loss: 2.312131\n",
      "Epoch: 8700 \tTraining Loss: 2.033741 \tValidation Loss: 2.312129\n",
      "Epoch: 8701 \tTraining Loss: 2.043217 \tValidation Loss: 2.312271\n",
      "Epoch: 8702 \tTraining Loss: 2.004515 \tValidation Loss: 2.312209\n",
      "Epoch: 8703 \tTraining Loss: 2.006687 \tValidation Loss: 2.312099\n",
      "Epoch: 8704 \tTraining Loss: 2.021222 \tValidation Loss: 2.311999\n",
      "Epoch: 8705 \tTraining Loss: 2.002057 \tValidation Loss: 2.311996\n",
      "Epoch: 8706 \tTraining Loss: 2.001713 \tValidation Loss: 2.312043\n",
      "Epoch: 8707 \tTraining Loss: 2.038253 \tValidation Loss: 2.312058\n",
      "Epoch: 8708 \tTraining Loss: 2.059853 \tValidation Loss: 2.311914\n",
      "Epoch: 8709 \tTraining Loss: 2.024365 \tValidation Loss: 2.311679\n",
      "Epoch: 8710 \tTraining Loss: 2.031722 \tValidation Loss: 2.311775\n",
      "Epoch: 8711 \tTraining Loss: 2.007413 \tValidation Loss: 2.311640\n",
      "Epoch: 8712 \tTraining Loss: 2.038062 \tValidation Loss: 2.311602\n",
      "Epoch: 8713 \tTraining Loss: 2.014012 \tValidation Loss: 2.311576\n",
      "Epoch: 8714 \tTraining Loss: 2.043987 \tValidation Loss: 2.311341\n",
      "Epoch: 8715 \tTraining Loss: 2.012138 \tValidation Loss: 2.311146\n",
      "Validation loss decreased (2.311285 --> 2.311146).  Saving model ...\n",
      "Epoch: 8716 \tTraining Loss: 2.029220 \tValidation Loss: 2.311104\n",
      "Validation loss decreased (2.311146 --> 2.311104).  Saving model ...\n",
      "Epoch: 8717 \tTraining Loss: 2.041080 \tValidation Loss: 2.311257\n",
      "Epoch: 8718 \tTraining Loss: 2.043029 \tValidation Loss: 2.311303\n",
      "Epoch: 8719 \tTraining Loss: 2.039110 \tValidation Loss: 2.311424\n",
      "Epoch: 8720 \tTraining Loss: 2.014580 \tValidation Loss: 2.311298\n",
      "Epoch: 8721 \tTraining Loss: 1.992194 \tValidation Loss: 2.311254\n",
      "Epoch: 8722 \tTraining Loss: 1.997980 \tValidation Loss: 2.311334\n",
      "Epoch: 8723 \tTraining Loss: 2.035721 \tValidation Loss: 2.311331\n",
      "Epoch: 8724 \tTraining Loss: 2.056718 \tValidation Loss: 2.311469\n",
      "Epoch: 8725 \tTraining Loss: 2.047443 \tValidation Loss: 2.311864\n",
      "Epoch: 8726 \tTraining Loss: 2.026987 \tValidation Loss: 2.311833\n",
      "Epoch: 8727 \tTraining Loss: 2.024975 \tValidation Loss: 2.311407\n",
      "Epoch: 8728 \tTraining Loss: 2.011745 \tValidation Loss: 2.311593\n",
      "Epoch: 8729 \tTraining Loss: 2.025626 \tValidation Loss: 2.311912\n",
      "Epoch: 8730 \tTraining Loss: 2.049003 \tValidation Loss: 2.311994\n",
      "Epoch: 8731 \tTraining Loss: 2.010796 \tValidation Loss: 2.311896\n",
      "Epoch: 8732 \tTraining Loss: 2.036815 \tValidation Loss: 2.311798\n",
      "Epoch: 8733 \tTraining Loss: 2.024966 \tValidation Loss: 2.311706\n",
      "Epoch: 8734 \tTraining Loss: 2.035178 \tValidation Loss: 2.311585\n",
      "Epoch: 8735 \tTraining Loss: 2.019634 \tValidation Loss: 2.311222\n",
      "Epoch: 8736 \tTraining Loss: 2.026322 \tValidation Loss: 2.311249\n",
      "Epoch: 8737 \tTraining Loss: 2.026563 \tValidation Loss: 2.311105\n",
      "Epoch: 8738 \tTraining Loss: 2.005347 \tValidation Loss: 2.311186\n",
      "Epoch: 8739 \tTraining Loss: 2.006576 \tValidation Loss: 2.311061\n",
      "Validation loss decreased (2.311104 --> 2.311061).  Saving model ...\n",
      "Epoch: 8740 \tTraining Loss: 2.043624 \tValidation Loss: 2.311203\n",
      "Epoch: 8741 \tTraining Loss: 2.046519 \tValidation Loss: 2.311146\n",
      "Epoch: 8742 \tTraining Loss: 2.042023 \tValidation Loss: 2.311336\n",
      "Epoch: 8743 \tTraining Loss: 2.032521 \tValidation Loss: 2.311063\n",
      "Epoch: 8744 \tTraining Loss: 2.032284 \tValidation Loss: 2.311145\n",
      "Epoch: 8745 \tTraining Loss: 2.026907 \tValidation Loss: 2.311237\n",
      "Epoch: 8746 \tTraining Loss: 1.997887 \tValidation Loss: 2.311143\n",
      "Epoch: 8747 \tTraining Loss: 2.014870 \tValidation Loss: 2.311339\n",
      "Epoch: 8748 \tTraining Loss: 2.024313 \tValidation Loss: 2.311355\n",
      "Epoch: 8749 \tTraining Loss: 2.029604 \tValidation Loss: 2.311482\n",
      "Epoch: 8750 \tTraining Loss: 2.010753 \tValidation Loss: 2.311453\n",
      "Epoch: 8751 \tTraining Loss: 2.015651 \tValidation Loss: 2.311686\n",
      "Epoch: 8752 \tTraining Loss: 2.043203 \tValidation Loss: 2.311630\n",
      "Epoch: 8753 \tTraining Loss: 2.033026 \tValidation Loss: 2.311874\n",
      "Epoch: 8754 \tTraining Loss: 2.022878 \tValidation Loss: 2.311871\n",
      "Epoch: 8755 \tTraining Loss: 1.985654 \tValidation Loss: 2.311949\n",
      "Epoch: 8756 \tTraining Loss: 2.039177 \tValidation Loss: 2.311643\n",
      "Epoch: 8757 \tTraining Loss: 2.023239 \tValidation Loss: 2.311622\n",
      "Epoch: 8758 \tTraining Loss: 2.035065 \tValidation Loss: 2.311370\n",
      "Epoch: 8759 \tTraining Loss: 2.022154 \tValidation Loss: 2.311587\n",
      "Epoch: 8760 \tTraining Loss: 2.037767 \tValidation Loss: 2.311461\n",
      "Epoch: 8761 \tTraining Loss: 2.019712 \tValidation Loss: 2.311495\n",
      "Epoch: 8762 \tTraining Loss: 1.995559 \tValidation Loss: 2.311181\n",
      "Epoch: 8763 \tTraining Loss: 2.027223 \tValidation Loss: 2.311282\n",
      "Epoch: 8764 \tTraining Loss: 2.023771 \tValidation Loss: 2.311180\n",
      "Epoch: 8765 \tTraining Loss: 2.020431 \tValidation Loss: 2.311223\n",
      "Epoch: 8766 \tTraining Loss: 2.041657 \tValidation Loss: 2.311289\n",
      "Epoch: 8767 \tTraining Loss: 2.031241 \tValidation Loss: 2.311078\n",
      "Epoch: 8768 \tTraining Loss: 2.029912 \tValidation Loss: 2.311041\n",
      "Validation loss decreased (2.311061 --> 2.311041).  Saving model ...\n",
      "Epoch: 8769 \tTraining Loss: 2.004973 \tValidation Loss: 2.311214\n",
      "Epoch: 8770 \tTraining Loss: 2.022121 \tValidation Loss: 2.311294\n",
      "Epoch: 8771 \tTraining Loss: 2.016656 \tValidation Loss: 2.311383\n",
      "Epoch: 8772 \tTraining Loss: 2.026145 \tValidation Loss: 2.311549\n",
      "Epoch: 8773 \tTraining Loss: 2.029489 \tValidation Loss: 2.311545\n",
      "Epoch: 8774 \tTraining Loss: 2.017337 \tValidation Loss: 2.311281\n",
      "Epoch: 8775 \tTraining Loss: 2.026859 \tValidation Loss: 2.311466\n",
      "Epoch: 8776 \tTraining Loss: 2.056147 \tValidation Loss: 2.311168\n",
      "Epoch: 8777 \tTraining Loss: 2.053988 \tValidation Loss: 2.311401\n",
      "Epoch: 8778 \tTraining Loss: 2.029898 \tValidation Loss: 2.311322\n",
      "Epoch: 8779 \tTraining Loss: 2.031945 \tValidation Loss: 2.311236\n",
      "Epoch: 8780 \tTraining Loss: 2.032134 \tValidation Loss: 2.311273\n",
      "Epoch: 8781 \tTraining Loss: 2.022472 \tValidation Loss: 2.311090\n",
      "Epoch: 8782 \tTraining Loss: 1.996406 \tValidation Loss: 2.311192\n",
      "Epoch: 8783 \tTraining Loss: 2.004836 \tValidation Loss: 2.311507\n",
      "Epoch: 8784 \tTraining Loss: 2.038993 \tValidation Loss: 2.311491\n",
      "Epoch: 8785 \tTraining Loss: 1.986984 \tValidation Loss: 2.311429\n",
      "Epoch: 8786 \tTraining Loss: 2.028593 \tValidation Loss: 2.311495\n",
      "Epoch: 8787 \tTraining Loss: 2.009025 \tValidation Loss: 2.311431\n",
      "Epoch: 8788 \tTraining Loss: 2.043919 \tValidation Loss: 2.311569\n",
      "Epoch: 8789 \tTraining Loss: 2.061074 \tValidation Loss: 2.311414\n",
      "Epoch: 8790 \tTraining Loss: 2.012031 \tValidation Loss: 2.311589\n",
      "Epoch: 8791 \tTraining Loss: 2.037531 \tValidation Loss: 2.311737\n",
      "Epoch: 8792 \tTraining Loss: 2.015338 \tValidation Loss: 2.311681\n",
      "Epoch: 8793 \tTraining Loss: 2.014624 \tValidation Loss: 2.311728\n",
      "Epoch: 8794 \tTraining Loss: 2.047601 \tValidation Loss: 2.311592\n",
      "Epoch: 8795 \tTraining Loss: 2.060310 \tValidation Loss: 2.311711\n",
      "Epoch: 8796 \tTraining Loss: 2.037101 \tValidation Loss: 2.311821\n",
      "Epoch: 8797 \tTraining Loss: 1.994985 \tValidation Loss: 2.311728\n",
      "Epoch: 8798 \tTraining Loss: 2.054009 \tValidation Loss: 2.311976\n",
      "Epoch: 8799 \tTraining Loss: 2.042317 \tValidation Loss: 2.311701\n",
      "Epoch: 8800 \tTraining Loss: 1.996404 \tValidation Loss: 2.311630\n",
      "Epoch: 8801 \tTraining Loss: 2.026307 \tValidation Loss: 2.311531\n",
      "Epoch: 8802 \tTraining Loss: 2.019049 \tValidation Loss: 2.311486\n",
      "Epoch: 8803 \tTraining Loss: 2.054232 \tValidation Loss: 2.311687\n",
      "Epoch: 8804 \tTraining Loss: 2.016696 \tValidation Loss: 2.311983\n",
      "Epoch: 8805 \tTraining Loss: 1.977252 \tValidation Loss: 2.311692\n",
      "Epoch: 8806 \tTraining Loss: 2.040019 \tValidation Loss: 2.311420\n",
      "Epoch: 8807 \tTraining Loss: 2.019204 \tValidation Loss: 2.311006\n",
      "Validation loss decreased (2.311041 --> 2.311006).  Saving model ...\n",
      "Epoch: 8808 \tTraining Loss: 2.027773 \tValidation Loss: 2.311065\n",
      "Epoch: 8809 \tTraining Loss: 2.043464 \tValidation Loss: 2.310942\n",
      "Validation loss decreased (2.311006 --> 2.310942).  Saving model ...\n",
      "Epoch: 8810 \tTraining Loss: 2.020927 \tValidation Loss: 2.311142\n",
      "Epoch: 8811 \tTraining Loss: 2.033449 \tValidation Loss: 2.311115\n",
      "Epoch: 8812 \tTraining Loss: 2.033728 \tValidation Loss: 2.311262\n",
      "Epoch: 8813 \tTraining Loss: 2.031375 \tValidation Loss: 2.311392\n",
      "Epoch: 8814 \tTraining Loss: 2.015542 \tValidation Loss: 2.311402\n",
      "Epoch: 8815 \tTraining Loss: 2.014865 \tValidation Loss: 2.311584\n",
      "Epoch: 8816 \tTraining Loss: 2.003621 \tValidation Loss: 2.311484\n",
      "Epoch: 8817 \tTraining Loss: 2.038247 \tValidation Loss: 2.311700\n",
      "Epoch: 8818 \tTraining Loss: 2.013172 \tValidation Loss: 2.311913\n",
      "Epoch: 8819 \tTraining Loss: 2.035465 \tValidation Loss: 2.311872\n",
      "Epoch: 8820 \tTraining Loss: 2.022414 \tValidation Loss: 2.311823\n",
      "Epoch: 8821 \tTraining Loss: 1.991858 \tValidation Loss: 2.312055\n",
      "Epoch: 8822 \tTraining Loss: 2.015884 \tValidation Loss: 2.311891\n",
      "Epoch: 8823 \tTraining Loss: 2.011717 \tValidation Loss: 2.311889\n",
      "Epoch: 8824 \tTraining Loss: 1.994466 \tValidation Loss: 2.311783\n",
      "Epoch: 8825 \tTraining Loss: 2.000867 \tValidation Loss: 2.311773\n",
      "Epoch: 8826 \tTraining Loss: 2.023363 \tValidation Loss: 2.311556\n",
      "Epoch: 8827 \tTraining Loss: 2.013220 \tValidation Loss: 2.311476\n",
      "Epoch: 8828 \tTraining Loss: 2.028610 \tValidation Loss: 2.311368\n",
      "Epoch: 8829 \tTraining Loss: 2.029425 \tValidation Loss: 2.311082\n",
      "Epoch: 8830 \tTraining Loss: 2.029106 \tValidation Loss: 2.311207\n",
      "Epoch: 8831 \tTraining Loss: 2.021482 \tValidation Loss: 2.311278\n",
      "Epoch: 8832 \tTraining Loss: 2.002933 \tValidation Loss: 2.311821\n",
      "Epoch: 8833 \tTraining Loss: 2.006093 \tValidation Loss: 2.311773\n",
      "Epoch: 8834 \tTraining Loss: 2.033598 \tValidation Loss: 2.311582\n",
      "Epoch: 8835 \tTraining Loss: 2.042009 \tValidation Loss: 2.311599\n",
      "Epoch: 8836 \tTraining Loss: 1.986287 \tValidation Loss: 2.311433\n",
      "Epoch: 8837 \tTraining Loss: 2.012291 \tValidation Loss: 2.311493\n",
      "Epoch: 8838 \tTraining Loss: 2.051163 \tValidation Loss: 2.311548\n",
      "Epoch: 8839 \tTraining Loss: 2.026252 \tValidation Loss: 2.311581\n",
      "Epoch: 8840 \tTraining Loss: 2.013044 \tValidation Loss: 2.312005\n",
      "Epoch: 8841 \tTraining Loss: 2.010773 \tValidation Loss: 2.311604\n",
      "Epoch: 8842 \tTraining Loss: 2.053200 \tValidation Loss: 2.311447\n",
      "Epoch: 8843 \tTraining Loss: 2.045647 \tValidation Loss: 2.311298\n",
      "Epoch: 8844 \tTraining Loss: 2.002517 \tValidation Loss: 2.311214\n",
      "Epoch: 8845 \tTraining Loss: 2.023517 \tValidation Loss: 2.311446\n",
      "Epoch: 8846 \tTraining Loss: 2.030117 \tValidation Loss: 2.311445\n",
      "Epoch: 8847 \tTraining Loss: 2.039388 \tValidation Loss: 2.311660\n",
      "Epoch: 8848 \tTraining Loss: 2.023321 \tValidation Loss: 2.311497\n",
      "Epoch: 8849 \tTraining Loss: 2.012518 \tValidation Loss: 2.311534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8850 \tTraining Loss: 2.038296 \tValidation Loss: 2.311489\n",
      "Epoch: 8851 \tTraining Loss: 1.995429 \tValidation Loss: 2.311169\n",
      "Epoch: 8852 \tTraining Loss: 2.056383 \tValidation Loss: 2.311261\n",
      "Epoch: 8853 \tTraining Loss: 2.016928 \tValidation Loss: 2.311561\n",
      "Epoch: 8854 \tTraining Loss: 2.042524 \tValidation Loss: 2.310980\n",
      "Epoch: 8855 \tTraining Loss: 2.048459 \tValidation Loss: 2.310684\n",
      "Validation loss decreased (2.310942 --> 2.310684).  Saving model ...\n",
      "Epoch: 8856 \tTraining Loss: 2.036593 \tValidation Loss: 2.310891\n",
      "Epoch: 8857 \tTraining Loss: 2.007500 \tValidation Loss: 2.311007\n",
      "Epoch: 8858 \tTraining Loss: 2.008538 \tValidation Loss: 2.311330\n",
      "Epoch: 8859 \tTraining Loss: 2.023294 \tValidation Loss: 2.311021\n",
      "Epoch: 8860 \tTraining Loss: 2.009141 \tValidation Loss: 2.311295\n",
      "Epoch: 8861 \tTraining Loss: 2.018684 \tValidation Loss: 2.311451\n",
      "Epoch: 8862 \tTraining Loss: 2.020100 \tValidation Loss: 2.311649\n",
      "Epoch: 8863 \tTraining Loss: 2.025693 \tValidation Loss: 2.311279\n",
      "Epoch: 8864 \tTraining Loss: 1.990120 \tValidation Loss: 2.311370\n",
      "Epoch: 8865 \tTraining Loss: 2.020522 \tValidation Loss: 2.311284\n",
      "Epoch: 8866 \tTraining Loss: 2.041283 \tValidation Loss: 2.311327\n",
      "Epoch: 8867 \tTraining Loss: 2.027361 \tValidation Loss: 2.310876\n",
      "Epoch: 8868 \tTraining Loss: 2.018525 \tValidation Loss: 2.311280\n",
      "Epoch: 8869 \tTraining Loss: 2.003915 \tValidation Loss: 2.311276\n",
      "Epoch: 8870 \tTraining Loss: 2.019362 \tValidation Loss: 2.311332\n",
      "Epoch: 8871 \tTraining Loss: 2.023173 \tValidation Loss: 2.311418\n",
      "Epoch: 8872 \tTraining Loss: 2.027209 \tValidation Loss: 2.311268\n",
      "Epoch: 8873 \tTraining Loss: 2.051233 \tValidation Loss: 2.311614\n",
      "Epoch: 8874 \tTraining Loss: 2.017442 \tValidation Loss: 2.311457\n",
      "Epoch: 8875 \tTraining Loss: 2.023637 \tValidation Loss: 2.310994\n",
      "Epoch: 8876 \tTraining Loss: 2.017653 \tValidation Loss: 2.311040\n",
      "Epoch: 8877 \tTraining Loss: 2.030334 \tValidation Loss: 2.311407\n",
      "Epoch: 8878 \tTraining Loss: 2.035106 \tValidation Loss: 2.311032\n",
      "Epoch: 8879 \tTraining Loss: 2.015163 \tValidation Loss: 2.310784\n",
      "Epoch: 8880 \tTraining Loss: 1.998618 \tValidation Loss: 2.310779\n",
      "Epoch: 8881 \tTraining Loss: 2.015338 \tValidation Loss: 2.310741\n",
      "Epoch: 8882 \tTraining Loss: 2.007787 \tValidation Loss: 2.310580\n",
      "Validation loss decreased (2.310684 --> 2.310580).  Saving model ...\n",
      "Epoch: 8883 \tTraining Loss: 2.022343 \tValidation Loss: 2.310859\n",
      "Epoch: 8884 \tTraining Loss: 2.023924 \tValidation Loss: 2.310952\n",
      "Epoch: 8885 \tTraining Loss: 2.004363 \tValidation Loss: 2.310917\n",
      "Epoch: 8886 \tTraining Loss: 2.000329 \tValidation Loss: 2.310951\n",
      "Epoch: 8887 \tTraining Loss: 2.030103 \tValidation Loss: 2.310796\n",
      "Epoch: 8888 \tTraining Loss: 2.003001 \tValidation Loss: 2.311109\n",
      "Epoch: 8889 \tTraining Loss: 2.018101 \tValidation Loss: 2.311128\n",
      "Epoch: 8890 \tTraining Loss: 2.015230 \tValidation Loss: 2.310560\n",
      "Validation loss decreased (2.310580 --> 2.310560).  Saving model ...\n",
      "Epoch: 8891 \tTraining Loss: 2.035676 \tValidation Loss: 2.310658\n",
      "Epoch: 8892 \tTraining Loss: 1.983017 \tValidation Loss: 2.310485\n",
      "Validation loss decreased (2.310560 --> 2.310485).  Saving model ...\n",
      "Epoch: 8893 \tTraining Loss: 2.003303 \tValidation Loss: 2.310208\n",
      "Validation loss decreased (2.310485 --> 2.310208).  Saving model ...\n",
      "Epoch: 8894 \tTraining Loss: 2.022100 \tValidation Loss: 2.310262\n",
      "Epoch: 8895 \tTraining Loss: 1.975915 \tValidation Loss: 2.310081\n",
      "Validation loss decreased (2.310208 --> 2.310081).  Saving model ...\n",
      "Epoch: 8896 \tTraining Loss: 1.989073 \tValidation Loss: 2.310382\n",
      "Epoch: 8897 \tTraining Loss: 2.022344 \tValidation Loss: 2.310619\n",
      "Epoch: 8898 \tTraining Loss: 2.020163 \tValidation Loss: 2.310741\n",
      "Epoch: 8899 \tTraining Loss: 2.014753 \tValidation Loss: 2.310884\n",
      "Epoch: 8900 \tTraining Loss: 2.037600 \tValidation Loss: 2.310871\n",
      "Epoch: 8901 \tTraining Loss: 1.997565 \tValidation Loss: 2.310845\n",
      "Epoch: 8902 \tTraining Loss: 2.022416 \tValidation Loss: 2.310858\n",
      "Epoch: 8903 \tTraining Loss: 2.025801 \tValidation Loss: 2.311041\n",
      "Epoch: 8904 \tTraining Loss: 2.007210 \tValidation Loss: 2.310853\n",
      "Epoch: 8905 \tTraining Loss: 2.021048 \tValidation Loss: 2.310885\n",
      "Epoch: 8906 \tTraining Loss: 1.981437 \tValidation Loss: 2.311043\n",
      "Epoch: 8907 \tTraining Loss: 2.027310 \tValidation Loss: 2.311077\n",
      "Epoch: 8908 \tTraining Loss: 2.026615 \tValidation Loss: 2.311112\n",
      "Epoch: 8909 \tTraining Loss: 2.020811 \tValidation Loss: 2.311018\n",
      "Epoch: 8910 \tTraining Loss: 2.009435 \tValidation Loss: 2.311034\n",
      "Epoch: 8911 \tTraining Loss: 2.024406 \tValidation Loss: 2.310828\n",
      "Epoch: 8912 \tTraining Loss: 2.019882 \tValidation Loss: 2.311248\n",
      "Epoch: 8913 \tTraining Loss: 2.005161 \tValidation Loss: 2.311490\n",
      "Epoch: 8914 \tTraining Loss: 2.006252 \tValidation Loss: 2.311784\n",
      "Epoch: 8915 \tTraining Loss: 1.990093 \tValidation Loss: 2.311530\n",
      "Epoch: 8916 \tTraining Loss: 2.047345 \tValidation Loss: 2.311485\n",
      "Epoch: 8917 \tTraining Loss: 1.987683 \tValidation Loss: 2.311198\n",
      "Epoch: 8918 \tTraining Loss: 2.053072 \tValidation Loss: 2.311460\n",
      "Epoch: 8919 \tTraining Loss: 1.990868 \tValidation Loss: 2.311490\n",
      "Epoch: 8920 \tTraining Loss: 2.007961 \tValidation Loss: 2.311736\n",
      "Epoch: 8921 \tTraining Loss: 2.034861 \tValidation Loss: 2.311805\n",
      "Epoch: 8922 \tTraining Loss: 2.013715 \tValidation Loss: 2.311462\n",
      "Epoch: 8923 \tTraining Loss: 2.027575 \tValidation Loss: 2.311404\n",
      "Epoch: 8924 \tTraining Loss: 2.005076 \tValidation Loss: 2.311264\n",
      "Epoch: 8925 \tTraining Loss: 2.008395 \tValidation Loss: 2.311517\n",
      "Epoch: 8926 \tTraining Loss: 2.039321 \tValidation Loss: 2.311069\n",
      "Epoch: 8927 \tTraining Loss: 2.042394 \tValidation Loss: 2.310725\n",
      "Epoch: 8928 \tTraining Loss: 2.003316 \tValidation Loss: 2.310990\n",
      "Epoch: 8929 \tTraining Loss: 1.994974 \tValidation Loss: 2.310918\n",
      "Epoch: 8930 \tTraining Loss: 2.002109 \tValidation Loss: 2.310862\n",
      "Epoch: 8931 \tTraining Loss: 2.020993 \tValidation Loss: 2.311079\n",
      "Epoch: 8932 \tTraining Loss: 1.989363 \tValidation Loss: 2.311105\n",
      "Epoch: 8933 \tTraining Loss: 2.008170 \tValidation Loss: 2.310948\n",
      "Epoch: 8934 \tTraining Loss: 1.995071 \tValidation Loss: 2.311037\n",
      "Epoch: 8935 \tTraining Loss: 2.022875 \tValidation Loss: 2.311547\n",
      "Epoch: 8936 \tTraining Loss: 2.031312 \tValidation Loss: 2.311502\n",
      "Epoch: 8937 \tTraining Loss: 2.001209 \tValidation Loss: 2.311271\n",
      "Epoch: 8938 \tTraining Loss: 1.986261 \tValidation Loss: 2.310882\n",
      "Epoch: 8939 \tTraining Loss: 2.035089 \tValidation Loss: 2.310964\n",
      "Epoch: 8940 \tTraining Loss: 1.998761 \tValidation Loss: 2.310906\n",
      "Epoch: 8941 \tTraining Loss: 2.011270 \tValidation Loss: 2.310894\n",
      "Epoch: 8942 \tTraining Loss: 1.986916 \tValidation Loss: 2.311125\n",
      "Epoch: 8943 \tTraining Loss: 2.017604 \tValidation Loss: 2.311173\n",
      "Epoch: 8944 \tTraining Loss: 2.024715 \tValidation Loss: 2.311292\n",
      "Epoch: 8945 \tTraining Loss: 2.015652 \tValidation Loss: 2.311603\n",
      "Epoch: 8946 \tTraining Loss: 2.015320 \tValidation Loss: 2.311373\n",
      "Epoch: 8947 \tTraining Loss: 2.035384 \tValidation Loss: 2.311540\n",
      "Epoch: 8948 \tTraining Loss: 2.023589 \tValidation Loss: 2.311817\n",
      "Epoch: 8949 \tTraining Loss: 2.019447 \tValidation Loss: 2.311783\n",
      "Epoch: 8950 \tTraining Loss: 1.978579 \tValidation Loss: 2.311915\n",
      "Epoch: 8951 \tTraining Loss: 2.013495 \tValidation Loss: 2.312162\n",
      "Epoch: 8952 \tTraining Loss: 2.000333 \tValidation Loss: 2.311809\n",
      "Epoch: 8953 \tTraining Loss: 2.042467 \tValidation Loss: 2.311903\n",
      "Epoch: 8954 \tTraining Loss: 1.989017 \tValidation Loss: 2.311941\n",
      "Epoch: 8955 \tTraining Loss: 1.983032 \tValidation Loss: 2.311766\n",
      "Epoch: 8956 \tTraining Loss: 2.013320 \tValidation Loss: 2.311825\n",
      "Epoch: 8957 \tTraining Loss: 2.003294 \tValidation Loss: 2.311865\n",
      "Epoch: 8958 \tTraining Loss: 2.013061 \tValidation Loss: 2.311562\n",
      "Epoch: 8959 \tTraining Loss: 2.025456 \tValidation Loss: 2.311408\n",
      "Epoch: 8960 \tTraining Loss: 2.018182 \tValidation Loss: 2.311409\n",
      "Epoch: 8961 \tTraining Loss: 2.036918 \tValidation Loss: 2.311633\n",
      "Epoch: 8962 \tTraining Loss: 2.024139 \tValidation Loss: 2.311470\n",
      "Epoch: 8963 \tTraining Loss: 2.010689 \tValidation Loss: 2.311298\n",
      "Epoch: 8964 \tTraining Loss: 2.050635 \tValidation Loss: 2.311217\n",
      "Epoch: 8965 \tTraining Loss: 1.990092 \tValidation Loss: 2.311094\n",
      "Epoch: 8966 \tTraining Loss: 1.986628 \tValidation Loss: 2.310875\n",
      "Epoch: 8967 \tTraining Loss: 2.008780 \tValidation Loss: 2.310918\n",
      "Epoch: 8968 \tTraining Loss: 1.990548 \tValidation Loss: 2.310667\n",
      "Epoch: 8969 \tTraining Loss: 2.002084 \tValidation Loss: 2.310995\n",
      "Epoch: 8970 \tTraining Loss: 1.998381 \tValidation Loss: 2.310654\n",
      "Epoch: 8971 \tTraining Loss: 1.985049 \tValidation Loss: 2.310868\n",
      "Epoch: 8972 \tTraining Loss: 2.039800 \tValidation Loss: 2.310748\n",
      "Epoch: 8973 \tTraining Loss: 2.019705 \tValidation Loss: 2.310870\n",
      "Epoch: 8974 \tTraining Loss: 1.990278 \tValidation Loss: 2.310557\n",
      "Epoch: 8975 \tTraining Loss: 2.005744 \tValidation Loss: 2.310822\n",
      "Epoch: 8976 \tTraining Loss: 2.020481 \tValidation Loss: 2.311069\n",
      "Epoch: 8977 \tTraining Loss: 2.027233 \tValidation Loss: 2.311201\n",
      "Epoch: 8978 \tTraining Loss: 2.013107 \tValidation Loss: 2.310868\n",
      "Epoch: 8979 \tTraining Loss: 1.991183 \tValidation Loss: 2.310789\n",
      "Epoch: 8980 \tTraining Loss: 2.015732 \tValidation Loss: 2.310662\n",
      "Epoch: 8981 \tTraining Loss: 2.024955 \tValidation Loss: 2.310747\n",
      "Epoch: 8982 \tTraining Loss: 1.984549 \tValidation Loss: 2.310578\n",
      "Epoch: 8983 \tTraining Loss: 2.041723 \tValidation Loss: 2.310626\n",
      "Epoch: 8984 \tTraining Loss: 2.037327 \tValidation Loss: 2.310354\n",
      "Epoch: 8985 \tTraining Loss: 1.974077 \tValidation Loss: 2.310879\n",
      "Epoch: 8986 \tTraining Loss: 1.994397 \tValidation Loss: 2.310571\n",
      "Epoch: 8987 \tTraining Loss: 2.017343 \tValidation Loss: 2.310747\n",
      "Epoch: 8988 \tTraining Loss: 1.986666 \tValidation Loss: 2.310487\n",
      "Epoch: 8989 \tTraining Loss: 2.038829 \tValidation Loss: 2.310602\n",
      "Epoch: 8990 \tTraining Loss: 2.002291 \tValidation Loss: 2.310396\n",
      "Epoch: 8991 \tTraining Loss: 2.009918 \tValidation Loss: 2.310743\n",
      "Epoch: 8992 \tTraining Loss: 2.037703 \tValidation Loss: 2.310484\n",
      "Epoch: 8993 \tTraining Loss: 2.006282 \tValidation Loss: 2.310360\n",
      "Epoch: 8994 \tTraining Loss: 2.010341 \tValidation Loss: 2.310362\n",
      "Epoch: 8995 \tTraining Loss: 2.016542 \tValidation Loss: 2.310397\n",
      "Epoch: 8996 \tTraining Loss: 1.997899 \tValidation Loss: 2.310493\n",
      "Epoch: 8997 \tTraining Loss: 1.987357 \tValidation Loss: 2.310363\n",
      "Epoch: 8998 \tTraining Loss: 1.996626 \tValidation Loss: 2.310746\n",
      "Epoch: 8999 \tTraining Loss: 2.034929 \tValidation Loss: 2.310723\n",
      "Epoch: 9000 \tTraining Loss: 1.977280 \tValidation Loss: 2.310805\n",
      "Epoch: 9001 \tTraining Loss: 2.017299 \tValidation Loss: 2.310729\n",
      "Epoch: 9002 \tTraining Loss: 2.005038 \tValidation Loss: 2.310467\n",
      "Epoch: 9003 \tTraining Loss: 2.042573 \tValidation Loss: 2.310267\n",
      "Epoch: 9004 \tTraining Loss: 2.020299 \tValidation Loss: 2.310385\n",
      "Epoch: 9005 \tTraining Loss: 1.992257 \tValidation Loss: 2.310237\n",
      "Epoch: 9006 \tTraining Loss: 2.025887 \tValidation Loss: 2.310428\n",
      "Epoch: 9007 \tTraining Loss: 2.030046 \tValidation Loss: 2.310120\n",
      "Epoch: 9008 \tTraining Loss: 2.010670 \tValidation Loss: 2.310112\n",
      "Epoch: 9009 \tTraining Loss: 2.013889 \tValidation Loss: 2.310298\n",
      "Epoch: 9010 \tTraining Loss: 2.013266 \tValidation Loss: 2.310309\n",
      "Epoch: 9011 \tTraining Loss: 2.004678 \tValidation Loss: 2.309981\n",
      "Validation loss decreased (2.310081 --> 2.309981).  Saving model ...\n",
      "Epoch: 9012 \tTraining Loss: 1.999089 \tValidation Loss: 2.310001\n",
      "Epoch: 9013 \tTraining Loss: 2.014449 \tValidation Loss: 2.310100\n",
      "Epoch: 9014 \tTraining Loss: 2.008667 \tValidation Loss: 2.310246\n",
      "Epoch: 9015 \tTraining Loss: 2.009181 \tValidation Loss: 2.310337\n",
      "Epoch: 9016 \tTraining Loss: 2.006480 \tValidation Loss: 2.310372\n",
      "Epoch: 9017 \tTraining Loss: 2.009081 \tValidation Loss: 2.310382\n",
      "Epoch: 9018 \tTraining Loss: 2.033196 \tValidation Loss: 2.310374\n",
      "Epoch: 9019 \tTraining Loss: 2.007324 \tValidation Loss: 2.310612\n",
      "Epoch: 9020 \tTraining Loss: 2.001383 \tValidation Loss: 2.310531\n",
      "Epoch: 9021 \tTraining Loss: 2.017794 \tValidation Loss: 2.310173\n",
      "Epoch: 9022 \tTraining Loss: 1.998309 \tValidation Loss: 2.310284\n",
      "Epoch: 9023 \tTraining Loss: 2.009895 \tValidation Loss: 2.310071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9024 \tTraining Loss: 2.005696 \tValidation Loss: 2.310492\n",
      "Epoch: 9025 \tTraining Loss: 2.008211 \tValidation Loss: 2.310450\n",
      "Epoch: 9026 \tTraining Loss: 2.009261 \tValidation Loss: 2.310595\n",
      "Epoch: 9027 \tTraining Loss: 2.037005 \tValidation Loss: 2.310738\n",
      "Epoch: 9028 \tTraining Loss: 1.995936 \tValidation Loss: 2.310781\n",
      "Epoch: 9029 \tTraining Loss: 1.999631 \tValidation Loss: 2.310899\n",
      "Epoch: 9030 \tTraining Loss: 1.977048 \tValidation Loss: 2.310565\n",
      "Epoch: 9031 \tTraining Loss: 1.997689 \tValidation Loss: 2.310498\n",
      "Epoch: 9032 \tTraining Loss: 2.009818 \tValidation Loss: 2.310557\n",
      "Epoch: 9033 \tTraining Loss: 2.022374 \tValidation Loss: 2.310328\n",
      "Epoch: 9034 \tTraining Loss: 2.019663 \tValidation Loss: 2.310318\n",
      "Epoch: 9035 \tTraining Loss: 1.997555 \tValidation Loss: 2.310512\n",
      "Epoch: 9036 \tTraining Loss: 2.053139 \tValidation Loss: 2.310482\n",
      "Epoch: 9037 \tTraining Loss: 2.002004 \tValidation Loss: 2.310880\n",
      "Epoch: 9038 \tTraining Loss: 2.006450 \tValidation Loss: 2.310691\n",
      "Epoch: 9039 \tTraining Loss: 2.029011 \tValidation Loss: 2.310957\n",
      "Epoch: 9040 \tTraining Loss: 2.012329 \tValidation Loss: 2.310784\n",
      "Epoch: 9041 \tTraining Loss: 2.004162 \tValidation Loss: 2.310712\n",
      "Epoch: 9042 \tTraining Loss: 2.014059 \tValidation Loss: 2.310868\n",
      "Epoch: 9043 \tTraining Loss: 2.002559 \tValidation Loss: 2.310859\n",
      "Epoch: 9044 \tTraining Loss: 2.005198 \tValidation Loss: 2.310688\n",
      "Epoch: 9045 \tTraining Loss: 1.999487 \tValidation Loss: 2.310694\n",
      "Epoch: 9046 \tTraining Loss: 2.015222 \tValidation Loss: 2.310934\n",
      "Epoch: 9047 \tTraining Loss: 1.990877 \tValidation Loss: 2.310764\n",
      "Epoch: 9048 \tTraining Loss: 2.004182 \tValidation Loss: 2.311034\n",
      "Epoch: 9049 \tTraining Loss: 2.003519 \tValidation Loss: 2.311221\n",
      "Epoch: 9050 \tTraining Loss: 2.022813 \tValidation Loss: 2.311203\n",
      "Epoch: 9051 \tTraining Loss: 2.024889 \tValidation Loss: 2.311168\n",
      "Epoch: 9052 \tTraining Loss: 1.991724 \tValidation Loss: 2.311277\n",
      "Epoch: 9053 \tTraining Loss: 2.024533 \tValidation Loss: 2.311852\n",
      "Epoch: 9054 \tTraining Loss: 2.010531 \tValidation Loss: 2.311608\n",
      "Epoch: 9055 \tTraining Loss: 2.002429 \tValidation Loss: 2.311367\n",
      "Epoch: 9056 \tTraining Loss: 2.025555 \tValidation Loss: 2.311330\n",
      "Epoch: 9057 \tTraining Loss: 1.984894 \tValidation Loss: 2.311651\n",
      "Epoch: 9058 \tTraining Loss: 1.993070 \tValidation Loss: 2.311474\n",
      "Epoch: 9059 \tTraining Loss: 2.014483 \tValidation Loss: 2.311394\n",
      "Epoch: 9060 \tTraining Loss: 2.015382 \tValidation Loss: 2.311044\n",
      "Epoch: 9061 \tTraining Loss: 2.002812 \tValidation Loss: 2.311120\n",
      "Epoch: 9062 \tTraining Loss: 2.002855 \tValidation Loss: 2.311377\n",
      "Epoch: 9063 \tTraining Loss: 2.012808 \tValidation Loss: 2.311384\n",
      "Epoch: 9064 \tTraining Loss: 1.995704 \tValidation Loss: 2.311725\n",
      "Epoch: 9065 \tTraining Loss: 2.014389 \tValidation Loss: 2.311893\n",
      "Epoch: 9066 \tTraining Loss: 2.019818 \tValidation Loss: 2.311534\n",
      "Epoch: 9067 \tTraining Loss: 2.009240 \tValidation Loss: 2.311406\n",
      "Epoch: 9068 \tTraining Loss: 2.024774 \tValidation Loss: 2.311467\n",
      "Epoch: 9069 \tTraining Loss: 2.031517 \tValidation Loss: 2.311446\n",
      "Epoch: 9070 \tTraining Loss: 1.981411 \tValidation Loss: 2.311250\n",
      "Epoch: 9071 \tTraining Loss: 2.027154 \tValidation Loss: 2.311371\n",
      "Epoch: 9072 \tTraining Loss: 2.025137 \tValidation Loss: 2.310942\n",
      "Epoch: 9073 \tTraining Loss: 1.997457 \tValidation Loss: 2.310906\n",
      "Epoch: 9074 \tTraining Loss: 2.010217 \tValidation Loss: 2.311300\n",
      "Epoch: 9075 \tTraining Loss: 2.017189 \tValidation Loss: 2.311219\n",
      "Epoch: 9076 \tTraining Loss: 2.002807 \tValidation Loss: 2.311424\n",
      "Epoch: 9077 \tTraining Loss: 2.001359 \tValidation Loss: 2.311102\n",
      "Epoch: 9078 \tTraining Loss: 2.018113 \tValidation Loss: 2.311085\n",
      "Epoch: 9079 \tTraining Loss: 2.007084 \tValidation Loss: 2.311239\n",
      "Epoch: 9080 \tTraining Loss: 1.985043 \tValidation Loss: 2.311114\n",
      "Epoch: 9081 \tTraining Loss: 2.028040 \tValidation Loss: 2.311390\n",
      "Epoch: 9082 \tTraining Loss: 2.030246 \tValidation Loss: 2.311125\n",
      "Epoch: 9083 \tTraining Loss: 2.008270 \tValidation Loss: 2.311039\n",
      "Epoch: 9084 \tTraining Loss: 1.996192 \tValidation Loss: 2.311040\n",
      "Epoch: 9085 \tTraining Loss: 1.980079 \tValidation Loss: 2.310632\n",
      "Epoch: 9086 \tTraining Loss: 2.003140 \tValidation Loss: 2.310514\n",
      "Epoch: 9087 \tTraining Loss: 2.019030 \tValidation Loss: 2.310425\n",
      "Epoch: 9088 \tTraining Loss: 1.991370 \tValidation Loss: 2.310317\n",
      "Epoch: 9089 \tTraining Loss: 1.982871 \tValidation Loss: 2.310230\n",
      "Epoch: 9090 \tTraining Loss: 1.992520 \tValidation Loss: 2.310257\n",
      "Epoch: 9091 \tTraining Loss: 1.999972 \tValidation Loss: 2.310407\n",
      "Epoch: 9092 \tTraining Loss: 2.012335 \tValidation Loss: 2.310016\n",
      "Epoch: 9093 \tTraining Loss: 1.996118 \tValidation Loss: 2.310426\n",
      "Epoch: 9094 \tTraining Loss: 2.032516 \tValidation Loss: 2.310835\n",
      "Epoch: 9095 \tTraining Loss: 2.022710 \tValidation Loss: 2.311040\n",
      "Epoch: 9096 \tTraining Loss: 2.000342 \tValidation Loss: 2.310933\n",
      "Epoch: 9097 \tTraining Loss: 2.026350 \tValidation Loss: 2.310689\n",
      "Epoch: 9098 \tTraining Loss: 1.967787 \tValidation Loss: 2.310937\n",
      "Epoch: 9099 \tTraining Loss: 1.997967 \tValidation Loss: 2.310689\n",
      "Epoch: 9100 \tTraining Loss: 2.000742 \tValidation Loss: 2.310800\n",
      "Epoch: 9101 \tTraining Loss: 2.012872 \tValidation Loss: 2.310590\n",
      "Epoch: 9102 \tTraining Loss: 1.999156 \tValidation Loss: 2.310427\n",
      "Epoch: 9103 \tTraining Loss: 1.989064 \tValidation Loss: 2.310220\n",
      "Epoch: 9104 \tTraining Loss: 1.992453 \tValidation Loss: 2.310578\n",
      "Epoch: 9105 \tTraining Loss: 2.007084 \tValidation Loss: 2.310354\n",
      "Epoch: 9106 \tTraining Loss: 2.018142 \tValidation Loss: 2.310532\n",
      "Epoch: 9107 \tTraining Loss: 2.019049 \tValidation Loss: 2.310428\n",
      "Epoch: 9108 \tTraining Loss: 2.004159 \tValidation Loss: 2.310412\n",
      "Epoch: 9109 \tTraining Loss: 1.997887 \tValidation Loss: 2.310429\n",
      "Epoch: 9110 \tTraining Loss: 1.980795 \tValidation Loss: 2.310712\n",
      "Epoch: 9111 \tTraining Loss: 2.013843 \tValidation Loss: 2.310560\n",
      "Epoch: 9112 \tTraining Loss: 2.009223 \tValidation Loss: 2.310464\n",
      "Epoch: 9113 \tTraining Loss: 2.010525 \tValidation Loss: 2.310349\n",
      "Epoch: 9114 \tTraining Loss: 2.000434 \tValidation Loss: 2.310322\n",
      "Epoch: 9115 \tTraining Loss: 2.019474 \tValidation Loss: 2.310376\n",
      "Epoch: 9116 \tTraining Loss: 2.002305 \tValidation Loss: 2.310013\n",
      "Epoch: 9117 \tTraining Loss: 1.992389 \tValidation Loss: 2.309767\n",
      "Validation loss decreased (2.309981 --> 2.309767).  Saving model ...\n",
      "Epoch: 9118 \tTraining Loss: 2.009543 \tValidation Loss: 2.309915\n",
      "Epoch: 9119 \tTraining Loss: 1.992411 \tValidation Loss: 2.310024\n",
      "Epoch: 9120 \tTraining Loss: 2.028875 \tValidation Loss: 2.310146\n",
      "Epoch: 9121 \tTraining Loss: 1.970690 \tValidation Loss: 2.310172\n",
      "Epoch: 9122 \tTraining Loss: 2.004924 \tValidation Loss: 2.310024\n",
      "Epoch: 9123 \tTraining Loss: 2.050544 \tValidation Loss: 2.310097\n",
      "Epoch: 9124 \tTraining Loss: 1.989214 \tValidation Loss: 2.310253\n",
      "Epoch: 9125 \tTraining Loss: 2.024518 \tValidation Loss: 2.310291\n",
      "Epoch: 9126 \tTraining Loss: 1.983634 \tValidation Loss: 2.310237\n",
      "Epoch: 9127 \tTraining Loss: 2.005670 \tValidation Loss: 2.310227\n",
      "Epoch: 9128 \tTraining Loss: 1.996497 \tValidation Loss: 2.310244\n",
      "Epoch: 9129 \tTraining Loss: 1.990940 \tValidation Loss: 2.310091\n",
      "Epoch: 9130 \tTraining Loss: 2.012212 \tValidation Loss: 2.310282\n",
      "Epoch: 9131 \tTraining Loss: 1.996976 \tValidation Loss: 2.310295\n",
      "Epoch: 9132 \tTraining Loss: 1.998835 \tValidation Loss: 2.310383\n",
      "Epoch: 9133 \tTraining Loss: 2.032657 \tValidation Loss: 2.310305\n",
      "Epoch: 9134 \tTraining Loss: 2.003980 \tValidation Loss: 2.310003\n",
      "Epoch: 9135 \tTraining Loss: 2.002252 \tValidation Loss: 2.309948\n",
      "Epoch: 9136 \tTraining Loss: 1.991807 \tValidation Loss: 2.309655\n",
      "Validation loss decreased (2.309767 --> 2.309655).  Saving model ...\n",
      "Epoch: 9137 \tTraining Loss: 1.998359 \tValidation Loss: 2.309941\n",
      "Epoch: 9138 \tTraining Loss: 2.005273 \tValidation Loss: 2.310072\n",
      "Epoch: 9139 \tTraining Loss: 1.975463 \tValidation Loss: 2.310449\n",
      "Epoch: 9140 \tTraining Loss: 2.023793 \tValidation Loss: 2.310241\n",
      "Epoch: 9141 \tTraining Loss: 2.021314 \tValidation Loss: 2.310151\n",
      "Epoch: 9142 \tTraining Loss: 2.011792 \tValidation Loss: 2.310391\n",
      "Epoch: 9143 \tTraining Loss: 1.997358 \tValidation Loss: 2.310465\n",
      "Epoch: 9144 \tTraining Loss: 2.002480 \tValidation Loss: 2.310649\n",
      "Epoch: 9145 \tTraining Loss: 1.987095 \tValidation Loss: 2.310381\n",
      "Epoch: 9146 \tTraining Loss: 1.989006 \tValidation Loss: 2.310709\n",
      "Epoch: 9147 \tTraining Loss: 2.025714 \tValidation Loss: 2.310741\n",
      "Epoch: 9148 \tTraining Loss: 1.987109 \tValidation Loss: 2.310811\n",
      "Epoch: 9149 \tTraining Loss: 1.988817 \tValidation Loss: 2.310661\n",
      "Epoch: 9150 \tTraining Loss: 2.011151 \tValidation Loss: 2.310174\n",
      "Epoch: 9151 \tTraining Loss: 2.001595 \tValidation Loss: 2.309810\n",
      "Epoch: 9152 \tTraining Loss: 1.994860 \tValidation Loss: 2.309936\n",
      "Epoch: 9153 \tTraining Loss: 2.006586 \tValidation Loss: 2.309783\n",
      "Epoch: 9154 \tTraining Loss: 2.005430 \tValidation Loss: 2.310004\n",
      "Epoch: 9155 \tTraining Loss: 2.024858 \tValidation Loss: 2.309905\n",
      "Epoch: 9156 \tTraining Loss: 1.995012 \tValidation Loss: 2.309926\n",
      "Epoch: 9157 \tTraining Loss: 2.016770 \tValidation Loss: 2.310251\n",
      "Epoch: 9158 \tTraining Loss: 2.012699 \tValidation Loss: 2.309952\n",
      "Epoch: 9159 \tTraining Loss: 2.014270 \tValidation Loss: 2.310145\n",
      "Epoch: 9160 \tTraining Loss: 1.998986 \tValidation Loss: 2.310096\n",
      "Epoch: 9161 \tTraining Loss: 2.018998 \tValidation Loss: 2.309959\n",
      "Epoch: 9162 \tTraining Loss: 2.005070 \tValidation Loss: 2.309500\n",
      "Validation loss decreased (2.309655 --> 2.309500).  Saving model ...\n",
      "Epoch: 9163 \tTraining Loss: 1.988853 \tValidation Loss: 2.309821\n",
      "Epoch: 9164 \tTraining Loss: 2.021639 \tValidation Loss: 2.309882\n",
      "Epoch: 9165 \tTraining Loss: 1.998384 \tValidation Loss: 2.309984\n",
      "Epoch: 9166 \tTraining Loss: 2.004448 \tValidation Loss: 2.310110\n",
      "Epoch: 9167 \tTraining Loss: 1.988888 \tValidation Loss: 2.310268\n",
      "Epoch: 9168 \tTraining Loss: 2.010744 \tValidation Loss: 2.310622\n",
      "Epoch: 9169 \tTraining Loss: 2.009031 \tValidation Loss: 2.311086\n",
      "Epoch: 9170 \tTraining Loss: 2.033296 \tValidation Loss: 2.310923\n",
      "Epoch: 9171 \tTraining Loss: 1.992274 \tValidation Loss: 2.310486\n",
      "Epoch: 9172 \tTraining Loss: 2.035980 \tValidation Loss: 2.310561\n",
      "Epoch: 9173 \tTraining Loss: 2.000773 \tValidation Loss: 2.310405\n",
      "Epoch: 9174 \tTraining Loss: 2.005586 \tValidation Loss: 2.310664\n",
      "Epoch: 9175 \tTraining Loss: 1.976456 \tValidation Loss: 2.310901\n",
      "Epoch: 9176 \tTraining Loss: 1.962681 \tValidation Loss: 2.310072\n",
      "Epoch: 9177 \tTraining Loss: 2.020975 \tValidation Loss: 2.310338\n",
      "Epoch: 9178 \tTraining Loss: 1.985304 \tValidation Loss: 2.310159\n",
      "Epoch: 9179 \tTraining Loss: 2.007174 \tValidation Loss: 2.310118\n",
      "Epoch: 9180 \tTraining Loss: 1.997035 \tValidation Loss: 2.309960\n",
      "Epoch: 9181 \tTraining Loss: 2.000748 \tValidation Loss: 2.310283\n",
      "Epoch: 9182 \tTraining Loss: 2.002581 \tValidation Loss: 2.310460\n",
      "Epoch: 9183 \tTraining Loss: 2.011948 \tValidation Loss: 2.310370\n",
      "Epoch: 9184 \tTraining Loss: 1.988230 \tValidation Loss: 2.310456\n",
      "Epoch: 9185 \tTraining Loss: 2.025931 \tValidation Loss: 2.310346\n",
      "Epoch: 9186 \tTraining Loss: 1.989130 \tValidation Loss: 2.310174\n",
      "Epoch: 9187 \tTraining Loss: 1.967792 \tValidation Loss: 2.309986\n",
      "Epoch: 9188 \tTraining Loss: 2.000602 \tValidation Loss: 2.310334\n",
      "Epoch: 9189 \tTraining Loss: 1.983476 \tValidation Loss: 2.310172\n",
      "Epoch: 9190 \tTraining Loss: 1.990696 \tValidation Loss: 2.309862\n",
      "Epoch: 9191 \tTraining Loss: 2.010253 \tValidation Loss: 2.309577\n",
      "Epoch: 9192 \tTraining Loss: 1.989814 \tValidation Loss: 2.309705\n",
      "Epoch: 9193 \tTraining Loss: 2.023884 \tValidation Loss: 2.309608\n",
      "Epoch: 9194 \tTraining Loss: 2.023030 \tValidation Loss: 2.309495\n",
      "Validation loss decreased (2.309500 --> 2.309495).  Saving model ...\n",
      "Epoch: 9195 \tTraining Loss: 2.004566 \tValidation Loss: 2.309438\n",
      "Validation loss decreased (2.309495 --> 2.309438).  Saving model ...\n",
      "Epoch: 9196 \tTraining Loss: 2.003243 \tValidation Loss: 2.309350\n",
      "Validation loss decreased (2.309438 --> 2.309350).  Saving model ...\n",
      "Epoch: 9197 \tTraining Loss: 2.009980 \tValidation Loss: 2.309148\n",
      "Validation loss decreased (2.309350 --> 2.309148).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9198 \tTraining Loss: 2.012669 \tValidation Loss: 2.309017\n",
      "Validation loss decreased (2.309148 --> 2.309017).  Saving model ...\n",
      "Epoch: 9199 \tTraining Loss: 2.020396 \tValidation Loss: 2.309305\n",
      "Epoch: 9200 \tTraining Loss: 1.998135 \tValidation Loss: 2.309262\n",
      "Epoch: 9201 \tTraining Loss: 2.011558 \tValidation Loss: 2.309636\n",
      "Epoch: 9202 \tTraining Loss: 2.003443 \tValidation Loss: 2.309823\n",
      "Epoch: 9203 \tTraining Loss: 2.021504 \tValidation Loss: 2.309676\n",
      "Epoch: 9204 \tTraining Loss: 1.997225 \tValidation Loss: 2.309376\n",
      "Epoch: 9205 \tTraining Loss: 2.003377 \tValidation Loss: 2.309672\n",
      "Epoch: 9206 \tTraining Loss: 1.967502 \tValidation Loss: 2.309532\n",
      "Epoch: 9207 \tTraining Loss: 1.986519 \tValidation Loss: 2.309821\n",
      "Epoch: 9208 \tTraining Loss: 2.008118 \tValidation Loss: 2.309886\n",
      "Epoch: 9209 \tTraining Loss: 1.994329 \tValidation Loss: 2.309803\n",
      "Epoch: 9210 \tTraining Loss: 1.989933 \tValidation Loss: 2.310214\n",
      "Epoch: 9211 \tTraining Loss: 2.004265 \tValidation Loss: 2.310270\n",
      "Epoch: 9212 \tTraining Loss: 2.020305 \tValidation Loss: 2.310475\n",
      "Epoch: 9213 \tTraining Loss: 1.981980 \tValidation Loss: 2.310002\n",
      "Epoch: 9214 \tTraining Loss: 1.990128 \tValidation Loss: 2.310023\n",
      "Epoch: 9215 \tTraining Loss: 1.992065 \tValidation Loss: 2.309932\n",
      "Epoch: 9216 \tTraining Loss: 2.002375 \tValidation Loss: 2.309691\n",
      "Epoch: 9217 \tTraining Loss: 1.994290 \tValidation Loss: 2.309596\n",
      "Epoch: 9218 \tTraining Loss: 2.040625 \tValidation Loss: 2.309426\n",
      "Epoch: 9219 \tTraining Loss: 2.018113 \tValidation Loss: 2.309544\n",
      "Epoch: 9220 \tTraining Loss: 2.016853 \tValidation Loss: 2.309679\n",
      "Epoch: 9221 \tTraining Loss: 1.990791 \tValidation Loss: 2.309529\n",
      "Epoch: 9222 \tTraining Loss: 2.020704 \tValidation Loss: 2.309798\n",
      "Epoch: 9223 \tTraining Loss: 2.012509 \tValidation Loss: 2.309981\n",
      "Epoch: 9224 \tTraining Loss: 2.011055 \tValidation Loss: 2.310206\n",
      "Epoch: 9225 \tTraining Loss: 1.993261 \tValidation Loss: 2.310458\n",
      "Epoch: 9226 \tTraining Loss: 1.960874 \tValidation Loss: 2.310495\n",
      "Epoch: 9227 \tTraining Loss: 1.990910 \tValidation Loss: 2.310311\n",
      "Epoch: 9228 \tTraining Loss: 1.998041 \tValidation Loss: 2.310560\n",
      "Epoch: 9229 \tTraining Loss: 1.994221 \tValidation Loss: 2.310815\n",
      "Epoch: 9230 \tTraining Loss: 1.990731 \tValidation Loss: 2.310410\n",
      "Epoch: 9231 \tTraining Loss: 1.972769 \tValidation Loss: 2.310253\n",
      "Epoch: 9232 \tTraining Loss: 2.009207 \tValidation Loss: 2.310272\n",
      "Epoch: 9233 \tTraining Loss: 2.008705 \tValidation Loss: 2.310211\n",
      "Epoch: 9234 \tTraining Loss: 2.031257 \tValidation Loss: 2.310323\n",
      "Epoch: 9235 \tTraining Loss: 1.989895 \tValidation Loss: 2.310339\n",
      "Epoch: 9236 \tTraining Loss: 2.029341 \tValidation Loss: 2.310288\n",
      "Epoch: 9237 \tTraining Loss: 2.002433 \tValidation Loss: 2.310304\n",
      "Epoch: 9238 \tTraining Loss: 2.006291 \tValidation Loss: 2.310220\n",
      "Epoch: 9239 \tTraining Loss: 1.993042 \tValidation Loss: 2.310242\n",
      "Epoch: 9240 \tTraining Loss: 2.008239 \tValidation Loss: 2.310424\n",
      "Epoch: 9241 \tTraining Loss: 1.997402 \tValidation Loss: 2.310541\n",
      "Epoch: 9242 \tTraining Loss: 2.014601 \tValidation Loss: 2.310709\n",
      "Epoch: 9243 \tTraining Loss: 1.989631 \tValidation Loss: 2.310541\n",
      "Epoch: 9244 \tTraining Loss: 1.991860 \tValidation Loss: 2.310456\n",
      "Epoch: 9245 \tTraining Loss: 1.992490 \tValidation Loss: 2.310574\n",
      "Epoch: 9246 \tTraining Loss: 1.999671 \tValidation Loss: 2.310628\n",
      "Epoch: 9247 \tTraining Loss: 1.997514 \tValidation Loss: 2.310180\n",
      "Epoch: 9248 \tTraining Loss: 1.996413 \tValidation Loss: 2.310304\n",
      "Epoch: 9249 \tTraining Loss: 1.993431 \tValidation Loss: 2.310140\n",
      "Epoch: 9250 \tTraining Loss: 1.996014 \tValidation Loss: 2.310050\n",
      "Epoch: 9251 \tTraining Loss: 1.979434 \tValidation Loss: 2.310013\n",
      "Epoch: 9252 \tTraining Loss: 2.010802 \tValidation Loss: 2.309714\n",
      "Epoch: 9253 \tTraining Loss: 1.988233 \tValidation Loss: 2.310084\n",
      "Epoch: 9254 \tTraining Loss: 2.019499 \tValidation Loss: 2.310179\n",
      "Epoch: 9255 \tTraining Loss: 1.987206 \tValidation Loss: 2.310687\n",
      "Epoch: 9256 \tTraining Loss: 1.974482 \tValidation Loss: 2.311169\n",
      "Epoch: 9257 \tTraining Loss: 1.968603 \tValidation Loss: 2.311423\n",
      "Epoch: 9258 \tTraining Loss: 1.986218 \tValidation Loss: 2.311139\n",
      "Epoch: 9259 \tTraining Loss: 2.015946 \tValidation Loss: 2.311311\n",
      "Epoch: 9260 \tTraining Loss: 2.043432 \tValidation Loss: 2.311325\n",
      "Epoch: 9261 \tTraining Loss: 1.975131 \tValidation Loss: 2.311189\n",
      "Epoch: 9262 \tTraining Loss: 1.995556 \tValidation Loss: 2.311263\n",
      "Epoch: 9263 \tTraining Loss: 1.987256 \tValidation Loss: 2.311128\n",
      "Epoch: 9264 \tTraining Loss: 2.005024 \tValidation Loss: 2.311166\n",
      "Epoch: 9265 \tTraining Loss: 1.997943 \tValidation Loss: 2.310937\n",
      "Epoch: 9266 \tTraining Loss: 1.991184 \tValidation Loss: 2.310973\n",
      "Epoch: 9267 \tTraining Loss: 1.990433 \tValidation Loss: 2.310765\n",
      "Epoch: 9268 \tTraining Loss: 2.021099 \tValidation Loss: 2.310722\n",
      "Epoch: 9269 \tTraining Loss: 2.008043 \tValidation Loss: 2.310602\n",
      "Epoch: 9270 \tTraining Loss: 1.984975 \tValidation Loss: 2.310862\n",
      "Epoch: 9271 \tTraining Loss: 1.977712 \tValidation Loss: 2.310936\n",
      "Epoch: 9272 \tTraining Loss: 1.992292 \tValidation Loss: 2.311057\n",
      "Epoch: 9273 \tTraining Loss: 1.971181 \tValidation Loss: 2.310359\n",
      "Epoch: 9274 \tTraining Loss: 1.999756 \tValidation Loss: 2.310319\n",
      "Epoch: 9275 \tTraining Loss: 2.008616 \tValidation Loss: 2.310333\n",
      "Epoch: 9276 \tTraining Loss: 2.017830 \tValidation Loss: 2.310463\n",
      "Epoch: 9277 \tTraining Loss: 1.976273 \tValidation Loss: 2.310480\n",
      "Epoch: 9278 \tTraining Loss: 1.976780 \tValidation Loss: 2.310479\n",
      "Epoch: 9279 \tTraining Loss: 2.000261 \tValidation Loss: 2.310092\n",
      "Epoch: 9280 \tTraining Loss: 1.997523 \tValidation Loss: 2.310039\n",
      "Epoch: 9281 \tTraining Loss: 1.992508 \tValidation Loss: 2.310203\n",
      "Epoch: 9282 \tTraining Loss: 2.000543 \tValidation Loss: 2.310545\n",
      "Epoch: 9283 \tTraining Loss: 1.994692 \tValidation Loss: 2.310669\n",
      "Epoch: 9284 \tTraining Loss: 1.990849 \tValidation Loss: 2.310458\n",
      "Epoch: 9285 \tTraining Loss: 2.027555 \tValidation Loss: 2.310327\n",
      "Epoch: 9286 \tTraining Loss: 1.952065 \tValidation Loss: 2.310544\n",
      "Epoch: 9287 \tTraining Loss: 1.976717 \tValidation Loss: 2.310542\n",
      "Epoch: 9288 \tTraining Loss: 1.983777 \tValidation Loss: 2.310024\n",
      "Epoch: 9289 \tTraining Loss: 2.001125 \tValidation Loss: 2.310212\n",
      "Epoch: 9290 \tTraining Loss: 1.989070 \tValidation Loss: 2.310127\n",
      "Epoch: 9291 \tTraining Loss: 1.975037 \tValidation Loss: 2.309911\n",
      "Epoch: 9292 \tTraining Loss: 2.000228 \tValidation Loss: 2.310400\n",
      "Epoch: 9293 \tTraining Loss: 1.990987 \tValidation Loss: 2.310471\n",
      "Epoch: 9294 \tTraining Loss: 2.008056 \tValidation Loss: 2.310302\n",
      "Epoch: 9295 \tTraining Loss: 2.018496 \tValidation Loss: 2.310138\n",
      "Epoch: 9296 \tTraining Loss: 2.008253 \tValidation Loss: 2.310252\n",
      "Epoch: 9297 \tTraining Loss: 1.993501 \tValidation Loss: 2.310558\n",
      "Epoch: 9298 \tTraining Loss: 1.981257 \tValidation Loss: 2.310612\n",
      "Epoch: 9299 \tTraining Loss: 2.022870 \tValidation Loss: 2.310414\n",
      "Epoch: 9300 \tTraining Loss: 1.977853 \tValidation Loss: 2.310779\n",
      "Epoch: 9301 \tTraining Loss: 2.018493 \tValidation Loss: 2.311095\n",
      "Epoch: 9302 \tTraining Loss: 1.979988 \tValidation Loss: 2.310815\n",
      "Epoch: 9303 \tTraining Loss: 1.993884 \tValidation Loss: 2.311039\n",
      "Epoch: 9304 \tTraining Loss: 2.001276 \tValidation Loss: 2.311194\n",
      "Epoch: 9305 \tTraining Loss: 1.980768 \tValidation Loss: 2.310934\n",
      "Epoch: 9306 \tTraining Loss: 1.984803 \tValidation Loss: 2.310822\n",
      "Epoch: 9307 \tTraining Loss: 1.988328 \tValidation Loss: 2.311100\n",
      "Epoch: 9308 \tTraining Loss: 2.002016 \tValidation Loss: 2.310941\n",
      "Epoch: 9309 \tTraining Loss: 2.004569 \tValidation Loss: 2.311029\n",
      "Epoch: 9310 \tTraining Loss: 2.000907 \tValidation Loss: 2.311002\n",
      "Epoch: 9311 \tTraining Loss: 1.976220 \tValidation Loss: 2.311112\n",
      "Epoch: 9312 \tTraining Loss: 1.996205 \tValidation Loss: 2.311135\n",
      "Epoch: 9313 \tTraining Loss: 1.996884 \tValidation Loss: 2.310859\n",
      "Epoch: 9314 \tTraining Loss: 1.991071 \tValidation Loss: 2.311121\n",
      "Epoch: 9315 \tTraining Loss: 2.015632 \tValidation Loss: 2.310789\n",
      "Epoch: 9316 \tTraining Loss: 1.985892 \tValidation Loss: 2.310886\n",
      "Epoch: 9317 \tTraining Loss: 1.991125 \tValidation Loss: 2.310385\n",
      "Epoch: 9318 \tTraining Loss: 1.996950 \tValidation Loss: 2.310355\n",
      "Epoch: 9319 \tTraining Loss: 2.002505 \tValidation Loss: 2.310187\n",
      "Epoch: 9320 \tTraining Loss: 1.970933 \tValidation Loss: 2.310148\n",
      "Epoch: 9321 \tTraining Loss: 2.011186 \tValidation Loss: 2.310072\n",
      "Epoch: 9322 \tTraining Loss: 1.986393 \tValidation Loss: 2.310126\n",
      "Epoch: 9323 \tTraining Loss: 2.014086 \tValidation Loss: 2.310058\n",
      "Epoch: 9324 \tTraining Loss: 1.997318 \tValidation Loss: 2.310408\n",
      "Epoch: 9325 \tTraining Loss: 1.996911 \tValidation Loss: 2.310142\n",
      "Epoch: 9326 \tTraining Loss: 2.000278 \tValidation Loss: 2.309977\n",
      "Epoch: 9327 \tTraining Loss: 1.972547 \tValidation Loss: 2.309876\n",
      "Epoch: 9328 \tTraining Loss: 1.990361 \tValidation Loss: 2.310656\n",
      "Epoch: 9329 \tTraining Loss: 2.000129 \tValidation Loss: 2.310536\n",
      "Epoch: 9330 \tTraining Loss: 2.000592 \tValidation Loss: 2.310767\n",
      "Epoch: 9331 \tTraining Loss: 1.989854 \tValidation Loss: 2.310282\n",
      "Epoch: 9332 \tTraining Loss: 1.944565 \tValidation Loss: 2.310101\n",
      "Epoch: 9333 \tTraining Loss: 2.019488 \tValidation Loss: 2.309892\n",
      "Epoch: 9334 \tTraining Loss: 1.988613 \tValidation Loss: 2.309646\n",
      "Epoch: 9335 \tTraining Loss: 1.987445 \tValidation Loss: 2.309952\n",
      "Epoch: 9336 \tTraining Loss: 1.982306 \tValidation Loss: 2.309648\n",
      "Epoch: 9337 \tTraining Loss: 1.996017 \tValidation Loss: 2.310050\n",
      "Epoch: 9338 \tTraining Loss: 2.001509 \tValidation Loss: 2.309816\n",
      "Epoch: 9339 \tTraining Loss: 1.976197 \tValidation Loss: 2.309386\n",
      "Epoch: 9340 \tTraining Loss: 1.968829 \tValidation Loss: 2.309529\n",
      "Epoch: 9341 \tTraining Loss: 1.980904 \tValidation Loss: 2.309180\n",
      "Epoch: 9342 \tTraining Loss: 1.962134 \tValidation Loss: 2.309281\n",
      "Epoch: 9343 \tTraining Loss: 1.996705 \tValidation Loss: 2.309586\n",
      "Epoch: 9344 \tTraining Loss: 2.005794 \tValidation Loss: 2.309775\n",
      "Epoch: 9345 \tTraining Loss: 1.996834 \tValidation Loss: 2.309615\n",
      "Epoch: 9346 \tTraining Loss: 1.961953 \tValidation Loss: 2.309631\n",
      "Epoch: 9347 \tTraining Loss: 2.001347 \tValidation Loss: 2.309805\n",
      "Epoch: 9348 \tTraining Loss: 2.000914 \tValidation Loss: 2.309666\n",
      "Epoch: 9349 \tTraining Loss: 2.015337 \tValidation Loss: 2.309840\n",
      "Epoch: 9350 \tTraining Loss: 2.009411 \tValidation Loss: 2.309729\n",
      "Epoch: 9351 \tTraining Loss: 1.974819 \tValidation Loss: 2.309661\n",
      "Epoch: 9352 \tTraining Loss: 1.962841 \tValidation Loss: 2.309774\n",
      "Epoch: 9353 \tTraining Loss: 1.993927 \tValidation Loss: 2.309941\n",
      "Epoch: 9354 \tTraining Loss: 2.012532 \tValidation Loss: 2.309790\n",
      "Epoch: 9355 \tTraining Loss: 2.004916 \tValidation Loss: 2.309765\n",
      "Epoch: 9356 \tTraining Loss: 1.996475 \tValidation Loss: 2.309739\n",
      "Epoch: 9357 \tTraining Loss: 2.010252 \tValidation Loss: 2.309603\n",
      "Epoch: 9358 \tTraining Loss: 1.990035 \tValidation Loss: 2.309620\n",
      "Epoch: 9359 \tTraining Loss: 1.993250 \tValidation Loss: 2.309764\n",
      "Epoch: 9360 \tTraining Loss: 1.966230 \tValidation Loss: 2.310037\n",
      "Epoch: 9361 \tTraining Loss: 2.021255 \tValidation Loss: 2.310189\n",
      "Epoch: 9362 \tTraining Loss: 2.001817 \tValidation Loss: 2.310416\n",
      "Epoch: 9363 \tTraining Loss: 2.014673 \tValidation Loss: 2.310483\n",
      "Epoch: 9364 \tTraining Loss: 1.982103 \tValidation Loss: 2.310183\n",
      "Epoch: 9365 \tTraining Loss: 2.002883 \tValidation Loss: 2.310312\n",
      "Epoch: 9366 \tTraining Loss: 1.989660 \tValidation Loss: 2.310235\n",
      "Epoch: 9367 \tTraining Loss: 2.012608 \tValidation Loss: 2.310601\n",
      "Epoch: 9368 \tTraining Loss: 1.998994 \tValidation Loss: 2.310611\n",
      "Epoch: 9369 \tTraining Loss: 2.003375 \tValidation Loss: 2.310889\n",
      "Epoch: 9370 \tTraining Loss: 1.993761 \tValidation Loss: 2.310839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9371 \tTraining Loss: 1.986346 \tValidation Loss: 2.310899\n",
      "Epoch: 9372 \tTraining Loss: 1.959458 \tValidation Loss: 2.310431\n",
      "Epoch: 9373 \tTraining Loss: 1.996347 \tValidation Loss: 2.310439\n",
      "Epoch: 9374 \tTraining Loss: 1.978971 \tValidation Loss: 2.310633\n",
      "Epoch: 9375 \tTraining Loss: 1.974414 \tValidation Loss: 2.310121\n",
      "Epoch: 9376 \tTraining Loss: 2.014717 \tValidation Loss: 2.309714\n",
      "Epoch: 9377 \tTraining Loss: 1.996504 \tValidation Loss: 2.309522\n",
      "Epoch: 9378 \tTraining Loss: 1.997655 \tValidation Loss: 2.309558\n",
      "Epoch: 9379 \tTraining Loss: 1.997779 \tValidation Loss: 2.309769\n",
      "Epoch: 9380 \tTraining Loss: 1.980111 \tValidation Loss: 2.309466\n",
      "Epoch: 9381 \tTraining Loss: 2.003026 \tValidation Loss: 2.309483\n",
      "Epoch: 9382 \tTraining Loss: 1.998204 \tValidation Loss: 2.309660\n",
      "Epoch: 9383 \tTraining Loss: 1.981548 \tValidation Loss: 2.309645\n",
      "Epoch: 9384 \tTraining Loss: 1.967748 \tValidation Loss: 2.310268\n",
      "Epoch: 9385 \tTraining Loss: 1.993444 \tValidation Loss: 2.309872\n",
      "Epoch: 9386 \tTraining Loss: 1.989624 \tValidation Loss: 2.309834\n",
      "Epoch: 9387 \tTraining Loss: 1.965596 \tValidation Loss: 2.310421\n",
      "Epoch: 9388 \tTraining Loss: 1.996143 \tValidation Loss: 2.310159\n",
      "Epoch: 9389 \tTraining Loss: 2.004998 \tValidation Loss: 2.310336\n",
      "Epoch: 9390 \tTraining Loss: 2.007649 \tValidation Loss: 2.310040\n",
      "Epoch: 9391 \tTraining Loss: 2.020394 \tValidation Loss: 2.310266\n",
      "Epoch: 9392 \tTraining Loss: 1.970255 \tValidation Loss: 2.310268\n",
      "Epoch: 9393 \tTraining Loss: 1.961223 \tValidation Loss: 2.310234\n",
      "Epoch: 9394 \tTraining Loss: 2.009602 \tValidation Loss: 2.310073\n",
      "Epoch: 9395 \tTraining Loss: 1.991285 \tValidation Loss: 2.310440\n",
      "Epoch: 9396 \tTraining Loss: 2.003727 \tValidation Loss: 2.310633\n",
      "Epoch: 9397 \tTraining Loss: 1.980868 \tValidation Loss: 2.310833\n",
      "Epoch: 9398 \tTraining Loss: 1.991516 \tValidation Loss: 2.310791\n",
      "Epoch: 9399 \tTraining Loss: 1.993555 \tValidation Loss: 2.310906\n",
      "Epoch: 9400 \tTraining Loss: 1.996610 \tValidation Loss: 2.311135\n",
      "Epoch: 9401 \tTraining Loss: 1.996158 \tValidation Loss: 2.311136\n",
      "Epoch: 9402 \tTraining Loss: 1.975996 \tValidation Loss: 2.311062\n",
      "Epoch: 9403 \tTraining Loss: 1.984857 \tValidation Loss: 2.311250\n",
      "Epoch: 9404 \tTraining Loss: 1.972988 \tValidation Loss: 2.311145\n",
      "Epoch: 9405 \tTraining Loss: 2.002297 \tValidation Loss: 2.311097\n",
      "Epoch: 9406 \tTraining Loss: 1.988283 \tValidation Loss: 2.310779\n",
      "Epoch: 9407 \tTraining Loss: 1.992527 \tValidation Loss: 2.310795\n",
      "Epoch: 9408 \tTraining Loss: 1.997241 \tValidation Loss: 2.310599\n",
      "Epoch: 9409 \tTraining Loss: 1.985482 \tValidation Loss: 2.310366\n",
      "Epoch: 9410 \tTraining Loss: 2.004799 \tValidation Loss: 2.310408\n",
      "Epoch: 9411 \tTraining Loss: 2.015110 \tValidation Loss: 2.310581\n",
      "Epoch: 9412 \tTraining Loss: 1.967598 \tValidation Loss: 2.310629\n",
      "Epoch: 9413 \tTraining Loss: 1.992596 \tValidation Loss: 2.310482\n",
      "Epoch: 9414 \tTraining Loss: 1.974319 \tValidation Loss: 2.310143\n",
      "Epoch: 9415 \tTraining Loss: 2.007476 \tValidation Loss: 2.310017\n",
      "Epoch: 9416 \tTraining Loss: 2.001880 \tValidation Loss: 2.310046\n",
      "Epoch: 9417 \tTraining Loss: 1.967302 \tValidation Loss: 2.310079\n",
      "Epoch: 9418 \tTraining Loss: 1.971869 \tValidation Loss: 2.310413\n",
      "Epoch: 9419 \tTraining Loss: 1.965455 \tValidation Loss: 2.310004\n",
      "Epoch: 9420 \tTraining Loss: 1.999751 \tValidation Loss: 2.309966\n",
      "Epoch: 9421 \tTraining Loss: 1.980450 \tValidation Loss: 2.310350\n",
      "Epoch: 9422 \tTraining Loss: 1.973607 \tValidation Loss: 2.310192\n",
      "Epoch: 9423 \tTraining Loss: 1.987822 \tValidation Loss: 2.309810\n",
      "Epoch: 9424 \tTraining Loss: 1.984603 \tValidation Loss: 2.310059\n",
      "Epoch: 9425 \tTraining Loss: 1.965308 \tValidation Loss: 2.309754\n",
      "Epoch: 9426 \tTraining Loss: 1.992657 \tValidation Loss: 2.309625\n",
      "Epoch: 9427 \tTraining Loss: 1.973251 \tValidation Loss: 2.309560\n",
      "Epoch: 9428 \tTraining Loss: 1.986541 \tValidation Loss: 2.309443\n",
      "Epoch: 9429 \tTraining Loss: 1.992417 \tValidation Loss: 2.309412\n",
      "Epoch: 9430 \tTraining Loss: 1.997534 \tValidation Loss: 2.309289\n",
      "Epoch: 9431 \tTraining Loss: 2.001839 \tValidation Loss: 2.309164\n",
      "Epoch: 9432 \tTraining Loss: 1.989375 \tValidation Loss: 2.309750\n",
      "Epoch: 9433 \tTraining Loss: 2.016136 \tValidation Loss: 2.309980\n",
      "Epoch: 9434 \tTraining Loss: 1.996712 \tValidation Loss: 2.310414\n",
      "Epoch: 9435 \tTraining Loss: 1.979868 \tValidation Loss: 2.310455\n",
      "Epoch: 9436 \tTraining Loss: 1.979913 \tValidation Loss: 2.310396\n",
      "Epoch: 9437 \tTraining Loss: 1.984651 \tValidation Loss: 2.310280\n",
      "Epoch: 9438 \tTraining Loss: 1.996255 \tValidation Loss: 2.310236\n",
      "Epoch: 9439 \tTraining Loss: 1.990722 \tValidation Loss: 2.309978\n",
      "Epoch: 9440 \tTraining Loss: 1.994254 \tValidation Loss: 2.309971\n",
      "Epoch: 9441 \tTraining Loss: 1.995604 \tValidation Loss: 2.309806\n",
      "Epoch: 9442 \tTraining Loss: 1.986107 \tValidation Loss: 2.309747\n",
      "Epoch: 9443 \tTraining Loss: 1.967672 \tValidation Loss: 2.309880\n",
      "Epoch: 9444 \tTraining Loss: 1.998226 \tValidation Loss: 2.309858\n",
      "Epoch: 9445 \tTraining Loss: 1.979791 \tValidation Loss: 2.309793\n",
      "Epoch: 9446 \tTraining Loss: 1.984113 \tValidation Loss: 2.309812\n",
      "Epoch: 9447 \tTraining Loss: 2.006690 \tValidation Loss: 2.309945\n",
      "Epoch: 9448 \tTraining Loss: 1.979009 \tValidation Loss: 2.310049\n",
      "Epoch: 9449 \tTraining Loss: 2.022300 \tValidation Loss: 2.310384\n",
      "Epoch: 9450 \tTraining Loss: 1.967272 \tValidation Loss: 2.309937\n",
      "Epoch: 9451 \tTraining Loss: 1.964711 \tValidation Loss: 2.309964\n",
      "Epoch: 9452 \tTraining Loss: 1.973914 \tValidation Loss: 2.309932\n",
      "Epoch: 9453 \tTraining Loss: 1.974036 \tValidation Loss: 2.309766\n",
      "Epoch: 9454 \tTraining Loss: 1.981213 \tValidation Loss: 2.310256\n",
      "Epoch: 9455 \tTraining Loss: 1.969770 \tValidation Loss: 2.309941\n",
      "Epoch: 9456 \tTraining Loss: 1.993345 \tValidation Loss: 2.310025\n",
      "Epoch: 9457 \tTraining Loss: 2.007467 \tValidation Loss: 2.309707\n",
      "Epoch: 9458 \tTraining Loss: 1.974555 \tValidation Loss: 2.310201\n",
      "Epoch: 9459 \tTraining Loss: 2.007516 \tValidation Loss: 2.309805\n",
      "Epoch: 9460 \tTraining Loss: 1.962436 \tValidation Loss: 2.309395\n",
      "Epoch: 9461 \tTraining Loss: 1.981531 \tValidation Loss: 2.309752\n",
      "Epoch: 9462 \tTraining Loss: 1.957499 \tValidation Loss: 2.309692\n",
      "Epoch: 9463 \tTraining Loss: 2.001185 \tValidation Loss: 2.309946\n",
      "Epoch: 9464 \tTraining Loss: 1.973153 \tValidation Loss: 2.310161\n",
      "Epoch: 9465 \tTraining Loss: 1.979074 \tValidation Loss: 2.310105\n",
      "Epoch: 9466 \tTraining Loss: 2.002666 \tValidation Loss: 2.309656\n",
      "Epoch: 9467 \tTraining Loss: 1.994349 \tValidation Loss: 2.309803\n",
      "Epoch: 9468 \tTraining Loss: 1.972140 \tValidation Loss: 2.309975\n",
      "Epoch: 9469 \tTraining Loss: 1.985146 \tValidation Loss: 2.310186\n",
      "Epoch: 9470 \tTraining Loss: 1.977098 \tValidation Loss: 2.310063\n",
      "Epoch: 9471 \tTraining Loss: 1.982017 \tValidation Loss: 2.309767\n",
      "Epoch: 9472 \tTraining Loss: 1.977694 \tValidation Loss: 2.309494\n",
      "Epoch: 9473 \tTraining Loss: 1.978459 \tValidation Loss: 2.309273\n",
      "Epoch: 9474 \tTraining Loss: 1.994573 \tValidation Loss: 2.309203\n",
      "Epoch: 9475 \tTraining Loss: 1.987570 \tValidation Loss: 2.309532\n",
      "Epoch: 9476 \tTraining Loss: 1.960587 \tValidation Loss: 2.309778\n",
      "Epoch: 9477 \tTraining Loss: 1.989422 \tValidation Loss: 2.309600\n",
      "Epoch: 9478 \tTraining Loss: 1.992195 \tValidation Loss: 2.309554\n",
      "Epoch: 9479 \tTraining Loss: 2.008260 \tValidation Loss: 2.309526\n",
      "Epoch: 9480 \tTraining Loss: 1.993459 \tValidation Loss: 2.309885\n",
      "Epoch: 9481 \tTraining Loss: 1.983522 \tValidation Loss: 2.309924\n",
      "Epoch: 9482 \tTraining Loss: 1.979242 \tValidation Loss: 2.310062\n",
      "Epoch: 9483 \tTraining Loss: 2.013741 \tValidation Loss: 2.309772\n",
      "Epoch: 9484 \tTraining Loss: 1.986595 \tValidation Loss: 2.309864\n",
      "Epoch: 9485 \tTraining Loss: 1.967177 \tValidation Loss: 2.310110\n",
      "Epoch: 9486 \tTraining Loss: 2.040763 \tValidation Loss: 2.309894\n",
      "Epoch: 9487 \tTraining Loss: 1.958138 \tValidation Loss: 2.310086\n",
      "Epoch: 9488 \tTraining Loss: 1.983035 \tValidation Loss: 2.310110\n",
      "Epoch: 9489 \tTraining Loss: 1.998035 \tValidation Loss: 2.309902\n",
      "Epoch: 9490 \tTraining Loss: 1.984577 \tValidation Loss: 2.309909\n",
      "Epoch: 9491 \tTraining Loss: 1.971236 \tValidation Loss: 2.309701\n",
      "Epoch: 9492 \tTraining Loss: 1.993365 \tValidation Loss: 2.309677\n",
      "Epoch: 9493 \tTraining Loss: 1.989859 \tValidation Loss: 2.309415\n",
      "Epoch: 9494 \tTraining Loss: 1.981713 \tValidation Loss: 2.309613\n",
      "Epoch: 9495 \tTraining Loss: 1.969986 \tValidation Loss: 2.309898\n",
      "Epoch: 9496 \tTraining Loss: 1.979581 \tValidation Loss: 2.309903\n",
      "Epoch: 9497 \tTraining Loss: 2.007914 \tValidation Loss: 2.310016\n",
      "Epoch: 9498 \tTraining Loss: 1.978427 \tValidation Loss: 2.309704\n",
      "Epoch: 9499 \tTraining Loss: 1.995736 \tValidation Loss: 2.309824\n",
      "Epoch: 9500 \tTraining Loss: 1.983560 \tValidation Loss: 2.310362\n",
      "Epoch: 9501 \tTraining Loss: 1.977587 \tValidation Loss: 2.309997\n",
      "Epoch: 9502 \tTraining Loss: 2.006527 \tValidation Loss: 2.309629\n",
      "Epoch: 9503 \tTraining Loss: 2.004366 \tValidation Loss: 2.309448\n",
      "Epoch: 9504 \tTraining Loss: 2.011771 \tValidation Loss: 2.309564\n",
      "Epoch: 9505 \tTraining Loss: 1.984061 \tValidation Loss: 2.309507\n",
      "Epoch: 9506 \tTraining Loss: 1.994740 \tValidation Loss: 2.309406\n",
      "Epoch: 9507 \tTraining Loss: 2.015867 \tValidation Loss: 2.309417\n",
      "Epoch: 9508 \tTraining Loss: 1.971749 \tValidation Loss: 2.309598\n",
      "Epoch: 9509 \tTraining Loss: 1.980894 \tValidation Loss: 2.309753\n",
      "Epoch: 9510 \tTraining Loss: 1.970686 \tValidation Loss: 2.309457\n",
      "Epoch: 9511 \tTraining Loss: 1.974399 \tValidation Loss: 2.309416\n",
      "Epoch: 9512 \tTraining Loss: 1.968500 \tValidation Loss: 2.309583\n",
      "Epoch: 9513 \tTraining Loss: 1.977418 \tValidation Loss: 2.309318\n",
      "Epoch: 9514 \tTraining Loss: 1.968059 \tValidation Loss: 2.309271\n",
      "Epoch: 9515 \tTraining Loss: 1.987546 \tValidation Loss: 2.308796\n",
      "Validation loss decreased (2.309017 --> 2.308796).  Saving model ...\n",
      "Epoch: 9516 \tTraining Loss: 1.982596 \tValidation Loss: 2.309052\n",
      "Epoch: 9517 \tTraining Loss: 1.981426 \tValidation Loss: 2.309448\n",
      "Epoch: 9518 \tTraining Loss: 1.991306 \tValidation Loss: 2.309563\n",
      "Epoch: 9519 \tTraining Loss: 2.001776 \tValidation Loss: 2.309425\n",
      "Epoch: 9520 \tTraining Loss: 1.965475 \tValidation Loss: 2.309288\n",
      "Epoch: 9521 \tTraining Loss: 1.975492 \tValidation Loss: 2.309447\n",
      "Epoch: 9522 \tTraining Loss: 1.985618 \tValidation Loss: 2.309227\n",
      "Epoch: 9523 \tTraining Loss: 1.970801 \tValidation Loss: 2.309118\n",
      "Epoch: 9524 \tTraining Loss: 1.989249 \tValidation Loss: 2.309546\n",
      "Epoch: 9525 \tTraining Loss: 1.944757 \tValidation Loss: 2.310087\n",
      "Epoch: 9526 \tTraining Loss: 1.990590 \tValidation Loss: 2.310198\n",
      "Epoch: 9527 \tTraining Loss: 1.972208 \tValidation Loss: 2.310307\n",
      "Epoch: 9528 \tTraining Loss: 2.009176 \tValidation Loss: 2.310090\n",
      "Epoch: 9529 \tTraining Loss: 1.997712 \tValidation Loss: 2.310046\n",
      "Epoch: 9530 \tTraining Loss: 1.996098 \tValidation Loss: 2.310290\n",
      "Epoch: 9531 \tTraining Loss: 1.961964 \tValidation Loss: 2.310214\n",
      "Epoch: 9532 \tTraining Loss: 2.000878 \tValidation Loss: 2.309920\n",
      "Epoch: 9533 \tTraining Loss: 1.965685 \tValidation Loss: 2.309849\n",
      "Epoch: 9534 \tTraining Loss: 1.980079 \tValidation Loss: 2.309827\n",
      "Epoch: 9535 \tTraining Loss: 1.956165 \tValidation Loss: 2.309829\n",
      "Epoch: 9536 \tTraining Loss: 1.963367 \tValidation Loss: 2.309610\n",
      "Epoch: 9537 \tTraining Loss: 1.997750 \tValidation Loss: 2.309705\n",
      "Epoch: 9538 \tTraining Loss: 1.966113 \tValidation Loss: 2.309844\n",
      "Epoch: 9539 \tTraining Loss: 1.992480 \tValidation Loss: 2.309771\n",
      "Epoch: 9540 \tTraining Loss: 2.003618 \tValidation Loss: 2.309713\n",
      "Epoch: 9541 \tTraining Loss: 1.981421 \tValidation Loss: 2.310067\n",
      "Epoch: 9542 \tTraining Loss: 1.979661 \tValidation Loss: 2.310429\n",
      "Epoch: 9543 \tTraining Loss: 1.988213 \tValidation Loss: 2.310078\n",
      "Epoch: 9544 \tTraining Loss: 1.969989 \tValidation Loss: 2.310172\n",
      "Epoch: 9545 \tTraining Loss: 2.006821 \tValidation Loss: 2.310118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9546 \tTraining Loss: 1.963471 \tValidation Loss: 2.310203\n",
      "Epoch: 9547 \tTraining Loss: 1.983195 \tValidation Loss: 2.310203\n",
      "Epoch: 9548 \tTraining Loss: 1.974900 \tValidation Loss: 2.310070\n",
      "Epoch: 9549 \tTraining Loss: 1.988299 \tValidation Loss: 2.310129\n",
      "Epoch: 9550 \tTraining Loss: 1.984529 \tValidation Loss: 2.310061\n",
      "Epoch: 9551 \tTraining Loss: 1.966351 \tValidation Loss: 2.310475\n",
      "Epoch: 9552 \tTraining Loss: 1.983390 \tValidation Loss: 2.310498\n",
      "Epoch: 9553 \tTraining Loss: 1.969624 \tValidation Loss: 2.310323\n",
      "Epoch: 9554 \tTraining Loss: 1.978274 \tValidation Loss: 2.310434\n",
      "Epoch: 9555 \tTraining Loss: 1.974154 \tValidation Loss: 2.310544\n",
      "Epoch: 9556 \tTraining Loss: 1.975718 \tValidation Loss: 2.310323\n",
      "Epoch: 9557 \tTraining Loss: 1.984140 \tValidation Loss: 2.310115\n",
      "Epoch: 9558 \tTraining Loss: 1.980994 \tValidation Loss: 2.310243\n",
      "Epoch: 9559 \tTraining Loss: 1.981534 \tValidation Loss: 2.310184\n",
      "Epoch: 9560 \tTraining Loss: 1.976719 \tValidation Loss: 2.310208\n",
      "Epoch: 9561 \tTraining Loss: 1.949322 \tValidation Loss: 2.310305\n",
      "Epoch: 9562 \tTraining Loss: 1.991981 \tValidation Loss: 2.310236\n",
      "Epoch: 9563 \tTraining Loss: 1.983039 \tValidation Loss: 2.310270\n",
      "Epoch: 9564 \tTraining Loss: 1.981086 \tValidation Loss: 2.310275\n",
      "Epoch: 9565 \tTraining Loss: 1.958629 \tValidation Loss: 2.309961\n",
      "Epoch: 9566 \tTraining Loss: 1.984755 \tValidation Loss: 2.310081\n",
      "Epoch: 9567 \tTraining Loss: 1.963121 \tValidation Loss: 2.310254\n",
      "Epoch: 9568 \tTraining Loss: 1.998985 \tValidation Loss: 2.310346\n",
      "Epoch: 9569 \tTraining Loss: 1.986059 \tValidation Loss: 2.310102\n",
      "Epoch: 9570 \tTraining Loss: 2.010140 \tValidation Loss: 2.310120\n",
      "Epoch: 9571 \tTraining Loss: 1.983796 \tValidation Loss: 2.309659\n",
      "Epoch: 9572 \tTraining Loss: 1.999092 \tValidation Loss: 2.309556\n",
      "Epoch: 9573 \tTraining Loss: 1.974128 \tValidation Loss: 2.309601\n",
      "Epoch: 9574 \tTraining Loss: 1.967702 \tValidation Loss: 2.309533\n",
      "Epoch: 9575 \tTraining Loss: 1.976651 \tValidation Loss: 2.309860\n",
      "Epoch: 9576 \tTraining Loss: 1.952631 \tValidation Loss: 2.309745\n",
      "Epoch: 9577 \tTraining Loss: 1.985951 \tValidation Loss: 2.309796\n",
      "Epoch: 9578 \tTraining Loss: 1.986175 \tValidation Loss: 2.309807\n",
      "Epoch: 9579 \tTraining Loss: 1.964097 \tValidation Loss: 2.310195\n",
      "Epoch: 9580 \tTraining Loss: 1.982699 \tValidation Loss: 2.310539\n",
      "Epoch: 9581 \tTraining Loss: 1.996761 \tValidation Loss: 2.310525\n",
      "Epoch: 9582 \tTraining Loss: 1.981778 \tValidation Loss: 2.310249\n",
      "Epoch: 9583 \tTraining Loss: 1.982241 \tValidation Loss: 2.310077\n",
      "Epoch: 9584 \tTraining Loss: 1.995022 \tValidation Loss: 2.310250\n",
      "Epoch: 9585 \tTraining Loss: 1.982529 \tValidation Loss: 2.309547\n",
      "Epoch: 9586 \tTraining Loss: 1.966727 \tValidation Loss: 2.309469\n",
      "Epoch: 9587 \tTraining Loss: 1.966517 \tValidation Loss: 2.309846\n",
      "Epoch: 9588 \tTraining Loss: 1.996160 \tValidation Loss: 2.309721\n",
      "Epoch: 9589 \tTraining Loss: 1.985771 \tValidation Loss: 2.309770\n",
      "Epoch: 9590 \tTraining Loss: 2.000195 \tValidation Loss: 2.309889\n",
      "Epoch: 9591 \tTraining Loss: 1.978635 \tValidation Loss: 2.309778\n",
      "Epoch: 9592 \tTraining Loss: 1.986519 \tValidation Loss: 2.309376\n",
      "Epoch: 9593 \tTraining Loss: 1.970494 \tValidation Loss: 2.309685\n",
      "Epoch: 9594 \tTraining Loss: 1.970821 \tValidation Loss: 2.309829\n",
      "Epoch: 9595 \tTraining Loss: 1.961164 \tValidation Loss: 2.309788\n",
      "Epoch: 9596 \tTraining Loss: 1.998453 \tValidation Loss: 2.309705\n",
      "Epoch: 9597 \tTraining Loss: 1.969153 \tValidation Loss: 2.309742\n",
      "Epoch: 9598 \tTraining Loss: 1.998496 \tValidation Loss: 2.309552\n",
      "Epoch: 9599 \tTraining Loss: 1.963693 \tValidation Loss: 2.310124\n",
      "Epoch: 9600 \tTraining Loss: 1.957169 \tValidation Loss: 2.310233\n",
      "Epoch: 9601 \tTraining Loss: 1.954983 \tValidation Loss: 2.310296\n",
      "Epoch: 9602 \tTraining Loss: 1.993957 \tValidation Loss: 2.310186\n",
      "Epoch: 9603 \tTraining Loss: 1.988059 \tValidation Loss: 2.310192\n",
      "Epoch: 9604 \tTraining Loss: 2.004261 \tValidation Loss: 2.309886\n",
      "Epoch: 9605 \tTraining Loss: 1.969784 \tValidation Loss: 2.309694\n",
      "Epoch: 9606 \tTraining Loss: 2.000161 \tValidation Loss: 2.309572\n",
      "Epoch: 9607 \tTraining Loss: 2.017077 \tValidation Loss: 2.309520\n",
      "Epoch: 9608 \tTraining Loss: 1.994348 \tValidation Loss: 2.309920\n",
      "Epoch: 9609 \tTraining Loss: 1.978135 \tValidation Loss: 2.310058\n",
      "Epoch: 9610 \tTraining Loss: 1.988316 \tValidation Loss: 2.310270\n",
      "Epoch: 9611 \tTraining Loss: 1.989750 \tValidation Loss: 2.310408\n",
      "Epoch: 9612 \tTraining Loss: 1.983169 \tValidation Loss: 2.310118\n",
      "Epoch: 9613 \tTraining Loss: 1.949320 \tValidation Loss: 2.309972\n",
      "Epoch: 9614 \tTraining Loss: 2.019255 \tValidation Loss: 2.309510\n",
      "Epoch: 9615 \tTraining Loss: 1.976554 \tValidation Loss: 2.309707\n",
      "Epoch: 9616 \tTraining Loss: 1.989075 \tValidation Loss: 2.310107\n",
      "Epoch: 9617 \tTraining Loss: 1.979715 \tValidation Loss: 2.309951\n",
      "Epoch: 9618 \tTraining Loss: 1.979157 \tValidation Loss: 2.310139\n",
      "Epoch: 9619 \tTraining Loss: 1.994994 \tValidation Loss: 2.310047\n",
      "Epoch: 9620 \tTraining Loss: 1.988708 \tValidation Loss: 2.309810\n",
      "Epoch: 9621 \tTraining Loss: 1.994708 \tValidation Loss: 2.309852\n",
      "Epoch: 9622 \tTraining Loss: 1.973896 \tValidation Loss: 2.309642\n",
      "Epoch: 9623 \tTraining Loss: 1.984217 \tValidation Loss: 2.310272\n",
      "Epoch: 9624 \tTraining Loss: 1.952161 \tValidation Loss: 2.310201\n",
      "Epoch: 9625 \tTraining Loss: 1.971786 \tValidation Loss: 2.310594\n",
      "Epoch: 9626 \tTraining Loss: 2.007200 \tValidation Loss: 2.310568\n",
      "Epoch: 9627 \tTraining Loss: 1.959799 \tValidation Loss: 2.310533\n",
      "Epoch: 9628 \tTraining Loss: 1.985230 \tValidation Loss: 2.310360\n",
      "Epoch: 9629 \tTraining Loss: 1.975695 \tValidation Loss: 2.310094\n",
      "Epoch: 9630 \tTraining Loss: 1.963149 \tValidation Loss: 2.309736\n",
      "Epoch: 9631 \tTraining Loss: 2.021184 \tValidation Loss: 2.309841\n",
      "Epoch: 9632 \tTraining Loss: 1.945916 \tValidation Loss: 2.309945\n",
      "Epoch: 9633 \tTraining Loss: 1.975692 \tValidation Loss: 2.309814\n",
      "Epoch: 9634 \tTraining Loss: 1.969256 \tValidation Loss: 2.309557\n",
      "Epoch: 9635 \tTraining Loss: 1.943226 \tValidation Loss: 2.309255\n",
      "Epoch: 9636 \tTraining Loss: 1.987649 \tValidation Loss: 2.309403\n",
      "Epoch: 9637 \tTraining Loss: 1.973377 \tValidation Loss: 2.309628\n",
      "Epoch: 9638 \tTraining Loss: 1.973363 \tValidation Loss: 2.309955\n",
      "Epoch: 9639 \tTraining Loss: 1.971401 \tValidation Loss: 2.309811\n",
      "Epoch: 9640 \tTraining Loss: 1.963003 \tValidation Loss: 2.309906\n",
      "Epoch: 9641 \tTraining Loss: 1.959718 \tValidation Loss: 2.309461\n",
      "Epoch: 9642 \tTraining Loss: 1.961741 \tValidation Loss: 2.309297\n",
      "Epoch: 9643 \tTraining Loss: 1.982456 \tValidation Loss: 2.309518\n",
      "Epoch: 9644 \tTraining Loss: 1.980143 \tValidation Loss: 2.309154\n",
      "Epoch: 9645 \tTraining Loss: 1.993125 \tValidation Loss: 2.309335\n",
      "Epoch: 9646 \tTraining Loss: 1.988043 \tValidation Loss: 2.309197\n",
      "Epoch: 9647 \tTraining Loss: 1.978695 \tValidation Loss: 2.309236\n",
      "Epoch: 9648 \tTraining Loss: 1.961080 \tValidation Loss: 2.309345\n",
      "Epoch: 9649 \tTraining Loss: 2.008380 \tValidation Loss: 2.309518\n",
      "Epoch: 9650 \tTraining Loss: 1.993154 \tValidation Loss: 2.309253\n",
      "Epoch: 9651 \tTraining Loss: 1.996307 \tValidation Loss: 2.309452\n",
      "Epoch: 9652 \tTraining Loss: 1.949451 \tValidation Loss: 2.309492\n",
      "Epoch: 9653 \tTraining Loss: 1.971721 \tValidation Loss: 2.309431\n",
      "Epoch: 9654 \tTraining Loss: 1.994446 \tValidation Loss: 2.309421\n",
      "Epoch: 9655 \tTraining Loss: 1.978606 \tValidation Loss: 2.309484\n",
      "Epoch: 9656 \tTraining Loss: 1.984052 \tValidation Loss: 2.309297\n",
      "Epoch: 9657 \tTraining Loss: 1.999656 \tValidation Loss: 2.309197\n",
      "Epoch: 9658 \tTraining Loss: 1.969063 \tValidation Loss: 2.309066\n",
      "Epoch: 9659 \tTraining Loss: 1.973636 \tValidation Loss: 2.309043\n",
      "Epoch: 9660 \tTraining Loss: 1.962021 \tValidation Loss: 2.308701\n",
      "Validation loss decreased (2.308796 --> 2.308701).  Saving model ...\n",
      "Epoch: 9661 \tTraining Loss: 1.992384 \tValidation Loss: 2.308558\n",
      "Validation loss decreased (2.308701 --> 2.308558).  Saving model ...\n",
      "Epoch: 9662 \tTraining Loss: 1.957592 \tValidation Loss: 2.308643\n",
      "Epoch: 9663 \tTraining Loss: 1.978326 \tValidation Loss: 2.309012\n",
      "Epoch: 9664 \tTraining Loss: 1.968433 \tValidation Loss: 2.308610\n",
      "Epoch: 9665 \tTraining Loss: 1.953416 \tValidation Loss: 2.309241\n",
      "Epoch: 9666 \tTraining Loss: 2.002176 \tValidation Loss: 2.309034\n",
      "Epoch: 9667 \tTraining Loss: 1.950878 \tValidation Loss: 2.309192\n",
      "Epoch: 9668 \tTraining Loss: 1.956329 \tValidation Loss: 2.309293\n",
      "Epoch: 9669 \tTraining Loss: 1.980716 \tValidation Loss: 2.309191\n",
      "Epoch: 9670 \tTraining Loss: 1.980435 \tValidation Loss: 2.309032\n",
      "Epoch: 9671 \tTraining Loss: 2.004684 \tValidation Loss: 2.309396\n",
      "Epoch: 9672 \tTraining Loss: 1.962218 \tValidation Loss: 2.309097\n",
      "Epoch: 9673 \tTraining Loss: 1.959351 \tValidation Loss: 2.309398\n",
      "Epoch: 9674 \tTraining Loss: 1.991063 \tValidation Loss: 2.309437\n",
      "Epoch: 9675 \tTraining Loss: 2.013278 \tValidation Loss: 2.309255\n",
      "Epoch: 9676 \tTraining Loss: 1.993544 \tValidation Loss: 2.309432\n",
      "Epoch: 9677 \tTraining Loss: 1.970137 \tValidation Loss: 2.309562\n",
      "Epoch: 9678 \tTraining Loss: 1.959085 \tValidation Loss: 2.309782\n",
      "Epoch: 9679 \tTraining Loss: 1.968144 \tValidation Loss: 2.309778\n",
      "Epoch: 9680 \tTraining Loss: 1.977648 \tValidation Loss: 2.309719\n",
      "Epoch: 9681 \tTraining Loss: 1.982496 \tValidation Loss: 2.309416\n",
      "Epoch: 9682 \tTraining Loss: 1.963230 \tValidation Loss: 2.309721\n",
      "Epoch: 9683 \tTraining Loss: 1.971399 \tValidation Loss: 2.309555\n",
      "Epoch: 9684 \tTraining Loss: 1.977881 \tValidation Loss: 2.309966\n",
      "Epoch: 9685 \tTraining Loss: 1.981003 \tValidation Loss: 2.310326\n",
      "Epoch: 9686 \tTraining Loss: 1.959948 \tValidation Loss: 2.310596\n",
      "Epoch: 9687 \tTraining Loss: 1.958674 \tValidation Loss: 2.310064\n",
      "Epoch: 9688 \tTraining Loss: 1.977887 \tValidation Loss: 2.309893\n",
      "Epoch: 9689 \tTraining Loss: 1.969484 \tValidation Loss: 2.310111\n",
      "Epoch: 9690 \tTraining Loss: 1.966070 \tValidation Loss: 2.309941\n",
      "Epoch: 9691 \tTraining Loss: 1.974562 \tValidation Loss: 2.309794\n",
      "Epoch: 9692 \tTraining Loss: 1.985346 \tValidation Loss: 2.309340\n",
      "Epoch: 9693 \tTraining Loss: 1.971579 \tValidation Loss: 2.309216\n",
      "Epoch: 9694 \tTraining Loss: 1.996115 \tValidation Loss: 2.309160\n",
      "Epoch: 9695 \tTraining Loss: 1.968307 \tValidation Loss: 2.309241\n",
      "Epoch: 9696 \tTraining Loss: 1.991562 \tValidation Loss: 2.309352\n",
      "Epoch: 9697 \tTraining Loss: 1.960827 \tValidation Loss: 2.309381\n",
      "Epoch: 9698 \tTraining Loss: 1.918954 \tValidation Loss: 2.309456\n",
      "Epoch: 9699 \tTraining Loss: 1.988478 \tValidation Loss: 2.308979\n",
      "Epoch: 9700 \tTraining Loss: 1.968241 \tValidation Loss: 2.309130\n",
      "Epoch: 9701 \tTraining Loss: 1.979443 \tValidation Loss: 2.309358\n",
      "Epoch: 9702 \tTraining Loss: 1.961809 \tValidation Loss: 2.309363\n",
      "Epoch: 9703 \tTraining Loss: 1.970617 \tValidation Loss: 2.309619\n",
      "Epoch: 9704 \tTraining Loss: 1.985980 \tValidation Loss: 2.309373\n",
      "Epoch: 9705 \tTraining Loss: 1.977292 \tValidation Loss: 2.309426\n",
      "Epoch: 9706 \tTraining Loss: 1.988581 \tValidation Loss: 2.309506\n",
      "Epoch: 9707 \tTraining Loss: 1.955523 \tValidation Loss: 2.309436\n",
      "Epoch: 9708 \tTraining Loss: 1.949972 \tValidation Loss: 2.308982\n",
      "Epoch: 9709 \tTraining Loss: 1.975990 \tValidation Loss: 2.308886\n",
      "Epoch: 9710 \tTraining Loss: 1.985734 \tValidation Loss: 2.308863\n",
      "Epoch: 9711 \tTraining Loss: 1.997884 \tValidation Loss: 2.309119\n",
      "Epoch: 9712 \tTraining Loss: 1.958128 \tValidation Loss: 2.309160\n",
      "Epoch: 9713 \tTraining Loss: 1.974021 \tValidation Loss: 2.309595\n",
      "Epoch: 9714 \tTraining Loss: 1.981183 \tValidation Loss: 2.309708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9715 \tTraining Loss: 1.982580 \tValidation Loss: 2.309509\n",
      "Epoch: 9716 \tTraining Loss: 1.985206 \tValidation Loss: 2.309392\n",
      "Epoch: 9717 \tTraining Loss: 1.976309 \tValidation Loss: 2.309087\n",
      "Epoch: 9718 \tTraining Loss: 1.961366 \tValidation Loss: 2.309192\n",
      "Epoch: 9719 \tTraining Loss: 1.950140 \tValidation Loss: 2.309174\n",
      "Epoch: 9720 \tTraining Loss: 1.985331 \tValidation Loss: 2.309343\n",
      "Epoch: 9721 \tTraining Loss: 1.953061 \tValidation Loss: 2.309136\n",
      "Epoch: 9722 \tTraining Loss: 1.977943 \tValidation Loss: 2.309106\n",
      "Epoch: 9723 \tTraining Loss: 1.937214 \tValidation Loss: 2.309011\n",
      "Epoch: 9724 \tTraining Loss: 1.964257 \tValidation Loss: 2.309001\n",
      "Epoch: 9725 \tTraining Loss: 1.979778 \tValidation Loss: 2.309108\n",
      "Epoch: 9726 \tTraining Loss: 1.987503 \tValidation Loss: 2.308883\n",
      "Epoch: 9727 \tTraining Loss: 1.974992 \tValidation Loss: 2.309246\n",
      "Epoch: 9728 \tTraining Loss: 1.977802 \tValidation Loss: 2.309919\n",
      "Epoch: 9729 \tTraining Loss: 1.990839 \tValidation Loss: 2.309618\n",
      "Epoch: 9730 \tTraining Loss: 1.973685 \tValidation Loss: 2.309721\n",
      "Epoch: 9731 \tTraining Loss: 1.987217 \tValidation Loss: 2.309816\n",
      "Epoch: 9732 \tTraining Loss: 1.948551 \tValidation Loss: 2.310181\n",
      "Epoch: 9733 \tTraining Loss: 1.951899 \tValidation Loss: 2.309856\n",
      "Epoch: 9734 \tTraining Loss: 1.946203 \tValidation Loss: 2.309777\n",
      "Epoch: 9735 \tTraining Loss: 1.968029 \tValidation Loss: 2.309899\n",
      "Epoch: 9736 \tTraining Loss: 1.968328 \tValidation Loss: 2.309510\n",
      "Epoch: 9737 \tTraining Loss: 1.973740 \tValidation Loss: 2.309719\n",
      "Epoch: 9738 \tTraining Loss: 1.935913 \tValidation Loss: 2.309822\n",
      "Epoch: 9739 \tTraining Loss: 1.982071 \tValidation Loss: 2.310129\n",
      "Epoch: 9740 \tTraining Loss: 1.961134 \tValidation Loss: 2.309891\n",
      "Epoch: 9741 \tTraining Loss: 1.994307 \tValidation Loss: 2.309603\n",
      "Epoch: 9742 \tTraining Loss: 1.989956 \tValidation Loss: 2.309579\n",
      "Epoch: 9743 \tTraining Loss: 1.933246 \tValidation Loss: 2.309526\n",
      "Epoch: 9744 \tTraining Loss: 1.987504 \tValidation Loss: 2.309159\n",
      "Epoch: 9745 \tTraining Loss: 1.979761 \tValidation Loss: 2.309180\n",
      "Epoch: 9746 \tTraining Loss: 1.980885 \tValidation Loss: 2.308926\n",
      "Epoch: 9747 \tTraining Loss: 1.972582 \tValidation Loss: 2.308925\n",
      "Epoch: 9748 \tTraining Loss: 1.973046 \tValidation Loss: 2.308810\n",
      "Epoch: 9749 \tTraining Loss: 1.967562 \tValidation Loss: 2.308930\n",
      "Epoch: 9750 \tTraining Loss: 1.963499 \tValidation Loss: 2.308567\n",
      "Epoch: 9751 \tTraining Loss: 1.988094 \tValidation Loss: 2.308343\n",
      "Validation loss decreased (2.308558 --> 2.308343).  Saving model ...\n",
      "Epoch: 9752 \tTraining Loss: 1.993183 \tValidation Loss: 2.308417\n",
      "Epoch: 9753 \tTraining Loss: 1.955415 \tValidation Loss: 2.308499\n",
      "Epoch: 9754 \tTraining Loss: 1.985777 \tValidation Loss: 2.308808\n",
      "Epoch: 9755 \tTraining Loss: 1.938646 \tValidation Loss: 2.309283\n",
      "Epoch: 9756 \tTraining Loss: 1.966727 \tValidation Loss: 2.309511\n",
      "Epoch: 9757 \tTraining Loss: 1.974121 \tValidation Loss: 2.309939\n",
      "Epoch: 9758 \tTraining Loss: 2.003518 \tValidation Loss: 2.310060\n",
      "Epoch: 9759 \tTraining Loss: 1.993058 \tValidation Loss: 2.310245\n",
      "Epoch: 9760 \tTraining Loss: 1.934159 \tValidation Loss: 2.310758\n",
      "Epoch: 9761 \tTraining Loss: 1.964403 \tValidation Loss: 2.310275\n",
      "Epoch: 9762 \tTraining Loss: 1.969375 \tValidation Loss: 2.310073\n",
      "Epoch: 9763 \tTraining Loss: 1.982253 \tValidation Loss: 2.309833\n",
      "Epoch: 9764 \tTraining Loss: 1.979507 \tValidation Loss: 2.309795\n",
      "Epoch: 9765 \tTraining Loss: 1.977079 \tValidation Loss: 2.309803\n",
      "Epoch: 9766 \tTraining Loss: 1.988107 \tValidation Loss: 2.309651\n",
      "Epoch: 9767 \tTraining Loss: 1.960262 \tValidation Loss: 2.309953\n",
      "Epoch: 9768 \tTraining Loss: 1.960833 \tValidation Loss: 2.309903\n",
      "Epoch: 9769 \tTraining Loss: 1.959739 \tValidation Loss: 2.309822\n",
      "Epoch: 9770 \tTraining Loss: 1.992115 \tValidation Loss: 2.309570\n",
      "Epoch: 9771 \tTraining Loss: 1.975503 \tValidation Loss: 2.309396\n",
      "Epoch: 9772 \tTraining Loss: 1.983291 \tValidation Loss: 2.309352\n",
      "Epoch: 9773 \tTraining Loss: 1.963926 \tValidation Loss: 2.309510\n",
      "Epoch: 9774 \tTraining Loss: 1.962312 \tValidation Loss: 2.309239\n",
      "Epoch: 9775 \tTraining Loss: 1.967111 \tValidation Loss: 2.309095\n",
      "Epoch: 9776 \tTraining Loss: 1.996108 \tValidation Loss: 2.309100\n",
      "Epoch: 9777 \tTraining Loss: 1.975762 \tValidation Loss: 2.308979\n",
      "Epoch: 9778 \tTraining Loss: 1.982705 \tValidation Loss: 2.308841\n",
      "Epoch: 9779 \tTraining Loss: 1.971396 \tValidation Loss: 2.308740\n",
      "Epoch: 9780 \tTraining Loss: 1.954302 \tValidation Loss: 2.308792\n",
      "Epoch: 9781 \tTraining Loss: 1.970846 \tValidation Loss: 2.308989\n",
      "Epoch: 9782 \tTraining Loss: 1.982837 \tValidation Loss: 2.309049\n",
      "Epoch: 9783 \tTraining Loss: 1.963891 \tValidation Loss: 2.309115\n",
      "Epoch: 9784 \tTraining Loss: 1.949181 \tValidation Loss: 2.308621\n",
      "Epoch: 9785 \tTraining Loss: 1.981030 \tValidation Loss: 2.308867\n",
      "Epoch: 9786 \tTraining Loss: 1.940909 \tValidation Loss: 2.309071\n",
      "Epoch: 9787 \tTraining Loss: 1.995116 \tValidation Loss: 2.309086\n",
      "Epoch: 9788 \tTraining Loss: 1.953874 \tValidation Loss: 2.308809\n",
      "Epoch: 9789 \tTraining Loss: 1.992402 \tValidation Loss: 2.308839\n",
      "Epoch: 9790 \tTraining Loss: 1.973052 \tValidation Loss: 2.309191\n",
      "Epoch: 9791 \tTraining Loss: 1.983642 \tValidation Loss: 2.309294\n",
      "Epoch: 9792 \tTraining Loss: 1.967875 \tValidation Loss: 2.309400\n",
      "Epoch: 9793 \tTraining Loss: 1.963942 \tValidation Loss: 2.309662\n",
      "Epoch: 9794 \tTraining Loss: 1.967964 \tValidation Loss: 2.309612\n",
      "Epoch: 9795 \tTraining Loss: 1.968228 \tValidation Loss: 2.309492\n",
      "Epoch: 9796 \tTraining Loss: 1.976908 \tValidation Loss: 2.309573\n",
      "Epoch: 9797 \tTraining Loss: 1.984507 \tValidation Loss: 2.309319\n",
      "Epoch: 9798 \tTraining Loss: 1.995835 \tValidation Loss: 2.309352\n",
      "Epoch: 9799 \tTraining Loss: 1.980181 \tValidation Loss: 2.309499\n",
      "Epoch: 9800 \tTraining Loss: 2.006552 \tValidation Loss: 2.309317\n",
      "Epoch: 9801 \tTraining Loss: 1.997866 \tValidation Loss: 2.309218\n",
      "Epoch: 9802 \tTraining Loss: 1.973853 \tValidation Loss: 2.308955\n",
      "Epoch: 9803 \tTraining Loss: 1.981879 \tValidation Loss: 2.309255\n",
      "Epoch: 9804 \tTraining Loss: 1.956600 \tValidation Loss: 2.309642\n",
      "Epoch: 9805 \tTraining Loss: 1.968199 \tValidation Loss: 2.309445\n",
      "Epoch: 9806 \tTraining Loss: 1.961791 \tValidation Loss: 2.309350\n",
      "Epoch: 9807 \tTraining Loss: 1.943112 \tValidation Loss: 2.309235\n",
      "Epoch: 9808 \tTraining Loss: 1.986936 \tValidation Loss: 2.309290\n",
      "Epoch: 9809 \tTraining Loss: 1.995890 \tValidation Loss: 2.309187\n",
      "Epoch: 9810 \tTraining Loss: 1.968541 \tValidation Loss: 2.308858\n",
      "Epoch: 9811 \tTraining Loss: 1.990831 \tValidation Loss: 2.308787\n",
      "Epoch: 9812 \tTraining Loss: 1.959226 \tValidation Loss: 2.308717\n",
      "Epoch: 9813 \tTraining Loss: 1.970476 \tValidation Loss: 2.308527\n",
      "Epoch: 9814 \tTraining Loss: 1.967785 \tValidation Loss: 2.308743\n",
      "Epoch: 9815 \tTraining Loss: 1.980138 \tValidation Loss: 2.308952\n",
      "Epoch: 9816 \tTraining Loss: 1.956740 \tValidation Loss: 2.308829\n",
      "Epoch: 9817 \tTraining Loss: 1.992809 \tValidation Loss: 2.308635\n",
      "Epoch: 9818 \tTraining Loss: 1.981533 \tValidation Loss: 2.308896\n",
      "Epoch: 9819 \tTraining Loss: 1.965832 \tValidation Loss: 2.308862\n",
      "Epoch: 9820 \tTraining Loss: 1.972411 \tValidation Loss: 2.309001\n",
      "Epoch: 9821 \tTraining Loss: 1.967483 \tValidation Loss: 2.309308\n",
      "Epoch: 9822 \tTraining Loss: 1.970748 \tValidation Loss: 2.309437\n",
      "Epoch: 9823 \tTraining Loss: 1.987995 \tValidation Loss: 2.309483\n",
      "Epoch: 9824 \tTraining Loss: 1.936082 \tValidation Loss: 2.309631\n",
      "Epoch: 9825 \tTraining Loss: 1.970580 \tValidation Loss: 2.309626\n",
      "Epoch: 9826 \tTraining Loss: 1.961233 \tValidation Loss: 2.309356\n",
      "Epoch: 9827 \tTraining Loss: 1.964173 \tValidation Loss: 2.309385\n",
      "Epoch: 9828 \tTraining Loss: 1.949108 \tValidation Loss: 2.309215\n",
      "Epoch: 9829 \tTraining Loss: 1.975600 \tValidation Loss: 2.309367\n",
      "Epoch: 9830 \tTraining Loss: 1.937346 \tValidation Loss: 2.309005\n",
      "Epoch: 9831 \tTraining Loss: 1.950419 \tValidation Loss: 2.308826\n",
      "Epoch: 9832 \tTraining Loss: 1.963563 \tValidation Loss: 2.308941\n",
      "Epoch: 9833 \tTraining Loss: 1.959313 \tValidation Loss: 2.309146\n",
      "Epoch: 9834 \tTraining Loss: 1.968843 \tValidation Loss: 2.308889\n",
      "Epoch: 9835 \tTraining Loss: 1.974584 \tValidation Loss: 2.308709\n",
      "Epoch: 9836 \tTraining Loss: 1.950587 \tValidation Loss: 2.308503\n",
      "Epoch: 9837 \tTraining Loss: 1.964239 \tValidation Loss: 2.308294\n",
      "Validation loss decreased (2.308343 --> 2.308294).  Saving model ...\n",
      "Epoch: 9838 \tTraining Loss: 1.966082 \tValidation Loss: 2.308823\n",
      "Epoch: 9839 \tTraining Loss: 1.963372 \tValidation Loss: 2.308722\n",
      "Epoch: 9840 \tTraining Loss: 1.953402 \tValidation Loss: 2.308505\n",
      "Epoch: 9841 \tTraining Loss: 1.975182 \tValidation Loss: 2.308582\n",
      "Epoch: 9842 \tTraining Loss: 1.973479 \tValidation Loss: 2.308877\n",
      "Epoch: 9843 \tTraining Loss: 2.007666 \tValidation Loss: 2.309089\n",
      "Epoch: 9844 \tTraining Loss: 1.948457 \tValidation Loss: 2.309417\n",
      "Epoch: 9845 \tTraining Loss: 1.964851 \tValidation Loss: 2.309577\n",
      "Epoch: 9846 \tTraining Loss: 1.977269 \tValidation Loss: 2.309608\n",
      "Epoch: 9847 \tTraining Loss: 1.968930 \tValidation Loss: 2.309562\n",
      "Epoch: 9848 \tTraining Loss: 1.947701 \tValidation Loss: 2.309447\n",
      "Epoch: 9849 \tTraining Loss: 1.991165 \tValidation Loss: 2.309370\n",
      "Epoch: 9850 \tTraining Loss: 1.966468 \tValidation Loss: 2.309667\n",
      "Epoch: 9851 \tTraining Loss: 1.968240 \tValidation Loss: 2.309628\n",
      "Epoch: 9852 \tTraining Loss: 1.931029 \tValidation Loss: 2.309671\n",
      "Epoch: 9853 \tTraining Loss: 1.949549 \tValidation Loss: 2.309351\n",
      "Epoch: 9854 \tTraining Loss: 1.971722 \tValidation Loss: 2.309372\n",
      "Epoch: 9855 \tTraining Loss: 2.010011 \tValidation Loss: 2.309560\n",
      "Epoch: 9856 \tTraining Loss: 1.972019 \tValidation Loss: 2.309162\n",
      "Epoch: 9857 \tTraining Loss: 1.984062 \tValidation Loss: 2.309256\n",
      "Epoch: 9858 \tTraining Loss: 1.970230 \tValidation Loss: 2.309424\n",
      "Epoch: 9859 \tTraining Loss: 1.952327 \tValidation Loss: 2.309460\n",
      "Epoch: 9860 \tTraining Loss: 1.966957 \tValidation Loss: 2.309449\n",
      "Epoch: 9861 \tTraining Loss: 1.973586 \tValidation Loss: 2.309112\n",
      "Epoch: 9862 \tTraining Loss: 1.966466 \tValidation Loss: 2.309009\n",
      "Epoch: 9863 \tTraining Loss: 1.935579 \tValidation Loss: 2.309387\n",
      "Epoch: 9864 \tTraining Loss: 1.934492 \tValidation Loss: 2.309446\n",
      "Epoch: 9865 \tTraining Loss: 1.979512 \tValidation Loss: 2.309426\n",
      "Epoch: 9866 \tTraining Loss: 1.968387 \tValidation Loss: 2.309439\n",
      "Epoch: 9867 \tTraining Loss: 1.954793 \tValidation Loss: 2.309416\n",
      "Epoch: 9868 \tTraining Loss: 1.967337 \tValidation Loss: 2.309289\n",
      "Epoch: 9869 \tTraining Loss: 1.959769 \tValidation Loss: 2.309230\n",
      "Epoch: 9870 \tTraining Loss: 1.985844 \tValidation Loss: 2.308928\n",
      "Epoch: 9871 \tTraining Loss: 1.961201 \tValidation Loss: 2.308719\n",
      "Epoch: 9872 \tTraining Loss: 1.990622 \tValidation Loss: 2.308513\n",
      "Epoch: 9873 \tTraining Loss: 1.972341 \tValidation Loss: 2.308787\n",
      "Epoch: 9874 \tTraining Loss: 1.986483 \tValidation Loss: 2.309162\n",
      "Epoch: 9875 \tTraining Loss: 1.998278 \tValidation Loss: 2.308773\n",
      "Epoch: 9876 \tTraining Loss: 1.961259 \tValidation Loss: 2.308821\n",
      "Epoch: 9877 \tTraining Loss: 1.968749 \tValidation Loss: 2.309176\n",
      "Epoch: 9878 \tTraining Loss: 1.968356 \tValidation Loss: 2.309386\n",
      "Epoch: 9879 \tTraining Loss: 1.956213 \tValidation Loss: 2.309072\n",
      "Epoch: 9880 \tTraining Loss: 1.973808 \tValidation Loss: 2.309183\n",
      "Epoch: 9881 \tTraining Loss: 1.974198 \tValidation Loss: 2.309093\n",
      "Epoch: 9882 \tTraining Loss: 1.985849 \tValidation Loss: 2.309276\n",
      "Epoch: 9883 \tTraining Loss: 1.957458 \tValidation Loss: 2.309363\n",
      "Epoch: 9884 \tTraining Loss: 1.966635 \tValidation Loss: 2.309488\n",
      "Epoch: 9885 \tTraining Loss: 1.951164 \tValidation Loss: 2.309546\n",
      "Epoch: 9886 \tTraining Loss: 1.977966 \tValidation Loss: 2.309473\n",
      "Epoch: 9887 \tTraining Loss: 1.915821 \tValidation Loss: 2.309407\n",
      "Epoch: 9888 \tTraining Loss: 1.967273 \tValidation Loss: 2.309631\n",
      "Epoch: 9889 \tTraining Loss: 1.975413 \tValidation Loss: 2.309545\n",
      "Epoch: 9890 \tTraining Loss: 1.969892 \tValidation Loss: 2.309444\n",
      "Epoch: 9891 \tTraining Loss: 1.974510 \tValidation Loss: 2.309360\n",
      "Epoch: 9892 \tTraining Loss: 1.984280 \tValidation Loss: 2.309521\n",
      "Epoch: 9893 \tTraining Loss: 1.953368 \tValidation Loss: 2.309831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9894 \tTraining Loss: 1.960382 \tValidation Loss: 2.309653\n",
      "Epoch: 9895 \tTraining Loss: 1.973415 \tValidation Loss: 2.309800\n",
      "Epoch: 9896 \tTraining Loss: 1.967475 \tValidation Loss: 2.310138\n",
      "Epoch: 9897 \tTraining Loss: 1.981466 \tValidation Loss: 2.309988\n",
      "Epoch: 9898 \tTraining Loss: 2.008515 \tValidation Loss: 2.309854\n",
      "Epoch: 9899 \tTraining Loss: 1.990526 \tValidation Loss: 2.309416\n",
      "Epoch: 9900 \tTraining Loss: 1.962904 \tValidation Loss: 2.309274\n",
      "Epoch: 9901 \tTraining Loss: 1.962792 \tValidation Loss: 2.309186\n",
      "Epoch: 9902 \tTraining Loss: 1.955290 \tValidation Loss: 2.309574\n",
      "Epoch: 9903 \tTraining Loss: 1.953950 \tValidation Loss: 2.309226\n",
      "Epoch: 9904 \tTraining Loss: 1.990814 \tValidation Loss: 2.308916\n",
      "Epoch: 9905 \tTraining Loss: 1.947881 \tValidation Loss: 2.308607\n",
      "Epoch: 9906 \tTraining Loss: 1.951345 \tValidation Loss: 2.308436\n",
      "Epoch: 9907 \tTraining Loss: 1.970428 \tValidation Loss: 2.308899\n",
      "Epoch: 9908 \tTraining Loss: 1.978107 \tValidation Loss: 2.308974\n",
      "Epoch: 9909 \tTraining Loss: 1.961742 \tValidation Loss: 2.308939\n",
      "Epoch: 9910 \tTraining Loss: 1.962417 \tValidation Loss: 2.309113\n",
      "Epoch: 9911 \tTraining Loss: 1.964178 \tValidation Loss: 2.308808\n",
      "Epoch: 9912 \tTraining Loss: 1.959492 \tValidation Loss: 2.309237\n",
      "Epoch: 9913 \tTraining Loss: 1.957238 \tValidation Loss: 2.309126\n",
      "Epoch: 9914 \tTraining Loss: 1.955286 \tValidation Loss: 2.309193\n",
      "Epoch: 9915 \tTraining Loss: 1.952435 \tValidation Loss: 2.309536\n",
      "Epoch: 9916 \tTraining Loss: 1.938432 \tValidation Loss: 2.309376\n",
      "Epoch: 9917 \tTraining Loss: 1.954013 \tValidation Loss: 2.309453\n",
      "Epoch: 9918 \tTraining Loss: 1.965345 \tValidation Loss: 2.309358\n",
      "Epoch: 9919 \tTraining Loss: 1.972168 \tValidation Loss: 2.309339\n",
      "Epoch: 9920 \tTraining Loss: 1.957947 \tValidation Loss: 2.308837\n",
      "Epoch: 9921 \tTraining Loss: 1.972195 \tValidation Loss: 2.308548\n",
      "Epoch: 9922 \tTraining Loss: 1.956928 \tValidation Loss: 2.308493\n",
      "Epoch: 9923 \tTraining Loss: 1.942210 \tValidation Loss: 2.308352\n",
      "Epoch: 9924 \tTraining Loss: 1.961001 \tValidation Loss: 2.308848\n",
      "Epoch: 9925 \tTraining Loss: 1.969636 \tValidation Loss: 2.308855\n",
      "Epoch: 9926 \tTraining Loss: 1.994238 \tValidation Loss: 2.308865\n",
      "Epoch: 9927 \tTraining Loss: 1.959114 \tValidation Loss: 2.308832\n",
      "Epoch: 9928 \tTraining Loss: 1.972799 \tValidation Loss: 2.308975\n",
      "Epoch: 9929 \tTraining Loss: 1.972313 \tValidation Loss: 2.309016\n",
      "Epoch: 9930 \tTraining Loss: 1.949233 \tValidation Loss: 2.308769\n",
      "Epoch: 9931 \tTraining Loss: 1.986069 \tValidation Loss: 2.308849\n",
      "Epoch: 9932 \tTraining Loss: 1.930839 \tValidation Loss: 2.308672\n",
      "Epoch: 9933 \tTraining Loss: 1.952556 \tValidation Loss: 2.308687\n",
      "Epoch: 9934 \tTraining Loss: 1.970154 \tValidation Loss: 2.308493\n",
      "Epoch: 9935 \tTraining Loss: 1.996055 \tValidation Loss: 2.308794\n",
      "Epoch: 9936 \tTraining Loss: 1.963831 \tValidation Loss: 2.308751\n",
      "Epoch: 9937 \tTraining Loss: 1.970684 \tValidation Loss: 2.308854\n",
      "Epoch: 9938 \tTraining Loss: 1.964567 \tValidation Loss: 2.308851\n",
      "Epoch: 9939 \tTraining Loss: 1.942168 \tValidation Loss: 2.309012\n",
      "Epoch: 9940 \tTraining Loss: 1.982032 \tValidation Loss: 2.309163\n",
      "Epoch: 9941 \tTraining Loss: 1.974390 \tValidation Loss: 2.309191\n",
      "Epoch: 9942 \tTraining Loss: 2.003227 \tValidation Loss: 2.308971\n",
      "Epoch: 9943 \tTraining Loss: 1.975888 \tValidation Loss: 2.309081\n",
      "Epoch: 9944 \tTraining Loss: 1.967872 \tValidation Loss: 2.309066\n",
      "Epoch: 9945 \tTraining Loss: 1.981112 \tValidation Loss: 2.309095\n",
      "Epoch: 9946 \tTraining Loss: 1.953277 \tValidation Loss: 2.308895\n",
      "Epoch: 9947 \tTraining Loss: 1.983852 \tValidation Loss: 2.308675\n",
      "Epoch: 9948 \tTraining Loss: 1.941699 \tValidation Loss: 2.308593\n",
      "Epoch: 9949 \tTraining Loss: 1.937967 \tValidation Loss: 2.308268\n",
      "Validation loss decreased (2.308294 --> 2.308268).  Saving model ...\n",
      "Epoch: 9950 \tTraining Loss: 1.953282 \tValidation Loss: 2.308232\n",
      "Validation loss decreased (2.308268 --> 2.308232).  Saving model ...\n",
      "Epoch: 9951 \tTraining Loss: 1.938956 \tValidation Loss: 2.308203\n",
      "Validation loss decreased (2.308232 --> 2.308203).  Saving model ...\n",
      "Epoch: 9952 \tTraining Loss: 1.977127 \tValidation Loss: 2.308264\n",
      "Epoch: 9953 \tTraining Loss: 1.960183 \tValidation Loss: 2.308147\n",
      "Validation loss decreased (2.308203 --> 2.308147).  Saving model ...\n",
      "Epoch: 9954 \tTraining Loss: 1.974169 \tValidation Loss: 2.307978\n",
      "Validation loss decreased (2.308147 --> 2.307978).  Saving model ...\n",
      "Epoch: 9955 \tTraining Loss: 1.974448 \tValidation Loss: 2.307719\n",
      "Validation loss decreased (2.307978 --> 2.307719).  Saving model ...\n",
      "Epoch: 9956 \tTraining Loss: 1.954466 \tValidation Loss: 2.307793\n",
      "Epoch: 9957 \tTraining Loss: 1.969538 \tValidation Loss: 2.307905\n",
      "Epoch: 9958 \tTraining Loss: 1.962252 \tValidation Loss: 2.308151\n",
      "Epoch: 9959 \tTraining Loss: 1.971449 \tValidation Loss: 2.308563\n",
      "Epoch: 9960 \tTraining Loss: 1.941857 \tValidation Loss: 2.308754\n",
      "Epoch: 9961 \tTraining Loss: 1.945799 \tValidation Loss: 2.309162\n",
      "Epoch: 9962 \tTraining Loss: 1.988274 \tValidation Loss: 2.308868\n",
      "Epoch: 9963 \tTraining Loss: 1.947875 \tValidation Loss: 2.308969\n",
      "Epoch: 9964 \tTraining Loss: 1.941871 \tValidation Loss: 2.308936\n",
      "Epoch: 9965 \tTraining Loss: 1.942848 \tValidation Loss: 2.308813\n",
      "Epoch: 9966 \tTraining Loss: 1.956220 \tValidation Loss: 2.308852\n",
      "Epoch: 9967 \tTraining Loss: 1.985086 \tValidation Loss: 2.308760\n",
      "Epoch: 9968 \tTraining Loss: 1.988282 \tValidation Loss: 2.308844\n",
      "Epoch: 9969 \tTraining Loss: 1.943202 \tValidation Loss: 2.309070\n",
      "Epoch: 9970 \tTraining Loss: 1.945439 \tValidation Loss: 2.309129\n",
      "Epoch: 9971 \tTraining Loss: 1.971641 \tValidation Loss: 2.309017\n",
      "Epoch: 9972 \tTraining Loss: 1.942023 \tValidation Loss: 2.309265\n",
      "Epoch: 9973 \tTraining Loss: 1.974978 \tValidation Loss: 2.309416\n",
      "Epoch: 9974 \tTraining Loss: 1.975985 \tValidation Loss: 2.308891\n",
      "Epoch: 9975 \tTraining Loss: 1.981933 \tValidation Loss: 2.308760\n",
      "Epoch: 9976 \tTraining Loss: 1.961926 \tValidation Loss: 2.308636\n",
      "Epoch: 9977 \tTraining Loss: 1.957667 \tValidation Loss: 2.308640\n",
      "Epoch: 9978 \tTraining Loss: 1.959248 \tValidation Loss: 2.308575\n",
      "Epoch: 9979 \tTraining Loss: 1.943524 \tValidation Loss: 2.308826\n",
      "Epoch: 9980 \tTraining Loss: 1.948187 \tValidation Loss: 2.308649\n",
      "Epoch: 9981 \tTraining Loss: 1.973946 \tValidation Loss: 2.308684\n",
      "Epoch: 9982 \tTraining Loss: 1.989182 \tValidation Loss: 2.309196\n",
      "Epoch: 9983 \tTraining Loss: 1.941913 \tValidation Loss: 2.309215\n",
      "Epoch: 9984 \tTraining Loss: 1.964721 \tValidation Loss: 2.308879\n",
      "Epoch: 9985 \tTraining Loss: 1.962630 \tValidation Loss: 2.309536\n",
      "Epoch: 9986 \tTraining Loss: 1.959969 \tValidation Loss: 2.309152\n",
      "Epoch: 9987 \tTraining Loss: 1.966911 \tValidation Loss: 2.309110\n",
      "Epoch: 9988 \tTraining Loss: 1.968785 \tValidation Loss: 2.309335\n",
      "Epoch: 9989 \tTraining Loss: 1.936181 \tValidation Loss: 2.309528\n",
      "Epoch: 9990 \tTraining Loss: 1.989329 \tValidation Loss: 2.309391\n",
      "Epoch: 9991 \tTraining Loss: 1.965500 \tValidation Loss: 2.309300\n",
      "Epoch: 9992 \tTraining Loss: 1.993259 \tValidation Loss: 2.309387\n",
      "Epoch: 9993 \tTraining Loss: 1.967462 \tValidation Loss: 2.309448\n",
      "Epoch: 9994 \tTraining Loss: 1.997527 \tValidation Loss: 2.309382\n",
      "Epoch: 9995 \tTraining Loss: 1.966312 \tValidation Loss: 2.309212\n",
      "Epoch: 9996 \tTraining Loss: 1.969115 \tValidation Loss: 2.309218\n",
      "Epoch: 9997 \tTraining Loss: 1.946086 \tValidation Loss: 2.308980\n",
      "Epoch: 9998 \tTraining Loss: 1.971631 \tValidation Loss: 2.309166\n",
      "Epoch: 9999 \tTraining Loss: 1.987752 \tValidation Loss: 2.309287\n",
      "Epoch: 10000 \tTraining Loss: 1.943908 \tValidation Loss: 2.309144\n",
      "Epoch: 10001 \tTraining Loss: 1.980063 \tValidation Loss: 2.308697\n",
      "Epoch: 10002 \tTraining Loss: 1.979917 \tValidation Loss: 2.309073\n",
      "Epoch: 10003 \tTraining Loss: 1.954141 \tValidation Loss: 2.309278\n",
      "Epoch: 10004 \tTraining Loss: 1.944850 \tValidation Loss: 2.309423\n",
      "Epoch: 10005 \tTraining Loss: 1.978922 \tValidation Loss: 2.309126\n",
      "Epoch: 10006 \tTraining Loss: 1.987432 \tValidation Loss: 2.309114\n",
      "Epoch: 10007 \tTraining Loss: 1.973271 \tValidation Loss: 2.309128\n",
      "Epoch: 10008 \tTraining Loss: 1.976159 \tValidation Loss: 2.308870\n",
      "Epoch: 10009 \tTraining Loss: 1.934187 \tValidation Loss: 2.308998\n",
      "Epoch: 10010 \tTraining Loss: 1.948018 \tValidation Loss: 2.309222\n",
      "Epoch: 10011 \tTraining Loss: 1.946792 \tValidation Loss: 2.309076\n",
      "Epoch: 10012 \tTraining Loss: 1.943259 \tValidation Loss: 2.308962\n",
      "Epoch: 10013 \tTraining Loss: 1.950933 \tValidation Loss: 2.309147\n",
      "Epoch: 10014 \tTraining Loss: 1.947685 \tValidation Loss: 2.309619\n",
      "Epoch: 10015 \tTraining Loss: 1.962741 \tValidation Loss: 2.309442\n",
      "Epoch: 10016 \tTraining Loss: 1.953101 \tValidation Loss: 2.309518\n",
      "Epoch: 10017 \tTraining Loss: 1.963934 \tValidation Loss: 2.309429\n",
      "Epoch: 10018 \tTraining Loss: 1.958714 \tValidation Loss: 2.309423\n",
      "Epoch: 10019 \tTraining Loss: 1.948518 \tValidation Loss: 2.309253\n",
      "Epoch: 10020 \tTraining Loss: 1.987913 \tValidation Loss: 2.309026\n",
      "Epoch: 10021 \tTraining Loss: 1.954586 \tValidation Loss: 2.309166\n",
      "Epoch: 10022 \tTraining Loss: 1.964468 \tValidation Loss: 2.309138\n",
      "Epoch: 10023 \tTraining Loss: 1.951991 \tValidation Loss: 2.308749\n",
      "Epoch: 10024 \tTraining Loss: 1.963564 \tValidation Loss: 2.308894\n",
      "Epoch: 10025 \tTraining Loss: 1.953743 \tValidation Loss: 2.309045\n",
      "Epoch: 10026 \tTraining Loss: 1.955370 \tValidation Loss: 2.309426\n",
      "Epoch: 10027 \tTraining Loss: 1.962214 \tValidation Loss: 2.309445\n",
      "Epoch: 10028 \tTraining Loss: 1.941074 \tValidation Loss: 2.309299\n",
      "Epoch: 10029 \tTraining Loss: 1.942737 \tValidation Loss: 2.309677\n",
      "Epoch: 10030 \tTraining Loss: 1.956649 \tValidation Loss: 2.309397\n",
      "Epoch: 10031 \tTraining Loss: 1.967726 \tValidation Loss: 2.309572\n",
      "Epoch: 10032 \tTraining Loss: 1.981245 \tValidation Loss: 2.309816\n",
      "Epoch: 10033 \tTraining Loss: 1.955504 \tValidation Loss: 2.309885\n",
      "Epoch: 10034 \tTraining Loss: 1.958404 \tValidation Loss: 2.309866\n",
      "Epoch: 10035 \tTraining Loss: 1.957153 \tValidation Loss: 2.309402\n",
      "Epoch: 10036 \tTraining Loss: 1.965003 \tValidation Loss: 2.309558\n",
      "Epoch: 10037 \tTraining Loss: 1.968060 \tValidation Loss: 2.309166\n",
      "Epoch: 10038 \tTraining Loss: 1.955835 \tValidation Loss: 2.309268\n",
      "Epoch: 10039 \tTraining Loss: 1.960476 \tValidation Loss: 2.309368\n",
      "Epoch: 10040 \tTraining Loss: 1.972773 \tValidation Loss: 2.309290\n",
      "Epoch: 10041 \tTraining Loss: 1.958829 \tValidation Loss: 2.309393\n",
      "Epoch: 10042 \tTraining Loss: 1.964747 \tValidation Loss: 2.309221\n",
      "Epoch: 10043 \tTraining Loss: 1.971195 \tValidation Loss: 2.309317\n",
      "Epoch: 10044 \tTraining Loss: 1.957383 \tValidation Loss: 2.309062\n",
      "Epoch: 10045 \tTraining Loss: 1.951487 \tValidation Loss: 2.309439\n",
      "Epoch: 10046 \tTraining Loss: 1.965945 \tValidation Loss: 2.308995\n",
      "Epoch: 10047 \tTraining Loss: 1.977563 \tValidation Loss: 2.309077\n",
      "Epoch: 10048 \tTraining Loss: 1.952383 \tValidation Loss: 2.309495\n",
      "Epoch: 10049 \tTraining Loss: 1.952052 \tValidation Loss: 2.309301\n",
      "Epoch: 10050 \tTraining Loss: 1.964360 \tValidation Loss: 2.309379\n",
      "Epoch: 10051 \tTraining Loss: 1.965205 \tValidation Loss: 2.309515\n",
      "Epoch: 10052 \tTraining Loss: 1.970844 \tValidation Loss: 2.309559\n",
      "Epoch: 10053 \tTraining Loss: 1.988762 \tValidation Loss: 2.309162\n",
      "Epoch: 10054 \tTraining Loss: 1.962490 \tValidation Loss: 2.309179\n",
      "Epoch: 10055 \tTraining Loss: 2.001703 \tValidation Loss: 2.309302\n",
      "Epoch: 10056 \tTraining Loss: 1.958622 \tValidation Loss: 2.309743\n",
      "Epoch: 10057 \tTraining Loss: 1.962781 \tValidation Loss: 2.309788\n",
      "Epoch: 10058 \tTraining Loss: 1.943494 \tValidation Loss: 2.309422\n",
      "Epoch: 10059 \tTraining Loss: 1.941100 \tValidation Loss: 2.309255\n",
      "Epoch: 10060 \tTraining Loss: 1.931234 \tValidation Loss: 2.309333\n",
      "Epoch: 10061 \tTraining Loss: 1.933962 \tValidation Loss: 2.309372\n",
      "Epoch: 10062 \tTraining Loss: 1.943009 \tValidation Loss: 2.309250\n",
      "Epoch: 10063 \tTraining Loss: 1.972521 \tValidation Loss: 2.309255\n",
      "Epoch: 10064 \tTraining Loss: 1.957457 \tValidation Loss: 2.308853\n",
      "Epoch: 10065 \tTraining Loss: 1.934748 \tValidation Loss: 2.308782\n",
      "Epoch: 10066 \tTraining Loss: 1.915585 \tValidation Loss: 2.309126\n",
      "Epoch: 10067 \tTraining Loss: 1.920741 \tValidation Loss: 2.309331\n",
      "Epoch: 10068 \tTraining Loss: 1.939759 \tValidation Loss: 2.309162\n",
      "Epoch: 10069 \tTraining Loss: 1.946136 \tValidation Loss: 2.309280\n",
      "Epoch: 10070 \tTraining Loss: 1.953077 \tValidation Loss: 2.309325\n",
      "Epoch: 10071 \tTraining Loss: 1.937678 \tValidation Loss: 2.309590\n",
      "Epoch: 10072 \tTraining Loss: 1.974307 \tValidation Loss: 2.309160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10073 \tTraining Loss: 1.969532 \tValidation Loss: 2.309218\n",
      "Epoch: 10074 \tTraining Loss: 1.958071 \tValidation Loss: 2.309403\n",
      "Epoch: 10075 \tTraining Loss: 1.955029 \tValidation Loss: 2.309499\n",
      "Epoch: 10076 \tTraining Loss: 1.960871 \tValidation Loss: 2.309450\n",
      "Epoch: 10077 \tTraining Loss: 1.934795 \tValidation Loss: 2.309554\n",
      "Epoch: 10078 \tTraining Loss: 1.968718 \tValidation Loss: 2.309765\n",
      "Epoch: 10079 \tTraining Loss: 1.944862 \tValidation Loss: 2.309658\n",
      "Epoch: 10080 \tTraining Loss: 1.974172 \tValidation Loss: 2.309949\n",
      "Epoch: 10081 \tTraining Loss: 1.947607 \tValidation Loss: 2.309397\n",
      "Epoch: 10082 \tTraining Loss: 1.953152 \tValidation Loss: 2.309299\n",
      "Epoch: 10083 \tTraining Loss: 1.964460 \tValidation Loss: 2.309530\n",
      "Epoch: 10084 \tTraining Loss: 1.927819 \tValidation Loss: 2.309512\n",
      "Epoch: 10085 \tTraining Loss: 1.956502 \tValidation Loss: 2.309064\n",
      "Epoch: 10086 \tTraining Loss: 1.961277 \tValidation Loss: 2.308945\n",
      "Epoch: 10087 \tTraining Loss: 1.945207 \tValidation Loss: 2.308703\n",
      "Epoch: 10088 \tTraining Loss: 1.961558 \tValidation Loss: 2.308191\n",
      "Epoch: 10089 \tTraining Loss: 1.927777 \tValidation Loss: 2.308602\n",
      "Epoch: 10090 \tTraining Loss: 1.924632 \tValidation Loss: 2.309062\n",
      "Epoch: 10091 \tTraining Loss: 1.965818 \tValidation Loss: 2.309007\n",
      "Epoch: 10092 \tTraining Loss: 1.961613 \tValidation Loss: 2.309153\n",
      "Epoch: 10093 \tTraining Loss: 1.961714 \tValidation Loss: 2.309315\n",
      "Epoch: 10094 \tTraining Loss: 1.933500 \tValidation Loss: 2.308886\n",
      "Epoch: 10095 \tTraining Loss: 1.934665 \tValidation Loss: 2.309062\n",
      "Epoch: 10096 \tTraining Loss: 1.972865 \tValidation Loss: 2.309231\n",
      "Epoch: 10097 \tTraining Loss: 1.914600 \tValidation Loss: 2.309359\n",
      "Epoch: 10098 \tTraining Loss: 1.946088 \tValidation Loss: 2.309060\n",
      "Epoch: 10099 \tTraining Loss: 1.956275 \tValidation Loss: 2.309170\n",
      "Epoch: 10100 \tTraining Loss: 1.948240 \tValidation Loss: 2.309059\n",
      "Epoch: 10101 \tTraining Loss: 1.986176 \tValidation Loss: 2.309065\n",
      "Epoch: 10102 \tTraining Loss: 1.947662 \tValidation Loss: 2.308873\n",
      "Epoch: 10103 \tTraining Loss: 1.981537 \tValidation Loss: 2.308897\n",
      "Epoch: 10104 \tTraining Loss: 1.967729 \tValidation Loss: 2.308836\n",
      "Epoch: 10105 \tTraining Loss: 1.959388 \tValidation Loss: 2.308959\n",
      "Epoch: 10106 \tTraining Loss: 1.926061 \tValidation Loss: 2.309018\n",
      "Epoch: 10107 \tTraining Loss: 1.924516 \tValidation Loss: 2.309160\n",
      "Epoch: 10108 \tTraining Loss: 1.994205 \tValidation Loss: 2.309059\n",
      "Epoch: 10109 \tTraining Loss: 1.962879 \tValidation Loss: 2.308867\n",
      "Epoch: 10110 \tTraining Loss: 1.939571 \tValidation Loss: 2.309167\n",
      "Epoch: 10111 \tTraining Loss: 1.966164 \tValidation Loss: 2.309232\n",
      "Epoch: 10112 \tTraining Loss: 1.961146 \tValidation Loss: 2.308798\n",
      "Epoch: 10113 \tTraining Loss: 1.929343 \tValidation Loss: 2.309258\n",
      "Epoch: 10114 \tTraining Loss: 1.953015 \tValidation Loss: 2.309001\n",
      "Epoch: 10115 \tTraining Loss: 1.931034 \tValidation Loss: 2.309092\n",
      "Epoch: 10116 \tTraining Loss: 1.931829 \tValidation Loss: 2.309433\n",
      "Epoch: 10117 \tTraining Loss: 1.936202 \tValidation Loss: 2.309534\n",
      "Epoch: 10118 \tTraining Loss: 1.972954 \tValidation Loss: 2.309722\n",
      "Epoch: 10119 \tTraining Loss: 1.967862 \tValidation Loss: 2.309747\n",
      "Epoch: 10120 \tTraining Loss: 1.959730 \tValidation Loss: 2.309295\n",
      "Epoch: 10121 \tTraining Loss: 1.934309 \tValidation Loss: 2.309314\n",
      "Epoch: 10122 \tTraining Loss: 1.933338 \tValidation Loss: 2.309421\n",
      "Epoch: 10123 \tTraining Loss: 1.951825 \tValidation Loss: 2.308961\n",
      "Epoch: 10124 \tTraining Loss: 1.962122 \tValidation Loss: 2.309099\n",
      "Epoch: 10125 \tTraining Loss: 1.933854 \tValidation Loss: 2.309141\n",
      "Epoch: 10126 \tTraining Loss: 1.935105 \tValidation Loss: 2.308734\n",
      "Epoch: 10127 \tTraining Loss: 1.949682 \tValidation Loss: 2.309080\n",
      "Epoch: 10128 \tTraining Loss: 1.960425 \tValidation Loss: 2.308841\n",
      "Epoch: 10129 \tTraining Loss: 1.941281 \tValidation Loss: 2.308643\n",
      "Epoch: 10130 \tTraining Loss: 1.987583 \tValidation Loss: 2.308798\n",
      "Epoch: 10131 \tTraining Loss: 1.952064 \tValidation Loss: 2.308902\n",
      "Epoch: 10132 \tTraining Loss: 1.972734 \tValidation Loss: 2.309121\n",
      "Epoch: 10133 \tTraining Loss: 1.917847 \tValidation Loss: 2.309268\n",
      "Epoch: 10134 \tTraining Loss: 1.944553 \tValidation Loss: 2.309236\n",
      "Epoch: 10135 \tTraining Loss: 1.946152 \tValidation Loss: 2.309364\n",
      "Epoch: 10136 \tTraining Loss: 1.910901 \tValidation Loss: 2.309539\n",
      "Epoch: 10137 \tTraining Loss: 1.955799 \tValidation Loss: 2.309268\n",
      "Epoch: 10138 \tTraining Loss: 1.941835 \tValidation Loss: 2.309112\n",
      "Epoch: 10139 \tTraining Loss: 1.975540 \tValidation Loss: 2.309045\n",
      "Epoch: 10140 \tTraining Loss: 1.942836 \tValidation Loss: 2.309075\n",
      "Epoch: 10141 \tTraining Loss: 1.950034 \tValidation Loss: 2.309026\n",
      "Epoch: 10142 \tTraining Loss: 1.941123 \tValidation Loss: 2.309168\n",
      "Epoch: 10143 \tTraining Loss: 1.941073 \tValidation Loss: 2.308874\n",
      "Epoch: 10144 \tTraining Loss: 1.955623 \tValidation Loss: 2.308805\n",
      "Epoch: 10145 \tTraining Loss: 1.957125 \tValidation Loss: 2.308896\n",
      "Epoch: 10146 \tTraining Loss: 1.958418 \tValidation Loss: 2.308947\n",
      "Epoch: 10147 \tTraining Loss: 1.969208 \tValidation Loss: 2.308855\n",
      "Epoch: 10148 \tTraining Loss: 1.960836 \tValidation Loss: 2.308928\n",
      "Epoch: 10149 \tTraining Loss: 1.915287 \tValidation Loss: 2.308595\n",
      "Epoch: 10150 \tTraining Loss: 1.910714 \tValidation Loss: 2.308824\n",
      "Epoch: 10151 \tTraining Loss: 1.936787 \tValidation Loss: 2.308629\n",
      "Epoch: 10152 \tTraining Loss: 1.961619 \tValidation Loss: 2.308854\n",
      "Epoch: 10153 \tTraining Loss: 1.933246 \tValidation Loss: 2.309345\n",
      "Epoch: 10154 \tTraining Loss: 1.968559 \tValidation Loss: 2.309392\n",
      "Epoch: 10155 \tTraining Loss: 1.961636 \tValidation Loss: 2.309661\n",
      "Epoch: 10156 \tTraining Loss: 1.957882 \tValidation Loss: 2.309280\n",
      "Epoch: 10157 \tTraining Loss: 1.977740 \tValidation Loss: 2.309166\n",
      "Epoch: 10158 \tTraining Loss: 1.936442 \tValidation Loss: 2.309475\n",
      "Epoch: 10159 \tTraining Loss: 1.927021 \tValidation Loss: 2.309866\n",
      "Epoch: 10160 \tTraining Loss: 1.953788 \tValidation Loss: 2.309576\n",
      "Epoch: 10161 \tTraining Loss: 1.952038 \tValidation Loss: 2.309816\n",
      "Epoch: 10162 \tTraining Loss: 1.936188 \tValidation Loss: 2.309580\n",
      "Epoch: 10163 \tTraining Loss: 1.932469 \tValidation Loss: 2.309595\n",
      "Epoch: 10164 \tTraining Loss: 1.945072 \tValidation Loss: 2.309235\n",
      "Epoch: 10165 \tTraining Loss: 1.979614 \tValidation Loss: 2.309067\n",
      "Epoch: 10166 \tTraining Loss: 1.939169 \tValidation Loss: 2.309325\n",
      "Epoch: 10167 \tTraining Loss: 1.959955 \tValidation Loss: 2.309060\n",
      "Epoch: 10168 \tTraining Loss: 1.979912 \tValidation Loss: 2.309250\n",
      "Epoch: 10169 \tTraining Loss: 1.958503 \tValidation Loss: 2.309040\n",
      "Epoch: 10170 \tTraining Loss: 1.967603 \tValidation Loss: 2.308965\n",
      "Epoch: 10171 \tTraining Loss: 1.974735 \tValidation Loss: 2.309115\n",
      "Epoch: 10172 \tTraining Loss: 1.968056 \tValidation Loss: 2.309153\n",
      "Epoch: 10173 \tTraining Loss: 1.927753 \tValidation Loss: 2.309103\n",
      "Epoch: 10174 \tTraining Loss: 1.948433 \tValidation Loss: 2.309266\n",
      "Epoch: 10175 \tTraining Loss: 1.983354 \tValidation Loss: 2.309113\n",
      "Epoch: 10176 \tTraining Loss: 1.960446 \tValidation Loss: 2.308964\n",
      "Epoch: 10177 \tTraining Loss: 1.975541 \tValidation Loss: 2.309031\n",
      "Epoch: 10178 \tTraining Loss: 1.956803 \tValidation Loss: 2.309013\n",
      "Epoch: 10179 \tTraining Loss: 1.960159 \tValidation Loss: 2.309181\n",
      "Epoch: 10180 \tTraining Loss: 1.955152 \tValidation Loss: 2.309164\n",
      "Epoch: 10181 \tTraining Loss: 1.973464 \tValidation Loss: 2.309230\n",
      "Epoch: 10182 \tTraining Loss: 1.933836 \tValidation Loss: 2.309424\n",
      "Epoch: 10183 \tTraining Loss: 1.934624 \tValidation Loss: 2.309500\n",
      "Epoch: 10184 \tTraining Loss: 1.976400 \tValidation Loss: 2.309402\n",
      "Epoch: 10185 \tTraining Loss: 1.969983 \tValidation Loss: 2.309256\n",
      "Epoch: 10186 \tTraining Loss: 1.914406 \tValidation Loss: 2.309307\n",
      "Epoch: 10187 \tTraining Loss: 1.949219 \tValidation Loss: 2.308873\n",
      "Epoch: 10188 \tTraining Loss: 1.942554 \tValidation Loss: 2.308741\n",
      "Epoch: 10189 \tTraining Loss: 1.963158 \tValidation Loss: 2.308732\n",
      "Epoch: 10190 \tTraining Loss: 1.924435 \tValidation Loss: 2.308813\n",
      "Epoch: 10191 \tTraining Loss: 1.947653 \tValidation Loss: 2.308862\n",
      "Epoch: 10192 \tTraining Loss: 1.945296 \tValidation Loss: 2.309105\n",
      "Epoch: 10193 \tTraining Loss: 1.938012 \tValidation Loss: 2.309048\n",
      "Epoch: 10194 \tTraining Loss: 1.996028 \tValidation Loss: 2.309186\n",
      "Epoch: 10195 \tTraining Loss: 1.944233 \tValidation Loss: 2.309266\n",
      "Epoch: 10196 \tTraining Loss: 1.961203 \tValidation Loss: 2.309379\n",
      "Epoch: 10197 \tTraining Loss: 1.929127 \tValidation Loss: 2.309376\n",
      "Epoch: 10198 \tTraining Loss: 1.977269 \tValidation Loss: 2.309629\n",
      "Epoch: 10199 \tTraining Loss: 1.932781 \tValidation Loss: 2.309791\n",
      "Epoch: 10200 \tTraining Loss: 1.932070 \tValidation Loss: 2.309589\n",
      "Epoch: 10201 \tTraining Loss: 1.940482 \tValidation Loss: 2.309111\n",
      "Epoch: 10202 \tTraining Loss: 1.942804 \tValidation Loss: 2.309098\n",
      "Epoch: 10203 \tTraining Loss: 1.957504 \tValidation Loss: 2.308922\n",
      "Epoch: 10204 \tTraining Loss: 1.958466 \tValidation Loss: 2.308695\n",
      "Epoch: 10205 \tTraining Loss: 1.980607 \tValidation Loss: 2.309224\n",
      "Epoch: 10206 \tTraining Loss: 1.960441 \tValidation Loss: 2.309326\n",
      "Epoch: 10207 \tTraining Loss: 1.943324 \tValidation Loss: 2.309146\n",
      "Epoch: 10208 \tTraining Loss: 1.948627 \tValidation Loss: 2.308885\n",
      "Epoch: 10209 \tTraining Loss: 1.958326 \tValidation Loss: 2.309061\n",
      "Epoch: 10210 \tTraining Loss: 1.945191 \tValidation Loss: 2.309356\n",
      "Epoch: 10211 \tTraining Loss: 1.949938 \tValidation Loss: 2.309461\n",
      "Epoch: 10212 \tTraining Loss: 1.950109 \tValidation Loss: 2.309747\n",
      "Epoch: 10213 \tTraining Loss: 1.961392 \tValidation Loss: 2.309695\n",
      "Epoch: 10214 \tTraining Loss: 1.936380 \tValidation Loss: 2.309397\n",
      "Epoch: 10215 \tTraining Loss: 1.927973 \tValidation Loss: 2.309333\n",
      "Epoch: 10216 \tTraining Loss: 1.934301 \tValidation Loss: 2.309248\n",
      "Epoch: 10217 \tTraining Loss: 1.952492 \tValidation Loss: 2.309417\n",
      "Epoch: 10218 \tTraining Loss: 1.961686 \tValidation Loss: 2.309408\n",
      "Epoch: 10219 \tTraining Loss: 1.932585 \tValidation Loss: 2.309835\n",
      "Epoch: 10220 \tTraining Loss: 1.923885 \tValidation Loss: 2.309774\n",
      "Epoch: 10221 \tTraining Loss: 1.946886 \tValidation Loss: 2.309557\n",
      "Epoch: 10222 \tTraining Loss: 1.934459 \tValidation Loss: 2.309111\n",
      "Epoch: 10223 \tTraining Loss: 1.928692 \tValidation Loss: 2.309217\n",
      "Epoch: 10224 \tTraining Loss: 1.912447 \tValidation Loss: 2.309391\n",
      "Epoch: 10225 \tTraining Loss: 1.950348 \tValidation Loss: 2.309807\n",
      "Epoch: 10226 \tTraining Loss: 1.980703 \tValidation Loss: 2.309751\n",
      "Epoch: 10227 \tTraining Loss: 1.937968 \tValidation Loss: 2.309489\n",
      "Epoch: 10228 \tTraining Loss: 1.940215 \tValidation Loss: 2.309589\n",
      "Epoch: 10229 \tTraining Loss: 1.969869 \tValidation Loss: 2.309425\n",
      "Epoch: 10230 \tTraining Loss: 1.952246 \tValidation Loss: 2.309453\n",
      "Epoch: 10231 \tTraining Loss: 1.952296 \tValidation Loss: 2.309332\n",
      "Epoch: 10232 \tTraining Loss: 1.935865 \tValidation Loss: 2.309255\n",
      "Epoch: 10233 \tTraining Loss: 1.948096 \tValidation Loss: 2.309302\n",
      "Epoch: 10234 \tTraining Loss: 1.984788 \tValidation Loss: 2.309495\n",
      "Epoch: 10235 \tTraining Loss: 1.962855 \tValidation Loss: 2.309576\n",
      "Epoch: 10236 \tTraining Loss: 1.944378 \tValidation Loss: 2.309898\n",
      "Epoch: 10237 \tTraining Loss: 1.912598 \tValidation Loss: 2.309597\n",
      "Epoch: 10238 \tTraining Loss: 1.949200 \tValidation Loss: 2.309717\n",
      "Epoch: 10239 \tTraining Loss: 1.940769 \tValidation Loss: 2.309679\n",
      "Epoch: 10240 \tTraining Loss: 1.938927 \tValidation Loss: 2.309449\n",
      "Epoch: 10241 \tTraining Loss: 1.950664 \tValidation Loss: 2.309511\n",
      "Epoch: 10242 \tTraining Loss: 1.981114 \tValidation Loss: 2.309488\n",
      "Epoch: 10243 \tTraining Loss: 1.933293 \tValidation Loss: 2.309339\n",
      "Epoch: 10244 \tTraining Loss: 1.982995 \tValidation Loss: 2.309036\n",
      "Epoch: 10245 \tTraining Loss: 1.934121 \tValidation Loss: 2.309107\n",
      "Epoch: 10246 \tTraining Loss: 1.957879 \tValidation Loss: 2.309004\n",
      "Epoch: 10247 \tTraining Loss: 1.940318 \tValidation Loss: 2.308892\n",
      "Epoch: 10248 \tTraining Loss: 1.921036 \tValidation Loss: 2.308872\n",
      "Epoch: 10249 \tTraining Loss: 1.931450 \tValidation Loss: 2.309019\n",
      "Epoch: 10250 \tTraining Loss: 1.935082 \tValidation Loss: 2.308817\n",
      "Epoch: 10251 \tTraining Loss: 1.947949 \tValidation Loss: 2.308930\n",
      "Epoch: 10252 \tTraining Loss: 1.934267 \tValidation Loss: 2.309125\n",
      "Epoch: 10253 \tTraining Loss: 1.958092 \tValidation Loss: 2.308612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10254 \tTraining Loss: 1.926942 \tValidation Loss: 2.308937\n",
      "Epoch: 10255 \tTraining Loss: 1.930853 \tValidation Loss: 2.309302\n",
      "Epoch: 10256 \tTraining Loss: 1.957757 \tValidation Loss: 2.309103\n",
      "Epoch: 10257 \tTraining Loss: 1.946964 \tValidation Loss: 2.309466\n",
      "Epoch: 10258 \tTraining Loss: 1.964546 \tValidation Loss: 2.309489\n",
      "Epoch: 10259 \tTraining Loss: 1.947818 \tValidation Loss: 2.309772\n",
      "Epoch: 10260 \tTraining Loss: 1.949904 \tValidation Loss: 2.310042\n",
      "Epoch: 10261 \tTraining Loss: 1.947755 \tValidation Loss: 2.309839\n",
      "Epoch: 10262 \tTraining Loss: 1.932141 \tValidation Loss: 2.309785\n",
      "Epoch: 10263 \tTraining Loss: 1.968255 \tValidation Loss: 2.309534\n",
      "Epoch: 10264 \tTraining Loss: 1.942110 \tValidation Loss: 2.309724\n",
      "Epoch: 10265 \tTraining Loss: 1.968199 \tValidation Loss: 2.309549\n",
      "Epoch: 10266 \tTraining Loss: 1.968577 \tValidation Loss: 2.309633\n",
      "Epoch: 10267 \tTraining Loss: 1.922553 \tValidation Loss: 2.309825\n",
      "Epoch: 10268 \tTraining Loss: 1.940037 \tValidation Loss: 2.309327\n",
      "Epoch: 10269 \tTraining Loss: 1.926547 \tValidation Loss: 2.309520\n",
      "Epoch: 10270 \tTraining Loss: 1.951872 \tValidation Loss: 2.309258\n",
      "Epoch: 10271 \tTraining Loss: 1.949194 \tValidation Loss: 2.309519\n",
      "Epoch: 10272 \tTraining Loss: 1.939148 \tValidation Loss: 2.309234\n",
      "Epoch: 10273 \tTraining Loss: 1.935574 \tValidation Loss: 2.308727\n",
      "Epoch: 10274 \tTraining Loss: 1.944076 \tValidation Loss: 2.308621\n",
      "Epoch: 10275 \tTraining Loss: 1.922989 \tValidation Loss: 2.308847\n",
      "Epoch: 10276 \tTraining Loss: 1.942672 \tValidation Loss: 2.309041\n",
      "Epoch: 10277 \tTraining Loss: 1.969740 \tValidation Loss: 2.309078\n",
      "Epoch: 10278 \tTraining Loss: 1.921090 \tValidation Loss: 2.308920\n",
      "Epoch: 10279 \tTraining Loss: 1.949080 \tValidation Loss: 2.308771\n",
      "Epoch: 10280 \tTraining Loss: 1.964993 \tValidation Loss: 2.308987\n",
      "Epoch: 10281 \tTraining Loss: 1.950054 \tValidation Loss: 2.309541\n",
      "Epoch: 10282 \tTraining Loss: 1.968237 \tValidation Loss: 2.309420\n",
      "Epoch: 10283 \tTraining Loss: 1.932262 \tValidation Loss: 2.309529\n",
      "Epoch: 10284 \tTraining Loss: 1.962683 \tValidation Loss: 2.309537\n",
      "Epoch: 10285 \tTraining Loss: 1.967459 \tValidation Loss: 2.309525\n",
      "Epoch: 10286 \tTraining Loss: 1.928761 \tValidation Loss: 2.309404\n",
      "Epoch: 10287 \tTraining Loss: 1.953685 \tValidation Loss: 2.309571\n",
      "Epoch: 10288 \tTraining Loss: 1.931035 \tValidation Loss: 2.309704\n",
      "Epoch: 10289 \tTraining Loss: 1.955927 \tValidation Loss: 2.309613\n",
      "Epoch: 10290 \tTraining Loss: 1.934615 \tValidation Loss: 2.309555\n",
      "Epoch: 10291 \tTraining Loss: 1.934601 \tValidation Loss: 2.309590\n",
      "Epoch: 10292 \tTraining Loss: 1.975504 \tValidation Loss: 2.309684\n",
      "Epoch: 10293 \tTraining Loss: 1.934564 \tValidation Loss: 2.309559\n",
      "Epoch: 10294 \tTraining Loss: 1.968296 \tValidation Loss: 2.309430\n",
      "Epoch: 10295 \tTraining Loss: 1.950412 \tValidation Loss: 2.309642\n",
      "Epoch: 10296 \tTraining Loss: 1.963956 \tValidation Loss: 2.309500\n",
      "Epoch: 10297 \tTraining Loss: 1.947831 \tValidation Loss: 2.309784\n",
      "Epoch: 10298 \tTraining Loss: 1.937061 \tValidation Loss: 2.309613\n",
      "Epoch: 10299 \tTraining Loss: 1.958628 \tValidation Loss: 2.309632\n",
      "Epoch: 10300 \tTraining Loss: 1.934400 \tValidation Loss: 2.309783\n",
      "Epoch: 10301 \tTraining Loss: 1.942647 \tValidation Loss: 2.309898\n",
      "Epoch: 10302 \tTraining Loss: 1.943138 \tValidation Loss: 2.309969\n",
      "Epoch: 10303 \tTraining Loss: 1.998473 \tValidation Loss: 2.309413\n",
      "Epoch: 10304 \tTraining Loss: 1.947578 \tValidation Loss: 2.309277\n",
      "Epoch: 10305 \tTraining Loss: 1.930373 \tValidation Loss: 2.309202\n",
      "Epoch: 10306 \tTraining Loss: 1.926630 \tValidation Loss: 2.309065\n",
      "Epoch: 10307 \tTraining Loss: 1.946014 \tValidation Loss: 2.309189\n",
      "Epoch: 10308 \tTraining Loss: 1.958352 \tValidation Loss: 2.309196\n",
      "Epoch: 10309 \tTraining Loss: 1.952025 \tValidation Loss: 2.309066\n",
      "Epoch: 10310 \tTraining Loss: 1.935717 \tValidation Loss: 2.309193\n",
      "Epoch: 10311 \tTraining Loss: 1.920578 \tValidation Loss: 2.309531\n",
      "Epoch: 10312 \tTraining Loss: 1.937900 \tValidation Loss: 2.309591\n",
      "Epoch: 10313 \tTraining Loss: 1.969932 \tValidation Loss: 2.309465\n",
      "Epoch: 10314 \tTraining Loss: 1.938995 \tValidation Loss: 2.309340\n",
      "Epoch: 10315 \tTraining Loss: 1.909107 \tValidation Loss: 2.309087\n",
      "Epoch: 10316 \tTraining Loss: 1.912072 \tValidation Loss: 2.309572\n",
      "Epoch: 10317 \tTraining Loss: 1.934384 \tValidation Loss: 2.309552\n",
      "Epoch: 10318 \tTraining Loss: 1.937874 \tValidation Loss: 2.309749\n",
      "Epoch: 10319 \tTraining Loss: 1.956011 \tValidation Loss: 2.309637\n",
      "Epoch: 10320 \tTraining Loss: 1.948517 \tValidation Loss: 2.309666\n",
      "Epoch: 10321 \tTraining Loss: 1.980996 \tValidation Loss: 2.309548\n",
      "Epoch: 10322 \tTraining Loss: 1.937732 \tValidation Loss: 2.309811\n",
      "Epoch: 10323 \tTraining Loss: 1.926267 \tValidation Loss: 2.309994\n",
      "Epoch: 10324 \tTraining Loss: 1.943403 \tValidation Loss: 2.309400\n",
      "Epoch: 10325 \tTraining Loss: 1.950008 \tValidation Loss: 2.309235\n",
      "Epoch: 10326 \tTraining Loss: 1.939569 \tValidation Loss: 2.309453\n",
      "Epoch: 10327 \tTraining Loss: 1.919731 \tValidation Loss: 2.309609\n",
      "Epoch: 10328 \tTraining Loss: 1.956613 \tValidation Loss: 2.309755\n",
      "Epoch: 10329 \tTraining Loss: 1.951995 \tValidation Loss: 2.309870\n",
      "Epoch: 10330 \tTraining Loss: 1.982769 \tValidation Loss: 2.309772\n",
      "Epoch: 10331 \tTraining Loss: 1.942718 \tValidation Loss: 2.309603\n",
      "Epoch: 10332 \tTraining Loss: 1.896896 \tValidation Loss: 2.309233\n",
      "Epoch: 10333 \tTraining Loss: 1.936209 \tValidation Loss: 2.309594\n",
      "Epoch: 10334 \tTraining Loss: 1.940320 \tValidation Loss: 2.309278\n",
      "Epoch: 10335 \tTraining Loss: 1.925122 \tValidation Loss: 2.309307\n",
      "Epoch: 10336 \tTraining Loss: 1.978305 \tValidation Loss: 2.309160\n",
      "Epoch: 10337 \tTraining Loss: 1.946361 \tValidation Loss: 2.309062\n",
      "Epoch: 10338 \tTraining Loss: 1.949956 \tValidation Loss: 2.309010\n",
      "Epoch: 10339 \tTraining Loss: 1.927675 \tValidation Loss: 2.309391\n",
      "Epoch: 10340 \tTraining Loss: 1.927010 \tValidation Loss: 2.309463\n",
      "Epoch: 10341 \tTraining Loss: 1.955878 \tValidation Loss: 2.309609\n",
      "Epoch: 10342 \tTraining Loss: 1.948189 \tValidation Loss: 2.309908\n",
      "Epoch: 10343 \tTraining Loss: 1.931033 \tValidation Loss: 2.309807\n",
      "Epoch: 10344 \tTraining Loss: 1.942902 \tValidation Loss: 2.309935\n",
      "Epoch: 10345 \tTraining Loss: 1.942569 \tValidation Loss: 2.309254\n",
      "Epoch: 10346 \tTraining Loss: 1.952692 \tValidation Loss: 2.309233\n",
      "Epoch: 10347 \tTraining Loss: 1.935883 \tValidation Loss: 2.309628\n",
      "Epoch: 10348 \tTraining Loss: 1.949437 \tValidation Loss: 2.309619\n",
      "Epoch: 10349 \tTraining Loss: 1.958918 \tValidation Loss: 2.309797\n",
      "Epoch: 10350 \tTraining Loss: 1.949244 \tValidation Loss: 2.309834\n",
      "Epoch: 10351 \tTraining Loss: 1.907593 \tValidation Loss: 2.309866\n",
      "Epoch: 10352 \tTraining Loss: 1.949529 \tValidation Loss: 2.309186\n",
      "Epoch: 10353 \tTraining Loss: 1.966190 \tValidation Loss: 2.309120\n",
      "Epoch: 10354 \tTraining Loss: 1.971884 \tValidation Loss: 2.309001\n",
      "Epoch: 10355 \tTraining Loss: 1.934927 \tValidation Loss: 2.309206\n",
      "Epoch: 10356 \tTraining Loss: 1.930219 \tValidation Loss: 2.309175\n",
      "Epoch: 10357 \tTraining Loss: 1.939906 \tValidation Loss: 2.309202\n",
      "Epoch: 10358 \tTraining Loss: 1.916435 \tValidation Loss: 2.308762\n",
      "Epoch: 10359 \tTraining Loss: 1.932392 \tValidation Loss: 2.308492\n",
      "Epoch: 10360 \tTraining Loss: 1.943638 \tValidation Loss: 2.308708\n",
      "Epoch: 10361 \tTraining Loss: 1.941579 \tValidation Loss: 2.308996\n",
      "Epoch: 10362 \tTraining Loss: 1.953270 \tValidation Loss: 2.309337\n",
      "Epoch: 10363 \tTraining Loss: 1.924187 \tValidation Loss: 2.309303\n",
      "Epoch: 10364 \tTraining Loss: 1.923054 \tValidation Loss: 2.309744\n",
      "Epoch: 10365 \tTraining Loss: 1.949552 \tValidation Loss: 2.309733\n",
      "Epoch: 10366 \tTraining Loss: 1.934982 \tValidation Loss: 2.309776\n",
      "Epoch: 10367 \tTraining Loss: 1.940001 \tValidation Loss: 2.309779\n",
      "Epoch: 10368 \tTraining Loss: 1.972546 \tValidation Loss: 2.309639\n",
      "Epoch: 10369 \tTraining Loss: 1.924178 \tValidation Loss: 2.309539\n",
      "Epoch: 10370 \tTraining Loss: 1.920945 \tValidation Loss: 2.309908\n",
      "Epoch: 10371 \tTraining Loss: 1.964096 \tValidation Loss: 2.309763\n",
      "Epoch: 10372 \tTraining Loss: 1.942408 \tValidation Loss: 2.309913\n",
      "Epoch: 10373 \tTraining Loss: 1.953017 \tValidation Loss: 2.309894\n",
      "Epoch: 10374 \tTraining Loss: 1.918811 \tValidation Loss: 2.309800\n",
      "Epoch: 10375 \tTraining Loss: 1.946793 \tValidation Loss: 2.309802\n",
      "Epoch: 10376 \tTraining Loss: 1.952165 \tValidation Loss: 2.309711\n",
      "Epoch: 10377 \tTraining Loss: 1.933385 \tValidation Loss: 2.309946\n",
      "Epoch: 10378 \tTraining Loss: 1.940785 \tValidation Loss: 2.310120\n",
      "Epoch: 10379 \tTraining Loss: 1.930223 \tValidation Loss: 2.310055\n",
      "Epoch: 10380 \tTraining Loss: 1.925558 \tValidation Loss: 2.310245\n",
      "Epoch: 10381 \tTraining Loss: 1.929297 \tValidation Loss: 2.310283\n",
      "Epoch: 10382 \tTraining Loss: 1.939732 \tValidation Loss: 2.309987\n",
      "Epoch: 10383 \tTraining Loss: 1.958042 \tValidation Loss: 2.309904\n",
      "Epoch: 10384 \tTraining Loss: 1.967155 \tValidation Loss: 2.310002\n",
      "Epoch: 10385 \tTraining Loss: 1.940648 \tValidation Loss: 2.310133\n",
      "Epoch: 10386 \tTraining Loss: 1.964159 \tValidation Loss: 2.309942\n",
      "Epoch: 10387 \tTraining Loss: 1.945827 \tValidation Loss: 2.309924\n",
      "Epoch: 10388 \tTraining Loss: 1.932635 \tValidation Loss: 2.309629\n",
      "Epoch: 10389 \tTraining Loss: 1.932323 \tValidation Loss: 2.309282\n",
      "Epoch: 10390 \tTraining Loss: 1.908241 \tValidation Loss: 2.309293\n",
      "Epoch: 10391 \tTraining Loss: 1.900778 \tValidation Loss: 2.309235\n",
      "Epoch: 10392 \tTraining Loss: 1.956443 \tValidation Loss: 2.309227\n",
      "Epoch: 10393 \tTraining Loss: 1.920643 \tValidation Loss: 2.309423\n",
      "Epoch: 10394 \tTraining Loss: 1.946375 \tValidation Loss: 2.309394\n",
      "Epoch: 10395 \tTraining Loss: 1.922114 \tValidation Loss: 2.309172\n",
      "Epoch: 10396 \tTraining Loss: 1.945137 \tValidation Loss: 2.309004\n",
      "Epoch: 10397 \tTraining Loss: 1.925071 \tValidation Loss: 2.309181\n",
      "Epoch: 10398 \tTraining Loss: 1.956716 \tValidation Loss: 2.309257\n",
      "Epoch: 10399 \tTraining Loss: 1.964214 \tValidation Loss: 2.309281\n",
      "Epoch: 10400 \tTraining Loss: 1.912318 \tValidation Loss: 2.309352\n",
      "Epoch: 10401 \tTraining Loss: 1.946769 \tValidation Loss: 2.309416\n",
      "Epoch: 10402 \tTraining Loss: 1.915108 \tValidation Loss: 2.308941\n",
      "Epoch: 10403 \tTraining Loss: 1.929559 \tValidation Loss: 2.309227\n",
      "Epoch: 10404 \tTraining Loss: 1.918076 \tValidation Loss: 2.309040\n",
      "Epoch: 10405 \tTraining Loss: 1.921760 \tValidation Loss: 2.309432\n",
      "Epoch: 10406 \tTraining Loss: 1.942214 \tValidation Loss: 2.309401\n",
      "Epoch: 10407 \tTraining Loss: 1.936521 \tValidation Loss: 2.309384\n",
      "Epoch: 10408 \tTraining Loss: 1.943301 \tValidation Loss: 2.309491\n",
      "Epoch: 10409 \tTraining Loss: 1.896061 \tValidation Loss: 2.309355\n",
      "Epoch: 10410 \tTraining Loss: 1.928206 \tValidation Loss: 2.309547\n",
      "Epoch: 10411 \tTraining Loss: 1.916121 \tValidation Loss: 2.309547\n",
      "Epoch: 10412 \tTraining Loss: 1.912174 \tValidation Loss: 2.309733\n",
      "Epoch: 10413 \tTraining Loss: 1.926362 \tValidation Loss: 2.309583\n",
      "Epoch: 10414 \tTraining Loss: 1.935654 \tValidation Loss: 2.309912\n",
      "Epoch: 10415 \tTraining Loss: 1.939307 \tValidation Loss: 2.309371\n",
      "Epoch: 10416 \tTraining Loss: 1.946607 \tValidation Loss: 2.309401\n",
      "Epoch: 10417 \tTraining Loss: 1.961037 \tValidation Loss: 2.309511\n",
      "Epoch: 10418 \tTraining Loss: 1.932477 \tValidation Loss: 2.309816\n",
      "Epoch: 10419 \tTraining Loss: 1.934578 \tValidation Loss: 2.309904\n",
      "Epoch: 10420 \tTraining Loss: 1.925866 \tValidation Loss: 2.309931\n",
      "Epoch: 10421 \tTraining Loss: 1.918715 \tValidation Loss: 2.309999\n",
      "Epoch: 10422 \tTraining Loss: 1.950862 \tValidation Loss: 2.309819\n",
      "Epoch: 10423 \tTraining Loss: 1.957800 \tValidation Loss: 2.309955\n",
      "Epoch: 10424 \tTraining Loss: 1.969258 \tValidation Loss: 2.309793\n",
      "Epoch: 10425 \tTraining Loss: 1.958689 \tValidation Loss: 2.309832\n",
      "Epoch: 10426 \tTraining Loss: 1.930017 \tValidation Loss: 2.309865\n",
      "Epoch: 10427 \tTraining Loss: 1.982498 \tValidation Loss: 2.310217\n",
      "Epoch: 10428 \tTraining Loss: 1.957474 \tValidation Loss: 2.310181\n",
      "Epoch: 10429 \tTraining Loss: 1.939260 \tValidation Loss: 2.309979\n",
      "Epoch: 10430 \tTraining Loss: 1.966474 \tValidation Loss: 2.309977\n",
      "Epoch: 10431 \tTraining Loss: 1.944175 \tValidation Loss: 2.309685\n",
      "Epoch: 10432 \tTraining Loss: 1.955077 \tValidation Loss: 2.309528\n",
      "Epoch: 10433 \tTraining Loss: 1.934224 \tValidation Loss: 2.309331\n",
      "Epoch: 10434 \tTraining Loss: 1.960433 \tValidation Loss: 2.309350\n",
      "Epoch: 10435 \tTraining Loss: 1.966795 \tValidation Loss: 2.309190\n",
      "Epoch: 10436 \tTraining Loss: 1.956746 \tValidation Loss: 2.308995\n",
      "Epoch: 10437 \tTraining Loss: 1.903035 \tValidation Loss: 2.309016\n",
      "Epoch: 10438 \tTraining Loss: 1.910326 \tValidation Loss: 2.309076\n",
      "Epoch: 10439 \tTraining Loss: 1.943047 \tValidation Loss: 2.308771\n",
      "Epoch: 10440 \tTraining Loss: 1.953778 \tValidation Loss: 2.308848\n",
      "Epoch: 10441 \tTraining Loss: 1.936566 \tValidation Loss: 2.308979\n",
      "Epoch: 10442 \tTraining Loss: 1.918797 \tValidation Loss: 2.308873\n",
      "Epoch: 10443 \tTraining Loss: 1.934633 \tValidation Loss: 2.308830\n",
      "Epoch: 10444 \tTraining Loss: 1.939813 \tValidation Loss: 2.308809\n",
      "Epoch: 10445 \tTraining Loss: 1.939592 \tValidation Loss: 2.308872\n",
      "Epoch: 10446 \tTraining Loss: 1.941609 \tValidation Loss: 2.309216\n",
      "Epoch: 10447 \tTraining Loss: 1.959257 \tValidation Loss: 2.309601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10448 \tTraining Loss: 1.923153 \tValidation Loss: 2.309628\n",
      "Epoch: 10449 \tTraining Loss: 1.933141 \tValidation Loss: 2.309806\n",
      "Epoch: 10450 \tTraining Loss: 1.945835 \tValidation Loss: 2.309486\n",
      "Epoch: 10451 \tTraining Loss: 1.922388 \tValidation Loss: 2.309434\n",
      "Epoch: 10452 \tTraining Loss: 1.905589 \tValidation Loss: 2.309422\n",
      "Epoch: 10453 \tTraining Loss: 1.936844 \tValidation Loss: 2.309498\n",
      "Epoch: 10454 \tTraining Loss: 1.952003 \tValidation Loss: 2.309338\n",
      "Epoch: 10455 \tTraining Loss: 1.945050 \tValidation Loss: 2.309242\n",
      "Epoch: 10456 \tTraining Loss: 1.945202 \tValidation Loss: 2.309175\n",
      "Epoch: 10457 \tTraining Loss: 1.939273 \tValidation Loss: 2.309419\n",
      "Epoch: 10458 \tTraining Loss: 1.927816 \tValidation Loss: 2.309646\n",
      "Epoch: 10459 \tTraining Loss: 1.928249 \tValidation Loss: 2.309690\n",
      "Epoch: 10460 \tTraining Loss: 1.916077 \tValidation Loss: 2.309583\n",
      "Epoch: 10461 \tTraining Loss: 1.946600 \tValidation Loss: 2.309438\n",
      "Epoch: 10462 \tTraining Loss: 1.958223 \tValidation Loss: 2.309389\n",
      "Epoch: 10463 \tTraining Loss: 1.927525 \tValidation Loss: 2.309165\n",
      "Epoch: 10464 \tTraining Loss: 1.918127 \tValidation Loss: 2.309087\n",
      "Epoch: 10465 \tTraining Loss: 1.943669 \tValidation Loss: 2.308749\n",
      "Epoch: 10466 \tTraining Loss: 1.939917 \tValidation Loss: 2.308935\n",
      "Epoch: 10467 \tTraining Loss: 1.950109 \tValidation Loss: 2.308621\n",
      "Epoch: 10468 \tTraining Loss: 1.957400 \tValidation Loss: 2.308936\n",
      "Epoch: 10469 \tTraining Loss: 1.941302 \tValidation Loss: 2.309230\n",
      "Epoch: 10470 \tTraining Loss: 1.955758 \tValidation Loss: 2.309381\n",
      "Epoch: 10471 \tTraining Loss: 1.936476 \tValidation Loss: 2.309159\n",
      "Epoch: 10472 \tTraining Loss: 1.971664 \tValidation Loss: 2.309059\n",
      "Epoch: 10473 \tTraining Loss: 1.937056 \tValidation Loss: 2.309316\n",
      "Epoch: 10474 \tTraining Loss: 1.938162 \tValidation Loss: 2.309402\n",
      "Epoch: 10475 \tTraining Loss: 1.951311 \tValidation Loss: 2.309613\n",
      "Epoch: 10476 \tTraining Loss: 1.939714 \tValidation Loss: 2.309676\n",
      "Epoch: 10477 \tTraining Loss: 1.961866 \tValidation Loss: 2.309394\n",
      "Epoch: 10478 \tTraining Loss: 1.943106 \tValidation Loss: 2.309195\n",
      "Epoch: 10479 \tTraining Loss: 1.946807 \tValidation Loss: 2.309190\n",
      "Epoch: 10480 \tTraining Loss: 1.945199 \tValidation Loss: 2.309289\n",
      "Epoch: 10481 \tTraining Loss: 1.902477 \tValidation Loss: 2.309274\n",
      "Epoch: 10482 \tTraining Loss: 1.915962 \tValidation Loss: 2.309570\n",
      "Epoch: 10483 \tTraining Loss: 1.959142 \tValidation Loss: 2.309847\n",
      "Epoch: 10484 \tTraining Loss: 1.923870 \tValidation Loss: 2.309519\n",
      "Epoch: 10485 \tTraining Loss: 1.937633 \tValidation Loss: 2.309676\n",
      "Epoch: 10486 \tTraining Loss: 1.957128 \tValidation Loss: 2.309700\n",
      "Epoch: 10487 \tTraining Loss: 1.934328 \tValidation Loss: 2.309524\n",
      "Epoch: 10488 \tTraining Loss: 1.920040 \tValidation Loss: 2.309635\n",
      "Epoch: 10489 \tTraining Loss: 1.920408 \tValidation Loss: 2.309751\n",
      "Epoch: 10490 \tTraining Loss: 1.962418 \tValidation Loss: 2.309432\n",
      "Epoch: 10491 \tTraining Loss: 1.932368 \tValidation Loss: 2.309124\n",
      "Epoch: 10492 \tTraining Loss: 1.942015 \tValidation Loss: 2.309027\n",
      "Epoch: 10493 \tTraining Loss: 1.922270 \tValidation Loss: 2.309255\n",
      "Epoch: 10494 \tTraining Loss: 1.970012 \tValidation Loss: 2.309412\n",
      "Epoch: 10495 \tTraining Loss: 1.907350 \tValidation Loss: 2.309344\n",
      "Epoch: 10496 \tTraining Loss: 1.947546 \tValidation Loss: 2.308851\n",
      "Epoch: 10497 \tTraining Loss: 1.953590 \tValidation Loss: 2.309120\n",
      "Epoch: 10498 \tTraining Loss: 1.917389 \tValidation Loss: 2.309416\n",
      "Epoch: 10499 \tTraining Loss: 1.956400 \tValidation Loss: 2.309526\n",
      "Epoch: 10500 \tTraining Loss: 1.926660 \tValidation Loss: 2.309481\n",
      "Epoch: 10501 \tTraining Loss: 1.945632 \tValidation Loss: 2.309130\n",
      "Epoch: 10502 \tTraining Loss: 1.948303 \tValidation Loss: 2.308754\n",
      "Epoch: 10503 \tTraining Loss: 1.939256 \tValidation Loss: 2.309310\n",
      "Epoch: 10504 \tTraining Loss: 1.946691 \tValidation Loss: 2.309419\n",
      "Epoch: 10505 \tTraining Loss: 1.914373 \tValidation Loss: 2.309678\n",
      "Epoch: 10506 \tTraining Loss: 1.906900 \tValidation Loss: 2.309403\n",
      "Epoch: 10507 \tTraining Loss: 1.920067 \tValidation Loss: 2.309626\n",
      "Epoch: 10508 \tTraining Loss: 1.945702 \tValidation Loss: 2.309418\n",
      "Epoch: 10509 \tTraining Loss: 1.958577 \tValidation Loss: 2.309524\n",
      "Epoch: 10510 \tTraining Loss: 1.948036 \tValidation Loss: 2.309780\n",
      "Epoch: 10511 \tTraining Loss: 1.927152 \tValidation Loss: 2.309153\n",
      "Epoch: 10512 \tTraining Loss: 1.927040 \tValidation Loss: 2.309316\n",
      "Epoch: 10513 \tTraining Loss: 1.919950 \tValidation Loss: 2.309220\n",
      "Epoch: 10514 \tTraining Loss: 1.935114 \tValidation Loss: 2.309206\n",
      "Epoch: 10515 \tTraining Loss: 1.948469 \tValidation Loss: 2.309453\n",
      "Epoch: 10516 \tTraining Loss: 1.948098 \tValidation Loss: 2.309384\n",
      "Epoch: 10517 \tTraining Loss: 1.950694 \tValidation Loss: 2.309635\n",
      "Epoch: 10518 \tTraining Loss: 1.941613 \tValidation Loss: 2.309570\n",
      "Epoch: 10519 \tTraining Loss: 1.938605 \tValidation Loss: 2.309612\n",
      "Epoch: 10520 \tTraining Loss: 1.959706 \tValidation Loss: 2.309486\n",
      "Epoch: 10521 \tTraining Loss: 1.939636 \tValidation Loss: 2.309508\n",
      "Epoch: 10522 \tTraining Loss: 1.926424 \tValidation Loss: 2.309465\n",
      "Epoch: 10523 \tTraining Loss: 1.915753 \tValidation Loss: 2.309423\n",
      "Epoch: 10524 \tTraining Loss: 1.937362 \tValidation Loss: 2.309621\n",
      "Epoch: 10525 \tTraining Loss: 1.930488 \tValidation Loss: 2.309805\n",
      "Epoch: 10526 \tTraining Loss: 1.929447 \tValidation Loss: 2.309650\n",
      "Epoch: 10527 \tTraining Loss: 1.956316 \tValidation Loss: 2.309454\n",
      "Epoch: 10528 \tTraining Loss: 1.940083 \tValidation Loss: 2.309136\n",
      "Epoch: 10529 \tTraining Loss: 1.929360 \tValidation Loss: 2.308927\n",
      "Epoch: 10530 \tTraining Loss: 1.947538 \tValidation Loss: 2.309299\n",
      "Epoch: 10531 \tTraining Loss: 1.907621 \tValidation Loss: 2.309614\n",
      "Epoch: 10532 \tTraining Loss: 1.916238 \tValidation Loss: 2.309568\n",
      "Epoch: 10533 \tTraining Loss: 1.931343 \tValidation Loss: 2.309511\n",
      "Epoch: 10534 \tTraining Loss: 1.956841 \tValidation Loss: 2.309565\n",
      "Epoch: 10535 \tTraining Loss: 1.913882 \tValidation Loss: 2.309347\n",
      "Epoch: 10536 \tTraining Loss: 1.927792 \tValidation Loss: 2.309398\n",
      "Epoch: 10537 \tTraining Loss: 1.910397 \tValidation Loss: 2.309569\n",
      "Epoch: 10538 \tTraining Loss: 1.926240 \tValidation Loss: 2.309484\n",
      "Epoch: 10539 \tTraining Loss: 1.894987 \tValidation Loss: 2.309799\n",
      "Epoch: 10540 \tTraining Loss: 1.945744 \tValidation Loss: 2.310289\n",
      "Epoch: 10541 \tTraining Loss: 1.901319 \tValidation Loss: 2.309967\n",
      "Epoch: 10542 \tTraining Loss: 1.939360 \tValidation Loss: 2.309730\n",
      "Epoch: 10543 \tTraining Loss: 1.956522 \tValidation Loss: 2.309856\n",
      "Epoch: 10544 \tTraining Loss: 1.939068 \tValidation Loss: 2.309617\n",
      "Epoch: 10545 \tTraining Loss: 1.929635 \tValidation Loss: 2.309862\n",
      "Epoch: 10546 \tTraining Loss: 1.956437 \tValidation Loss: 2.309727\n",
      "Epoch: 10547 \tTraining Loss: 1.932563 \tValidation Loss: 2.309285\n",
      "Epoch: 10548 \tTraining Loss: 1.917342 \tValidation Loss: 2.309227\n",
      "Epoch: 10549 \tTraining Loss: 1.928428 \tValidation Loss: 2.309449\n",
      "Epoch: 10550 \tTraining Loss: 1.964568 \tValidation Loss: 2.309451\n",
      "Epoch: 10551 \tTraining Loss: 1.929572 \tValidation Loss: 2.309529\n",
      "Epoch: 10552 \tTraining Loss: 1.926100 \tValidation Loss: 2.309309\n",
      "Epoch: 10553 \tTraining Loss: 1.935377 \tValidation Loss: 2.309150\n",
      "Epoch: 10554 \tTraining Loss: 1.943258 \tValidation Loss: 2.309664\n",
      "Epoch: 10555 \tTraining Loss: 1.933787 \tValidation Loss: 2.309568\n",
      "Epoch: 10556 \tTraining Loss: 1.914609 \tValidation Loss: 2.309616\n",
      "Epoch: 10557 \tTraining Loss: 1.910190 \tValidation Loss: 2.309273\n",
      "Epoch: 10558 \tTraining Loss: 1.940668 \tValidation Loss: 2.309550\n",
      "Epoch: 10559 \tTraining Loss: 1.916150 \tValidation Loss: 2.309714\n",
      "Epoch: 10560 \tTraining Loss: 1.925409 \tValidation Loss: 2.309500\n",
      "Epoch: 10561 \tTraining Loss: 1.911291 \tValidation Loss: 2.309720\n",
      "Epoch: 10562 \tTraining Loss: 1.936168 \tValidation Loss: 2.309627\n",
      "Epoch: 10563 \tTraining Loss: 1.898697 \tValidation Loss: 2.309813\n",
      "Epoch: 10564 \tTraining Loss: 1.911213 \tValidation Loss: 2.310207\n",
      "Epoch: 10565 \tTraining Loss: 1.934107 \tValidation Loss: 2.309995\n",
      "Epoch: 10566 \tTraining Loss: 1.931002 \tValidation Loss: 2.309740\n",
      "Epoch: 10567 \tTraining Loss: 1.946086 \tValidation Loss: 2.309534\n",
      "Epoch: 10568 \tTraining Loss: 1.910198 \tValidation Loss: 2.309666\n",
      "Epoch: 10569 \tTraining Loss: 1.942802 \tValidation Loss: 2.309160\n",
      "Epoch: 10570 \tTraining Loss: 1.926803 \tValidation Loss: 2.309016\n",
      "Epoch: 10571 \tTraining Loss: 1.918463 \tValidation Loss: 2.309285\n",
      "Epoch: 10572 \tTraining Loss: 1.973744 \tValidation Loss: 2.309307\n",
      "Epoch: 10573 \tTraining Loss: 1.946188 \tValidation Loss: 2.309373\n",
      "Epoch: 10574 \tTraining Loss: 1.906290 \tValidation Loss: 2.309352\n",
      "Epoch: 10575 \tTraining Loss: 1.916667 \tValidation Loss: 2.309698\n",
      "Epoch: 10576 \tTraining Loss: 1.945886 \tValidation Loss: 2.309741\n",
      "Epoch: 10577 \tTraining Loss: 1.928361 \tValidation Loss: 2.309770\n",
      "Epoch: 10578 \tTraining Loss: 1.951867 \tValidation Loss: 2.309726\n",
      "Epoch: 10579 \tTraining Loss: 1.939258 \tValidation Loss: 2.309519\n",
      "Epoch: 10580 \tTraining Loss: 1.935650 \tValidation Loss: 2.309615\n",
      "Epoch: 10581 \tTraining Loss: 1.934167 \tValidation Loss: 2.309389\n",
      "Epoch: 10582 \tTraining Loss: 1.956544 \tValidation Loss: 2.309552\n",
      "Epoch: 10583 \tTraining Loss: 1.936430 \tValidation Loss: 2.309561\n",
      "Epoch: 10584 \tTraining Loss: 1.946984 \tValidation Loss: 2.309675\n",
      "Epoch: 10585 \tTraining Loss: 1.925588 \tValidation Loss: 2.309400\n",
      "Epoch: 10586 \tTraining Loss: 1.941326 \tValidation Loss: 2.309337\n",
      "Epoch: 10587 \tTraining Loss: 1.909700 \tValidation Loss: 2.309594\n",
      "Epoch: 10588 \tTraining Loss: 1.958260 \tValidation Loss: 2.309402\n",
      "Epoch: 10589 \tTraining Loss: 1.916713 \tValidation Loss: 2.309647\n",
      "Epoch: 10590 \tTraining Loss: 1.977290 \tValidation Loss: 2.309186\n",
      "Epoch: 10591 \tTraining Loss: 1.940426 \tValidation Loss: 2.309376\n",
      "Epoch: 10592 \tTraining Loss: 1.909267 \tValidation Loss: 2.309257\n",
      "Epoch: 10593 \tTraining Loss: 1.917693 \tValidation Loss: 2.309146\n",
      "Epoch: 10594 \tTraining Loss: 1.907193 \tValidation Loss: 2.309074\n",
      "Epoch: 10595 \tTraining Loss: 1.909183 \tValidation Loss: 2.308855\n",
      "Epoch: 10596 \tTraining Loss: 1.955011 \tValidation Loss: 2.309310\n",
      "Epoch: 10597 \tTraining Loss: 1.946193 \tValidation Loss: 2.308685\n",
      "Epoch: 10598 \tTraining Loss: 1.945656 \tValidation Loss: 2.308553\n",
      "Epoch: 10599 \tTraining Loss: 1.940537 \tValidation Loss: 2.308589\n",
      "Epoch: 10600 \tTraining Loss: 1.927357 \tValidation Loss: 2.308602\n",
      "Epoch: 10601 \tTraining Loss: 1.939108 \tValidation Loss: 2.308570\n",
      "Epoch: 10602 \tTraining Loss: 1.917896 \tValidation Loss: 2.308626\n",
      "Epoch: 10603 \tTraining Loss: 1.932150 \tValidation Loss: 2.308643\n",
      "Epoch: 10604 \tTraining Loss: 1.945047 \tValidation Loss: 2.308888\n",
      "Epoch: 10605 \tTraining Loss: 1.937414 \tValidation Loss: 2.308988\n",
      "Epoch: 10606 \tTraining Loss: 1.918150 \tValidation Loss: 2.309253\n",
      "Epoch: 10607 \tTraining Loss: 1.947827 \tValidation Loss: 2.309599\n",
      "Epoch: 10608 \tTraining Loss: 1.946986 \tValidation Loss: 2.309396\n",
      "Epoch: 10609 \tTraining Loss: 1.935799 \tValidation Loss: 2.309593\n",
      "Epoch: 10610 \tTraining Loss: 1.944637 \tValidation Loss: 2.309665\n",
      "Epoch: 10611 \tTraining Loss: 1.920591 \tValidation Loss: 2.309514\n",
      "Epoch: 10612 \tTraining Loss: 1.962965 \tValidation Loss: 2.309528\n",
      "Epoch: 10613 \tTraining Loss: 1.913685 \tValidation Loss: 2.309186\n",
      "Epoch: 10614 \tTraining Loss: 1.919257 \tValidation Loss: 2.309171\n",
      "Epoch: 10615 \tTraining Loss: 1.940699 \tValidation Loss: 2.309406\n",
      "Epoch: 10616 \tTraining Loss: 1.926276 \tValidation Loss: 2.309159\n",
      "Epoch: 10617 \tTraining Loss: 1.945767 \tValidation Loss: 2.309177\n",
      "Epoch: 10618 \tTraining Loss: 1.939405 \tValidation Loss: 2.308979\n",
      "Epoch: 10619 \tTraining Loss: 1.924748 \tValidation Loss: 2.308892\n",
      "Epoch: 10620 \tTraining Loss: 1.966097 \tValidation Loss: 2.309219\n",
      "Epoch: 10621 \tTraining Loss: 1.938879 \tValidation Loss: 2.308945\n",
      "Epoch: 10622 \tTraining Loss: 1.897982 \tValidation Loss: 2.309201\n",
      "Epoch: 10623 \tTraining Loss: 1.918616 \tValidation Loss: 2.309463\n",
      "Epoch: 10624 \tTraining Loss: 1.933877 \tValidation Loss: 2.309465\n",
      "Epoch: 10625 \tTraining Loss: 1.959234 \tValidation Loss: 2.309215\n",
      "Epoch: 10626 \tTraining Loss: 1.923585 \tValidation Loss: 2.309314\n",
      "Epoch: 10627 \tTraining Loss: 1.946787 \tValidation Loss: 2.309292\n",
      "Epoch: 10628 \tTraining Loss: 1.936014 \tValidation Loss: 2.309222\n",
      "Epoch: 10629 \tTraining Loss: 1.936116 \tValidation Loss: 2.309210\n",
      "Epoch: 10630 \tTraining Loss: 1.944902 \tValidation Loss: 2.309594\n",
      "Epoch: 10631 \tTraining Loss: 1.937409 \tValidation Loss: 2.309851\n",
      "Epoch: 10632 \tTraining Loss: 1.944850 \tValidation Loss: 2.309915\n",
      "Epoch: 10633 \tTraining Loss: 1.936126 \tValidation Loss: 2.309639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10634 \tTraining Loss: 1.956200 \tValidation Loss: 2.309532\n",
      "Epoch: 10635 \tTraining Loss: 1.915601 \tValidation Loss: 2.309120\n",
      "Epoch: 10636 \tTraining Loss: 1.954389 \tValidation Loss: 2.308886\n",
      "Epoch: 10637 \tTraining Loss: 1.919005 \tValidation Loss: 2.309158\n",
      "Epoch: 10638 \tTraining Loss: 1.955032 \tValidation Loss: 2.308830\n",
      "Epoch: 10639 \tTraining Loss: 1.902628 \tValidation Loss: 2.308657\n",
      "Epoch: 10640 \tTraining Loss: 1.935200 \tValidation Loss: 2.308893\n",
      "Epoch: 10641 \tTraining Loss: 1.910706 \tValidation Loss: 2.309220\n",
      "Epoch: 10642 \tTraining Loss: 1.915431 \tValidation Loss: 2.309222\n",
      "Epoch: 10643 \tTraining Loss: 1.910388 \tValidation Loss: 2.309390\n",
      "Epoch: 10644 \tTraining Loss: 1.916723 \tValidation Loss: 2.309542\n",
      "Epoch: 10645 \tTraining Loss: 1.917246 \tValidation Loss: 2.309833\n",
      "Epoch: 10646 \tTraining Loss: 1.938979 \tValidation Loss: 2.309582\n",
      "Epoch: 10647 \tTraining Loss: 1.919708 \tValidation Loss: 2.309691\n",
      "Epoch: 10648 \tTraining Loss: 1.903960 \tValidation Loss: 2.309582\n",
      "Epoch: 10649 \tTraining Loss: 1.912381 \tValidation Loss: 2.309568\n",
      "Epoch: 10650 \tTraining Loss: 1.919410 \tValidation Loss: 2.309481\n",
      "Epoch: 10651 \tTraining Loss: 1.924334 \tValidation Loss: 2.309502\n",
      "Epoch: 10652 \tTraining Loss: 1.940282 \tValidation Loss: 2.309276\n",
      "Epoch: 10653 \tTraining Loss: 1.942927 \tValidation Loss: 2.309272\n",
      "Epoch: 10654 \tTraining Loss: 1.920554 \tValidation Loss: 2.309495\n",
      "Epoch: 10655 \tTraining Loss: 1.940631 \tValidation Loss: 2.309314\n",
      "Epoch: 10656 \tTraining Loss: 1.922009 \tValidation Loss: 2.309294\n",
      "Epoch: 10657 \tTraining Loss: 1.911201 \tValidation Loss: 2.309203\n",
      "Epoch: 10658 \tTraining Loss: 1.925750 \tValidation Loss: 2.309337\n",
      "Epoch: 10659 \tTraining Loss: 1.896435 \tValidation Loss: 2.309056\n",
      "Epoch: 10660 \tTraining Loss: 1.935108 \tValidation Loss: 2.309321\n",
      "Epoch: 10661 \tTraining Loss: 1.900549 \tValidation Loss: 2.309045\n",
      "Epoch: 10662 \tTraining Loss: 1.907845 \tValidation Loss: 2.309419\n",
      "Epoch: 10663 \tTraining Loss: 1.939029 \tValidation Loss: 2.309736\n",
      "Epoch: 10664 \tTraining Loss: 1.923520 \tValidation Loss: 2.309388\n",
      "Epoch: 10665 \tTraining Loss: 1.936126 \tValidation Loss: 2.309262\n",
      "Epoch: 10666 \tTraining Loss: 1.918283 \tValidation Loss: 2.309388\n",
      "Epoch: 10667 \tTraining Loss: 1.895205 \tValidation Loss: 2.309445\n",
      "Epoch: 10668 \tTraining Loss: 1.894772 \tValidation Loss: 2.309251\n",
      "Epoch: 10669 \tTraining Loss: 1.937160 \tValidation Loss: 2.308951\n",
      "Epoch: 10670 \tTraining Loss: 1.929592 \tValidation Loss: 2.309084\n",
      "Epoch: 10671 \tTraining Loss: 1.907803 \tValidation Loss: 2.308884\n",
      "Epoch: 10672 \tTraining Loss: 1.941652 \tValidation Loss: 2.308765\n",
      "Epoch: 10673 \tTraining Loss: 1.951498 \tValidation Loss: 2.309143\n",
      "Epoch: 10674 \tTraining Loss: 1.904343 \tValidation Loss: 2.309147\n",
      "Epoch: 10675 \tTraining Loss: 1.895168 \tValidation Loss: 2.309475\n",
      "Epoch: 10676 \tTraining Loss: 1.960844 \tValidation Loss: 2.309154\n",
      "Epoch: 10677 \tTraining Loss: 1.918148 \tValidation Loss: 2.309335\n",
      "Epoch: 10678 \tTraining Loss: 1.933625 \tValidation Loss: 2.309511\n",
      "Epoch: 10679 \tTraining Loss: 1.914437 \tValidation Loss: 2.309793\n",
      "Epoch: 10680 \tTraining Loss: 1.931401 \tValidation Loss: 2.309495\n",
      "Epoch: 10681 \tTraining Loss: 1.910477 \tValidation Loss: 2.310096\n",
      "Epoch: 10682 \tTraining Loss: 1.948090 \tValidation Loss: 2.309613\n",
      "Epoch: 10683 \tTraining Loss: 1.904512 \tValidation Loss: 2.309372\n",
      "Epoch: 10684 \tTraining Loss: 1.925276 \tValidation Loss: 2.309291\n",
      "Epoch: 10685 \tTraining Loss: 1.915922 \tValidation Loss: 2.309428\n",
      "Epoch: 10686 \tTraining Loss: 1.926342 \tValidation Loss: 2.309429\n",
      "Epoch: 10687 \tTraining Loss: 1.922512 \tValidation Loss: 2.309639\n",
      "Epoch: 10688 \tTraining Loss: 1.930122 \tValidation Loss: 2.309770\n",
      "Epoch: 10689 \tTraining Loss: 1.913297 \tValidation Loss: 2.309404\n",
      "Epoch: 10690 \tTraining Loss: 1.955377 \tValidation Loss: 2.309115\n",
      "Epoch: 10691 \tTraining Loss: 1.918898 \tValidation Loss: 2.309025\n",
      "Epoch: 10692 \tTraining Loss: 1.919311 \tValidation Loss: 2.309147\n",
      "Epoch: 10693 \tTraining Loss: 1.941555 \tValidation Loss: 2.309056\n",
      "Epoch: 10694 \tTraining Loss: 1.916298 \tValidation Loss: 2.309119\n",
      "Epoch: 10695 \tTraining Loss: 1.920518 \tValidation Loss: 2.309507\n",
      "Epoch: 10696 \tTraining Loss: 1.940928 \tValidation Loss: 2.309225\n",
      "Epoch: 10697 \tTraining Loss: 1.905404 \tValidation Loss: 2.309316\n",
      "Epoch: 10698 \tTraining Loss: 1.934011 \tValidation Loss: 2.309260\n",
      "Epoch: 10699 \tTraining Loss: 1.937953 \tValidation Loss: 2.309112\n",
      "Epoch: 10700 \tTraining Loss: 1.921175 \tValidation Loss: 2.309127\n",
      "Epoch: 10701 \tTraining Loss: 1.920800 \tValidation Loss: 2.309385\n",
      "Epoch: 10702 \tTraining Loss: 1.938600 \tValidation Loss: 2.309349\n",
      "Epoch: 10703 \tTraining Loss: 1.933713 \tValidation Loss: 2.309279\n",
      "Epoch: 10704 \tTraining Loss: 1.930015 \tValidation Loss: 2.309004\n",
      "Epoch: 10705 \tTraining Loss: 1.938220 \tValidation Loss: 2.309206\n",
      "Epoch: 10706 \tTraining Loss: 1.916624 \tValidation Loss: 2.309427\n",
      "Epoch: 10707 \tTraining Loss: 1.893520 \tValidation Loss: 2.309376\n",
      "Epoch: 10708 \tTraining Loss: 1.929804 \tValidation Loss: 2.309285\n",
      "Epoch: 10709 \tTraining Loss: 1.982857 \tValidation Loss: 2.309167\n",
      "Epoch: 10710 \tTraining Loss: 1.902915 \tValidation Loss: 2.309501\n",
      "Epoch: 10711 \tTraining Loss: 1.933015 \tValidation Loss: 2.309547\n",
      "Epoch: 10712 \tTraining Loss: 1.949207 \tValidation Loss: 2.309353\n",
      "Epoch: 10713 \tTraining Loss: 1.893273 \tValidation Loss: 2.309143\n",
      "Epoch: 10714 \tTraining Loss: 1.955476 \tValidation Loss: 2.309013\n",
      "Epoch: 10715 \tTraining Loss: 1.915006 \tValidation Loss: 2.308915\n",
      "Epoch: 10716 \tTraining Loss: 1.937846 \tValidation Loss: 2.309116\n",
      "Epoch: 10717 \tTraining Loss: 1.910827 \tValidation Loss: 2.309101\n",
      "Epoch: 10718 \tTraining Loss: 1.935120 \tValidation Loss: 2.308933\n",
      "Epoch: 10719 \tTraining Loss: 1.908496 \tValidation Loss: 2.308856\n",
      "Epoch: 10720 \tTraining Loss: 1.935921 \tValidation Loss: 2.308940\n",
      "Epoch: 10721 \tTraining Loss: 1.935216 \tValidation Loss: 2.309257\n",
      "Epoch: 10722 \tTraining Loss: 1.937731 \tValidation Loss: 2.309100\n",
      "Epoch: 10723 \tTraining Loss: 1.930300 \tValidation Loss: 2.308887\n",
      "Epoch: 10724 \tTraining Loss: 1.938378 \tValidation Loss: 2.308972\n",
      "Epoch: 10725 \tTraining Loss: 1.911238 \tValidation Loss: 2.309309\n",
      "Epoch: 10726 \tTraining Loss: 1.933095 \tValidation Loss: 2.309099\n",
      "Epoch: 10727 \tTraining Loss: 1.901618 \tValidation Loss: 2.309479\n",
      "Epoch: 10728 \tTraining Loss: 1.959495 \tValidation Loss: 2.309425\n",
      "Epoch: 10729 \tTraining Loss: 1.915843 \tValidation Loss: 2.309348\n",
      "Epoch: 10730 \tTraining Loss: 1.912973 \tValidation Loss: 2.309340\n",
      "Epoch: 10731 \tTraining Loss: 1.935178 \tValidation Loss: 2.309271\n",
      "Epoch: 10732 \tTraining Loss: 1.913304 \tValidation Loss: 2.309261\n",
      "Epoch: 10733 \tTraining Loss: 1.904267 \tValidation Loss: 2.309198\n",
      "Epoch: 10734 \tTraining Loss: 1.895951 \tValidation Loss: 2.309201\n",
      "Epoch: 10735 \tTraining Loss: 1.914814 \tValidation Loss: 2.309467\n",
      "Epoch: 10736 \tTraining Loss: 1.914215 \tValidation Loss: 2.309533\n",
      "Epoch: 10737 \tTraining Loss: 1.935118 \tValidation Loss: 2.309068\n",
      "Epoch: 10738 \tTraining Loss: 1.921790 \tValidation Loss: 2.308887\n",
      "Epoch: 10739 \tTraining Loss: 1.929785 \tValidation Loss: 2.308904\n",
      "Epoch: 10740 \tTraining Loss: 1.906957 \tValidation Loss: 2.309120\n",
      "Epoch: 10741 \tTraining Loss: 1.919954 \tValidation Loss: 2.309141\n",
      "Epoch: 10742 \tTraining Loss: 1.922207 \tValidation Loss: 2.309283\n",
      "Epoch: 10743 \tTraining Loss: 1.898933 \tValidation Loss: 2.309153\n",
      "Epoch: 10744 \tTraining Loss: 1.916491 \tValidation Loss: 2.309101\n",
      "Epoch: 10745 \tTraining Loss: 1.923831 \tValidation Loss: 2.308909\n",
      "Epoch: 10746 \tTraining Loss: 1.944969 \tValidation Loss: 2.308857\n",
      "Epoch: 10747 \tTraining Loss: 1.902143 \tValidation Loss: 2.308923\n",
      "Epoch: 10748 \tTraining Loss: 1.927404 \tValidation Loss: 2.309216\n",
      "Epoch: 10749 \tTraining Loss: 1.934444 \tValidation Loss: 2.309004\n",
      "Epoch: 10750 \tTraining Loss: 1.909150 \tValidation Loss: 2.308901\n",
      "Epoch: 10751 \tTraining Loss: 1.911500 \tValidation Loss: 2.309011\n",
      "Epoch: 10752 \tTraining Loss: 1.918882 \tValidation Loss: 2.309253\n",
      "Epoch: 10753 \tTraining Loss: 1.909630 \tValidation Loss: 2.309067\n",
      "Epoch: 10754 \tTraining Loss: 1.944058 \tValidation Loss: 2.309179\n",
      "Epoch: 10755 \tTraining Loss: 1.944633 \tValidation Loss: 2.309321\n",
      "Epoch: 10756 \tTraining Loss: 1.930583 \tValidation Loss: 2.309432\n",
      "Epoch: 10757 \tTraining Loss: 1.929837 \tValidation Loss: 2.309250\n",
      "Epoch: 10758 \tTraining Loss: 1.890188 \tValidation Loss: 2.309323\n",
      "Epoch: 10759 \tTraining Loss: 1.931830 \tValidation Loss: 2.309601\n",
      "Epoch: 10760 \tTraining Loss: 1.903856 \tValidation Loss: 2.309764\n",
      "Epoch: 10761 \tTraining Loss: 1.921389 \tValidation Loss: 2.309407\n",
      "Epoch: 10762 \tTraining Loss: 1.927727 \tValidation Loss: 2.309387\n",
      "Epoch: 10763 \tTraining Loss: 1.935205 \tValidation Loss: 2.309491\n",
      "Epoch: 10764 \tTraining Loss: 1.926447 \tValidation Loss: 2.309503\n",
      "Epoch: 10765 \tTraining Loss: 1.932391 \tValidation Loss: 2.308696\n",
      "Epoch: 10766 \tTraining Loss: 1.941275 \tValidation Loss: 2.309027\n",
      "Epoch: 10767 \tTraining Loss: 1.910812 \tValidation Loss: 2.309024\n",
      "Epoch: 10768 \tTraining Loss: 1.940738 \tValidation Loss: 2.309095\n",
      "Epoch: 10769 \tTraining Loss: 1.900131 \tValidation Loss: 2.309610\n",
      "Epoch: 10770 \tTraining Loss: 1.921270 \tValidation Loss: 2.309893\n",
      "Epoch: 10771 \tTraining Loss: 1.928852 \tValidation Loss: 2.309738\n",
      "Epoch: 10772 \tTraining Loss: 1.944638 \tValidation Loss: 2.309853\n",
      "Epoch: 10773 \tTraining Loss: 1.929238 \tValidation Loss: 2.309652\n",
      "Epoch: 10774 \tTraining Loss: 1.931636 \tValidation Loss: 2.309463\n",
      "Epoch: 10775 \tTraining Loss: 1.934947 \tValidation Loss: 2.310006\n",
      "Epoch: 10776 \tTraining Loss: 1.906078 \tValidation Loss: 2.309912\n",
      "Epoch: 10777 \tTraining Loss: 1.948563 \tValidation Loss: 2.309921\n",
      "Epoch: 10778 \tTraining Loss: 1.917092 \tValidation Loss: 2.309993\n",
      "Epoch: 10779 \tTraining Loss: 1.964761 \tValidation Loss: 2.309314\n",
      "Epoch: 10780 \tTraining Loss: 1.904669 \tValidation Loss: 2.309695\n",
      "Epoch: 10781 \tTraining Loss: 1.942760 \tValidation Loss: 2.309968\n",
      "Epoch: 10782 \tTraining Loss: 1.903507 \tValidation Loss: 2.309838\n",
      "Epoch: 10783 \tTraining Loss: 1.937698 \tValidation Loss: 2.309870\n",
      "Epoch: 10784 \tTraining Loss: 1.907873 \tValidation Loss: 2.309673\n",
      "Epoch: 10785 \tTraining Loss: 1.889688 \tValidation Loss: 2.309709\n",
      "Epoch: 10786 \tTraining Loss: 1.927518 \tValidation Loss: 2.309694\n",
      "Epoch: 10787 \tTraining Loss: 1.956156 \tValidation Loss: 2.309756\n",
      "Epoch: 10788 \tTraining Loss: 1.932974 \tValidation Loss: 2.309618\n",
      "Epoch: 10789 \tTraining Loss: 1.954502 \tValidation Loss: 2.309740\n",
      "Epoch: 10790 \tTraining Loss: 1.914115 \tValidation Loss: 2.309777\n",
      "Epoch: 10791 \tTraining Loss: 1.917657 \tValidation Loss: 2.309751\n",
      "Epoch: 10792 \tTraining Loss: 1.936576 \tValidation Loss: 2.309733\n",
      "Epoch: 10793 \tTraining Loss: 1.933348 \tValidation Loss: 2.309628\n",
      "Epoch: 10794 \tTraining Loss: 1.904360 \tValidation Loss: 2.309525\n",
      "Epoch: 10795 \tTraining Loss: 1.925959 \tValidation Loss: 2.309459\n",
      "Epoch: 10796 \tTraining Loss: 1.923140 \tValidation Loss: 2.309667\n",
      "Epoch: 10797 \tTraining Loss: 1.911584 \tValidation Loss: 2.309569\n",
      "Epoch: 10798 \tTraining Loss: 1.915784 \tValidation Loss: 2.309463\n",
      "Epoch: 10799 \tTraining Loss: 1.903772 \tValidation Loss: 2.309501\n",
      "Epoch: 10800 \tTraining Loss: 1.911786 \tValidation Loss: 2.309494\n",
      "Epoch: 10801 \tTraining Loss: 1.931194 \tValidation Loss: 2.309617\n",
      "Epoch: 10802 \tTraining Loss: 1.888364 \tValidation Loss: 2.309844\n",
      "Epoch: 10803 \tTraining Loss: 1.915805 \tValidation Loss: 2.309479\n",
      "Epoch: 10804 \tTraining Loss: 1.928978 \tValidation Loss: 2.309369\n",
      "Epoch: 10805 \tTraining Loss: 1.914776 \tValidation Loss: 2.309382\n",
      "Epoch: 10806 \tTraining Loss: 1.921635 \tValidation Loss: 2.309472\n",
      "Epoch: 10807 \tTraining Loss: 1.948003 \tValidation Loss: 2.309605\n",
      "Epoch: 10808 \tTraining Loss: 1.925076 \tValidation Loss: 2.309356\n",
      "Epoch: 10809 \tTraining Loss: 1.923298 \tValidation Loss: 2.309468\n",
      "Epoch: 10810 \tTraining Loss: 1.934996 \tValidation Loss: 2.309228\n",
      "Epoch: 10811 \tTraining Loss: 1.934972 \tValidation Loss: 2.309225\n",
      "Epoch: 10812 \tTraining Loss: 1.915455 \tValidation Loss: 2.309560\n",
      "Epoch: 10813 \tTraining Loss: 1.916425 \tValidation Loss: 2.309728\n",
      "Epoch: 10814 \tTraining Loss: 1.917998 \tValidation Loss: 2.309727\n",
      "Epoch: 10815 \tTraining Loss: 1.920836 \tValidation Loss: 2.309587\n",
      "Epoch: 10816 \tTraining Loss: 1.921376 \tValidation Loss: 2.309356\n",
      "Epoch: 10817 \tTraining Loss: 1.904681 \tValidation Loss: 2.309369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10818 \tTraining Loss: 1.930878 \tValidation Loss: 2.309237\n",
      "Epoch: 10819 \tTraining Loss: 1.938331 \tValidation Loss: 2.309222\n",
      "Epoch: 10820 \tTraining Loss: 1.929515 \tValidation Loss: 2.309186\n",
      "Epoch: 10821 \tTraining Loss: 1.928812 \tValidation Loss: 2.309238\n",
      "Epoch: 10822 \tTraining Loss: 1.913531 \tValidation Loss: 2.309287\n",
      "Epoch: 10823 \tTraining Loss: 1.933447 \tValidation Loss: 2.309497\n",
      "Epoch: 10824 \tTraining Loss: 1.943506 \tValidation Loss: 2.309081\n",
      "Epoch: 10825 \tTraining Loss: 1.905275 \tValidation Loss: 2.309495\n",
      "Epoch: 10826 \tTraining Loss: 1.937539 \tValidation Loss: 2.309051\n",
      "Epoch: 10827 \tTraining Loss: 1.886680 \tValidation Loss: 2.309247\n",
      "Epoch: 10828 \tTraining Loss: 1.945030 \tValidation Loss: 2.309224\n",
      "Epoch: 10829 \tTraining Loss: 1.905589 \tValidation Loss: 2.309263\n",
      "Epoch: 10830 \tTraining Loss: 1.927813 \tValidation Loss: 2.309100\n",
      "Epoch: 10831 \tTraining Loss: 1.913412 \tValidation Loss: 2.309323\n",
      "Epoch: 10832 \tTraining Loss: 1.885804 \tValidation Loss: 2.309602\n",
      "Epoch: 10833 \tTraining Loss: 1.927899 \tValidation Loss: 2.309534\n",
      "Epoch: 10834 \tTraining Loss: 1.909870 \tValidation Loss: 2.309365\n",
      "Epoch: 10835 \tTraining Loss: 1.915688 \tValidation Loss: 2.309356\n",
      "Epoch: 10836 \tTraining Loss: 1.890310 \tValidation Loss: 2.309290\n",
      "Epoch: 10837 \tTraining Loss: 1.944077 \tValidation Loss: 2.309148\n",
      "Epoch: 10838 \tTraining Loss: 1.921978 \tValidation Loss: 2.309345\n",
      "Epoch: 10839 \tTraining Loss: 1.922721 \tValidation Loss: 2.309285\n",
      "Epoch: 10840 \tTraining Loss: 1.912861 \tValidation Loss: 2.309525\n",
      "Epoch: 10841 \tTraining Loss: 1.893539 \tValidation Loss: 2.310001\n",
      "Epoch: 10842 \tTraining Loss: 1.885248 \tValidation Loss: 2.309996\n",
      "Epoch: 10843 \tTraining Loss: 1.892543 \tValidation Loss: 2.309882\n",
      "Epoch: 10844 \tTraining Loss: 1.913881 \tValidation Loss: 2.309680\n",
      "Epoch: 10845 \tTraining Loss: 1.905668 \tValidation Loss: 2.309851\n",
      "Epoch: 10846 \tTraining Loss: 1.930139 \tValidation Loss: 2.309588\n",
      "Epoch: 10847 \tTraining Loss: 1.888072 \tValidation Loss: 2.309838\n",
      "Epoch: 10848 \tTraining Loss: 1.897273 \tValidation Loss: 2.309632\n",
      "Epoch: 10849 \tTraining Loss: 1.938543 \tValidation Loss: 2.309647\n",
      "Epoch: 10850 \tTraining Loss: 1.917857 \tValidation Loss: 2.309305\n",
      "Epoch: 10851 \tTraining Loss: 1.909395 \tValidation Loss: 2.309738\n",
      "Epoch: 10852 \tTraining Loss: 1.929982 \tValidation Loss: 2.310059\n",
      "Epoch: 10853 \tTraining Loss: 1.912422 \tValidation Loss: 2.310107\n",
      "Epoch: 10854 \tTraining Loss: 1.910174 \tValidation Loss: 2.310090\n",
      "Epoch: 10855 \tTraining Loss: 1.933974 \tValidation Loss: 2.310091\n",
      "Epoch: 10856 \tTraining Loss: 1.933037 \tValidation Loss: 2.310087\n",
      "Epoch: 10857 \tTraining Loss: 1.905913 \tValidation Loss: 2.310268\n",
      "Epoch: 10858 \tTraining Loss: 1.933697 \tValidation Loss: 2.310538\n",
      "Epoch: 10859 \tTraining Loss: 1.965953 \tValidation Loss: 2.310188\n",
      "Epoch: 10860 \tTraining Loss: 1.894761 \tValidation Loss: 2.310334\n",
      "Epoch: 10861 \tTraining Loss: 1.889953 \tValidation Loss: 2.310526\n",
      "Epoch: 10862 \tTraining Loss: 1.912575 \tValidation Loss: 2.310220\n",
      "Epoch: 10863 \tTraining Loss: 1.910770 \tValidation Loss: 2.310337\n",
      "Epoch: 10864 \tTraining Loss: 1.896643 \tValidation Loss: 2.310444\n",
      "Epoch: 10865 \tTraining Loss: 1.924421 \tValidation Loss: 2.309961\n",
      "Epoch: 10866 \tTraining Loss: 1.920390 \tValidation Loss: 2.309842\n",
      "Epoch: 10867 \tTraining Loss: 1.924035 \tValidation Loss: 2.309931\n",
      "Epoch: 10868 \tTraining Loss: 1.875670 \tValidation Loss: 2.309766\n",
      "Epoch: 10869 \tTraining Loss: 1.941788 \tValidation Loss: 2.309402\n",
      "Epoch: 10870 \tTraining Loss: 1.900788 \tValidation Loss: 2.309336\n",
      "Epoch: 10871 \tTraining Loss: 1.909545 \tValidation Loss: 2.309730\n",
      "Epoch: 10872 \tTraining Loss: 1.929845 \tValidation Loss: 2.310045\n",
      "Epoch: 10873 \tTraining Loss: 1.916833 \tValidation Loss: 2.309744\n",
      "Epoch: 10874 \tTraining Loss: 1.920950 \tValidation Loss: 2.309700\n",
      "Epoch: 10875 \tTraining Loss: 1.923271 \tValidation Loss: 2.309490\n",
      "Epoch: 10876 \tTraining Loss: 1.926206 \tValidation Loss: 2.309402\n",
      "Epoch: 10877 \tTraining Loss: 1.949745 \tValidation Loss: 2.309435\n",
      "Epoch: 10878 \tTraining Loss: 1.894358 \tValidation Loss: 2.309593\n",
      "Epoch: 10879 \tTraining Loss: 1.920622 \tValidation Loss: 2.309622\n",
      "Epoch: 10880 \tTraining Loss: 1.938177 \tValidation Loss: 2.309500\n",
      "Epoch: 10881 \tTraining Loss: 1.930404 \tValidation Loss: 2.309743\n",
      "Epoch: 10882 \tTraining Loss: 1.955721 \tValidation Loss: 2.309556\n",
      "Epoch: 10883 \tTraining Loss: 1.930799 \tValidation Loss: 2.309250\n",
      "Epoch: 10884 \tTraining Loss: 1.929182 \tValidation Loss: 2.309310\n",
      "Epoch: 10885 \tTraining Loss: 1.911187 \tValidation Loss: 2.309373\n",
      "Epoch: 10886 \tTraining Loss: 1.928483 \tValidation Loss: 2.309465\n",
      "Epoch: 10887 \tTraining Loss: 1.895722 \tValidation Loss: 2.309444\n",
      "Epoch: 10888 \tTraining Loss: 1.929639 \tValidation Loss: 2.309320\n",
      "Epoch: 10889 \tTraining Loss: 1.893510 \tValidation Loss: 2.309528\n",
      "Epoch: 10890 \tTraining Loss: 1.894866 \tValidation Loss: 2.309735\n",
      "Epoch: 10891 \tTraining Loss: 1.912540 \tValidation Loss: 2.309609\n",
      "Epoch: 10892 \tTraining Loss: 1.905247 \tValidation Loss: 2.309941\n",
      "Epoch: 10893 \tTraining Loss: 1.910090 \tValidation Loss: 2.310308\n",
      "Epoch: 10894 \tTraining Loss: 1.861451 \tValidation Loss: 2.309958\n",
      "Epoch: 10895 \tTraining Loss: 1.926813 \tValidation Loss: 2.309894\n",
      "Epoch: 10896 \tTraining Loss: 1.882634 \tValidation Loss: 2.309938\n",
      "Epoch: 10897 \tTraining Loss: 1.937625 \tValidation Loss: 2.310033\n",
      "Epoch: 10898 \tTraining Loss: 1.927787 \tValidation Loss: 2.309809\n",
      "Epoch: 10899 \tTraining Loss: 1.932120 \tValidation Loss: 2.310143\n",
      "Epoch: 10900 \tTraining Loss: 1.912315 \tValidation Loss: 2.309905\n",
      "Epoch: 10901 \tTraining Loss: 1.906356 \tValidation Loss: 2.310006\n",
      "Epoch: 10902 \tTraining Loss: 1.913347 \tValidation Loss: 2.309622\n",
      "Epoch: 10903 \tTraining Loss: 1.901020 \tValidation Loss: 2.309557\n",
      "Epoch: 10904 \tTraining Loss: 1.950310 \tValidation Loss: 2.309402\n",
      "Epoch: 10905 \tTraining Loss: 1.916815 \tValidation Loss: 2.309436\n",
      "Epoch: 10906 \tTraining Loss: 1.898772 \tValidation Loss: 2.309506\n",
      "Epoch: 10907 \tTraining Loss: 1.924824 \tValidation Loss: 2.309384\n",
      "Epoch: 10908 \tTraining Loss: 1.905722 \tValidation Loss: 2.309524\n",
      "Epoch: 10909 \tTraining Loss: 1.940128 \tValidation Loss: 2.309823\n",
      "Epoch: 10910 \tTraining Loss: 1.908347 \tValidation Loss: 2.309906\n",
      "Epoch: 10911 \tTraining Loss: 1.910775 \tValidation Loss: 2.309996\n",
      "Epoch: 10912 \tTraining Loss: 1.936490 \tValidation Loss: 2.309747\n",
      "Epoch: 10913 \tTraining Loss: 1.897878 \tValidation Loss: 2.309883\n",
      "Epoch: 10914 \tTraining Loss: 1.879112 \tValidation Loss: 2.310359\n",
      "Epoch: 10915 \tTraining Loss: 1.912686 \tValidation Loss: 2.310508\n",
      "Epoch: 10916 \tTraining Loss: 1.935659 \tValidation Loss: 2.310371\n",
      "Epoch: 10917 \tTraining Loss: 1.872772 \tValidation Loss: 2.310278\n",
      "Epoch: 10918 \tTraining Loss: 1.934607 \tValidation Loss: 2.310102\n",
      "Epoch: 10919 \tTraining Loss: 1.906749 \tValidation Loss: 2.310177\n",
      "Epoch: 10920 \tTraining Loss: 1.881225 \tValidation Loss: 2.309818\n",
      "Epoch: 10921 \tTraining Loss: 1.922330 \tValidation Loss: 2.309920\n",
      "Epoch: 10922 \tTraining Loss: 1.925621 \tValidation Loss: 2.310020\n",
      "Epoch: 10923 \tTraining Loss: 1.899256 \tValidation Loss: 2.310065\n",
      "Epoch: 10924 \tTraining Loss: 1.933246 \tValidation Loss: 2.309986\n",
      "Epoch: 10925 \tTraining Loss: 1.908680 \tValidation Loss: 2.309950\n",
      "Epoch: 10926 \tTraining Loss: 1.921660 \tValidation Loss: 2.310044\n",
      "Epoch: 10927 \tTraining Loss: 1.923917 \tValidation Loss: 2.310240\n",
      "Epoch: 10928 \tTraining Loss: 1.959291 \tValidation Loss: 2.309962\n",
      "Epoch: 10929 \tTraining Loss: 1.912711 \tValidation Loss: 2.310090\n",
      "Epoch: 10930 \tTraining Loss: 1.932645 \tValidation Loss: 2.310081\n",
      "Epoch: 10931 \tTraining Loss: 1.906583 \tValidation Loss: 2.310250\n",
      "Epoch: 10932 \tTraining Loss: 1.901933 \tValidation Loss: 2.310194\n",
      "Epoch: 10933 \tTraining Loss: 1.910960 \tValidation Loss: 2.310158\n",
      "Epoch: 10934 \tTraining Loss: 1.912760 \tValidation Loss: 2.310220\n",
      "Epoch: 10935 \tTraining Loss: 1.927138 \tValidation Loss: 2.310176\n",
      "Epoch: 10936 \tTraining Loss: 1.902306 \tValidation Loss: 2.310249\n",
      "Epoch: 10937 \tTraining Loss: 1.908844 \tValidation Loss: 2.310076\n",
      "Epoch: 10938 \tTraining Loss: 1.929297 \tValidation Loss: 2.309909\n",
      "Epoch: 10939 \tTraining Loss: 1.921622 \tValidation Loss: 2.309974\n",
      "Epoch: 10940 \tTraining Loss: 1.925767 \tValidation Loss: 2.310068\n",
      "Epoch: 10941 \tTraining Loss: 1.867985 \tValidation Loss: 2.310668\n",
      "Epoch: 10942 \tTraining Loss: 1.935857 \tValidation Loss: 2.310521\n",
      "Epoch: 10943 \tTraining Loss: 1.917761 \tValidation Loss: 2.310473\n",
      "Epoch: 10944 \tTraining Loss: 1.904527 \tValidation Loss: 2.310752\n",
      "Epoch: 10945 \tTraining Loss: 1.895414 \tValidation Loss: 2.310472\n",
      "Epoch: 10946 \tTraining Loss: 1.904554 \tValidation Loss: 2.310341\n",
      "Epoch: 10947 \tTraining Loss: 1.906556 \tValidation Loss: 2.310465\n",
      "Epoch: 10948 \tTraining Loss: 1.904373 \tValidation Loss: 2.310162\n",
      "Epoch: 10949 \tTraining Loss: 1.953672 \tValidation Loss: 2.310259\n",
      "Epoch: 10950 \tTraining Loss: 1.957757 \tValidation Loss: 2.310153\n",
      "Epoch: 10951 \tTraining Loss: 1.910299 \tValidation Loss: 2.309993\n",
      "Epoch: 10952 \tTraining Loss: 1.918357 \tValidation Loss: 2.309759\n",
      "Epoch: 10953 \tTraining Loss: 1.923417 \tValidation Loss: 2.309672\n",
      "Epoch: 10954 \tTraining Loss: 1.890540 \tValidation Loss: 2.309735\n",
      "Epoch: 10955 \tTraining Loss: 1.888824 \tValidation Loss: 2.309906\n",
      "Epoch: 10956 \tTraining Loss: 1.900494 \tValidation Loss: 2.310057\n",
      "Epoch: 10957 \tTraining Loss: 1.915608 \tValidation Loss: 2.309499\n",
      "Epoch: 10958 \tTraining Loss: 1.912910 \tValidation Loss: 2.309480\n",
      "Epoch: 10959 \tTraining Loss: 1.912049 \tValidation Loss: 2.309626\n",
      "Epoch: 10960 \tTraining Loss: 1.940003 \tValidation Loss: 2.309718\n",
      "Epoch: 10961 \tTraining Loss: 1.919345 \tValidation Loss: 2.309734\n",
      "Epoch: 10962 \tTraining Loss: 1.925509 \tValidation Loss: 2.309933\n",
      "Epoch: 10963 \tTraining Loss: 1.907700 \tValidation Loss: 2.310213\n",
      "Epoch: 10964 \tTraining Loss: 1.883040 \tValidation Loss: 2.309974\n",
      "Epoch: 10965 \tTraining Loss: 1.915148 \tValidation Loss: 2.309642\n",
      "Epoch: 10966 \tTraining Loss: 1.896314 \tValidation Loss: 2.309705\n",
      "Epoch: 10967 \tTraining Loss: 1.882455 \tValidation Loss: 2.309848\n",
      "Epoch: 10968 \tTraining Loss: 1.892865 \tValidation Loss: 2.309594\n",
      "Epoch: 10969 \tTraining Loss: 1.924790 \tValidation Loss: 2.309336\n",
      "Epoch: 10970 \tTraining Loss: 1.914041 \tValidation Loss: 2.309601\n",
      "Epoch: 10971 \tTraining Loss: 1.917720 \tValidation Loss: 2.309734\n",
      "Epoch: 10972 \tTraining Loss: 1.919447 \tValidation Loss: 2.309726\n",
      "Epoch: 10973 \tTraining Loss: 1.947656 \tValidation Loss: 2.309776\n",
      "Epoch: 10974 \tTraining Loss: 1.927567 \tValidation Loss: 2.310055\n",
      "Epoch: 10975 \tTraining Loss: 1.906897 \tValidation Loss: 2.309928\n",
      "Epoch: 10976 \tTraining Loss: 1.921458 \tValidation Loss: 2.309450\n",
      "Epoch: 10977 \tTraining Loss: 1.908259 \tValidation Loss: 2.309487\n",
      "Epoch: 10978 \tTraining Loss: 1.925918 \tValidation Loss: 2.309864\n",
      "Epoch: 10979 \tTraining Loss: 1.926085 \tValidation Loss: 2.309601\n",
      "Epoch: 10980 \tTraining Loss: 1.906856 \tValidation Loss: 2.309793\n",
      "Epoch: 10981 \tTraining Loss: 1.899244 \tValidation Loss: 2.309940\n",
      "Epoch: 10982 \tTraining Loss: 1.889567 \tValidation Loss: 2.310022\n",
      "Epoch: 10983 \tTraining Loss: 1.885636 \tValidation Loss: 2.310097\n",
      "Epoch: 10984 \tTraining Loss: 1.896261 \tValidation Loss: 2.310081\n",
      "Epoch: 10985 \tTraining Loss: 1.905823 \tValidation Loss: 2.309790\n",
      "Epoch: 10986 \tTraining Loss: 1.928668 \tValidation Loss: 2.309723\n",
      "Epoch: 10987 \tTraining Loss: 1.893863 \tValidation Loss: 2.309713\n",
      "Epoch: 10988 \tTraining Loss: 1.921251 \tValidation Loss: 2.309919\n",
      "Epoch: 10989 \tTraining Loss: 1.906602 \tValidation Loss: 2.309747\n",
      "Epoch: 10990 \tTraining Loss: 1.902323 \tValidation Loss: 2.310004\n",
      "Epoch: 10991 \tTraining Loss: 1.910705 \tValidation Loss: 2.310011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10992 \tTraining Loss: 1.917933 \tValidation Loss: 2.309831\n",
      "Epoch: 10993 \tTraining Loss: 1.926157 \tValidation Loss: 2.309836\n",
      "Epoch: 10994 \tTraining Loss: 1.897350 \tValidation Loss: 2.309795\n",
      "Epoch: 10995 \tTraining Loss: 1.909076 \tValidation Loss: 2.309829\n",
      "Epoch: 10996 \tTraining Loss: 1.911865 \tValidation Loss: 2.309714\n",
      "Epoch: 10997 \tTraining Loss: 1.953491 \tValidation Loss: 2.309701\n",
      "Epoch: 10998 \tTraining Loss: 1.958161 \tValidation Loss: 2.309878\n",
      "Epoch: 10999 \tTraining Loss: 1.886438 \tValidation Loss: 2.309763\n",
      "Epoch: 11000 \tTraining Loss: 1.916250 \tValidation Loss: 2.309750\n",
      "Epoch: 11001 \tTraining Loss: 1.908581 \tValidation Loss: 2.309686\n",
      "Epoch: 11002 \tTraining Loss: 1.917842 \tValidation Loss: 2.309966\n",
      "Epoch: 11003 \tTraining Loss: 1.897583 \tValidation Loss: 2.310162\n",
      "Epoch: 11004 \tTraining Loss: 1.909399 \tValidation Loss: 2.310190\n",
      "Epoch: 11005 \tTraining Loss: 1.883492 \tValidation Loss: 2.310210\n",
      "Epoch: 11006 \tTraining Loss: 1.905914 \tValidation Loss: 2.310116\n",
      "Epoch: 11007 \tTraining Loss: 1.909693 \tValidation Loss: 2.310250\n",
      "Epoch: 11008 \tTraining Loss: 1.918829 \tValidation Loss: 2.310022\n",
      "Epoch: 11009 \tTraining Loss: 1.879095 \tValidation Loss: 2.310196\n",
      "Epoch: 11010 \tTraining Loss: 1.952205 \tValidation Loss: 2.310228\n",
      "Epoch: 11011 \tTraining Loss: 1.891848 \tValidation Loss: 2.310049\n",
      "Epoch: 11012 \tTraining Loss: 1.936695 \tValidation Loss: 2.310139\n",
      "Epoch: 11013 \tTraining Loss: 1.922465 \tValidation Loss: 2.310228\n",
      "Epoch: 11014 \tTraining Loss: 1.944951 \tValidation Loss: 2.310148\n",
      "Epoch: 11015 \tTraining Loss: 1.921305 \tValidation Loss: 2.309910\n",
      "Epoch: 11016 \tTraining Loss: 1.906685 \tValidation Loss: 2.309922\n",
      "Epoch: 11017 \tTraining Loss: 1.901489 \tValidation Loss: 2.309998\n",
      "Epoch: 11018 \tTraining Loss: 1.933432 \tValidation Loss: 2.310252\n",
      "Epoch: 11019 \tTraining Loss: 1.896449 \tValidation Loss: 2.310330\n",
      "Epoch: 11020 \tTraining Loss: 1.933248 \tValidation Loss: 2.310503\n",
      "Epoch: 11021 \tTraining Loss: 1.907060 \tValidation Loss: 2.310515\n",
      "Epoch: 11022 \tTraining Loss: 1.910476 \tValidation Loss: 2.310400\n",
      "Epoch: 11023 \tTraining Loss: 1.893903 \tValidation Loss: 2.310591\n",
      "Epoch: 11024 \tTraining Loss: 1.906296 \tValidation Loss: 2.310339\n",
      "Epoch: 11025 \tTraining Loss: 1.890467 \tValidation Loss: 2.310781\n",
      "Epoch: 11026 \tTraining Loss: 1.935623 \tValidation Loss: 2.310796\n",
      "Epoch: 11027 \tTraining Loss: 1.920674 \tValidation Loss: 2.310562\n",
      "Epoch: 11028 \tTraining Loss: 1.925897 \tValidation Loss: 2.310489\n",
      "Epoch: 11029 \tTraining Loss: 1.930529 \tValidation Loss: 2.310155\n",
      "Epoch: 11030 \tTraining Loss: 1.896192 \tValidation Loss: 2.310106\n",
      "Epoch: 11031 \tTraining Loss: 1.895447 \tValidation Loss: 2.310559\n",
      "Epoch: 11032 \tTraining Loss: 1.875923 \tValidation Loss: 2.310381\n",
      "Epoch: 11033 \tTraining Loss: 1.926883 \tValidation Loss: 2.310337\n",
      "Epoch: 11034 \tTraining Loss: 1.895147 \tValidation Loss: 2.310020\n",
      "Epoch: 11035 \tTraining Loss: 1.909560 \tValidation Loss: 2.310100\n",
      "Epoch: 11036 \tTraining Loss: 1.919570 \tValidation Loss: 2.310399\n",
      "Epoch: 11037 \tTraining Loss: 1.884085 \tValidation Loss: 2.310358\n",
      "Epoch: 11038 \tTraining Loss: 1.902686 \tValidation Loss: 2.310138\n",
      "Epoch: 11039 \tTraining Loss: 1.896945 \tValidation Loss: 2.310283\n",
      "Epoch: 11040 \tTraining Loss: 1.891334 \tValidation Loss: 2.310158\n",
      "Epoch: 11041 \tTraining Loss: 1.907818 \tValidation Loss: 2.310200\n",
      "Epoch: 11042 \tTraining Loss: 1.918515 \tValidation Loss: 2.310034\n",
      "Epoch: 11043 \tTraining Loss: 1.922763 \tValidation Loss: 2.309992\n",
      "Epoch: 11044 \tTraining Loss: 1.885524 \tValidation Loss: 2.310065\n",
      "Epoch: 11045 \tTraining Loss: 1.931106 \tValidation Loss: 2.310190\n",
      "Epoch: 11046 \tTraining Loss: 1.942216 \tValidation Loss: 2.310306\n",
      "Epoch: 11047 \tTraining Loss: 1.920672 \tValidation Loss: 2.310312\n",
      "Epoch: 11048 \tTraining Loss: 1.935645 \tValidation Loss: 2.310206\n",
      "Epoch: 11049 \tTraining Loss: 1.928051 \tValidation Loss: 2.310059\n",
      "Epoch: 11050 \tTraining Loss: 1.939372 \tValidation Loss: 2.309883\n",
      "Epoch: 11051 \tTraining Loss: 1.894773 \tValidation Loss: 2.309904\n",
      "Epoch: 11052 \tTraining Loss: 1.913248 \tValidation Loss: 2.309984\n",
      "Epoch: 11053 \tTraining Loss: 1.923671 \tValidation Loss: 2.310144\n",
      "Epoch: 11054 \tTraining Loss: 1.925813 \tValidation Loss: 2.309984\n",
      "Epoch: 11055 \tTraining Loss: 1.896690 \tValidation Loss: 2.309846\n",
      "Epoch: 11056 \tTraining Loss: 1.875065 \tValidation Loss: 2.309587\n",
      "Epoch: 11057 \tTraining Loss: 1.927833 \tValidation Loss: 2.309863\n",
      "Epoch: 11058 \tTraining Loss: 1.917534 \tValidation Loss: 2.309625\n",
      "Epoch: 11059 \tTraining Loss: 1.873897 \tValidation Loss: 2.309887\n",
      "Epoch: 11060 \tTraining Loss: 1.890320 \tValidation Loss: 2.310066\n",
      "Epoch: 11061 \tTraining Loss: 1.892749 \tValidation Loss: 2.309983\n",
      "Epoch: 11062 \tTraining Loss: 1.894455 \tValidation Loss: 2.310109\n",
      "Epoch: 11063 \tTraining Loss: 1.901283 \tValidation Loss: 2.310225\n",
      "Epoch: 11064 \tTraining Loss: 1.935002 \tValidation Loss: 2.310222\n",
      "Epoch: 11065 \tTraining Loss: 1.889614 \tValidation Loss: 2.310160\n",
      "Epoch: 11066 \tTraining Loss: 1.903905 \tValidation Loss: 2.310375\n",
      "Epoch: 11067 \tTraining Loss: 1.899720 \tValidation Loss: 2.310499\n",
      "Epoch: 11068 \tTraining Loss: 1.907493 \tValidation Loss: 2.310343\n",
      "Epoch: 11069 \tTraining Loss: 1.945828 \tValidation Loss: 2.309995\n",
      "Epoch: 11070 \tTraining Loss: 1.901025 \tValidation Loss: 2.310087\n",
      "Epoch: 11071 \tTraining Loss: 1.928757 \tValidation Loss: 2.310209\n",
      "Epoch: 11072 \tTraining Loss: 1.907045 \tValidation Loss: 2.310210\n",
      "Epoch: 11073 \tTraining Loss: 1.915793 \tValidation Loss: 2.310505\n",
      "Epoch: 11074 \tTraining Loss: 1.887617 \tValidation Loss: 2.310713\n",
      "Epoch: 11075 \tTraining Loss: 1.908389 \tValidation Loss: 2.310772\n",
      "Epoch: 11076 \tTraining Loss: 1.930484 \tValidation Loss: 2.311035\n",
      "Epoch: 11077 \tTraining Loss: 1.899131 \tValidation Loss: 2.310754\n",
      "Epoch: 11078 \tTraining Loss: 1.910561 \tValidation Loss: 2.310789\n",
      "Epoch: 11079 \tTraining Loss: 1.922724 \tValidation Loss: 2.310868\n",
      "Epoch: 11080 \tTraining Loss: 1.892506 \tValidation Loss: 2.311008\n",
      "Epoch: 11081 \tTraining Loss: 1.901177 \tValidation Loss: 2.310759\n",
      "Epoch: 11082 \tTraining Loss: 1.937660 \tValidation Loss: 2.310637\n",
      "Epoch: 11083 \tTraining Loss: 1.947636 \tValidation Loss: 2.310672\n",
      "Epoch: 11084 \tTraining Loss: 1.933359 \tValidation Loss: 2.310598\n",
      "Epoch: 11085 \tTraining Loss: 1.944356 \tValidation Loss: 2.310860\n",
      "Epoch: 11086 \tTraining Loss: 1.891841 \tValidation Loss: 2.310884\n",
      "Epoch: 11087 \tTraining Loss: 1.907725 \tValidation Loss: 2.310739\n",
      "Epoch: 11088 \tTraining Loss: 1.874359 \tValidation Loss: 2.310755\n",
      "Epoch: 11089 \tTraining Loss: 1.933313 \tValidation Loss: 2.310454\n",
      "Epoch: 11090 \tTraining Loss: 1.939466 \tValidation Loss: 2.310241\n",
      "Epoch: 11091 \tTraining Loss: 1.896054 \tValidation Loss: 2.310351\n",
      "Epoch: 11092 \tTraining Loss: 1.888072 \tValidation Loss: 2.310511\n",
      "Epoch: 11093 \tTraining Loss: 1.895100 \tValidation Loss: 2.310846\n",
      "Epoch: 11094 \tTraining Loss: 1.855211 \tValidation Loss: 2.310842\n",
      "Epoch: 11095 \tTraining Loss: 1.887198 \tValidation Loss: 2.310868\n",
      "Epoch: 11096 \tTraining Loss: 1.904748 \tValidation Loss: 2.310598\n",
      "Epoch: 11097 \tTraining Loss: 1.911600 \tValidation Loss: 2.310595\n",
      "Epoch: 11098 \tTraining Loss: 1.919198 \tValidation Loss: 2.310447\n",
      "Epoch: 11099 \tTraining Loss: 1.885172 \tValidation Loss: 2.310586\n",
      "Epoch: 11100 \tTraining Loss: 1.909486 \tValidation Loss: 2.310663\n",
      "Epoch: 11101 \tTraining Loss: 1.918914 \tValidation Loss: 2.310388\n",
      "Epoch: 11102 \tTraining Loss: 1.939705 \tValidation Loss: 2.310350\n",
      "Epoch: 11103 \tTraining Loss: 1.900812 \tValidation Loss: 2.310325\n",
      "Epoch: 11104 \tTraining Loss: 1.880964 \tValidation Loss: 2.310121\n",
      "Epoch: 11105 \tTraining Loss: 1.908442 \tValidation Loss: 2.309990\n",
      "Epoch: 11106 \tTraining Loss: 1.929293 \tValidation Loss: 2.310132\n",
      "Epoch: 11107 \tTraining Loss: 1.875025 \tValidation Loss: 2.310032\n",
      "Epoch: 11108 \tTraining Loss: 1.924840 \tValidation Loss: 2.310150\n",
      "Epoch: 11109 \tTraining Loss: 1.884665 \tValidation Loss: 2.310071\n",
      "Epoch: 11110 \tTraining Loss: 1.894434 \tValidation Loss: 2.310266\n",
      "Epoch: 11111 \tTraining Loss: 1.891901 \tValidation Loss: 2.310384\n",
      "Epoch: 11112 \tTraining Loss: 1.916617 \tValidation Loss: 2.310506\n",
      "Epoch: 11113 \tTraining Loss: 1.912635 \tValidation Loss: 2.310585\n",
      "Epoch: 11114 \tTraining Loss: 1.925190 \tValidation Loss: 2.310501\n",
      "Epoch: 11115 \tTraining Loss: 1.887160 \tValidation Loss: 2.310470\n",
      "Epoch: 11116 \tTraining Loss: 1.936439 \tValidation Loss: 2.310440\n",
      "Epoch: 11117 \tTraining Loss: 1.887146 \tValidation Loss: 2.310305\n",
      "Epoch: 11118 \tTraining Loss: 1.909822 \tValidation Loss: 2.310367\n",
      "Epoch: 11119 \tTraining Loss: 1.902069 \tValidation Loss: 2.310254\n",
      "Epoch: 11120 \tTraining Loss: 1.904344 \tValidation Loss: 2.310378\n",
      "Epoch: 11121 \tTraining Loss: 1.880201 \tValidation Loss: 2.310443\n",
      "Epoch: 11122 \tTraining Loss: 1.877173 \tValidation Loss: 2.310512\n",
      "Epoch: 11123 \tTraining Loss: 1.925974 \tValidation Loss: 2.310368\n",
      "Epoch: 11124 \tTraining Loss: 1.900075 \tValidation Loss: 2.310502\n",
      "Epoch: 11125 \tTraining Loss: 1.901045 \tValidation Loss: 2.310476\n",
      "Epoch: 11126 \tTraining Loss: 1.891849 \tValidation Loss: 2.310089\n",
      "Epoch: 11127 \tTraining Loss: 1.906993 \tValidation Loss: 2.310005\n",
      "Epoch: 11128 \tTraining Loss: 1.936024 \tValidation Loss: 2.310204\n",
      "Epoch: 11129 \tTraining Loss: 1.886075 \tValidation Loss: 2.310531\n",
      "Epoch: 11130 \tTraining Loss: 1.907493 \tValidation Loss: 2.310551\n",
      "Epoch: 11131 \tTraining Loss: 1.894605 \tValidation Loss: 2.310590\n",
      "Epoch: 11132 \tTraining Loss: 1.898186 \tValidation Loss: 2.310378\n",
      "Epoch: 11133 \tTraining Loss: 1.891353 \tValidation Loss: 2.310424\n",
      "Epoch: 11134 \tTraining Loss: 1.897940 \tValidation Loss: 2.310296\n",
      "Epoch: 11135 \tTraining Loss: 1.909610 \tValidation Loss: 2.310181\n",
      "Epoch: 11136 \tTraining Loss: 1.916959 \tValidation Loss: 2.310493\n",
      "Epoch: 11137 \tTraining Loss: 1.882093 \tValidation Loss: 2.310429\n",
      "Epoch: 11138 \tTraining Loss: 1.917112 \tValidation Loss: 2.310179\n",
      "Epoch: 11139 \tTraining Loss: 1.917458 \tValidation Loss: 2.310552\n",
      "Epoch: 11140 \tTraining Loss: 1.921049 \tValidation Loss: 2.310296\n",
      "Epoch: 11141 \tTraining Loss: 1.907919 \tValidation Loss: 2.310004\n",
      "Epoch: 11142 \tTraining Loss: 1.919290 \tValidation Loss: 2.310325\n",
      "Epoch: 11143 \tTraining Loss: 1.905837 \tValidation Loss: 2.310349\n",
      "Epoch: 11144 \tTraining Loss: 1.916996 \tValidation Loss: 2.310936\n",
      "Epoch: 11145 \tTraining Loss: 1.912597 \tValidation Loss: 2.310263\n",
      "Epoch: 11146 \tTraining Loss: 1.913326 \tValidation Loss: 2.310273\n",
      "Epoch: 11147 \tTraining Loss: 1.916704 \tValidation Loss: 2.310433\n",
      "Epoch: 11148 \tTraining Loss: 1.923414 \tValidation Loss: 2.310884\n",
      "Epoch: 11149 \tTraining Loss: 1.879609 \tValidation Loss: 2.311149\n",
      "Epoch: 11150 \tTraining Loss: 1.934966 \tValidation Loss: 2.311045\n",
      "Epoch: 11151 \tTraining Loss: 1.889992 \tValidation Loss: 2.310831\n",
      "Epoch: 11152 \tTraining Loss: 1.928664 \tValidation Loss: 2.310713\n",
      "Epoch: 11153 \tTraining Loss: 1.888199 \tValidation Loss: 2.310856\n",
      "Epoch: 11154 \tTraining Loss: 1.869514 \tValidation Loss: 2.310903\n",
      "Epoch: 11155 \tTraining Loss: 1.893876 \tValidation Loss: 2.310623\n",
      "Epoch: 11156 \tTraining Loss: 1.892442 \tValidation Loss: 2.310682\n",
      "Epoch: 11157 \tTraining Loss: 1.905963 \tValidation Loss: 2.310763\n",
      "Epoch: 11158 \tTraining Loss: 1.920513 \tValidation Loss: 2.310816\n",
      "Epoch: 11159 \tTraining Loss: 1.883493 \tValidation Loss: 2.311066\n",
      "Epoch: 11160 \tTraining Loss: 1.869262 \tValidation Loss: 2.311320\n",
      "Epoch: 11161 \tTraining Loss: 1.890755 \tValidation Loss: 2.311330\n",
      "Epoch: 11162 \tTraining Loss: 1.907929 \tValidation Loss: 2.311274\n",
      "Epoch: 11163 \tTraining Loss: 1.889979 \tValidation Loss: 2.311263\n",
      "Epoch: 11164 \tTraining Loss: 1.921320 \tValidation Loss: 2.311163\n",
      "Epoch: 11165 \tTraining Loss: 1.908462 \tValidation Loss: 2.311051\n",
      "Epoch: 11166 \tTraining Loss: 1.930140 \tValidation Loss: 2.311166\n",
      "Epoch: 11167 \tTraining Loss: 1.889136 \tValidation Loss: 2.311028\n",
      "Epoch: 11168 \tTraining Loss: 1.905447 \tValidation Loss: 2.311303\n",
      "Epoch: 11169 \tTraining Loss: 1.894382 \tValidation Loss: 2.311085\n",
      "Epoch: 11170 \tTraining Loss: 1.906326 \tValidation Loss: 2.310679\n",
      "Epoch: 11171 \tTraining Loss: 1.906181 \tValidation Loss: 2.310815\n",
      "Epoch: 11172 \tTraining Loss: 1.961404 \tValidation Loss: 2.310570\n",
      "Epoch: 11173 \tTraining Loss: 1.906485 \tValidation Loss: 2.310896\n",
      "Epoch: 11174 \tTraining Loss: 1.911207 \tValidation Loss: 2.310793\n",
      "Epoch: 11175 \tTraining Loss: 1.869257 \tValidation Loss: 2.310879\n",
      "Epoch: 11176 \tTraining Loss: 1.871334 \tValidation Loss: 2.310927\n",
      "Epoch: 11177 \tTraining Loss: 1.893970 \tValidation Loss: 2.310985\n",
      "Epoch: 11178 \tTraining Loss: 1.902032 \tValidation Loss: 2.310960\n",
      "Epoch: 11179 \tTraining Loss: 1.898300 \tValidation Loss: 2.311061\n",
      "Epoch: 11180 \tTraining Loss: 1.893082 \tValidation Loss: 2.311152\n",
      "Epoch: 11181 \tTraining Loss: 1.898393 \tValidation Loss: 2.311183\n",
      "Epoch: 11182 \tTraining Loss: 1.895483 \tValidation Loss: 2.311053\n",
      "Epoch: 11183 \tTraining Loss: 1.901108 \tValidation Loss: 2.310704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11184 \tTraining Loss: 1.904796 \tValidation Loss: 2.310843\n",
      "Epoch: 11185 \tTraining Loss: 1.882709 \tValidation Loss: 2.311359\n",
      "Epoch: 11186 \tTraining Loss: 1.890902 \tValidation Loss: 2.311074\n",
      "Epoch: 11187 \tTraining Loss: 1.880967 \tValidation Loss: 2.310945\n",
      "Epoch: 11188 \tTraining Loss: 1.928152 \tValidation Loss: 2.311250\n",
      "Epoch: 11189 \tTraining Loss: 1.880348 \tValidation Loss: 2.311459\n",
      "Epoch: 11190 \tTraining Loss: 1.892651 \tValidation Loss: 2.310948\n",
      "Epoch: 11191 \tTraining Loss: 1.930393 \tValidation Loss: 2.310917\n",
      "Epoch: 11192 \tTraining Loss: 1.896784 \tValidation Loss: 2.311137\n",
      "Epoch: 11193 \tTraining Loss: 1.914886 \tValidation Loss: 2.311253\n",
      "Epoch: 11194 \tTraining Loss: 1.875597 \tValidation Loss: 2.311384\n",
      "Epoch: 11195 \tTraining Loss: 1.893215 \tValidation Loss: 2.311475\n",
      "Epoch: 11196 \tTraining Loss: 1.924067 \tValidation Loss: 2.311816\n",
      "Epoch: 11197 \tTraining Loss: 1.920533 \tValidation Loss: 2.311116\n",
      "Epoch: 11198 \tTraining Loss: 1.900123 \tValidation Loss: 2.311321\n",
      "Epoch: 11199 \tTraining Loss: 1.893050 \tValidation Loss: 2.311186\n",
      "Epoch: 11200 \tTraining Loss: 1.908480 \tValidation Loss: 2.311165\n",
      "Epoch: 11201 \tTraining Loss: 1.880525 \tValidation Loss: 2.310783\n",
      "Epoch: 11202 \tTraining Loss: 1.932182 \tValidation Loss: 2.310928\n",
      "Epoch: 11203 \tTraining Loss: 1.924280 \tValidation Loss: 2.311331\n",
      "Epoch: 11204 \tTraining Loss: 1.934816 \tValidation Loss: 2.311059\n",
      "Epoch: 11205 \tTraining Loss: 1.863896 \tValidation Loss: 2.310991\n",
      "Epoch: 11206 \tTraining Loss: 1.909965 \tValidation Loss: 2.310724\n",
      "Epoch: 11207 \tTraining Loss: 1.905348 \tValidation Loss: 2.310602\n",
      "Epoch: 11208 \tTraining Loss: 1.896157 \tValidation Loss: 2.310850\n",
      "Epoch: 11209 \tTraining Loss: 1.901701 \tValidation Loss: 2.310627\n",
      "Epoch: 11210 \tTraining Loss: 1.869795 \tValidation Loss: 2.310670\n",
      "Epoch: 11211 \tTraining Loss: 1.897905 \tValidation Loss: 2.310774\n",
      "Epoch: 11212 \tTraining Loss: 1.901600 \tValidation Loss: 2.310590\n",
      "Epoch: 11213 \tTraining Loss: 1.905053 \tValidation Loss: 2.310576\n",
      "Epoch: 11214 \tTraining Loss: 1.907979 \tValidation Loss: 2.310898\n",
      "Epoch: 11215 \tTraining Loss: 1.913079 \tValidation Loss: 2.310769\n",
      "Epoch: 11216 \tTraining Loss: 1.938233 \tValidation Loss: 2.310758\n",
      "Epoch: 11217 \tTraining Loss: 1.899657 \tValidation Loss: 2.310818\n",
      "Epoch: 11218 \tTraining Loss: 1.899669 \tValidation Loss: 2.311278\n",
      "Epoch: 11219 \tTraining Loss: 1.891642 \tValidation Loss: 2.311278\n",
      "Epoch: 11220 \tTraining Loss: 1.913801 \tValidation Loss: 2.311276\n",
      "Epoch: 11221 \tTraining Loss: 1.912146 \tValidation Loss: 2.311136\n",
      "Epoch: 11222 \tTraining Loss: 1.908870 \tValidation Loss: 2.310959\n",
      "Epoch: 11223 \tTraining Loss: 1.887812 \tValidation Loss: 2.310739\n",
      "Epoch: 11224 \tTraining Loss: 1.906953 \tValidation Loss: 2.310730\n",
      "Epoch: 11225 \tTraining Loss: 1.895342 \tValidation Loss: 2.310657\n",
      "Epoch: 11226 \tTraining Loss: 1.890597 \tValidation Loss: 2.311155\n",
      "Epoch: 11227 \tTraining Loss: 1.890692 \tValidation Loss: 2.311113\n",
      "Epoch: 11228 \tTraining Loss: 1.886134 \tValidation Loss: 2.311289\n",
      "Epoch: 11229 \tTraining Loss: 1.922023 \tValidation Loss: 2.311457\n",
      "Epoch: 11230 \tTraining Loss: 1.895011 \tValidation Loss: 2.311372\n",
      "Epoch: 11231 \tTraining Loss: 1.898347 \tValidation Loss: 2.311498\n",
      "Epoch: 11232 \tTraining Loss: 1.880087 \tValidation Loss: 2.311249\n",
      "Epoch: 11233 \tTraining Loss: 1.865182 \tValidation Loss: 2.310993\n",
      "Epoch: 11234 \tTraining Loss: 1.903788 \tValidation Loss: 2.311090\n",
      "Epoch: 11235 \tTraining Loss: 1.937125 \tValidation Loss: 2.310998\n",
      "Epoch: 11236 \tTraining Loss: 1.927406 \tValidation Loss: 2.310918\n",
      "Epoch: 11237 \tTraining Loss: 1.859387 \tValidation Loss: 2.311113\n",
      "Epoch: 11238 \tTraining Loss: 1.883834 \tValidation Loss: 2.311044\n",
      "Epoch: 11239 \tTraining Loss: 1.917003 \tValidation Loss: 2.311241\n",
      "Epoch: 11240 \tTraining Loss: 1.906522 \tValidation Loss: 2.311257\n",
      "Epoch: 11241 \tTraining Loss: 1.898794 \tValidation Loss: 2.310805\n",
      "Epoch: 11242 \tTraining Loss: 1.895765 \tValidation Loss: 2.311142\n",
      "Epoch: 11243 \tTraining Loss: 1.882723 \tValidation Loss: 2.310524\n",
      "Epoch: 11244 \tTraining Loss: 1.907640 \tValidation Loss: 2.310365\n",
      "Epoch: 11245 \tTraining Loss: 1.893369 \tValidation Loss: 2.310429\n",
      "Epoch: 11246 \tTraining Loss: 1.885097 \tValidation Loss: 2.310342\n",
      "Epoch: 11247 \tTraining Loss: 1.900090 \tValidation Loss: 2.310705\n",
      "Epoch: 11248 \tTraining Loss: 1.906219 \tValidation Loss: 2.310945\n",
      "Epoch: 11249 \tTraining Loss: 1.875899 \tValidation Loss: 2.310989\n",
      "Epoch: 11250 \tTraining Loss: 1.906230 \tValidation Loss: 2.310177\n",
      "Epoch: 11251 \tTraining Loss: 1.898243 \tValidation Loss: 2.310333\n",
      "Epoch: 11252 \tTraining Loss: 1.886560 \tValidation Loss: 2.310801\n",
      "Epoch: 11253 \tTraining Loss: 1.912418 \tValidation Loss: 2.310754\n",
      "Epoch: 11254 \tTraining Loss: 1.898016 \tValidation Loss: 2.310804\n",
      "Epoch: 11255 \tTraining Loss: 1.892645 \tValidation Loss: 2.310858\n",
      "Epoch: 11256 \tTraining Loss: 1.896739 \tValidation Loss: 2.310755\n",
      "Epoch: 11257 \tTraining Loss: 1.924636 \tValidation Loss: 2.310773\n",
      "Epoch: 11258 \tTraining Loss: 1.918553 \tValidation Loss: 2.310733\n",
      "Epoch: 11259 \tTraining Loss: 1.943876 \tValidation Loss: 2.310654\n",
      "Epoch: 11260 \tTraining Loss: 1.903907 \tValidation Loss: 2.310562\n",
      "Epoch: 11261 \tTraining Loss: 1.892465 \tValidation Loss: 2.310865\n",
      "Epoch: 11262 \tTraining Loss: 1.868015 \tValidation Loss: 2.310959\n",
      "Epoch: 11263 \tTraining Loss: 1.911341 \tValidation Loss: 2.311265\n",
      "Epoch: 11264 \tTraining Loss: 1.871529 \tValidation Loss: 2.310894\n",
      "Epoch: 11265 \tTraining Loss: 1.939843 \tValidation Loss: 2.310764\n",
      "Epoch: 11266 \tTraining Loss: 1.909756 \tValidation Loss: 2.310856\n",
      "Epoch: 11267 \tTraining Loss: 1.884705 \tValidation Loss: 2.310580\n",
      "Epoch: 11268 \tTraining Loss: 1.907758 \tValidation Loss: 2.310560\n",
      "Epoch: 11269 \tTraining Loss: 1.871837 \tValidation Loss: 2.310911\n",
      "Epoch: 11270 \tTraining Loss: 1.873806 \tValidation Loss: 2.311347\n",
      "Epoch: 11271 \tTraining Loss: 1.877570 \tValidation Loss: 2.311298\n",
      "Epoch: 11272 \tTraining Loss: 1.942674 \tValidation Loss: 2.311226\n",
      "Epoch: 11273 \tTraining Loss: 1.908502 \tValidation Loss: 2.311050\n",
      "Epoch: 11274 \tTraining Loss: 1.913129 \tValidation Loss: 2.311189\n",
      "Epoch: 11275 \tTraining Loss: 1.910949 \tValidation Loss: 2.311261\n",
      "Epoch: 11276 \tTraining Loss: 1.884059 \tValidation Loss: 2.311374\n",
      "Epoch: 11277 \tTraining Loss: 1.923152 \tValidation Loss: 2.311052\n",
      "Epoch: 11278 \tTraining Loss: 1.899181 \tValidation Loss: 2.311127\n",
      "Epoch: 11279 \tTraining Loss: 1.910348 \tValidation Loss: 2.311314\n",
      "Epoch: 11280 \tTraining Loss: 1.926127 \tValidation Loss: 2.311189\n",
      "Epoch: 11281 \tTraining Loss: 1.920939 \tValidation Loss: 2.310953\n",
      "Epoch: 11282 \tTraining Loss: 1.882961 \tValidation Loss: 2.311362\n",
      "Epoch: 11283 \tTraining Loss: 1.864608 \tValidation Loss: 2.311353\n",
      "Epoch: 11284 \tTraining Loss: 1.913735 \tValidation Loss: 2.311086\n",
      "Epoch: 11285 \tTraining Loss: 1.874607 \tValidation Loss: 2.311088\n",
      "Epoch: 11286 \tTraining Loss: 1.903008 \tValidation Loss: 2.311502\n",
      "Epoch: 11287 \tTraining Loss: 1.937508 \tValidation Loss: 2.311178\n",
      "Epoch: 11288 \tTraining Loss: 1.884174 \tValidation Loss: 2.311045\n",
      "Epoch: 11289 \tTraining Loss: 1.872214 \tValidation Loss: 2.310743\n",
      "Epoch: 11290 \tTraining Loss: 1.888532 \tValidation Loss: 2.311211\n",
      "Epoch: 11291 \tTraining Loss: 1.935736 \tValidation Loss: 2.311101\n",
      "Epoch: 11292 \tTraining Loss: 1.914771 \tValidation Loss: 2.310812\n",
      "Epoch: 11293 \tTraining Loss: 1.933004 \tValidation Loss: 2.311011\n",
      "Epoch: 11294 \tTraining Loss: 1.906996 \tValidation Loss: 2.311139\n",
      "Epoch: 11295 \tTraining Loss: 1.893250 \tValidation Loss: 2.311301\n",
      "Epoch: 11296 \tTraining Loss: 1.899162 \tValidation Loss: 2.311262\n",
      "Epoch: 11297 \tTraining Loss: 1.892316 \tValidation Loss: 2.311163\n",
      "Epoch: 11298 \tTraining Loss: 1.901416 \tValidation Loss: 2.310941\n",
      "Epoch: 11299 \tTraining Loss: 1.919463 \tValidation Loss: 2.311043\n",
      "Epoch: 11300 \tTraining Loss: 1.918731 \tValidation Loss: 2.311065\n",
      "Epoch: 11301 \tTraining Loss: 1.909324 \tValidation Loss: 2.311465\n",
      "Epoch: 11302 \tTraining Loss: 1.901487 \tValidation Loss: 2.311315\n",
      "Epoch: 11303 \tTraining Loss: 1.900201 \tValidation Loss: 2.311446\n",
      "Epoch: 11304 \tTraining Loss: 1.906587 \tValidation Loss: 2.311570\n",
      "Epoch: 11305 \tTraining Loss: 1.877354 \tValidation Loss: 2.311511\n",
      "Epoch: 11306 \tTraining Loss: 1.875364 \tValidation Loss: 2.311664\n",
      "Epoch: 11307 \tTraining Loss: 1.886190 \tValidation Loss: 2.311553\n",
      "Epoch: 11308 \tTraining Loss: 1.887941 \tValidation Loss: 2.311434\n",
      "Epoch: 11309 \tTraining Loss: 1.881127 \tValidation Loss: 2.311244\n",
      "Epoch: 11310 \tTraining Loss: 1.879625 \tValidation Loss: 2.311069\n",
      "Epoch: 11311 \tTraining Loss: 1.891733 \tValidation Loss: 2.311403\n",
      "Epoch: 11312 \tTraining Loss: 1.905420 \tValidation Loss: 2.311340\n",
      "Epoch: 11313 \tTraining Loss: 1.884952 \tValidation Loss: 2.311132\n",
      "Epoch: 11314 \tTraining Loss: 1.878925 \tValidation Loss: 2.311314\n",
      "Epoch: 11315 \tTraining Loss: 1.927299 \tValidation Loss: 2.311203\n",
      "Epoch: 11316 \tTraining Loss: 1.909827 \tValidation Loss: 2.311426\n",
      "Epoch: 11317 \tTraining Loss: 1.887049 \tValidation Loss: 2.311188\n",
      "Epoch: 11318 \tTraining Loss: 1.892147 \tValidation Loss: 2.311042\n",
      "Epoch: 11319 \tTraining Loss: 1.890014 \tValidation Loss: 2.311179\n",
      "Epoch: 11320 \tTraining Loss: 1.877508 \tValidation Loss: 2.311332\n",
      "Epoch: 11321 \tTraining Loss: 1.898741 \tValidation Loss: 2.311466\n",
      "Epoch: 11322 \tTraining Loss: 1.903191 \tValidation Loss: 2.311339\n",
      "Epoch: 11323 \tTraining Loss: 1.892350 \tValidation Loss: 2.311348\n",
      "Epoch: 11324 \tTraining Loss: 1.920471 \tValidation Loss: 2.311090\n",
      "Epoch: 11325 \tTraining Loss: 1.873317 \tValidation Loss: 2.310805\n",
      "Epoch: 11326 \tTraining Loss: 1.914928 \tValidation Loss: 2.311375\n",
      "Epoch: 11327 \tTraining Loss: 1.858126 \tValidation Loss: 2.311803\n",
      "Epoch: 11328 \tTraining Loss: 1.896982 \tValidation Loss: 2.312132\n",
      "Epoch: 11329 \tTraining Loss: 1.896962 \tValidation Loss: 2.311975\n",
      "Epoch: 11330 \tTraining Loss: 1.889763 \tValidation Loss: 2.311985\n",
      "Epoch: 11331 \tTraining Loss: 1.908912 \tValidation Loss: 2.311971\n",
      "Epoch: 11332 \tTraining Loss: 1.871673 \tValidation Loss: 2.311503\n",
      "Epoch: 11333 \tTraining Loss: 1.887785 \tValidation Loss: 2.311278\n",
      "Epoch: 11334 \tTraining Loss: 1.897181 \tValidation Loss: 2.311456\n",
      "Epoch: 11335 \tTraining Loss: 1.865442 \tValidation Loss: 2.311664\n",
      "Epoch: 11336 \tTraining Loss: 1.888531 \tValidation Loss: 2.311535\n",
      "Epoch: 11337 \tTraining Loss: 1.902935 \tValidation Loss: 2.311615\n",
      "Epoch: 11338 \tTraining Loss: 1.887711 \tValidation Loss: 2.311777\n",
      "Epoch: 11339 \tTraining Loss: 1.883893 \tValidation Loss: 2.311642\n",
      "Epoch: 11340 \tTraining Loss: 1.901031 \tValidation Loss: 2.311637\n",
      "Epoch: 11341 \tTraining Loss: 1.902160 \tValidation Loss: 2.311102\n",
      "Epoch: 11342 \tTraining Loss: 1.893877 \tValidation Loss: 2.311146\n",
      "Epoch: 11343 \tTraining Loss: 1.887283 \tValidation Loss: 2.311232\n",
      "Epoch: 11344 \tTraining Loss: 1.896644 \tValidation Loss: 2.311098\n",
      "Epoch: 11345 \tTraining Loss: 1.909805 \tValidation Loss: 2.311179\n",
      "Epoch: 11346 \tTraining Loss: 1.913964 \tValidation Loss: 2.311137\n",
      "Epoch: 11347 \tTraining Loss: 1.878858 \tValidation Loss: 2.311010\n",
      "Epoch: 11348 \tTraining Loss: 1.882595 \tValidation Loss: 2.311111\n",
      "Epoch: 11349 \tTraining Loss: 1.863484 \tValidation Loss: 2.311094\n",
      "Epoch: 11350 \tTraining Loss: 1.889661 \tValidation Loss: 2.311340\n",
      "Epoch: 11351 \tTraining Loss: 1.896658 \tValidation Loss: 2.311337\n",
      "Epoch: 11352 \tTraining Loss: 1.880595 \tValidation Loss: 2.311453\n",
      "Epoch: 11353 \tTraining Loss: 1.889883 \tValidation Loss: 2.311610\n",
      "Epoch: 11354 \tTraining Loss: 1.889405 \tValidation Loss: 2.311421\n",
      "Epoch: 11355 \tTraining Loss: 1.894747 \tValidation Loss: 2.311525\n",
      "Epoch: 11356 \tTraining Loss: 1.894068 \tValidation Loss: 2.311550\n",
      "Epoch: 11357 \tTraining Loss: 1.872626 \tValidation Loss: 2.311666\n",
      "Epoch: 11358 \tTraining Loss: 1.883686 \tValidation Loss: 2.311534\n",
      "Epoch: 11359 \tTraining Loss: 1.876160 \tValidation Loss: 2.311294\n",
      "Epoch: 11360 \tTraining Loss: 1.891254 \tValidation Loss: 2.311376\n",
      "Epoch: 11361 \tTraining Loss: 1.892887 \tValidation Loss: 2.311686\n",
      "Epoch: 11362 \tTraining Loss: 1.871049 \tValidation Loss: 2.311963\n",
      "Epoch: 11363 \tTraining Loss: 1.867714 \tValidation Loss: 2.311926\n",
      "Epoch: 11364 \tTraining Loss: 1.907264 \tValidation Loss: 2.311707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11365 \tTraining Loss: 1.893634 \tValidation Loss: 2.311900\n",
      "Epoch: 11366 \tTraining Loss: 1.850124 \tValidation Loss: 2.311771\n",
      "Epoch: 11367 \tTraining Loss: 1.856734 \tValidation Loss: 2.311574\n",
      "Epoch: 11368 \tTraining Loss: 1.886613 \tValidation Loss: 2.311779\n",
      "Epoch: 11369 \tTraining Loss: 1.921111 \tValidation Loss: 2.311776\n",
      "Epoch: 11370 \tTraining Loss: 1.897807 \tValidation Loss: 2.311665\n",
      "Epoch: 11371 \tTraining Loss: 1.886297 \tValidation Loss: 2.311727\n",
      "Epoch: 11372 \tTraining Loss: 1.911502 \tValidation Loss: 2.311527\n",
      "Epoch: 11373 \tTraining Loss: 1.893086 \tValidation Loss: 2.311327\n",
      "Epoch: 11374 \tTraining Loss: 1.907735 \tValidation Loss: 2.311327\n",
      "Epoch: 11375 \tTraining Loss: 1.905602 \tValidation Loss: 2.311348\n",
      "Epoch: 11376 \tTraining Loss: 1.858097 \tValidation Loss: 2.311937\n",
      "Epoch: 11377 \tTraining Loss: 1.912566 \tValidation Loss: 2.312073\n",
      "Epoch: 11378 \tTraining Loss: 1.889716 \tValidation Loss: 2.311926\n",
      "Epoch: 11379 \tTraining Loss: 1.893359 \tValidation Loss: 2.311668\n",
      "Epoch: 11380 \tTraining Loss: 1.920329 \tValidation Loss: 2.311556\n",
      "Epoch: 11381 \tTraining Loss: 1.929066 \tValidation Loss: 2.311567\n",
      "Epoch: 11382 \tTraining Loss: 1.880942 \tValidation Loss: 2.311468\n",
      "Epoch: 11383 \tTraining Loss: 1.878338 \tValidation Loss: 2.311545\n",
      "Epoch: 11384 \tTraining Loss: 1.891085 \tValidation Loss: 2.311518\n",
      "Epoch: 11385 \tTraining Loss: 1.907512 \tValidation Loss: 2.311412\n",
      "Epoch: 11386 \tTraining Loss: 1.916324 \tValidation Loss: 2.311481\n",
      "Epoch: 11387 \tTraining Loss: 1.887908 \tValidation Loss: 2.311717\n",
      "Epoch: 11388 \tTraining Loss: 1.911415 \tValidation Loss: 2.311804\n",
      "Epoch: 11389 \tTraining Loss: 1.908330 \tValidation Loss: 2.311601\n",
      "Epoch: 11390 \tTraining Loss: 1.906466 \tValidation Loss: 2.311410\n",
      "Epoch: 11391 \tTraining Loss: 1.912198 \tValidation Loss: 2.311186\n",
      "Epoch: 11392 \tTraining Loss: 1.887046 \tValidation Loss: 2.311188\n",
      "Epoch: 11393 \tTraining Loss: 1.881320 \tValidation Loss: 2.311021\n",
      "Epoch: 11394 \tTraining Loss: 1.880459 \tValidation Loss: 2.311038\n",
      "Epoch: 11395 \tTraining Loss: 1.900836 \tValidation Loss: 2.311629\n",
      "Epoch: 11396 \tTraining Loss: 1.878444 \tValidation Loss: 2.311611\n",
      "Epoch: 11397 \tTraining Loss: 1.907052 \tValidation Loss: 2.311303\n",
      "Epoch: 11398 \tTraining Loss: 1.909141 \tValidation Loss: 2.311461\n",
      "Epoch: 11399 \tTraining Loss: 1.873858 \tValidation Loss: 2.311779\n",
      "Epoch: 11400 \tTraining Loss: 1.859199 \tValidation Loss: 2.312277\n",
      "Epoch: 11401 \tTraining Loss: 1.878209 \tValidation Loss: 2.311872\n",
      "Epoch: 11402 \tTraining Loss: 1.910215 \tValidation Loss: 2.311429\n",
      "Epoch: 11403 \tTraining Loss: 1.911466 \tValidation Loss: 2.311400\n",
      "Epoch: 11404 \tTraining Loss: 1.917255 \tValidation Loss: 2.311625\n",
      "Epoch: 11405 \tTraining Loss: 1.873950 \tValidation Loss: 2.311924\n",
      "Epoch: 11406 \tTraining Loss: 1.906062 \tValidation Loss: 2.311570\n",
      "Epoch: 11407 \tTraining Loss: 1.917053 \tValidation Loss: 2.311577\n",
      "Epoch: 11408 \tTraining Loss: 1.902873 \tValidation Loss: 2.311533\n",
      "Epoch: 11409 \tTraining Loss: 1.890404 \tValidation Loss: 2.311555\n",
      "Epoch: 11410 \tTraining Loss: 1.896285 \tValidation Loss: 2.311809\n",
      "Epoch: 11411 \tTraining Loss: 1.898380 \tValidation Loss: 2.311615\n",
      "Epoch: 11412 \tTraining Loss: 1.874835 \tValidation Loss: 2.311710\n",
      "Epoch: 11413 \tTraining Loss: 1.895622 \tValidation Loss: 2.311450\n",
      "Epoch: 11414 \tTraining Loss: 1.902170 \tValidation Loss: 2.311314\n",
      "Epoch: 11415 \tTraining Loss: 1.904358 \tValidation Loss: 2.311602\n",
      "Epoch: 11416 \tTraining Loss: 1.911202 \tValidation Loss: 2.311406\n",
      "Epoch: 11417 \tTraining Loss: 1.910868 \tValidation Loss: 2.311510\n",
      "Epoch: 11418 \tTraining Loss: 1.891487 \tValidation Loss: 2.311619\n",
      "Epoch: 11419 \tTraining Loss: 1.888305 \tValidation Loss: 2.311213\n",
      "Epoch: 11420 \tTraining Loss: 1.898377 \tValidation Loss: 2.311353\n",
      "Epoch: 11421 \tTraining Loss: 1.922120 \tValidation Loss: 2.310956\n",
      "Epoch: 11422 \tTraining Loss: 1.874692 \tValidation Loss: 2.311150\n",
      "Epoch: 11423 \tTraining Loss: 1.920764 \tValidation Loss: 2.311380\n",
      "Epoch: 11424 \tTraining Loss: 1.894945 \tValidation Loss: 2.311054\n",
      "Epoch: 11425 \tTraining Loss: 1.898193 \tValidation Loss: 2.311335\n",
      "Epoch: 11426 \tTraining Loss: 1.883770 \tValidation Loss: 2.311490\n",
      "Epoch: 11427 \tTraining Loss: 1.880460 \tValidation Loss: 2.311456\n",
      "Epoch: 11428 \tTraining Loss: 1.889492 \tValidation Loss: 2.311443\n",
      "Epoch: 11429 \tTraining Loss: 1.879679 \tValidation Loss: 2.311842\n",
      "Epoch: 11430 \tTraining Loss: 1.893288 \tValidation Loss: 2.312114\n",
      "Epoch: 11431 \tTraining Loss: 1.893834 \tValidation Loss: 2.311879\n",
      "Epoch: 11432 \tTraining Loss: 1.858225 \tValidation Loss: 2.311675\n",
      "Epoch: 11433 \tTraining Loss: 1.838605 \tValidation Loss: 2.311524\n",
      "Epoch: 11434 \tTraining Loss: 1.894278 \tValidation Loss: 2.311492\n",
      "Epoch: 11435 \tTraining Loss: 1.888495 \tValidation Loss: 2.311445\n",
      "Epoch: 11436 \tTraining Loss: 1.860623 \tValidation Loss: 2.311911\n",
      "Epoch: 11437 \tTraining Loss: 1.894645 \tValidation Loss: 2.311739\n",
      "Epoch: 11438 \tTraining Loss: 1.908646 \tValidation Loss: 2.311698\n",
      "Epoch: 11439 \tTraining Loss: 1.893283 \tValidation Loss: 2.311635\n",
      "Epoch: 11440 \tTraining Loss: 1.908463 \tValidation Loss: 2.312030\n",
      "Epoch: 11441 \tTraining Loss: 1.879559 \tValidation Loss: 2.312125\n",
      "Epoch: 11442 \tTraining Loss: 1.867309 \tValidation Loss: 2.311867\n",
      "Epoch: 11443 \tTraining Loss: 1.885336 \tValidation Loss: 2.311573\n",
      "Epoch: 11444 \tTraining Loss: 1.945380 \tValidation Loss: 2.311751\n",
      "Epoch: 11445 \tTraining Loss: 1.915136 \tValidation Loss: 2.311272\n",
      "Epoch: 11446 \tTraining Loss: 1.875180 \tValidation Loss: 2.311750\n",
      "Epoch: 11447 \tTraining Loss: 1.881777 \tValidation Loss: 2.311539\n",
      "Epoch: 11448 \tTraining Loss: 1.879237 \tValidation Loss: 2.311453\n",
      "Epoch: 11449 \tTraining Loss: 1.860306 \tValidation Loss: 2.311810\n",
      "Epoch: 11450 \tTraining Loss: 1.882512 \tValidation Loss: 2.311972\n",
      "Epoch: 11451 \tTraining Loss: 1.881085 \tValidation Loss: 2.312238\n",
      "Epoch: 11452 \tTraining Loss: 1.896392 \tValidation Loss: 2.312267\n",
      "Epoch: 11453 \tTraining Loss: 1.921398 \tValidation Loss: 2.312071\n",
      "Epoch: 11454 \tTraining Loss: 1.877441 \tValidation Loss: 2.312359\n",
      "Epoch: 11455 \tTraining Loss: 1.905087 \tValidation Loss: 2.312138\n",
      "Epoch: 11456 \tTraining Loss: 1.872650 \tValidation Loss: 2.312252\n",
      "Epoch: 11457 \tTraining Loss: 1.861488 \tValidation Loss: 2.312387\n",
      "Epoch: 11458 \tTraining Loss: 1.898531 \tValidation Loss: 2.312253\n",
      "Epoch: 11459 \tTraining Loss: 1.890141 \tValidation Loss: 2.312249\n",
      "Epoch: 11460 \tTraining Loss: 1.892595 \tValidation Loss: 2.312172\n",
      "Epoch: 11461 \tTraining Loss: 1.869801 \tValidation Loss: 2.312058\n",
      "Epoch: 11462 \tTraining Loss: 1.891813 \tValidation Loss: 2.311838\n",
      "Epoch: 11463 \tTraining Loss: 1.879367 \tValidation Loss: 2.311721\n",
      "Epoch: 11464 \tTraining Loss: 1.894175 \tValidation Loss: 2.311749\n",
      "Epoch: 11465 \tTraining Loss: 1.858817 \tValidation Loss: 2.311609\n",
      "Epoch: 11466 \tTraining Loss: 1.861759 \tValidation Loss: 2.311878\n",
      "Epoch: 11467 \tTraining Loss: 1.895176 \tValidation Loss: 2.311969\n",
      "Epoch: 11468 \tTraining Loss: 1.895144 \tValidation Loss: 2.312011\n",
      "Epoch: 11469 \tTraining Loss: 1.888584 \tValidation Loss: 2.311904\n",
      "Epoch: 11470 \tTraining Loss: 1.863432 \tValidation Loss: 2.311736\n",
      "Epoch: 11471 \tTraining Loss: 1.905108 \tValidation Loss: 2.311777\n",
      "Epoch: 11472 \tTraining Loss: 1.885641 \tValidation Loss: 2.311815\n",
      "Epoch: 11473 \tTraining Loss: 1.901185 \tValidation Loss: 2.311698\n",
      "Epoch: 11474 \tTraining Loss: 1.905976 \tValidation Loss: 2.311545\n",
      "Epoch: 11475 \tTraining Loss: 1.853325 \tValidation Loss: 2.311502\n",
      "Epoch: 11476 \tTraining Loss: 1.899576 \tValidation Loss: 2.311737\n",
      "Epoch: 11477 \tTraining Loss: 1.889357 \tValidation Loss: 2.312045\n",
      "Epoch: 11478 \tTraining Loss: 1.875623 \tValidation Loss: 2.311987\n",
      "Epoch: 11479 \tTraining Loss: 1.879367 \tValidation Loss: 2.311941\n",
      "Epoch: 11480 \tTraining Loss: 1.879574 \tValidation Loss: 2.312155\n",
      "Epoch: 11481 \tTraining Loss: 1.863410 \tValidation Loss: 2.312181\n",
      "Epoch: 11482 \tTraining Loss: 1.871874 \tValidation Loss: 2.312129\n",
      "Epoch: 11483 \tTraining Loss: 1.886169 \tValidation Loss: 2.312392\n",
      "Epoch: 11484 \tTraining Loss: 1.914251 \tValidation Loss: 2.312052\n",
      "Epoch: 11485 \tTraining Loss: 1.912152 \tValidation Loss: 2.311735\n",
      "Epoch: 11486 \tTraining Loss: 1.880058 \tValidation Loss: 2.311943\n",
      "Epoch: 11487 \tTraining Loss: 1.872669 \tValidation Loss: 2.312058\n",
      "Epoch: 11488 \tTraining Loss: 1.863089 \tValidation Loss: 2.311875\n",
      "Epoch: 11489 \tTraining Loss: 1.899400 \tValidation Loss: 2.311896\n",
      "Epoch: 11490 \tTraining Loss: 1.888911 \tValidation Loss: 2.312194\n",
      "Epoch: 11491 \tTraining Loss: 1.886911 \tValidation Loss: 2.312208\n",
      "Epoch: 11492 \tTraining Loss: 1.912758 \tValidation Loss: 2.311958\n",
      "Epoch: 11493 \tTraining Loss: 1.926765 \tValidation Loss: 2.311903\n",
      "Epoch: 11494 \tTraining Loss: 1.878334 \tValidation Loss: 2.312052\n",
      "Epoch: 11495 \tTraining Loss: 1.891033 \tValidation Loss: 2.312023\n",
      "Epoch: 11496 \tTraining Loss: 1.869179 \tValidation Loss: 2.312243\n",
      "Epoch: 11497 \tTraining Loss: 1.883840 \tValidation Loss: 2.312607\n",
      "Epoch: 11498 \tTraining Loss: 1.875554 \tValidation Loss: 2.312987\n",
      "Epoch: 11499 \tTraining Loss: 1.921222 \tValidation Loss: 2.312770\n",
      "Epoch: 11500 \tTraining Loss: 1.897059 \tValidation Loss: 2.312458\n",
      "Epoch: 11501 \tTraining Loss: 1.872735 \tValidation Loss: 2.312001\n",
      "Epoch: 11502 \tTraining Loss: 1.860129 \tValidation Loss: 2.312170\n",
      "Epoch: 11503 \tTraining Loss: 1.888429 \tValidation Loss: 2.311972\n",
      "Epoch: 11504 \tTraining Loss: 1.868690 \tValidation Loss: 2.311733\n",
      "Epoch: 11505 \tTraining Loss: 1.875902 \tValidation Loss: 2.311695\n",
      "Epoch: 11506 \tTraining Loss: 1.894469 \tValidation Loss: 2.311591\n",
      "Epoch: 11507 \tTraining Loss: 1.890125 \tValidation Loss: 2.311528\n",
      "Epoch: 11508 \tTraining Loss: 1.874153 \tValidation Loss: 2.311933\n",
      "Epoch: 11509 \tTraining Loss: 1.883461 \tValidation Loss: 2.312089\n",
      "Epoch: 11510 \tTraining Loss: 1.886945 \tValidation Loss: 2.312170\n",
      "Epoch: 11511 \tTraining Loss: 1.878235 \tValidation Loss: 2.312084\n",
      "Epoch: 11512 \tTraining Loss: 1.867828 \tValidation Loss: 2.312126\n",
      "Epoch: 11513 \tTraining Loss: 1.887205 \tValidation Loss: 2.312118\n",
      "Epoch: 11514 \tTraining Loss: 1.861426 \tValidation Loss: 2.312121\n",
      "Epoch: 11515 \tTraining Loss: 1.882679 \tValidation Loss: 2.312150\n",
      "Epoch: 11516 \tTraining Loss: 1.898071 \tValidation Loss: 2.311852\n",
      "Epoch: 11517 \tTraining Loss: 1.891016 \tValidation Loss: 2.311942\n",
      "Epoch: 11518 \tTraining Loss: 1.913349 \tValidation Loss: 2.311858\n",
      "Epoch: 11519 \tTraining Loss: 1.893724 \tValidation Loss: 2.312053\n",
      "Epoch: 11520 \tTraining Loss: 1.913431 \tValidation Loss: 2.312331\n",
      "Epoch: 11521 \tTraining Loss: 1.891554 \tValidation Loss: 2.312063\n",
      "Epoch: 11522 \tTraining Loss: 1.930416 \tValidation Loss: 2.311857\n",
      "Epoch: 11523 \tTraining Loss: 1.877573 \tValidation Loss: 2.312011\n",
      "Epoch: 11524 \tTraining Loss: 1.862383 \tValidation Loss: 2.312117\n",
      "Epoch: 11525 \tTraining Loss: 1.891581 \tValidation Loss: 2.312065\n",
      "Epoch: 11526 \tTraining Loss: 1.861499 \tValidation Loss: 2.312473\n",
      "Epoch: 11527 \tTraining Loss: 1.903645 \tValidation Loss: 2.312390\n",
      "Epoch: 11528 \tTraining Loss: 1.894324 \tValidation Loss: 2.312375\n",
      "Epoch: 11529 \tTraining Loss: 1.894943 \tValidation Loss: 2.312294\n",
      "Epoch: 11530 \tTraining Loss: 1.862795 \tValidation Loss: 2.312249\n",
      "Epoch: 11531 \tTraining Loss: 1.885766 \tValidation Loss: 2.311994\n",
      "Epoch: 11532 \tTraining Loss: 1.899643 \tValidation Loss: 2.312128\n",
      "Epoch: 11533 \tTraining Loss: 1.868344 \tValidation Loss: 2.311883\n",
      "Epoch: 11534 \tTraining Loss: 1.889291 \tValidation Loss: 2.312263\n",
      "Epoch: 11535 \tTraining Loss: 1.882388 \tValidation Loss: 2.312178\n",
      "Epoch: 11536 \tTraining Loss: 1.893836 \tValidation Loss: 2.312379\n",
      "Epoch: 11537 \tTraining Loss: 1.877058 \tValidation Loss: 2.312401\n",
      "Epoch: 11538 \tTraining Loss: 1.873512 \tValidation Loss: 2.312358\n",
      "Epoch: 11539 \tTraining Loss: 1.854407 \tValidation Loss: 2.312077\n",
      "Epoch: 11540 \tTraining Loss: 1.874125 \tValidation Loss: 2.312347\n",
      "Epoch: 11541 \tTraining Loss: 1.899863 \tValidation Loss: 2.312454\n",
      "Epoch: 11542 \tTraining Loss: 1.874354 \tValidation Loss: 2.312438\n",
      "Epoch: 11543 \tTraining Loss: 1.919293 \tValidation Loss: 2.312793\n",
      "Epoch: 11544 \tTraining Loss: 1.925231 \tValidation Loss: 2.312669\n",
      "Epoch: 11545 \tTraining Loss: 1.868286 \tValidation Loss: 2.312791\n",
      "Epoch: 11546 \tTraining Loss: 1.904298 \tValidation Loss: 2.313079\n",
      "Epoch: 11547 \tTraining Loss: 1.892540 \tValidation Loss: 2.312767\n",
      "Epoch: 11548 \tTraining Loss: 1.917873 \tValidation Loss: 2.312366\n",
      "Epoch: 11549 \tTraining Loss: 1.901796 \tValidation Loss: 2.312384\n",
      "Epoch: 11550 \tTraining Loss: 1.906211 \tValidation Loss: 2.312307\n",
      "Epoch: 11551 \tTraining Loss: 1.869339 \tValidation Loss: 2.312480\n",
      "Epoch: 11552 \tTraining Loss: 1.876666 \tValidation Loss: 2.312322\n",
      "Epoch: 11553 \tTraining Loss: 1.853249 \tValidation Loss: 2.312196\n",
      "Epoch: 11554 \tTraining Loss: 1.889831 \tValidation Loss: 2.312137\n",
      "Epoch: 11555 \tTraining Loss: 1.899693 \tValidation Loss: 2.312408\n",
      "Epoch: 11556 \tTraining Loss: 1.884483 \tValidation Loss: 2.312301\n",
      "Epoch: 11557 \tTraining Loss: 1.914227 \tValidation Loss: 2.312318\n",
      "Epoch: 11558 \tTraining Loss: 1.894824 \tValidation Loss: 2.312451\n",
      "Epoch: 11559 \tTraining Loss: 1.873132 \tValidation Loss: 2.312118\n",
      "Epoch: 11560 \tTraining Loss: 1.907588 \tValidation Loss: 2.312292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11561 \tTraining Loss: 1.875561 \tValidation Loss: 2.312546\n",
      "Epoch: 11562 \tTraining Loss: 1.859356 \tValidation Loss: 2.312652\n",
      "Epoch: 11563 \tTraining Loss: 1.914995 \tValidation Loss: 2.312575\n",
      "Epoch: 11564 \tTraining Loss: 1.850927 \tValidation Loss: 2.312599\n",
      "Epoch: 11565 \tTraining Loss: 1.851620 \tValidation Loss: 2.312715\n",
      "Epoch: 11566 \tTraining Loss: 1.893544 \tValidation Loss: 2.312449\n",
      "Epoch: 11567 \tTraining Loss: 1.902481 \tValidation Loss: 2.312207\n",
      "Epoch: 11568 \tTraining Loss: 1.881790 \tValidation Loss: 2.312330\n",
      "Epoch: 11569 \tTraining Loss: 1.887076 \tValidation Loss: 2.312143\n",
      "Epoch: 11570 \tTraining Loss: 1.882418 \tValidation Loss: 2.311949\n",
      "Epoch: 11571 \tTraining Loss: 1.912250 \tValidation Loss: 2.311738\n",
      "Epoch: 11572 \tTraining Loss: 1.857372 \tValidation Loss: 2.312127\n",
      "Epoch: 11573 \tTraining Loss: 1.891044 \tValidation Loss: 2.312579\n",
      "Epoch: 11574 \tTraining Loss: 1.876443 \tValidation Loss: 2.312515\n",
      "Epoch: 11575 \tTraining Loss: 1.908909 \tValidation Loss: 2.312637\n",
      "Epoch: 11576 \tTraining Loss: 1.875192 \tValidation Loss: 2.312702\n",
      "Epoch: 11577 \tTraining Loss: 1.900663 \tValidation Loss: 2.312905\n",
      "Epoch: 11578 \tTraining Loss: 1.890895 \tValidation Loss: 2.312610\n",
      "Epoch: 11579 \tTraining Loss: 1.895638 \tValidation Loss: 2.312212\n",
      "Epoch: 11580 \tTraining Loss: 1.883211 \tValidation Loss: 2.312509\n",
      "Epoch: 11581 \tTraining Loss: 1.861658 \tValidation Loss: 2.312683\n",
      "Epoch: 11582 \tTraining Loss: 1.888044 \tValidation Loss: 2.312649\n",
      "Epoch: 11583 \tTraining Loss: 1.864597 \tValidation Loss: 2.312527\n",
      "Epoch: 11584 \tTraining Loss: 1.893553 \tValidation Loss: 2.312362\n",
      "Epoch: 11585 \tTraining Loss: 1.884672 \tValidation Loss: 2.312096\n",
      "Epoch: 11586 \tTraining Loss: 1.875214 \tValidation Loss: 2.312033\n",
      "Epoch: 11587 \tTraining Loss: 1.874892 \tValidation Loss: 2.311637\n",
      "Epoch: 11588 \tTraining Loss: 1.871904 \tValidation Loss: 2.311797\n",
      "Epoch: 11589 \tTraining Loss: 1.877073 \tValidation Loss: 2.312068\n",
      "Epoch: 11590 \tTraining Loss: 1.908484 \tValidation Loss: 2.312204\n",
      "Epoch: 11591 \tTraining Loss: 1.909970 \tValidation Loss: 2.312021\n",
      "Epoch: 11592 \tTraining Loss: 1.852099 \tValidation Loss: 2.312301\n",
      "Epoch: 11593 \tTraining Loss: 1.905986 \tValidation Loss: 2.312799\n",
      "Epoch: 11594 \tTraining Loss: 1.884879 \tValidation Loss: 2.312765\n",
      "Epoch: 11595 \tTraining Loss: 1.897153 \tValidation Loss: 2.312696\n",
      "Epoch: 11596 \tTraining Loss: 1.877652 \tValidation Loss: 2.313063\n",
      "Epoch: 11597 \tTraining Loss: 1.862291 \tValidation Loss: 2.312775\n",
      "Epoch: 11598 \tTraining Loss: 1.865187 \tValidation Loss: 2.312716\n",
      "Epoch: 11599 \tTraining Loss: 1.891859 \tValidation Loss: 2.312716\n",
      "Epoch: 11600 \tTraining Loss: 1.895248 \tValidation Loss: 2.312889\n",
      "Epoch: 11601 \tTraining Loss: 1.846128 \tValidation Loss: 2.312864\n",
      "Epoch: 11602 \tTraining Loss: 1.908175 \tValidation Loss: 2.313180\n",
      "Epoch: 11603 \tTraining Loss: 1.854906 \tValidation Loss: 2.313062\n",
      "Epoch: 11604 \tTraining Loss: 1.878477 \tValidation Loss: 2.312641\n",
      "Epoch: 11605 \tTraining Loss: 1.860854 \tValidation Loss: 2.312313\n",
      "Epoch: 11606 \tTraining Loss: 1.870032 \tValidation Loss: 2.312479\n",
      "Epoch: 11607 \tTraining Loss: 1.852082 \tValidation Loss: 2.312603\n",
      "Epoch: 11608 \tTraining Loss: 1.853055 \tValidation Loss: 2.312559\n",
      "Epoch: 11609 \tTraining Loss: 1.904194 \tValidation Loss: 2.312509\n",
      "Epoch: 11610 \tTraining Loss: 1.840552 \tValidation Loss: 2.312564\n",
      "Epoch: 11611 \tTraining Loss: 1.879764 \tValidation Loss: 2.312483\n",
      "Epoch: 11612 \tTraining Loss: 1.893994 \tValidation Loss: 2.312244\n",
      "Epoch: 11613 \tTraining Loss: 1.882944 \tValidation Loss: 2.312433\n",
      "Epoch: 11614 \tTraining Loss: 1.885901 \tValidation Loss: 2.312523\n",
      "Epoch: 11615 \tTraining Loss: 1.875752 \tValidation Loss: 2.312478\n",
      "Epoch: 11616 \tTraining Loss: 1.883008 \tValidation Loss: 2.312326\n",
      "Epoch: 11617 \tTraining Loss: 1.899177 \tValidation Loss: 2.312049\n",
      "Epoch: 11618 \tTraining Loss: 1.876942 \tValidation Loss: 2.312179\n",
      "Epoch: 11619 \tTraining Loss: 1.881251 \tValidation Loss: 2.312179\n",
      "Epoch: 11620 \tTraining Loss: 1.894728 \tValidation Loss: 2.312360\n",
      "Epoch: 11621 \tTraining Loss: 1.873863 \tValidation Loss: 2.312176\n",
      "Epoch: 11622 \tTraining Loss: 1.906274 \tValidation Loss: 2.312468\n",
      "Epoch: 11623 \tTraining Loss: 1.905732 \tValidation Loss: 2.312894\n",
      "Epoch: 11624 \tTraining Loss: 1.862905 \tValidation Loss: 2.313334\n",
      "Epoch: 11625 \tTraining Loss: 1.902857 \tValidation Loss: 2.313376\n",
      "Epoch: 11626 \tTraining Loss: 1.870649 \tValidation Loss: 2.313004\n",
      "Epoch: 11627 \tTraining Loss: 1.870301 \tValidation Loss: 2.312942\n",
      "Epoch: 11628 \tTraining Loss: 1.924430 \tValidation Loss: 2.312345\n",
      "Epoch: 11629 \tTraining Loss: 1.881799 \tValidation Loss: 2.312652\n",
      "Epoch: 11630 \tTraining Loss: 1.832396 \tValidation Loss: 2.312978\n",
      "Epoch: 11631 \tTraining Loss: 1.905631 \tValidation Loss: 2.312517\n",
      "Epoch: 11632 \tTraining Loss: 1.859075 \tValidation Loss: 2.312286\n",
      "Epoch: 11633 \tTraining Loss: 1.902246 \tValidation Loss: 2.312209\n",
      "Epoch: 11634 \tTraining Loss: 1.835868 \tValidation Loss: 2.312113\n",
      "Epoch: 11635 \tTraining Loss: 1.887871 \tValidation Loss: 2.312299\n",
      "Epoch: 11636 \tTraining Loss: 1.879785 \tValidation Loss: 2.312012\n",
      "Epoch: 11637 \tTraining Loss: 1.909752 \tValidation Loss: 2.311826\n",
      "Epoch: 11638 \tTraining Loss: 1.885411 \tValidation Loss: 2.312247\n",
      "Epoch: 11639 \tTraining Loss: 1.880270 \tValidation Loss: 2.312479\n",
      "Epoch: 11640 \tTraining Loss: 1.843049 \tValidation Loss: 2.312080\n",
      "Epoch: 11641 \tTraining Loss: 1.886366 \tValidation Loss: 2.312330\n",
      "Epoch: 11642 \tTraining Loss: 1.869063 \tValidation Loss: 2.312495\n",
      "Epoch: 11643 \tTraining Loss: 1.883697 \tValidation Loss: 2.312184\n",
      "Epoch: 11644 \tTraining Loss: 1.888512 \tValidation Loss: 2.312470\n",
      "Epoch: 11645 \tTraining Loss: 1.865514 \tValidation Loss: 2.312200\n",
      "Epoch: 11646 \tTraining Loss: 1.879682 \tValidation Loss: 2.312126\n",
      "Epoch: 11647 \tTraining Loss: 1.886991 \tValidation Loss: 2.311973\n",
      "Epoch: 11648 \tTraining Loss: 1.868083 \tValidation Loss: 2.312094\n",
      "Epoch: 11649 \tTraining Loss: 1.883512 \tValidation Loss: 2.312260\n",
      "Epoch: 11650 \tTraining Loss: 1.901168 \tValidation Loss: 2.312106\n",
      "Epoch: 11651 \tTraining Loss: 1.867150 \tValidation Loss: 2.312020\n",
      "Epoch: 11652 \tTraining Loss: 1.857917 \tValidation Loss: 2.311689\n",
      "Epoch: 11653 \tTraining Loss: 1.875213 \tValidation Loss: 2.311776\n",
      "Epoch: 11654 \tTraining Loss: 1.887535 \tValidation Loss: 2.311901\n",
      "Epoch: 11655 \tTraining Loss: 1.866282 \tValidation Loss: 2.312119\n",
      "Epoch: 11656 \tTraining Loss: 1.884100 \tValidation Loss: 2.312128\n",
      "Epoch: 11657 \tTraining Loss: 1.872938 \tValidation Loss: 2.312369\n",
      "Epoch: 11658 \tTraining Loss: 1.898535 \tValidation Loss: 2.312503\n",
      "Epoch: 11659 \tTraining Loss: 1.878674 \tValidation Loss: 2.312302\n",
      "Epoch: 11660 \tTraining Loss: 1.835866 \tValidation Loss: 2.312674\n",
      "Epoch: 11661 \tTraining Loss: 1.886556 \tValidation Loss: 2.312128\n",
      "Epoch: 11662 \tTraining Loss: 1.906526 \tValidation Loss: 2.312183\n",
      "Epoch: 11663 \tTraining Loss: 1.872402 \tValidation Loss: 2.312421\n",
      "Epoch: 11664 \tTraining Loss: 1.869220 \tValidation Loss: 2.312183\n",
      "Epoch: 11665 \tTraining Loss: 1.865539 \tValidation Loss: 2.312518\n",
      "Epoch: 11666 \tTraining Loss: 1.907104 \tValidation Loss: 2.312952\n",
      "Epoch: 11667 \tTraining Loss: 1.866581 \tValidation Loss: 2.313122\n",
      "Epoch: 11668 \tTraining Loss: 1.885754 \tValidation Loss: 2.312896\n",
      "Epoch: 11669 \tTraining Loss: 1.862111 \tValidation Loss: 2.312639\n",
      "Epoch: 11670 \tTraining Loss: 1.875946 \tValidation Loss: 2.312755\n",
      "Epoch: 11671 \tTraining Loss: 1.886522 \tValidation Loss: 2.312586\n",
      "Epoch: 11672 \tTraining Loss: 1.867366 \tValidation Loss: 2.312676\n",
      "Epoch: 11673 \tTraining Loss: 1.875493 \tValidation Loss: 2.312631\n",
      "Epoch: 11674 \tTraining Loss: 1.849234 \tValidation Loss: 2.312737\n",
      "Epoch: 11675 \tTraining Loss: 1.878908 \tValidation Loss: 2.312944\n",
      "Epoch: 11676 \tTraining Loss: 1.900181 \tValidation Loss: 2.312740\n",
      "Epoch: 11677 \tTraining Loss: 1.876682 \tValidation Loss: 2.312609\n",
      "Epoch: 11678 \tTraining Loss: 1.894453 \tValidation Loss: 2.312776\n",
      "Epoch: 11679 \tTraining Loss: 1.868481 \tValidation Loss: 2.312691\n",
      "Epoch: 11680 \tTraining Loss: 1.851133 \tValidation Loss: 2.312463\n",
      "Epoch: 11681 \tTraining Loss: 1.903314 \tValidation Loss: 2.312669\n",
      "Epoch: 11682 \tTraining Loss: 1.863189 \tValidation Loss: 2.313096\n",
      "Epoch: 11683 \tTraining Loss: 1.873046 \tValidation Loss: 2.313202\n",
      "Epoch: 11684 \tTraining Loss: 1.875199 \tValidation Loss: 2.312995\n",
      "Epoch: 11685 \tTraining Loss: 1.874469 \tValidation Loss: 2.312648\n",
      "Epoch: 11686 \tTraining Loss: 1.867586 \tValidation Loss: 2.313078\n",
      "Epoch: 11687 \tTraining Loss: 1.877984 \tValidation Loss: 2.313024\n",
      "Epoch: 11688 \tTraining Loss: 1.876820 \tValidation Loss: 2.313224\n",
      "Epoch: 11689 \tTraining Loss: 1.874972 \tValidation Loss: 2.313153\n",
      "Epoch: 11690 \tTraining Loss: 1.884574 \tValidation Loss: 2.312821\n",
      "Epoch: 11691 \tTraining Loss: 1.891723 \tValidation Loss: 2.312530\n",
      "Epoch: 11692 \tTraining Loss: 1.863904 \tValidation Loss: 2.312618\n",
      "Epoch: 11693 \tTraining Loss: 1.889167 \tValidation Loss: 2.312323\n",
      "Epoch: 11694 \tTraining Loss: 1.876503 \tValidation Loss: 2.312226\n",
      "Epoch: 11695 \tTraining Loss: 1.895535 \tValidation Loss: 2.312189\n",
      "Epoch: 11696 \tTraining Loss: 1.889338 \tValidation Loss: 2.312243\n",
      "Epoch: 11697 \tTraining Loss: 1.877190 \tValidation Loss: 2.312205\n",
      "Epoch: 11698 \tTraining Loss: 1.892548 \tValidation Loss: 2.312123\n",
      "Epoch: 11699 \tTraining Loss: 1.859934 \tValidation Loss: 2.311724\n",
      "Epoch: 11700 \tTraining Loss: 1.879652 \tValidation Loss: 2.311607\n",
      "Epoch: 11701 \tTraining Loss: 1.902501 \tValidation Loss: 2.311931\n",
      "Epoch: 11702 \tTraining Loss: 1.889468 \tValidation Loss: 2.312095\n",
      "Epoch: 11703 \tTraining Loss: 1.891098 \tValidation Loss: 2.312232\n",
      "Epoch: 11704 \tTraining Loss: 1.880967 \tValidation Loss: 2.312310\n",
      "Epoch: 11705 \tTraining Loss: 1.868477 \tValidation Loss: 2.312711\n",
      "Epoch: 11706 \tTraining Loss: 1.886693 \tValidation Loss: 2.312789\n",
      "Epoch: 11707 \tTraining Loss: 1.874326 \tValidation Loss: 2.312594\n",
      "Epoch: 11708 \tTraining Loss: 1.869681 \tValidation Loss: 2.312703\n",
      "Epoch: 11709 \tTraining Loss: 1.886533 \tValidation Loss: 2.313036\n",
      "Epoch: 11710 \tTraining Loss: 1.884292 \tValidation Loss: 2.313041\n",
      "Epoch: 11711 \tTraining Loss: 1.867372 \tValidation Loss: 2.312968\n",
      "Epoch: 11712 \tTraining Loss: 1.875928 \tValidation Loss: 2.312790\n",
      "Epoch: 11713 \tTraining Loss: 1.865813 \tValidation Loss: 2.312976\n",
      "Epoch: 11714 \tTraining Loss: 1.879610 \tValidation Loss: 2.312420\n",
      "Epoch: 11715 \tTraining Loss: 1.900531 \tValidation Loss: 2.312655\n",
      "Epoch: 11716 \tTraining Loss: 1.872288 \tValidation Loss: 2.312669\n",
      "Epoch: 11717 \tTraining Loss: 1.916739 \tValidation Loss: 2.312423\n",
      "Epoch: 11718 \tTraining Loss: 1.870389 \tValidation Loss: 2.312410\n",
      "Epoch: 11719 \tTraining Loss: 1.906498 \tValidation Loss: 2.312300\n",
      "Epoch: 11720 \tTraining Loss: 1.886783 \tValidation Loss: 2.312562\n",
      "Epoch: 11721 \tTraining Loss: 1.875873 \tValidation Loss: 2.312629\n",
      "Epoch: 11722 \tTraining Loss: 1.871747 \tValidation Loss: 2.312813\n",
      "Epoch: 11723 \tTraining Loss: 1.890131 \tValidation Loss: 2.312971\n",
      "Epoch: 11724 \tTraining Loss: 1.888705 \tValidation Loss: 2.312806\n",
      "Epoch: 11725 \tTraining Loss: 1.868423 \tValidation Loss: 2.313139\n",
      "Epoch: 11726 \tTraining Loss: 1.879765 \tValidation Loss: 2.312822\n",
      "Epoch: 11727 \tTraining Loss: 1.867058 \tValidation Loss: 2.312589\n",
      "Epoch: 11728 \tTraining Loss: 1.876593 \tValidation Loss: 2.312883\n",
      "Epoch: 11729 \tTraining Loss: 1.881832 \tValidation Loss: 2.312646\n",
      "Epoch: 11730 \tTraining Loss: 1.847171 \tValidation Loss: 2.312996\n",
      "Epoch: 11731 \tTraining Loss: 1.855162 \tValidation Loss: 2.313224\n",
      "Epoch: 11732 \tTraining Loss: 1.896989 \tValidation Loss: 2.313159\n",
      "Epoch: 11733 \tTraining Loss: 1.889999 \tValidation Loss: 2.312883\n",
      "Epoch: 11734 \tTraining Loss: 1.856541 \tValidation Loss: 2.313086\n",
      "Epoch: 11735 \tTraining Loss: 1.855516 \tValidation Loss: 2.313149\n",
      "Epoch: 11736 \tTraining Loss: 1.858345 \tValidation Loss: 2.313325\n",
      "Epoch: 11737 \tTraining Loss: 1.888540 \tValidation Loss: 2.313090\n",
      "Epoch: 11738 \tTraining Loss: 1.882772 \tValidation Loss: 2.313033\n",
      "Epoch: 11739 \tTraining Loss: 1.817748 \tValidation Loss: 2.313023\n",
      "Epoch: 11740 \tTraining Loss: 1.909017 \tValidation Loss: 2.312650\n",
      "Epoch: 11741 \tTraining Loss: 1.883573 \tValidation Loss: 2.313275\n",
      "Epoch: 11742 \tTraining Loss: 1.867989 \tValidation Loss: 2.312879\n",
      "Epoch: 11743 \tTraining Loss: 1.855744 \tValidation Loss: 2.312956\n",
      "Epoch: 11744 \tTraining Loss: 1.910648 \tValidation Loss: 2.312932\n",
      "Epoch: 11745 \tTraining Loss: 1.850502 \tValidation Loss: 2.312924\n",
      "Epoch: 11746 \tTraining Loss: 1.892452 \tValidation Loss: 2.312660\n",
      "Epoch: 11747 \tTraining Loss: 1.872227 \tValidation Loss: 2.313053\n",
      "Epoch: 11748 \tTraining Loss: 1.912427 \tValidation Loss: 2.313158\n",
      "Epoch: 11749 \tTraining Loss: 1.871220 \tValidation Loss: 2.312950\n",
      "Epoch: 11750 \tTraining Loss: 1.858007 \tValidation Loss: 2.312822\n",
      "Epoch: 11751 \tTraining Loss: 1.870935 \tValidation Loss: 2.312915\n",
      "Epoch: 11752 \tTraining Loss: 1.875901 \tValidation Loss: 2.313228\n",
      "Epoch: 11753 \tTraining Loss: 1.855312 \tValidation Loss: 2.313054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11754 \tTraining Loss: 1.892088 \tValidation Loss: 2.313113\n",
      "Epoch: 11755 \tTraining Loss: 1.859594 \tValidation Loss: 2.312952\n",
      "Epoch: 11756 \tTraining Loss: 1.887710 \tValidation Loss: 2.312877\n",
      "Epoch: 11757 \tTraining Loss: 1.892431 \tValidation Loss: 2.312878\n",
      "Epoch: 11758 \tTraining Loss: 1.929091 \tValidation Loss: 2.313012\n",
      "Epoch: 11759 \tTraining Loss: 1.848117 \tValidation Loss: 2.313328\n",
      "Epoch: 11760 \tTraining Loss: 1.870957 \tValidation Loss: 2.313201\n",
      "Epoch: 11761 \tTraining Loss: 1.858152 \tValidation Loss: 2.313367\n",
      "Epoch: 11762 \tTraining Loss: 1.851958 \tValidation Loss: 2.313484\n",
      "Epoch: 11763 \tTraining Loss: 1.896415 \tValidation Loss: 2.313352\n",
      "Epoch: 11764 \tTraining Loss: 1.841084 \tValidation Loss: 2.313588\n",
      "Epoch: 11765 \tTraining Loss: 1.863163 \tValidation Loss: 2.313569\n",
      "Epoch: 11766 \tTraining Loss: 1.877540 \tValidation Loss: 2.313939\n",
      "Epoch: 11767 \tTraining Loss: 1.870862 \tValidation Loss: 2.313911\n",
      "Epoch: 11768 \tTraining Loss: 1.851068 \tValidation Loss: 2.313720\n",
      "Epoch: 11769 \tTraining Loss: 1.851108 \tValidation Loss: 2.313697\n",
      "Epoch: 11770 \tTraining Loss: 1.859060 \tValidation Loss: 2.313619\n",
      "Epoch: 11771 \tTraining Loss: 1.868738 \tValidation Loss: 2.313179\n",
      "Epoch: 11772 \tTraining Loss: 1.889177 \tValidation Loss: 2.313078\n",
      "Epoch: 11773 \tTraining Loss: 1.849528 \tValidation Loss: 2.313095\n",
      "Epoch: 11774 \tTraining Loss: 1.894211 \tValidation Loss: 2.313019\n",
      "Epoch: 11775 \tTraining Loss: 1.898318 \tValidation Loss: 2.313062\n",
      "Epoch: 11776 \tTraining Loss: 1.878469 \tValidation Loss: 2.313587\n",
      "Epoch: 11777 \tTraining Loss: 1.864074 \tValidation Loss: 2.313186\n",
      "Epoch: 11778 \tTraining Loss: 1.845286 \tValidation Loss: 2.313042\n",
      "Epoch: 11779 \tTraining Loss: 1.848056 \tValidation Loss: 2.312953\n",
      "Epoch: 11780 \tTraining Loss: 1.881180 \tValidation Loss: 2.313328\n",
      "Epoch: 11781 \tTraining Loss: 1.824908 \tValidation Loss: 2.313578\n",
      "Epoch: 11782 \tTraining Loss: 1.863565 \tValidation Loss: 2.313156\n",
      "Epoch: 11783 \tTraining Loss: 1.863270 \tValidation Loss: 2.313227\n",
      "Epoch: 11784 \tTraining Loss: 1.893267 \tValidation Loss: 2.313451\n",
      "Epoch: 11785 \tTraining Loss: 1.861482 \tValidation Loss: 2.313283\n",
      "Epoch: 11786 \tTraining Loss: 1.918966 \tValidation Loss: 2.313095\n",
      "Epoch: 11787 \tTraining Loss: 1.822852 \tValidation Loss: 2.312932\n",
      "Epoch: 11788 \tTraining Loss: 1.904435 \tValidation Loss: 2.312699\n",
      "Epoch: 11789 \tTraining Loss: 1.885499 \tValidation Loss: 2.312994\n",
      "Epoch: 11790 \tTraining Loss: 1.918124 \tValidation Loss: 2.313198\n",
      "Epoch: 11791 \tTraining Loss: 1.869451 \tValidation Loss: 2.313423\n",
      "Epoch: 11792 \tTraining Loss: 1.883951 \tValidation Loss: 2.313324\n",
      "Epoch: 11793 \tTraining Loss: 1.887423 \tValidation Loss: 2.313467\n",
      "Epoch: 11794 \tTraining Loss: 1.884039 \tValidation Loss: 2.313556\n",
      "Epoch: 11795 \tTraining Loss: 1.893600 \tValidation Loss: 2.313313\n",
      "Epoch: 11796 \tTraining Loss: 1.886727 \tValidation Loss: 2.313206\n",
      "Epoch: 11797 \tTraining Loss: 1.854322 \tValidation Loss: 2.313442\n",
      "Epoch: 11798 \tTraining Loss: 1.895509 \tValidation Loss: 2.313349\n",
      "Epoch: 11799 \tTraining Loss: 1.913289 \tValidation Loss: 2.313446\n",
      "Epoch: 11800 \tTraining Loss: 1.882317 \tValidation Loss: 2.313333\n",
      "Epoch: 11801 \tTraining Loss: 1.869543 \tValidation Loss: 2.313473\n",
      "Epoch: 11802 \tTraining Loss: 1.851029 \tValidation Loss: 2.313460\n",
      "Epoch: 11803 \tTraining Loss: 1.875901 \tValidation Loss: 2.313482\n",
      "Epoch: 11804 \tTraining Loss: 1.890097 \tValidation Loss: 2.313661\n",
      "Epoch: 11805 \tTraining Loss: 1.864391 \tValidation Loss: 2.313644\n",
      "Epoch: 11806 \tTraining Loss: 1.875541 \tValidation Loss: 2.313556\n",
      "Epoch: 11807 \tTraining Loss: 1.888904 \tValidation Loss: 2.313518\n",
      "Epoch: 11808 \tTraining Loss: 1.917867 \tValidation Loss: 2.313673\n",
      "Epoch: 11809 \tTraining Loss: 1.853906 \tValidation Loss: 2.313976\n",
      "Epoch: 11810 \tTraining Loss: 1.912733 \tValidation Loss: 2.314037\n",
      "Epoch: 11811 \tTraining Loss: 1.841405 \tValidation Loss: 2.314116\n",
      "Epoch: 11812 \tTraining Loss: 1.879714 \tValidation Loss: 2.314192\n",
      "Epoch: 11813 \tTraining Loss: 1.895629 \tValidation Loss: 2.313749\n",
      "Epoch: 11814 \tTraining Loss: 1.863373 \tValidation Loss: 2.313698\n",
      "Epoch: 11815 \tTraining Loss: 1.876239 \tValidation Loss: 2.313716\n",
      "Epoch: 11816 \tTraining Loss: 1.882252 \tValidation Loss: 2.313450\n",
      "Epoch: 11817 \tTraining Loss: 1.864530 \tValidation Loss: 2.313557\n",
      "Epoch: 11818 \tTraining Loss: 1.871025 \tValidation Loss: 2.313476\n",
      "Epoch: 11819 \tTraining Loss: 1.889252 \tValidation Loss: 2.313292\n",
      "Epoch: 11820 \tTraining Loss: 1.900485 \tValidation Loss: 2.313033\n",
      "Epoch: 11821 \tTraining Loss: 1.873766 \tValidation Loss: 2.313275\n",
      "Epoch: 11822 \tTraining Loss: 1.895518 \tValidation Loss: 2.312803\n",
      "Epoch: 11823 \tTraining Loss: 1.863624 \tValidation Loss: 2.313062\n",
      "Epoch: 11824 \tTraining Loss: 1.891692 \tValidation Loss: 2.313337\n",
      "Epoch: 11825 \tTraining Loss: 1.872062 \tValidation Loss: 2.313142\n",
      "Epoch: 11826 \tTraining Loss: 1.848108 \tValidation Loss: 2.313548\n",
      "Epoch: 11827 \tTraining Loss: 1.881099 \tValidation Loss: 2.313789\n",
      "Epoch: 11828 \tTraining Loss: 1.893713 \tValidation Loss: 2.313601\n",
      "Epoch: 11829 \tTraining Loss: 1.876111 \tValidation Loss: 2.313523\n",
      "Epoch: 11830 \tTraining Loss: 1.870169 \tValidation Loss: 2.313814\n",
      "Epoch: 11831 \tTraining Loss: 1.885280 \tValidation Loss: 2.313732\n",
      "Epoch: 11832 \tTraining Loss: 1.850499 \tValidation Loss: 2.314040\n",
      "Epoch: 11833 \tTraining Loss: 1.915301 \tValidation Loss: 2.313963\n",
      "Epoch: 11834 \tTraining Loss: 1.911367 \tValidation Loss: 2.313662\n",
      "Epoch: 11835 \tTraining Loss: 1.862927 \tValidation Loss: 2.314083\n",
      "Epoch: 11836 \tTraining Loss: 1.863306 \tValidation Loss: 2.313827\n",
      "Epoch: 11837 \tTraining Loss: 1.886078 \tValidation Loss: 2.313948\n",
      "Epoch: 11838 \tTraining Loss: 1.855363 \tValidation Loss: 2.313679\n",
      "Epoch: 11839 \tTraining Loss: 1.857133 \tValidation Loss: 2.313302\n",
      "Epoch: 11840 \tTraining Loss: 1.876321 \tValidation Loss: 2.313362\n",
      "Epoch: 11841 \tTraining Loss: 1.882441 \tValidation Loss: 2.313299\n",
      "Epoch: 11842 \tTraining Loss: 1.911557 \tValidation Loss: 2.313082\n",
      "Epoch: 11843 \tTraining Loss: 1.894738 \tValidation Loss: 2.313067\n",
      "Epoch: 11844 \tTraining Loss: 1.858679 \tValidation Loss: 2.313262\n",
      "Epoch: 11845 \tTraining Loss: 1.868306 \tValidation Loss: 2.313281\n",
      "Epoch: 11846 \tTraining Loss: 1.864835 \tValidation Loss: 2.313563\n",
      "Epoch: 11847 \tTraining Loss: 1.885477 \tValidation Loss: 2.313734\n",
      "Epoch: 11848 \tTraining Loss: 1.859674 \tValidation Loss: 2.313934\n",
      "Epoch: 11849 \tTraining Loss: 1.916482 \tValidation Loss: 2.313786\n",
      "Epoch: 11850 \tTraining Loss: 1.883487 \tValidation Loss: 2.313371\n",
      "Epoch: 11851 \tTraining Loss: 1.848505 \tValidation Loss: 2.313671\n",
      "Epoch: 11852 \tTraining Loss: 1.860074 \tValidation Loss: 2.313848\n",
      "Epoch: 11853 \tTraining Loss: 1.887161 \tValidation Loss: 2.313952\n",
      "Epoch: 11854 \tTraining Loss: 1.864657 \tValidation Loss: 2.313812\n",
      "Epoch: 11855 \tTraining Loss: 1.878145 \tValidation Loss: 2.313651\n",
      "Epoch: 11856 \tTraining Loss: 1.886426 \tValidation Loss: 2.313823\n",
      "Epoch: 11857 \tTraining Loss: 1.877420 \tValidation Loss: 2.314219\n",
      "Epoch: 11858 \tTraining Loss: 1.859503 \tValidation Loss: 2.314218\n",
      "Epoch: 11859 \tTraining Loss: 1.855163 \tValidation Loss: 2.313986\n",
      "Epoch: 11860 \tTraining Loss: 1.869631 \tValidation Loss: 2.313751\n",
      "Epoch: 11861 \tTraining Loss: 1.892948 \tValidation Loss: 2.313691\n",
      "Epoch: 11862 \tTraining Loss: 1.828198 \tValidation Loss: 2.313400\n",
      "Epoch: 11863 \tTraining Loss: 1.897551 \tValidation Loss: 2.313641\n",
      "Epoch: 11864 \tTraining Loss: 1.874407 \tValidation Loss: 2.313827\n",
      "Epoch: 11865 \tTraining Loss: 1.858032 \tValidation Loss: 2.313774\n",
      "Epoch: 11866 \tTraining Loss: 1.866991 \tValidation Loss: 2.314030\n",
      "Epoch: 11867 \tTraining Loss: 1.865531 \tValidation Loss: 2.314442\n",
      "Epoch: 11868 \tTraining Loss: 1.876870 \tValidation Loss: 2.314113\n",
      "Epoch: 11869 \tTraining Loss: 1.879545 \tValidation Loss: 2.314096\n",
      "Epoch: 11870 \tTraining Loss: 1.876711 \tValidation Loss: 2.313624\n",
      "Epoch: 11871 \tTraining Loss: 1.876402 \tValidation Loss: 2.313547\n",
      "Epoch: 11872 \tTraining Loss: 1.871021 \tValidation Loss: 2.313509\n",
      "Epoch: 11873 \tTraining Loss: 1.924663 \tValidation Loss: 2.313389\n",
      "Epoch: 11874 \tTraining Loss: 1.864289 \tValidation Loss: 2.314127\n",
      "Epoch: 11875 \tTraining Loss: 1.909777 \tValidation Loss: 2.314142\n",
      "Epoch: 11876 \tTraining Loss: 1.876285 \tValidation Loss: 2.314295\n",
      "Epoch: 11877 \tTraining Loss: 1.840992 \tValidation Loss: 2.314381\n",
      "Epoch: 11878 \tTraining Loss: 1.860405 \tValidation Loss: 2.314008\n",
      "Epoch: 11879 \tTraining Loss: 1.860593 \tValidation Loss: 2.314215\n",
      "Epoch: 11880 \tTraining Loss: 1.872481 \tValidation Loss: 2.314041\n",
      "Epoch: 11881 \tTraining Loss: 1.901003 \tValidation Loss: 2.314017\n",
      "Epoch: 11882 \tTraining Loss: 1.860572 \tValidation Loss: 2.314386\n",
      "Epoch: 11883 \tTraining Loss: 1.862744 \tValidation Loss: 2.314080\n",
      "Epoch: 11884 \tTraining Loss: 1.851924 \tValidation Loss: 2.314075\n",
      "Epoch: 11885 \tTraining Loss: 1.862272 \tValidation Loss: 2.314099\n",
      "Epoch: 11886 \tTraining Loss: 1.876714 \tValidation Loss: 2.314276\n",
      "Epoch: 11887 \tTraining Loss: 1.874120 \tValidation Loss: 2.314432\n",
      "Epoch: 11888 \tTraining Loss: 1.841058 \tValidation Loss: 2.314314\n",
      "Epoch: 11889 \tTraining Loss: 1.896034 \tValidation Loss: 2.314569\n",
      "Epoch: 11890 \tTraining Loss: 1.836251 \tValidation Loss: 2.314391\n",
      "Epoch: 11891 \tTraining Loss: 1.848463 \tValidation Loss: 2.314419\n",
      "Epoch: 11892 \tTraining Loss: 1.893085 \tValidation Loss: 2.314384\n",
      "Epoch: 11893 \tTraining Loss: 1.860592 \tValidation Loss: 2.314510\n",
      "Epoch: 11894 \tTraining Loss: 1.845139 \tValidation Loss: 2.314385\n",
      "Epoch: 11895 \tTraining Loss: 1.876902 \tValidation Loss: 2.314598\n",
      "Epoch: 11896 \tTraining Loss: 1.867049 \tValidation Loss: 2.314403\n",
      "Epoch: 11897 \tTraining Loss: 1.844443 \tValidation Loss: 2.314598\n",
      "Epoch: 11898 \tTraining Loss: 1.893459 \tValidation Loss: 2.314588\n",
      "Epoch: 11899 \tTraining Loss: 1.869366 \tValidation Loss: 2.314567\n",
      "Epoch: 11900 \tTraining Loss: 1.859932 \tValidation Loss: 2.314435\n",
      "Epoch: 11901 \tTraining Loss: 1.891388 \tValidation Loss: 2.314379\n",
      "Epoch: 11902 \tTraining Loss: 1.847514 \tValidation Loss: 2.314214\n",
      "Epoch: 11903 \tTraining Loss: 1.857143 \tValidation Loss: 2.314340\n",
      "Epoch: 11904 \tTraining Loss: 1.880896 \tValidation Loss: 2.314216\n",
      "Epoch: 11905 \tTraining Loss: 1.869368 \tValidation Loss: 2.314619\n",
      "Epoch: 11906 \tTraining Loss: 1.892704 \tValidation Loss: 2.314332\n",
      "Epoch: 11907 \tTraining Loss: 1.877405 \tValidation Loss: 2.314510\n",
      "Epoch: 11908 \tTraining Loss: 1.869037 \tValidation Loss: 2.314648\n",
      "Epoch: 11909 \tTraining Loss: 1.855470 \tValidation Loss: 2.314307\n",
      "Epoch: 11910 \tTraining Loss: 1.868290 \tValidation Loss: 2.314391\n",
      "Epoch: 11911 \tTraining Loss: 1.876276 \tValidation Loss: 2.314449\n",
      "Epoch: 11912 \tTraining Loss: 1.870418 \tValidation Loss: 2.314445\n",
      "Epoch: 11913 \tTraining Loss: 1.855377 \tValidation Loss: 2.314378\n",
      "Epoch: 11914 \tTraining Loss: 1.901070 \tValidation Loss: 2.313925\n",
      "Epoch: 11915 \tTraining Loss: 1.866247 \tValidation Loss: 2.313941\n",
      "Epoch: 11916 \tTraining Loss: 1.861541 \tValidation Loss: 2.314410\n",
      "Epoch: 11917 \tTraining Loss: 1.845403 \tValidation Loss: 2.314358\n",
      "Epoch: 11918 \tTraining Loss: 1.888992 \tValidation Loss: 2.314592\n",
      "Epoch: 11919 \tTraining Loss: 1.903231 \tValidation Loss: 2.314756\n",
      "Epoch: 11920 \tTraining Loss: 1.901684 \tValidation Loss: 2.314678\n",
      "Epoch: 11921 \tTraining Loss: 1.893839 \tValidation Loss: 2.314636\n",
      "Epoch: 11922 \tTraining Loss: 1.908286 \tValidation Loss: 2.314402\n",
      "Epoch: 11923 \tTraining Loss: 1.888972 \tValidation Loss: 2.314377\n",
      "Epoch: 11924 \tTraining Loss: 1.867254 \tValidation Loss: 2.314124\n",
      "Epoch: 11925 \tTraining Loss: 1.844585 \tValidation Loss: 2.314435\n",
      "Epoch: 11926 \tTraining Loss: 1.837339 \tValidation Loss: 2.314322\n",
      "Epoch: 11927 \tTraining Loss: 1.867345 \tValidation Loss: 2.314475\n",
      "Epoch: 11928 \tTraining Loss: 1.888340 \tValidation Loss: 2.314113\n",
      "Epoch: 11929 \tTraining Loss: 1.924622 \tValidation Loss: 2.314014\n",
      "Epoch: 11930 \tTraining Loss: 1.867990 \tValidation Loss: 2.314225\n",
      "Epoch: 11931 \tTraining Loss: 1.836253 \tValidation Loss: 2.314288\n",
      "Epoch: 11932 \tTraining Loss: 1.840220 \tValidation Loss: 2.314080\n",
      "Epoch: 11933 \tTraining Loss: 1.831239 \tValidation Loss: 2.314272\n",
      "Epoch: 11934 \tTraining Loss: 1.848134 \tValidation Loss: 2.314461\n",
      "Epoch: 11935 \tTraining Loss: 1.894347 \tValidation Loss: 2.314389\n",
      "Epoch: 11936 \tTraining Loss: 1.866849 \tValidation Loss: 2.314398\n",
      "Epoch: 11937 \tTraining Loss: 1.865154 \tValidation Loss: 2.314397\n",
      "Epoch: 11938 \tTraining Loss: 1.868819 \tValidation Loss: 2.314732\n",
      "Epoch: 11939 \tTraining Loss: 1.883015 \tValidation Loss: 2.315147\n",
      "Epoch: 11940 \tTraining Loss: 1.881099 \tValidation Loss: 2.314607\n",
      "Epoch: 11941 \tTraining Loss: 1.855003 \tValidation Loss: 2.314729\n",
      "Epoch: 11942 \tTraining Loss: 1.868801 \tValidation Loss: 2.314370\n",
      "Epoch: 11943 \tTraining Loss: 1.872908 \tValidation Loss: 2.313884\n",
      "Epoch: 11944 \tTraining Loss: 1.866604 \tValidation Loss: 2.314300\n",
      "Epoch: 11945 \tTraining Loss: 1.878561 \tValidation Loss: 2.314043\n",
      "Epoch: 11946 \tTraining Loss: 1.884677 \tValidation Loss: 2.314827\n",
      "Epoch: 11947 \tTraining Loss: 1.858026 \tValidation Loss: 2.314398\n",
      "Epoch: 11948 \tTraining Loss: 1.850326 \tValidation Loss: 2.314141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11949 \tTraining Loss: 1.865681 \tValidation Loss: 2.314720\n",
      "Epoch: 11950 \tTraining Loss: 1.878678 \tValidation Loss: 2.314456\n",
      "Epoch: 11951 \tTraining Loss: 1.857399 \tValidation Loss: 2.314687\n",
      "Epoch: 11952 \tTraining Loss: 1.846974 \tValidation Loss: 2.314816\n",
      "Epoch: 11953 \tTraining Loss: 1.877012 \tValidation Loss: 2.314836\n",
      "Epoch: 11954 \tTraining Loss: 1.853160 \tValidation Loss: 2.314648\n",
      "Epoch: 11955 \tTraining Loss: 1.849630 \tValidation Loss: 2.314321\n",
      "Epoch: 11956 \tTraining Loss: 1.849768 \tValidation Loss: 2.314040\n",
      "Epoch: 11957 \tTraining Loss: 1.841646 \tValidation Loss: 2.314328\n",
      "Epoch: 11958 \tTraining Loss: 1.856317 \tValidation Loss: 2.314374\n",
      "Epoch: 11959 \tTraining Loss: 1.873197 \tValidation Loss: 2.314727\n",
      "Epoch: 11960 \tTraining Loss: 1.860700 \tValidation Loss: 2.314852\n",
      "Epoch: 11961 \tTraining Loss: 1.897007 \tValidation Loss: 2.314900\n",
      "Epoch: 11962 \tTraining Loss: 1.888041 \tValidation Loss: 2.315151\n",
      "Epoch: 11963 \tTraining Loss: 1.868136 \tValidation Loss: 2.314941\n",
      "Epoch: 11964 \tTraining Loss: 1.838032 \tValidation Loss: 2.314330\n",
      "Epoch: 11965 \tTraining Loss: 1.884899 \tValidation Loss: 2.314523\n",
      "Epoch: 11966 \tTraining Loss: 1.846680 \tValidation Loss: 2.314527\n",
      "Epoch: 11967 \tTraining Loss: 1.892681 \tValidation Loss: 2.314674\n",
      "Epoch: 11968 \tTraining Loss: 1.886897 \tValidation Loss: 2.314676\n",
      "Epoch: 11969 \tTraining Loss: 1.864643 \tValidation Loss: 2.314882\n",
      "Epoch: 11970 \tTraining Loss: 1.858948 \tValidation Loss: 2.315104\n",
      "Epoch: 11971 \tTraining Loss: 1.914390 \tValidation Loss: 2.314867\n",
      "Epoch: 11972 \tTraining Loss: 1.869169 \tValidation Loss: 2.314260\n",
      "Epoch: 11973 \tTraining Loss: 1.859956 \tValidation Loss: 2.315032\n",
      "Epoch: 11974 \tTraining Loss: 1.848932 \tValidation Loss: 2.314828\n",
      "Epoch: 11975 \tTraining Loss: 1.882633 \tValidation Loss: 2.314676\n",
      "Epoch: 11976 \tTraining Loss: 1.832760 \tValidation Loss: 2.314643\n",
      "Epoch: 11977 \tTraining Loss: 1.856123 \tValidation Loss: 2.314733\n",
      "Epoch: 11978 \tTraining Loss: 1.835773 \tValidation Loss: 2.315209\n",
      "Epoch: 11979 \tTraining Loss: 1.855862 \tValidation Loss: 2.314754\n",
      "Epoch: 11980 \tTraining Loss: 1.851838 \tValidation Loss: 2.315029\n",
      "Epoch: 11981 \tTraining Loss: 1.865025 \tValidation Loss: 2.315269\n",
      "Epoch: 11982 \tTraining Loss: 1.846937 \tValidation Loss: 2.315272\n",
      "Epoch: 11983 \tTraining Loss: 1.873359 \tValidation Loss: 2.314998\n",
      "Epoch: 11984 \tTraining Loss: 1.849815 \tValidation Loss: 2.315277\n",
      "Epoch: 11985 \tTraining Loss: 1.873217 \tValidation Loss: 2.315490\n",
      "Epoch: 11986 \tTraining Loss: 1.859984 \tValidation Loss: 2.315525\n",
      "Epoch: 11987 \tTraining Loss: 1.842727 \tValidation Loss: 2.315408\n",
      "Epoch: 11988 \tTraining Loss: 1.863579 \tValidation Loss: 2.315257\n",
      "Epoch: 11989 \tTraining Loss: 1.886450 \tValidation Loss: 2.314999\n",
      "Epoch: 11990 \tTraining Loss: 1.858216 \tValidation Loss: 2.314357\n",
      "Epoch: 11991 \tTraining Loss: 1.882746 \tValidation Loss: 2.314743\n",
      "Epoch: 11992 \tTraining Loss: 1.861607 \tValidation Loss: 2.314982\n",
      "Epoch: 11993 \tTraining Loss: 1.851244 \tValidation Loss: 2.314909\n",
      "Epoch: 11994 \tTraining Loss: 1.875461 \tValidation Loss: 2.314547\n",
      "Epoch: 11995 \tTraining Loss: 1.859007 \tValidation Loss: 2.314628\n",
      "Epoch: 11996 \tTraining Loss: 1.863250 \tValidation Loss: 2.314685\n",
      "Epoch: 11997 \tTraining Loss: 1.879963 \tValidation Loss: 2.314723\n",
      "Epoch: 11998 \tTraining Loss: 1.864551 \tValidation Loss: 2.314573\n",
      "Epoch: 11999 \tTraining Loss: 1.862929 \tValidation Loss: 2.314506\n",
      "Epoch: 12000 \tTraining Loss: 1.883200 \tValidation Loss: 2.314726\n",
      "Epoch: 12001 \tTraining Loss: 1.874671 \tValidation Loss: 2.314487\n",
      "Epoch: 12002 \tTraining Loss: 1.885663 \tValidation Loss: 2.315246\n",
      "Epoch: 12003 \tTraining Loss: 1.865871 \tValidation Loss: 2.315399\n",
      "Epoch: 12004 \tTraining Loss: 1.841324 \tValidation Loss: 2.315222\n",
      "Epoch: 12005 \tTraining Loss: 1.839991 \tValidation Loss: 2.315423\n",
      "Epoch: 12006 \tTraining Loss: 1.881863 \tValidation Loss: 2.315484\n",
      "Epoch: 12007 \tTraining Loss: 1.887873 \tValidation Loss: 2.315309\n",
      "Epoch: 12008 \tTraining Loss: 1.857303 \tValidation Loss: 2.315658\n",
      "Epoch: 12009 \tTraining Loss: 1.884621 \tValidation Loss: 2.315503\n",
      "Epoch: 12010 \tTraining Loss: 1.905931 \tValidation Loss: 2.315123\n",
      "Epoch: 12011 \tTraining Loss: 1.889349 \tValidation Loss: 2.314985\n",
      "Epoch: 12012 \tTraining Loss: 1.877041 \tValidation Loss: 2.314765\n",
      "Epoch: 12013 \tTraining Loss: 1.842943 \tValidation Loss: 2.315136\n",
      "Epoch: 12014 \tTraining Loss: 1.861861 \tValidation Loss: 2.315205\n",
      "Epoch: 12015 \tTraining Loss: 1.883345 \tValidation Loss: 2.315005\n",
      "Epoch: 12016 \tTraining Loss: 1.851727 \tValidation Loss: 2.314934\n",
      "Epoch: 12017 \tTraining Loss: 1.832324 \tValidation Loss: 2.314873\n",
      "Epoch: 12018 \tTraining Loss: 1.870706 \tValidation Loss: 2.314717\n",
      "Epoch: 12019 \tTraining Loss: 1.898508 \tValidation Loss: 2.314937\n",
      "Epoch: 12020 \tTraining Loss: 1.832192 \tValidation Loss: 2.315039\n",
      "Epoch: 12021 \tTraining Loss: 1.862396 \tValidation Loss: 2.314988\n",
      "Epoch: 12022 \tTraining Loss: 1.836541 \tValidation Loss: 2.314865\n",
      "Epoch: 12023 \tTraining Loss: 1.882602 \tValidation Loss: 2.314983\n",
      "Epoch: 12024 \tTraining Loss: 1.856743 \tValidation Loss: 2.315220\n",
      "Epoch: 12025 \tTraining Loss: 1.870055 \tValidation Loss: 2.314944\n",
      "Epoch: 12026 \tTraining Loss: 1.861961 \tValidation Loss: 2.315153\n",
      "Epoch: 12027 \tTraining Loss: 1.843181 \tValidation Loss: 2.315016\n",
      "Epoch: 12028 \tTraining Loss: 1.889087 \tValidation Loss: 2.314981\n",
      "Epoch: 12029 \tTraining Loss: 1.838282 \tValidation Loss: 2.315274\n",
      "Epoch: 12030 \tTraining Loss: 1.847872 \tValidation Loss: 2.315536\n",
      "Epoch: 12031 \tTraining Loss: 1.891772 \tValidation Loss: 2.315312\n",
      "Epoch: 12032 \tTraining Loss: 1.854007 \tValidation Loss: 2.315143\n",
      "Epoch: 12033 \tTraining Loss: 1.837484 \tValidation Loss: 2.315258\n",
      "Epoch: 12034 \tTraining Loss: 1.864213 \tValidation Loss: 2.315608\n",
      "Epoch: 12035 \tTraining Loss: 1.876973 \tValidation Loss: 2.315542\n",
      "Epoch: 12036 \tTraining Loss: 1.878155 \tValidation Loss: 2.315273\n",
      "Epoch: 12037 \tTraining Loss: 1.857733 \tValidation Loss: 2.315089\n",
      "Epoch: 12038 \tTraining Loss: 1.880561 \tValidation Loss: 2.315244\n",
      "Epoch: 12039 \tTraining Loss: 1.876416 \tValidation Loss: 2.315472\n",
      "Epoch: 12040 \tTraining Loss: 1.889756 \tValidation Loss: 2.315117\n",
      "Epoch: 12041 \tTraining Loss: 1.883127 \tValidation Loss: 2.315029\n",
      "Epoch: 12042 \tTraining Loss: 1.804417 \tValidation Loss: 2.315241\n",
      "Epoch: 12043 \tTraining Loss: 1.906360 \tValidation Loss: 2.315096\n",
      "Epoch: 12044 \tTraining Loss: 1.870895 \tValidation Loss: 2.315558\n",
      "Epoch: 12045 \tTraining Loss: 1.872451 \tValidation Loss: 2.315808\n",
      "Epoch: 12046 \tTraining Loss: 1.854345 \tValidation Loss: 2.315439\n",
      "Epoch: 12047 \tTraining Loss: 1.868085 \tValidation Loss: 2.315012\n",
      "Epoch: 12048 \tTraining Loss: 1.878436 \tValidation Loss: 2.314615\n",
      "Epoch: 12049 \tTraining Loss: 1.858090 \tValidation Loss: 2.314764\n",
      "Epoch: 12050 \tTraining Loss: 1.892190 \tValidation Loss: 2.315034\n",
      "Epoch: 12051 \tTraining Loss: 1.880190 \tValidation Loss: 2.315290\n",
      "Epoch: 12052 \tTraining Loss: 1.860640 \tValidation Loss: 2.315823\n",
      "Epoch: 12053 \tTraining Loss: 1.823072 \tValidation Loss: 2.315305\n",
      "Epoch: 12054 \tTraining Loss: 1.823781 \tValidation Loss: 2.315539\n",
      "Epoch: 12055 \tTraining Loss: 1.891570 \tValidation Loss: 2.315476\n",
      "Epoch: 12056 \tTraining Loss: 1.858753 \tValidation Loss: 2.315349\n",
      "Epoch: 12057 \tTraining Loss: 1.843014 \tValidation Loss: 2.315475\n",
      "Epoch: 12058 \tTraining Loss: 1.854132 \tValidation Loss: 2.315575\n",
      "Epoch: 12059 \tTraining Loss: 1.853167 \tValidation Loss: 2.315336\n",
      "Epoch: 12060 \tTraining Loss: 1.849584 \tValidation Loss: 2.315456\n",
      "Epoch: 12061 \tTraining Loss: 1.852745 \tValidation Loss: 2.315023\n",
      "Epoch: 12062 \tTraining Loss: 1.888185 \tValidation Loss: 2.314849\n",
      "Epoch: 12063 \tTraining Loss: 1.875728 \tValidation Loss: 2.315176\n",
      "Epoch: 12064 \tTraining Loss: 1.869340 \tValidation Loss: 2.314965\n",
      "Epoch: 12065 \tTraining Loss: 1.842181 \tValidation Loss: 2.314794\n",
      "Epoch: 12066 \tTraining Loss: 1.866182 \tValidation Loss: 2.314315\n",
      "Epoch: 12067 \tTraining Loss: 1.866310 \tValidation Loss: 2.314454\n",
      "Epoch: 12068 \tTraining Loss: 1.847120 \tValidation Loss: 2.314795\n",
      "Epoch: 12069 \tTraining Loss: 1.878683 \tValidation Loss: 2.314818\n",
      "Epoch: 12070 \tTraining Loss: 1.873215 \tValidation Loss: 2.314641\n",
      "Epoch: 12071 \tTraining Loss: 1.853732 \tValidation Loss: 2.314583\n",
      "Epoch: 12072 \tTraining Loss: 1.854807 \tValidation Loss: 2.314726\n",
      "Epoch: 12073 \tTraining Loss: 1.859884 \tValidation Loss: 2.314893\n",
      "Epoch: 12074 \tTraining Loss: 1.811041 \tValidation Loss: 2.315070\n",
      "Epoch: 12075 \tTraining Loss: 1.873938 \tValidation Loss: 2.315395\n",
      "Epoch: 12076 \tTraining Loss: 1.909703 \tValidation Loss: 2.315489\n",
      "Epoch: 12077 \tTraining Loss: 1.847264 \tValidation Loss: 2.315397\n",
      "Epoch: 12078 \tTraining Loss: 1.846534 \tValidation Loss: 2.315331\n",
      "Epoch: 12079 \tTraining Loss: 1.864992 \tValidation Loss: 2.315034\n",
      "Epoch: 12080 \tTraining Loss: 1.841137 \tValidation Loss: 2.315568\n",
      "Epoch: 12081 \tTraining Loss: 1.880102 \tValidation Loss: 2.315756\n",
      "Epoch: 12082 \tTraining Loss: 1.854785 \tValidation Loss: 2.315348\n",
      "Epoch: 12083 \tTraining Loss: 1.851646 \tValidation Loss: 2.315182\n",
      "Epoch: 12084 \tTraining Loss: 1.857768 \tValidation Loss: 2.315470\n",
      "Epoch: 12085 \tTraining Loss: 1.902251 \tValidation Loss: 2.315073\n",
      "Epoch: 12086 \tTraining Loss: 1.864971 \tValidation Loss: 2.314911\n",
      "Epoch: 12087 \tTraining Loss: 1.856983 \tValidation Loss: 2.315241\n",
      "Epoch: 12088 \tTraining Loss: 1.865917 \tValidation Loss: 2.315450\n",
      "Epoch: 12089 \tTraining Loss: 1.860868 \tValidation Loss: 2.315590\n",
      "Epoch: 12090 \tTraining Loss: 1.855010 \tValidation Loss: 2.315559\n",
      "Epoch: 12091 \tTraining Loss: 1.855265 \tValidation Loss: 2.315601\n",
      "Epoch: 12092 \tTraining Loss: 1.874742 \tValidation Loss: 2.315674\n",
      "Epoch: 12093 \tTraining Loss: 1.856763 \tValidation Loss: 2.315900\n",
      "Epoch: 12094 \tTraining Loss: 1.827954 \tValidation Loss: 2.316067\n",
      "Epoch: 12095 \tTraining Loss: 1.849149 \tValidation Loss: 2.315799\n",
      "Epoch: 12096 \tTraining Loss: 1.848555 \tValidation Loss: 2.315924\n",
      "Epoch: 12097 \tTraining Loss: 1.826146 \tValidation Loss: 2.315961\n",
      "Epoch: 12098 \tTraining Loss: 1.884724 \tValidation Loss: 2.315645\n",
      "Epoch: 12099 \tTraining Loss: 1.861250 \tValidation Loss: 2.315390\n",
      "Epoch: 12100 \tTraining Loss: 1.863352 \tValidation Loss: 2.315499\n",
      "Epoch: 12101 \tTraining Loss: 1.863900 \tValidation Loss: 2.315979\n",
      "Epoch: 12102 \tTraining Loss: 1.879917 \tValidation Loss: 2.315772\n",
      "Epoch: 12103 \tTraining Loss: 1.864113 \tValidation Loss: 2.315649\n",
      "Epoch: 12104 \tTraining Loss: 1.886682 \tValidation Loss: 2.315246\n",
      "Epoch: 12105 \tTraining Loss: 1.865793 \tValidation Loss: 2.315141\n",
      "Epoch: 12106 \tTraining Loss: 1.820344 \tValidation Loss: 2.315676\n",
      "Epoch: 12107 \tTraining Loss: 1.897163 \tValidation Loss: 2.315471\n",
      "Epoch: 12108 \tTraining Loss: 1.845816 \tValidation Loss: 2.315520\n",
      "Epoch: 12109 \tTraining Loss: 1.861950 \tValidation Loss: 2.315451\n",
      "Epoch: 12110 \tTraining Loss: 1.894431 \tValidation Loss: 2.315857\n",
      "Epoch: 12111 \tTraining Loss: 1.876372 \tValidation Loss: 2.315302\n",
      "Epoch: 12112 \tTraining Loss: 1.834897 \tValidation Loss: 2.314821\n",
      "Epoch: 12113 \tTraining Loss: 1.880564 \tValidation Loss: 2.314756\n",
      "Epoch: 12114 \tTraining Loss: 1.857277 \tValidation Loss: 2.314736\n",
      "Epoch: 12115 \tTraining Loss: 1.857374 \tValidation Loss: 2.315062\n",
      "Epoch: 12116 \tTraining Loss: 1.851520 \tValidation Loss: 2.315044\n",
      "Epoch: 12117 \tTraining Loss: 1.860469 \tValidation Loss: 2.315245\n",
      "Epoch: 12118 \tTraining Loss: 1.900140 \tValidation Loss: 2.314954\n",
      "Epoch: 12119 \tTraining Loss: 1.842665 \tValidation Loss: 2.315249\n",
      "Epoch: 12120 \tTraining Loss: 1.844964 \tValidation Loss: 2.315499\n",
      "Epoch: 12121 \tTraining Loss: 1.864405 \tValidation Loss: 2.315416\n",
      "Epoch: 12122 \tTraining Loss: 1.878159 \tValidation Loss: 2.315171\n",
      "Epoch: 12123 \tTraining Loss: 1.877981 \tValidation Loss: 2.314837\n",
      "Epoch: 12124 \tTraining Loss: 1.842459 \tValidation Loss: 2.315062\n",
      "Epoch: 12125 \tTraining Loss: 1.843547 \tValidation Loss: 2.315483\n",
      "Epoch: 12126 \tTraining Loss: 1.884105 \tValidation Loss: 2.315630\n",
      "Epoch: 12127 \tTraining Loss: 1.824205 \tValidation Loss: 2.315998\n",
      "Epoch: 12128 \tTraining Loss: 1.819820 \tValidation Loss: 2.316046\n",
      "Epoch: 12129 \tTraining Loss: 1.865517 \tValidation Loss: 2.315603\n",
      "Epoch: 12130 \tTraining Loss: 1.844802 \tValidation Loss: 2.315771\n",
      "Epoch: 12131 \tTraining Loss: 1.844768 \tValidation Loss: 2.315526\n",
      "Epoch: 12132 \tTraining Loss: 1.873932 \tValidation Loss: 2.315090\n",
      "Epoch: 12133 \tTraining Loss: 1.864309 \tValidation Loss: 2.315126\n",
      "Epoch: 12134 \tTraining Loss: 1.828377 \tValidation Loss: 2.315485\n",
      "Epoch: 12135 \tTraining Loss: 1.858753 \tValidation Loss: 2.315263\n",
      "Epoch: 12136 \tTraining Loss: 1.874391 \tValidation Loss: 2.315265\n",
      "Epoch: 12137 \tTraining Loss: 1.864624 \tValidation Loss: 2.315575\n",
      "Epoch: 12138 \tTraining Loss: 1.863048 \tValidation Loss: 2.315755\n",
      "Epoch: 12139 \tTraining Loss: 1.862850 \tValidation Loss: 2.315497\n",
      "Epoch: 12140 \tTraining Loss: 1.857730 \tValidation Loss: 2.315408\n",
      "Epoch: 12141 \tTraining Loss: 1.866000 \tValidation Loss: 2.315398\n",
      "Epoch: 12142 \tTraining Loss: 1.852149 \tValidation Loss: 2.315698\n",
      "Epoch: 12143 \tTraining Loss: 1.878502 \tValidation Loss: 2.315173\n",
      "Epoch: 12144 \tTraining Loss: 1.841834 \tValidation Loss: 2.315527\n",
      "Epoch: 12145 \tTraining Loss: 1.857287 \tValidation Loss: 2.315436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12146 \tTraining Loss: 1.854789 \tValidation Loss: 2.315400\n",
      "Epoch: 12147 \tTraining Loss: 1.873452 \tValidation Loss: 2.315637\n",
      "Epoch: 12148 \tTraining Loss: 1.861347 \tValidation Loss: 2.315595\n",
      "Epoch: 12149 \tTraining Loss: 1.864791 \tValidation Loss: 2.315699\n",
      "Epoch: 12150 \tTraining Loss: 1.869123 \tValidation Loss: 2.315310\n",
      "Epoch: 12151 \tTraining Loss: 1.837337 \tValidation Loss: 2.315560\n",
      "Epoch: 12152 \tTraining Loss: 1.873091 \tValidation Loss: 2.315583\n",
      "Epoch: 12153 \tTraining Loss: 1.840400 \tValidation Loss: 2.315934\n",
      "Epoch: 12154 \tTraining Loss: 1.884970 \tValidation Loss: 2.315901\n",
      "Epoch: 12155 \tTraining Loss: 1.832901 \tValidation Loss: 2.315600\n",
      "Epoch: 12156 \tTraining Loss: 1.845191 \tValidation Loss: 2.315428\n",
      "Epoch: 12157 \tTraining Loss: 1.840147 \tValidation Loss: 2.315815\n",
      "Epoch: 12158 \tTraining Loss: 1.878125 \tValidation Loss: 2.315641\n",
      "Epoch: 12159 \tTraining Loss: 1.868852 \tValidation Loss: 2.315817\n",
      "Epoch: 12160 \tTraining Loss: 1.834574 \tValidation Loss: 2.316104\n",
      "Epoch: 12161 \tTraining Loss: 1.871756 \tValidation Loss: 2.315890\n",
      "Epoch: 12162 \tTraining Loss: 1.870803 \tValidation Loss: 2.315823\n",
      "Epoch: 12163 \tTraining Loss: 1.844621 \tValidation Loss: 2.315977\n",
      "Epoch: 12164 \tTraining Loss: 1.846773 \tValidation Loss: 2.316116\n",
      "Epoch: 12165 \tTraining Loss: 1.831855 \tValidation Loss: 2.316070\n",
      "Epoch: 12166 \tTraining Loss: 1.876992 \tValidation Loss: 2.315781\n",
      "Epoch: 12167 \tTraining Loss: 1.852105 \tValidation Loss: 2.315896\n",
      "Epoch: 12168 \tTraining Loss: 1.862960 \tValidation Loss: 2.315522\n",
      "Epoch: 12169 \tTraining Loss: 1.853407 \tValidation Loss: 2.315525\n",
      "Epoch: 12170 \tTraining Loss: 1.831724 \tValidation Loss: 2.315512\n",
      "Epoch: 12171 \tTraining Loss: 1.854313 \tValidation Loss: 2.315946\n",
      "Epoch: 12172 \tTraining Loss: 1.857661 \tValidation Loss: 2.315757\n",
      "Epoch: 12173 \tTraining Loss: 1.829887 \tValidation Loss: 2.315819\n",
      "Epoch: 12174 \tTraining Loss: 1.853379 \tValidation Loss: 2.315996\n",
      "Epoch: 12175 \tTraining Loss: 1.835145 \tValidation Loss: 2.316332\n",
      "Epoch: 12176 \tTraining Loss: 1.876247 \tValidation Loss: 2.315810\n",
      "Epoch: 12177 \tTraining Loss: 1.866290 \tValidation Loss: 2.315625\n",
      "Epoch: 12178 \tTraining Loss: 1.838104 \tValidation Loss: 2.316083\n",
      "Epoch: 12179 \tTraining Loss: 1.890179 \tValidation Loss: 2.316060\n",
      "Epoch: 12180 \tTraining Loss: 1.847979 \tValidation Loss: 2.316274\n",
      "Epoch: 12181 \tTraining Loss: 1.848370 \tValidation Loss: 2.315959\n",
      "Epoch: 12182 \tTraining Loss: 1.833702 \tValidation Loss: 2.315540\n",
      "Epoch: 12183 \tTraining Loss: 1.887813 \tValidation Loss: 2.315855\n",
      "Epoch: 12184 \tTraining Loss: 1.851712 \tValidation Loss: 2.315807\n",
      "Epoch: 12185 \tTraining Loss: 1.846884 \tValidation Loss: 2.315868\n",
      "Epoch: 12186 \tTraining Loss: 1.867083 \tValidation Loss: 2.316263\n",
      "Epoch: 12187 \tTraining Loss: 1.836333 \tValidation Loss: 2.316194\n",
      "Epoch: 12188 \tTraining Loss: 1.884767 \tValidation Loss: 2.315549\n",
      "Epoch: 12189 \tTraining Loss: 1.849140 \tValidation Loss: 2.315421\n",
      "Epoch: 12190 \tTraining Loss: 1.865127 \tValidation Loss: 2.315554\n",
      "Epoch: 12191 \tTraining Loss: 1.871034 \tValidation Loss: 2.315523\n",
      "Epoch: 12192 \tTraining Loss: 1.838932 \tValidation Loss: 2.315513\n",
      "Epoch: 12193 \tTraining Loss: 1.841924 \tValidation Loss: 2.316044\n",
      "Epoch: 12194 \tTraining Loss: 1.823722 \tValidation Loss: 2.316304\n",
      "Epoch: 12195 \tTraining Loss: 1.848586 \tValidation Loss: 2.316113\n",
      "Epoch: 12196 \tTraining Loss: 1.885191 \tValidation Loss: 2.315845\n",
      "Epoch: 12197 \tTraining Loss: 1.863360 \tValidation Loss: 2.315856\n",
      "Epoch: 12198 \tTraining Loss: 1.891099 \tValidation Loss: 2.315964\n",
      "Epoch: 12199 \tTraining Loss: 1.883527 \tValidation Loss: 2.315776\n",
      "Epoch: 12200 \tTraining Loss: 1.871412 \tValidation Loss: 2.316110\n",
      "Epoch: 12201 \tTraining Loss: 1.887387 \tValidation Loss: 2.315979\n",
      "Epoch: 12202 \tTraining Loss: 1.842619 \tValidation Loss: 2.315936\n",
      "Epoch: 12203 \tTraining Loss: 1.845904 \tValidation Loss: 2.316248\n",
      "Epoch: 12204 \tTraining Loss: 1.847227 \tValidation Loss: 2.315945\n",
      "Epoch: 12205 \tTraining Loss: 1.879050 \tValidation Loss: 2.315936\n",
      "Epoch: 12206 \tTraining Loss: 1.829938 \tValidation Loss: 2.316110\n",
      "Epoch: 12207 \tTraining Loss: 1.845369 \tValidation Loss: 2.316026\n",
      "Epoch: 12208 \tTraining Loss: 1.839514 \tValidation Loss: 2.316324\n",
      "Epoch: 12209 \tTraining Loss: 1.858217 \tValidation Loss: 2.316458\n",
      "Epoch: 12210 \tTraining Loss: 1.876519 \tValidation Loss: 2.316447\n",
      "Epoch: 12211 \tTraining Loss: 1.846973 \tValidation Loss: 2.316385\n",
      "Epoch: 12212 \tTraining Loss: 1.856608 \tValidation Loss: 2.315721\n",
      "Epoch: 12213 \tTraining Loss: 1.878359 \tValidation Loss: 2.316127\n",
      "Epoch: 12214 \tTraining Loss: 1.859124 \tValidation Loss: 2.315924\n",
      "Epoch: 12215 \tTraining Loss: 1.858667 \tValidation Loss: 2.315808\n",
      "Epoch: 12216 \tTraining Loss: 1.853821 \tValidation Loss: 2.315929\n",
      "Epoch: 12217 \tTraining Loss: 1.840307 \tValidation Loss: 2.315752\n",
      "Epoch: 12218 \tTraining Loss: 1.867911 \tValidation Loss: 2.316048\n",
      "Epoch: 12219 \tTraining Loss: 1.839080 \tValidation Loss: 2.316138\n",
      "Epoch: 12220 \tTraining Loss: 1.866533 \tValidation Loss: 2.315798\n",
      "Epoch: 12221 \tTraining Loss: 1.848781 \tValidation Loss: 2.315917\n",
      "Epoch: 12222 \tTraining Loss: 1.845279 \tValidation Loss: 2.316134\n",
      "Epoch: 12223 \tTraining Loss: 1.842248 \tValidation Loss: 2.316145\n",
      "Epoch: 12224 \tTraining Loss: 1.841494 \tValidation Loss: 2.316170\n",
      "Epoch: 12225 \tTraining Loss: 1.864392 \tValidation Loss: 2.316439\n",
      "Epoch: 12226 \tTraining Loss: 1.833443 \tValidation Loss: 2.316331\n",
      "Epoch: 12227 \tTraining Loss: 1.856293 \tValidation Loss: 2.316488\n",
      "Epoch: 12228 \tTraining Loss: 1.841907 \tValidation Loss: 2.316645\n",
      "Epoch: 12229 \tTraining Loss: 1.867014 \tValidation Loss: 2.316231\n",
      "Epoch: 12230 \tTraining Loss: 1.840285 \tValidation Loss: 2.315994\n",
      "Epoch: 12231 \tTraining Loss: 1.858878 \tValidation Loss: 2.316049\n",
      "Epoch: 12232 \tTraining Loss: 1.908088 \tValidation Loss: 2.316082\n",
      "Epoch: 12233 \tTraining Loss: 1.867141 \tValidation Loss: 2.316018\n",
      "Epoch: 12234 \tTraining Loss: 1.904877 \tValidation Loss: 2.315801\n",
      "Epoch: 12235 \tTraining Loss: 1.874136 \tValidation Loss: 2.316171\n",
      "Epoch: 12236 \tTraining Loss: 1.865611 \tValidation Loss: 2.316227\n",
      "Epoch: 12237 \tTraining Loss: 1.891979 \tValidation Loss: 2.316203\n",
      "Epoch: 12238 \tTraining Loss: 1.840044 \tValidation Loss: 2.316154\n",
      "Epoch: 12239 \tTraining Loss: 1.864927 \tValidation Loss: 2.315837\n",
      "Epoch: 12240 \tTraining Loss: 1.892443 \tValidation Loss: 2.315542\n",
      "Epoch: 12241 \tTraining Loss: 1.868209 \tValidation Loss: 2.315699\n",
      "Epoch: 12242 \tTraining Loss: 1.872928 \tValidation Loss: 2.315537\n",
      "Epoch: 12243 \tTraining Loss: 1.900644 \tValidation Loss: 2.315787\n",
      "Epoch: 12244 \tTraining Loss: 1.876000 \tValidation Loss: 2.315655\n",
      "Epoch: 12245 \tTraining Loss: 1.843807 \tValidation Loss: 2.315861\n",
      "Epoch: 12246 \tTraining Loss: 1.873932 \tValidation Loss: 2.315597\n",
      "Epoch: 12247 \tTraining Loss: 1.840372 \tValidation Loss: 2.315877\n",
      "Epoch: 12248 \tTraining Loss: 1.872417 \tValidation Loss: 2.315794\n",
      "Epoch: 12249 \tTraining Loss: 1.870341 \tValidation Loss: 2.316015\n",
      "Epoch: 12250 \tTraining Loss: 1.853222 \tValidation Loss: 2.315906\n",
      "Epoch: 12251 \tTraining Loss: 1.825567 \tValidation Loss: 2.315893\n",
      "Epoch: 12252 \tTraining Loss: 1.861342 \tValidation Loss: 2.316234\n",
      "Epoch: 12253 \tTraining Loss: 1.836589 \tValidation Loss: 2.316580\n",
      "Epoch: 12254 \tTraining Loss: 1.868629 \tValidation Loss: 2.316508\n",
      "Epoch: 12255 \tTraining Loss: 1.848357 \tValidation Loss: 2.316120\n",
      "Epoch: 12256 \tTraining Loss: 1.838369 \tValidation Loss: 2.316365\n",
      "Epoch: 12257 \tTraining Loss: 1.834953 \tValidation Loss: 2.316115\n",
      "Epoch: 12258 \tTraining Loss: 1.845364 \tValidation Loss: 2.316274\n",
      "Epoch: 12259 \tTraining Loss: 1.847732 \tValidation Loss: 2.316909\n",
      "Epoch: 12260 \tTraining Loss: 1.868172 \tValidation Loss: 2.317196\n",
      "Epoch: 12261 \tTraining Loss: 1.857869 \tValidation Loss: 2.317233\n",
      "Epoch: 12262 \tTraining Loss: 1.851870 \tValidation Loss: 2.317006\n",
      "Epoch: 12263 \tTraining Loss: 1.852149 \tValidation Loss: 2.316720\n",
      "Epoch: 12264 \tTraining Loss: 1.837604 \tValidation Loss: 2.316985\n",
      "Epoch: 12265 \tTraining Loss: 1.834066 \tValidation Loss: 2.316744\n",
      "Epoch: 12266 \tTraining Loss: 1.876875 \tValidation Loss: 2.316354\n",
      "Epoch: 12267 \tTraining Loss: 1.811851 \tValidation Loss: 2.316889\n",
      "Epoch: 12268 \tTraining Loss: 1.889722 \tValidation Loss: 2.316606\n",
      "Epoch: 12269 \tTraining Loss: 1.828170 \tValidation Loss: 2.316765\n",
      "Epoch: 12270 \tTraining Loss: 1.856416 \tValidation Loss: 2.316776\n",
      "Epoch: 12271 \tTraining Loss: 1.847412 \tValidation Loss: 2.316751\n",
      "Epoch: 12272 \tTraining Loss: 1.897185 \tValidation Loss: 2.316669\n",
      "Epoch: 12273 \tTraining Loss: 1.833116 \tValidation Loss: 2.316904\n",
      "Epoch: 12274 \tTraining Loss: 1.853558 \tValidation Loss: 2.316590\n",
      "Epoch: 12275 \tTraining Loss: 1.842307 \tValidation Loss: 2.316515\n",
      "Epoch: 12276 \tTraining Loss: 1.832080 \tValidation Loss: 2.316400\n",
      "Epoch: 12277 \tTraining Loss: 1.811378 \tValidation Loss: 2.316272\n",
      "Epoch: 12278 \tTraining Loss: 1.852841 \tValidation Loss: 2.316588\n",
      "Epoch: 12279 \tTraining Loss: 1.849404 \tValidation Loss: 2.316223\n",
      "Epoch: 12280 \tTraining Loss: 1.822479 \tValidation Loss: 2.316228\n",
      "Epoch: 12281 \tTraining Loss: 1.840546 \tValidation Loss: 2.316448\n",
      "Epoch: 12282 \tTraining Loss: 1.857247 \tValidation Loss: 2.316395\n",
      "Epoch: 12283 \tTraining Loss: 1.805181 \tValidation Loss: 2.316577\n",
      "Epoch: 12284 \tTraining Loss: 1.854491 \tValidation Loss: 2.316393\n",
      "Epoch: 12285 \tTraining Loss: 1.846830 \tValidation Loss: 2.316300\n",
      "Epoch: 12286 \tTraining Loss: 1.851481 \tValidation Loss: 2.316402\n",
      "Epoch: 12287 \tTraining Loss: 1.817894 \tValidation Loss: 2.316404\n",
      "Epoch: 12288 \tTraining Loss: 1.848853 \tValidation Loss: 2.316225\n",
      "Epoch: 12289 \tTraining Loss: 1.838849 \tValidation Loss: 2.316429\n",
      "Epoch: 12290 \tTraining Loss: 1.831008 \tValidation Loss: 2.316136\n",
      "Epoch: 12291 \tTraining Loss: 1.848523 \tValidation Loss: 2.316340\n",
      "Epoch: 12292 \tTraining Loss: 1.847390 \tValidation Loss: 2.316720\n",
      "Epoch: 12293 \tTraining Loss: 1.855362 \tValidation Loss: 2.316972\n",
      "Epoch: 12294 \tTraining Loss: 1.840057 \tValidation Loss: 2.316805\n",
      "Epoch: 12295 \tTraining Loss: 1.865428 \tValidation Loss: 2.316766\n",
      "Epoch: 12296 \tTraining Loss: 1.875319 \tValidation Loss: 2.316520\n",
      "Epoch: 12297 \tTraining Loss: 1.834297 \tValidation Loss: 2.316634\n",
      "Epoch: 12298 \tTraining Loss: 1.841883 \tValidation Loss: 2.316430\n",
      "Epoch: 12299 \tTraining Loss: 1.825023 \tValidation Loss: 2.316700\n",
      "Epoch: 12300 \tTraining Loss: 1.838089 \tValidation Loss: 2.316602\n",
      "Epoch: 12301 \tTraining Loss: 1.833317 \tValidation Loss: 2.316413\n",
      "Epoch: 12302 \tTraining Loss: 1.847817 \tValidation Loss: 2.316378\n",
      "Epoch: 12303 \tTraining Loss: 1.817822 \tValidation Loss: 2.316713\n",
      "Epoch: 12304 \tTraining Loss: 1.866841 \tValidation Loss: 2.316427\n",
      "Epoch: 12305 \tTraining Loss: 1.801876 \tValidation Loss: 2.316784\n",
      "Epoch: 12306 \tTraining Loss: 1.848272 \tValidation Loss: 2.316657\n",
      "Epoch: 12307 \tTraining Loss: 1.859733 \tValidation Loss: 2.316543\n",
      "Epoch: 12308 \tTraining Loss: 1.838267 \tValidation Loss: 2.316749\n",
      "Epoch: 12309 \tTraining Loss: 1.833795 \tValidation Loss: 2.316931\n",
      "Epoch: 12310 \tTraining Loss: 1.852876 \tValidation Loss: 2.316778\n",
      "Epoch: 12311 \tTraining Loss: 1.847639 \tValidation Loss: 2.316733\n",
      "Epoch: 12312 \tTraining Loss: 1.876755 \tValidation Loss: 2.316282\n",
      "Epoch: 12313 \tTraining Loss: 1.866338 \tValidation Loss: 2.316482\n",
      "Epoch: 12314 \tTraining Loss: 1.846952 \tValidation Loss: 2.316163\n",
      "Epoch: 12315 \tTraining Loss: 1.822247 \tValidation Loss: 2.316158\n",
      "Epoch: 12316 \tTraining Loss: 1.860274 \tValidation Loss: 2.316101\n",
      "Epoch: 12317 \tTraining Loss: 1.875531 \tValidation Loss: 2.316036\n",
      "Epoch: 12318 \tTraining Loss: 1.822324 \tValidation Loss: 2.316259\n",
      "Epoch: 12319 \tTraining Loss: 1.879660 \tValidation Loss: 2.316514\n",
      "Epoch: 12320 \tTraining Loss: 1.851823 \tValidation Loss: 2.316917\n",
      "Epoch: 12321 \tTraining Loss: 1.849003 \tValidation Loss: 2.317303\n",
      "Epoch: 12322 \tTraining Loss: 1.870062 \tValidation Loss: 2.317421\n",
      "Epoch: 12323 \tTraining Loss: 1.856798 \tValidation Loss: 2.317755\n",
      "Epoch: 12324 \tTraining Loss: 1.866334 \tValidation Loss: 2.317415\n",
      "Epoch: 12325 \tTraining Loss: 1.863648 \tValidation Loss: 2.316997\n",
      "Epoch: 12326 \tTraining Loss: 1.884102 \tValidation Loss: 2.316650\n",
      "Epoch: 12327 \tTraining Loss: 1.844483 \tValidation Loss: 2.316558\n",
      "Epoch: 12328 \tTraining Loss: 1.861221 \tValidation Loss: 2.316499\n",
      "Epoch: 12329 \tTraining Loss: 1.835308 \tValidation Loss: 2.316350\n",
      "Epoch: 12330 \tTraining Loss: 1.820376 \tValidation Loss: 2.316892\n",
      "Epoch: 12331 \tTraining Loss: 1.841168 \tValidation Loss: 2.316810\n",
      "Epoch: 12332 \tTraining Loss: 1.849741 \tValidation Loss: 2.316402\n",
      "Epoch: 12333 \tTraining Loss: 1.834942 \tValidation Loss: 2.316743\n",
      "Epoch: 12334 \tTraining Loss: 1.841319 \tValidation Loss: 2.316614\n",
      "Epoch: 12335 \tTraining Loss: 1.866014 \tValidation Loss: 2.316720\n",
      "Epoch: 12336 \tTraining Loss: 1.847968 \tValidation Loss: 2.316382\n",
      "Epoch: 12337 \tTraining Loss: 1.853272 \tValidation Loss: 2.316938\n",
      "Epoch: 12338 \tTraining Loss: 1.889767 \tValidation Loss: 2.316901\n",
      "Epoch: 12339 \tTraining Loss: 1.866700 \tValidation Loss: 2.316699\n",
      "Epoch: 12340 \tTraining Loss: 1.854249 \tValidation Loss: 2.317353\n",
      "Epoch: 12341 \tTraining Loss: 1.809398 \tValidation Loss: 2.317159\n",
      "Epoch: 12342 \tTraining Loss: 1.821862 \tValidation Loss: 2.317287\n",
      "Epoch: 12343 \tTraining Loss: 1.850248 \tValidation Loss: 2.317255\n",
      "Epoch: 12344 \tTraining Loss: 1.860040 \tValidation Loss: 2.317665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12345 \tTraining Loss: 1.862822 \tValidation Loss: 2.317715\n",
      "Epoch: 12346 \tTraining Loss: 1.856135 \tValidation Loss: 2.317446\n",
      "Epoch: 12347 \tTraining Loss: 1.885402 \tValidation Loss: 2.317443\n",
      "Epoch: 12348 \tTraining Loss: 1.821890 \tValidation Loss: 2.317084\n",
      "Epoch: 12349 \tTraining Loss: 1.884678 \tValidation Loss: 2.317042\n",
      "Epoch: 12350 \tTraining Loss: 1.834397 \tValidation Loss: 2.317280\n",
      "Epoch: 12351 \tTraining Loss: 1.855972 \tValidation Loss: 2.317714\n",
      "Epoch: 12352 \tTraining Loss: 1.885081 \tValidation Loss: 2.317511\n",
      "Epoch: 12353 \tTraining Loss: 1.823994 \tValidation Loss: 2.317806\n",
      "Epoch: 12354 \tTraining Loss: 1.852102 \tValidation Loss: 2.317372\n",
      "Epoch: 12355 \tTraining Loss: 1.846376 \tValidation Loss: 2.317657\n",
      "Epoch: 12356 \tTraining Loss: 1.846618 \tValidation Loss: 2.316971\n",
      "Epoch: 12357 \tTraining Loss: 1.845012 \tValidation Loss: 2.317303\n",
      "Epoch: 12358 \tTraining Loss: 1.848676 \tValidation Loss: 2.317541\n",
      "Epoch: 12359 \tTraining Loss: 1.834274 \tValidation Loss: 2.317769\n",
      "Epoch: 12360 \tTraining Loss: 1.865328 \tValidation Loss: 2.317598\n",
      "Epoch: 12361 \tTraining Loss: 1.848776 \tValidation Loss: 2.317487\n",
      "Epoch: 12362 \tTraining Loss: 1.834342 \tValidation Loss: 2.317873\n",
      "Epoch: 12363 \tTraining Loss: 1.882104 \tValidation Loss: 2.317481\n",
      "Epoch: 12364 \tTraining Loss: 1.839030 \tValidation Loss: 2.317241\n",
      "Epoch: 12365 \tTraining Loss: 1.857227 \tValidation Loss: 2.317273\n",
      "Epoch: 12366 \tTraining Loss: 1.857231 \tValidation Loss: 2.317056\n",
      "Epoch: 12367 \tTraining Loss: 1.845886 \tValidation Loss: 2.317394\n",
      "Epoch: 12368 \tTraining Loss: 1.819875 \tValidation Loss: 2.317578\n",
      "Epoch: 12369 \tTraining Loss: 1.859998 \tValidation Loss: 2.317515\n",
      "Epoch: 12370 \tTraining Loss: 1.852162 \tValidation Loss: 2.318202\n",
      "Epoch: 12371 \tTraining Loss: 1.842444 \tValidation Loss: 2.317882\n",
      "Epoch: 12372 \tTraining Loss: 1.826977 \tValidation Loss: 2.317606\n",
      "Epoch: 12373 \tTraining Loss: 1.844190 \tValidation Loss: 2.317599\n",
      "Epoch: 12374 \tTraining Loss: 1.857546 \tValidation Loss: 2.317348\n",
      "Epoch: 12375 \tTraining Loss: 1.853667 \tValidation Loss: 2.317487\n",
      "Epoch: 12376 \tTraining Loss: 1.850151 \tValidation Loss: 2.317515\n",
      "Epoch: 12377 \tTraining Loss: 1.849372 \tValidation Loss: 2.317306\n",
      "Epoch: 12378 \tTraining Loss: 1.836868 \tValidation Loss: 2.317566\n",
      "Epoch: 12379 \tTraining Loss: 1.867426 \tValidation Loss: 2.317432\n",
      "Epoch: 12380 \tTraining Loss: 1.867358 \tValidation Loss: 2.316988\n",
      "Epoch: 12381 \tTraining Loss: 1.852824 \tValidation Loss: 2.317209\n",
      "Epoch: 12382 \tTraining Loss: 1.835756 \tValidation Loss: 2.316985\n",
      "Epoch: 12383 \tTraining Loss: 1.833120 \tValidation Loss: 2.317246\n",
      "Epoch: 12384 \tTraining Loss: 1.879801 \tValidation Loss: 2.317321\n",
      "Epoch: 12385 \tTraining Loss: 1.851753 \tValidation Loss: 2.317487\n",
      "Epoch: 12386 \tTraining Loss: 1.837209 \tValidation Loss: 2.317635\n",
      "Epoch: 12387 \tTraining Loss: 1.837430 \tValidation Loss: 2.317562\n",
      "Epoch: 12388 \tTraining Loss: 1.850840 \tValidation Loss: 2.317603\n",
      "Epoch: 12389 \tTraining Loss: 1.874085 \tValidation Loss: 2.317739\n",
      "Epoch: 12390 \tTraining Loss: 1.865533 \tValidation Loss: 2.317877\n",
      "Epoch: 12391 \tTraining Loss: 1.831930 \tValidation Loss: 2.318130\n",
      "Epoch: 12392 \tTraining Loss: 1.866255 \tValidation Loss: 2.318459\n",
      "Epoch: 12393 \tTraining Loss: 1.836131 \tValidation Loss: 2.318602\n",
      "Epoch: 12394 \tTraining Loss: 1.823899 \tValidation Loss: 2.318032\n",
      "Epoch: 12395 \tTraining Loss: 1.840153 \tValidation Loss: 2.318138\n",
      "Epoch: 12396 \tTraining Loss: 1.876696 \tValidation Loss: 2.317726\n",
      "Epoch: 12397 \tTraining Loss: 1.888734 \tValidation Loss: 2.317807\n",
      "Epoch: 12398 \tTraining Loss: 1.820269 \tValidation Loss: 2.317699\n",
      "Epoch: 12399 \tTraining Loss: 1.838816 \tValidation Loss: 2.317812\n",
      "Epoch: 12400 \tTraining Loss: 1.832902 \tValidation Loss: 2.318129\n",
      "Epoch: 12401 \tTraining Loss: 1.844517 \tValidation Loss: 2.318044\n",
      "Epoch: 12402 \tTraining Loss: 1.873856 \tValidation Loss: 2.318123\n",
      "Epoch: 12403 \tTraining Loss: 1.845397 \tValidation Loss: 2.318343\n",
      "Epoch: 12404 \tTraining Loss: 1.842160 \tValidation Loss: 2.318288\n",
      "Epoch: 12405 \tTraining Loss: 1.871738 \tValidation Loss: 2.317844\n",
      "Epoch: 12406 \tTraining Loss: 1.840121 \tValidation Loss: 2.318092\n",
      "Epoch: 12407 \tTraining Loss: 1.803653 \tValidation Loss: 2.318087\n",
      "Epoch: 12408 \tTraining Loss: 1.856257 \tValidation Loss: 2.317381\n",
      "Epoch: 12409 \tTraining Loss: 1.891749 \tValidation Loss: 2.316910\n",
      "Epoch: 12410 \tTraining Loss: 1.842071 \tValidation Loss: 2.317557\n",
      "Epoch: 12411 \tTraining Loss: 1.875746 \tValidation Loss: 2.317848\n",
      "Epoch: 12412 \tTraining Loss: 1.855188 \tValidation Loss: 2.318072\n",
      "Epoch: 12413 \tTraining Loss: 1.830396 \tValidation Loss: 2.318485\n",
      "Epoch: 12414 \tTraining Loss: 1.851435 \tValidation Loss: 2.318747\n",
      "Epoch: 12415 \tTraining Loss: 1.858334 \tValidation Loss: 2.318492\n",
      "Epoch: 12416 \tTraining Loss: 1.850776 \tValidation Loss: 2.318601\n",
      "Epoch: 12417 \tTraining Loss: 1.829920 \tValidation Loss: 2.318521\n",
      "Epoch: 12418 \tTraining Loss: 1.839301 \tValidation Loss: 2.318326\n",
      "Epoch: 12419 \tTraining Loss: 1.870538 \tValidation Loss: 2.318432\n",
      "Epoch: 12420 \tTraining Loss: 1.835900 \tValidation Loss: 2.318408\n",
      "Epoch: 12421 \tTraining Loss: 1.824328 \tValidation Loss: 2.318130\n",
      "Epoch: 12422 \tTraining Loss: 1.853128 \tValidation Loss: 2.317949\n",
      "Epoch: 12423 \tTraining Loss: 1.832589 \tValidation Loss: 2.317465\n",
      "Epoch: 12424 \tTraining Loss: 1.826486 \tValidation Loss: 2.317335\n",
      "Epoch: 12425 \tTraining Loss: 1.851012 \tValidation Loss: 2.317614\n",
      "Epoch: 12426 \tTraining Loss: 1.805113 \tValidation Loss: 2.317607\n",
      "Epoch: 12427 \tTraining Loss: 1.855723 \tValidation Loss: 2.317741\n",
      "Epoch: 12428 \tTraining Loss: 1.846309 \tValidation Loss: 2.317851\n",
      "Epoch: 12429 \tTraining Loss: 1.846014 \tValidation Loss: 2.317779\n",
      "Epoch: 12430 \tTraining Loss: 1.841977 \tValidation Loss: 2.317389\n",
      "Epoch: 12431 \tTraining Loss: 1.846482 \tValidation Loss: 2.317917\n",
      "Epoch: 12432 \tTraining Loss: 1.850232 \tValidation Loss: 2.318076\n",
      "Epoch: 12433 \tTraining Loss: 1.819478 \tValidation Loss: 2.317919\n",
      "Epoch: 12434 \tTraining Loss: 1.842480 \tValidation Loss: 2.317885\n",
      "Epoch: 12435 \tTraining Loss: 1.859344 \tValidation Loss: 2.317755\n",
      "Epoch: 12436 \tTraining Loss: 1.874825 \tValidation Loss: 2.318130\n",
      "Epoch: 12437 \tTraining Loss: 1.834966 \tValidation Loss: 2.317966\n",
      "Epoch: 12438 \tTraining Loss: 1.865047 \tValidation Loss: 2.317717\n",
      "Epoch: 12439 \tTraining Loss: 1.845630 \tValidation Loss: 2.317647\n",
      "Epoch: 12440 \tTraining Loss: 1.823293 \tValidation Loss: 2.318036\n",
      "Epoch: 12441 \tTraining Loss: 1.885358 \tValidation Loss: 2.317971\n",
      "Epoch: 12442 \tTraining Loss: 1.864488 \tValidation Loss: 2.317860\n",
      "Epoch: 12443 \tTraining Loss: 1.865329 \tValidation Loss: 2.318401\n",
      "Epoch: 12444 \tTraining Loss: 1.867055 \tValidation Loss: 2.318028\n",
      "Epoch: 12445 \tTraining Loss: 1.808009 \tValidation Loss: 2.318714\n",
      "Epoch: 12446 \tTraining Loss: 1.879453 \tValidation Loss: 2.318488\n",
      "Epoch: 12447 \tTraining Loss: 1.872319 \tValidation Loss: 2.318393\n",
      "Epoch: 12448 \tTraining Loss: 1.797706 \tValidation Loss: 2.318430\n",
      "Epoch: 12449 \tTraining Loss: 1.830409 \tValidation Loss: 2.318500\n",
      "Epoch: 12450 \tTraining Loss: 1.844941 \tValidation Loss: 2.318961\n",
      "Epoch: 12451 \tTraining Loss: 1.861587 \tValidation Loss: 2.318858\n",
      "Epoch: 12452 \tTraining Loss: 1.862855 \tValidation Loss: 2.318623\n",
      "Epoch: 12453 \tTraining Loss: 1.852942 \tValidation Loss: 2.318395\n",
      "Epoch: 12454 \tTraining Loss: 1.858835 \tValidation Loss: 2.318287\n",
      "Epoch: 12455 \tTraining Loss: 1.867317 \tValidation Loss: 2.318091\n",
      "Epoch: 12456 \tTraining Loss: 1.858565 \tValidation Loss: 2.318100\n",
      "Epoch: 12457 \tTraining Loss: 1.845147 \tValidation Loss: 2.318524\n",
      "Epoch: 12458 \tTraining Loss: 1.823756 \tValidation Loss: 2.317930\n",
      "Epoch: 12459 \tTraining Loss: 1.872394 \tValidation Loss: 2.318114\n",
      "Epoch: 12460 \tTraining Loss: 1.839930 \tValidation Loss: 2.318212\n",
      "Epoch: 12461 \tTraining Loss: 1.854440 \tValidation Loss: 2.317894\n",
      "Epoch: 12462 \tTraining Loss: 1.829279 \tValidation Loss: 2.317773\n",
      "Epoch: 12463 \tTraining Loss: 1.862796 \tValidation Loss: 2.318216\n",
      "Epoch: 12464 \tTraining Loss: 1.829862 \tValidation Loss: 2.318283\n",
      "Epoch: 12465 \tTraining Loss: 1.836560 \tValidation Loss: 2.318045\n",
      "Epoch: 12466 \tTraining Loss: 1.863290 \tValidation Loss: 2.318064\n",
      "Epoch: 12467 \tTraining Loss: 1.833770 \tValidation Loss: 2.318291\n",
      "Epoch: 12468 \tTraining Loss: 1.847428 \tValidation Loss: 2.318358\n",
      "Epoch: 12469 \tTraining Loss: 1.838804 \tValidation Loss: 2.318144\n",
      "Epoch: 12470 \tTraining Loss: 1.844935 \tValidation Loss: 2.317996\n",
      "Epoch: 12471 \tTraining Loss: 1.816553 \tValidation Loss: 2.318342\n",
      "Epoch: 12472 \tTraining Loss: 1.834766 \tValidation Loss: 2.318534\n",
      "Epoch: 12473 \tTraining Loss: 1.838477 \tValidation Loss: 2.318561\n",
      "Epoch: 12474 \tTraining Loss: 1.844584 \tValidation Loss: 2.318225\n",
      "Epoch: 12475 \tTraining Loss: 1.845430 \tValidation Loss: 2.318228\n",
      "Epoch: 12476 \tTraining Loss: 1.824881 \tValidation Loss: 2.318300\n",
      "Epoch: 12477 \tTraining Loss: 1.833672 \tValidation Loss: 2.318129\n",
      "Epoch: 12478 \tTraining Loss: 1.852396 \tValidation Loss: 2.318226\n",
      "Epoch: 12479 \tTraining Loss: 1.801544 \tValidation Loss: 2.318690\n",
      "Epoch: 12480 \tTraining Loss: 1.827842 \tValidation Loss: 2.318658\n",
      "Epoch: 12481 \tTraining Loss: 1.874082 \tValidation Loss: 2.318380\n",
      "Epoch: 12482 \tTraining Loss: 1.816898 \tValidation Loss: 2.318381\n",
      "Epoch: 12483 \tTraining Loss: 1.839723 \tValidation Loss: 2.317687\n",
      "Epoch: 12484 \tTraining Loss: 1.864281 \tValidation Loss: 2.317813\n",
      "Epoch: 12485 \tTraining Loss: 1.835736 \tValidation Loss: 2.317863\n",
      "Epoch: 12486 \tTraining Loss: 1.790578 \tValidation Loss: 2.318437\n",
      "Epoch: 12487 \tTraining Loss: 1.820168 \tValidation Loss: 2.318434\n",
      "Epoch: 12488 \tTraining Loss: 1.847564 \tValidation Loss: 2.318604\n",
      "Epoch: 12489 \tTraining Loss: 1.852953 \tValidation Loss: 2.318671\n",
      "Epoch: 12490 \tTraining Loss: 1.838594 \tValidation Loss: 2.318597\n",
      "Epoch: 12491 \tTraining Loss: 1.806681 \tValidation Loss: 2.318659\n",
      "Epoch: 12492 \tTraining Loss: 1.884099 \tValidation Loss: 2.318455\n",
      "Epoch: 12493 \tTraining Loss: 1.850050 \tValidation Loss: 2.318725\n",
      "Epoch: 12494 \tTraining Loss: 1.869700 \tValidation Loss: 2.318517\n",
      "Epoch: 12495 \tTraining Loss: 1.829673 \tValidation Loss: 2.318567\n",
      "Epoch: 12496 \tTraining Loss: 1.853712 \tValidation Loss: 2.318619\n",
      "Epoch: 12497 \tTraining Loss: 1.830206 \tValidation Loss: 2.318547\n",
      "Epoch: 12498 \tTraining Loss: 1.845238 \tValidation Loss: 2.318217\n",
      "Epoch: 12499 \tTraining Loss: 1.867311 \tValidation Loss: 2.318115\n",
      "Epoch: 12500 \tTraining Loss: 1.831102 \tValidation Loss: 2.318644\n",
      "Epoch: 12501 \tTraining Loss: 1.823488 \tValidation Loss: 2.318532\n",
      "Epoch: 12502 \tTraining Loss: 1.835094 \tValidation Loss: 2.318466\n",
      "Epoch: 12503 \tTraining Loss: 1.832788 \tValidation Loss: 2.318780\n",
      "Epoch: 12504 \tTraining Loss: 1.817772 \tValidation Loss: 2.318397\n",
      "Epoch: 12505 \tTraining Loss: 1.827730 \tValidation Loss: 2.318638\n",
      "Epoch: 12506 \tTraining Loss: 1.850749 \tValidation Loss: 2.318702\n",
      "Epoch: 12507 \tTraining Loss: 1.837231 \tValidation Loss: 2.318674\n",
      "Epoch: 12508 \tTraining Loss: 1.816348 \tValidation Loss: 2.318607\n",
      "Epoch: 12509 \tTraining Loss: 1.831236 \tValidation Loss: 2.318622\n",
      "Epoch: 12510 \tTraining Loss: 1.796795 \tValidation Loss: 2.318591\n",
      "Epoch: 12511 \tTraining Loss: 1.827175 \tValidation Loss: 2.319117\n",
      "Epoch: 12512 \tTraining Loss: 1.834044 \tValidation Loss: 2.318808\n",
      "Epoch: 12513 \tTraining Loss: 1.852965 \tValidation Loss: 2.318830\n",
      "Epoch: 12514 \tTraining Loss: 1.816466 \tValidation Loss: 2.319132\n",
      "Epoch: 12515 \tTraining Loss: 1.814028 \tValidation Loss: 2.319326\n",
      "Epoch: 12516 \tTraining Loss: 1.821426 \tValidation Loss: 2.318823\n",
      "Epoch: 12517 \tTraining Loss: 1.813805 \tValidation Loss: 2.318638\n",
      "Epoch: 12518 \tTraining Loss: 1.818278 \tValidation Loss: 2.318415\n",
      "Epoch: 12519 \tTraining Loss: 1.878662 \tValidation Loss: 2.318363\n",
      "Epoch: 12520 \tTraining Loss: 1.831271 \tValidation Loss: 2.318445\n",
      "Epoch: 12521 \tTraining Loss: 1.827719 \tValidation Loss: 2.318455\n",
      "Epoch: 12522 \tTraining Loss: 1.827181 \tValidation Loss: 2.318637\n",
      "Epoch: 12523 \tTraining Loss: 1.822775 \tValidation Loss: 2.318621\n",
      "Epoch: 12524 \tTraining Loss: 1.841249 \tValidation Loss: 2.318433\n",
      "Epoch: 12525 \tTraining Loss: 1.842425 \tValidation Loss: 2.318509\n",
      "Epoch: 12526 \tTraining Loss: 1.874085 \tValidation Loss: 2.318193\n",
      "Epoch: 12527 \tTraining Loss: 1.834314 \tValidation Loss: 2.318573\n",
      "Epoch: 12528 \tTraining Loss: 1.837683 \tValidation Loss: 2.318911\n",
      "Epoch: 12529 \tTraining Loss: 1.827467 \tValidation Loss: 2.319066\n",
      "Epoch: 12530 \tTraining Loss: 1.844808 \tValidation Loss: 2.318597\n",
      "Epoch: 12531 \tTraining Loss: 1.867519 \tValidation Loss: 2.318989\n",
      "Epoch: 12532 \tTraining Loss: 1.857538 \tValidation Loss: 2.319288\n",
      "Epoch: 12533 \tTraining Loss: 1.828271 \tValidation Loss: 2.319083\n",
      "Epoch: 12534 \tTraining Loss: 1.828357 \tValidation Loss: 2.318748\n",
      "Epoch: 12535 \tTraining Loss: 1.871670 \tValidation Loss: 2.318982\n",
      "Epoch: 12536 \tTraining Loss: 1.851327 \tValidation Loss: 2.318907\n",
      "Epoch: 12537 \tTraining Loss: 1.841128 \tValidation Loss: 2.318891\n",
      "Epoch: 12538 \tTraining Loss: 1.869009 \tValidation Loss: 2.318839\n",
      "Epoch: 12539 \tTraining Loss: 1.832646 \tValidation Loss: 2.318653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12540 \tTraining Loss: 1.830944 \tValidation Loss: 2.318303\n",
      "Epoch: 12541 \tTraining Loss: 1.827049 \tValidation Loss: 2.318010\n",
      "Epoch: 12542 \tTraining Loss: 1.843579 \tValidation Loss: 2.318115\n",
      "Epoch: 12543 \tTraining Loss: 1.839944 \tValidation Loss: 2.318264\n",
      "Epoch: 12544 \tTraining Loss: 1.843038 \tValidation Loss: 2.318359\n",
      "Epoch: 12545 \tTraining Loss: 1.815329 \tValidation Loss: 2.318588\n",
      "Epoch: 12546 \tTraining Loss: 1.813706 \tValidation Loss: 2.318532\n",
      "Epoch: 12547 \tTraining Loss: 1.833465 \tValidation Loss: 2.318611\n",
      "Epoch: 12548 \tTraining Loss: 1.791950 \tValidation Loss: 2.318753\n",
      "Epoch: 12549 \tTraining Loss: 1.803910 \tValidation Loss: 2.318882\n",
      "Epoch: 12550 \tTraining Loss: 1.837682 \tValidation Loss: 2.319340\n",
      "Epoch: 12551 \tTraining Loss: 1.863581 \tValidation Loss: 2.319043\n",
      "Epoch: 12552 \tTraining Loss: 1.872678 \tValidation Loss: 2.319237\n",
      "Epoch: 12553 \tTraining Loss: 1.874688 \tValidation Loss: 2.319078\n",
      "Epoch: 12554 \tTraining Loss: 1.829555 \tValidation Loss: 2.318934\n",
      "Epoch: 12555 \tTraining Loss: 1.811719 \tValidation Loss: 2.318723\n",
      "Epoch: 12556 \tTraining Loss: 1.832873 \tValidation Loss: 2.319149\n",
      "Epoch: 12557 \tTraining Loss: 1.835974 \tValidation Loss: 2.319057\n",
      "Epoch: 12558 \tTraining Loss: 1.827537 \tValidation Loss: 2.318504\n",
      "Epoch: 12559 \tTraining Loss: 1.833555 \tValidation Loss: 2.318231\n",
      "Epoch: 12560 \tTraining Loss: 1.833673 \tValidation Loss: 2.318326\n",
      "Epoch: 12561 \tTraining Loss: 1.816257 \tValidation Loss: 2.318290\n",
      "Epoch: 12562 \tTraining Loss: 1.798057 \tValidation Loss: 2.318105\n",
      "Epoch: 12563 \tTraining Loss: 1.835946 \tValidation Loss: 2.318148\n",
      "Epoch: 12564 \tTraining Loss: 1.829299 \tValidation Loss: 2.318434\n",
      "Epoch: 12565 \tTraining Loss: 1.833673 \tValidation Loss: 2.318977\n",
      "Epoch: 12566 \tTraining Loss: 1.832721 \tValidation Loss: 2.319195\n",
      "Epoch: 12567 \tTraining Loss: 1.858901 \tValidation Loss: 2.318719\n",
      "Epoch: 12568 \tTraining Loss: 1.827541 \tValidation Loss: 2.318709\n",
      "Epoch: 12569 \tTraining Loss: 1.823545 \tValidation Loss: 2.318699\n",
      "Epoch: 12570 \tTraining Loss: 1.867465 \tValidation Loss: 2.318333\n",
      "Epoch: 12571 \tTraining Loss: 1.859575 \tValidation Loss: 2.318236\n",
      "Epoch: 12572 \tTraining Loss: 1.850396 \tValidation Loss: 2.318516\n",
      "Epoch: 12573 \tTraining Loss: 1.882314 \tValidation Loss: 2.318379\n",
      "Epoch: 12574 \tTraining Loss: 1.821956 \tValidation Loss: 2.318302\n",
      "Epoch: 12575 \tTraining Loss: 1.838367 \tValidation Loss: 2.318690\n",
      "Epoch: 12576 \tTraining Loss: 1.856891 \tValidation Loss: 2.318980\n",
      "Epoch: 12577 \tTraining Loss: 1.861216 \tValidation Loss: 2.318931\n",
      "Epoch: 12578 \tTraining Loss: 1.887051 \tValidation Loss: 2.318598\n",
      "Epoch: 12579 \tTraining Loss: 1.813446 \tValidation Loss: 2.318854\n",
      "Epoch: 12580 \tTraining Loss: 1.846806 \tValidation Loss: 2.318558\n",
      "Epoch: 12581 \tTraining Loss: 1.825259 \tValidation Loss: 2.318279\n",
      "Epoch: 12582 \tTraining Loss: 1.811539 \tValidation Loss: 2.318386\n",
      "Epoch: 12583 \tTraining Loss: 1.832672 \tValidation Loss: 2.318525\n",
      "Epoch: 12584 \tTraining Loss: 1.805038 \tValidation Loss: 2.318458\n",
      "Epoch: 12585 \tTraining Loss: 1.823202 \tValidation Loss: 2.318770\n",
      "Epoch: 12586 \tTraining Loss: 1.821860 \tValidation Loss: 2.318286\n",
      "Epoch: 12587 \tTraining Loss: 1.858072 \tValidation Loss: 2.318465\n",
      "Epoch: 12588 \tTraining Loss: 1.843688 \tValidation Loss: 2.318537\n",
      "Epoch: 12589 \tTraining Loss: 1.821787 \tValidation Loss: 2.318945\n",
      "Epoch: 12590 \tTraining Loss: 1.850295 \tValidation Loss: 2.319017\n",
      "Epoch: 12591 \tTraining Loss: 1.851235 \tValidation Loss: 2.318776\n",
      "Epoch: 12592 \tTraining Loss: 1.864812 \tValidation Loss: 2.318594\n",
      "Epoch: 12593 \tTraining Loss: 1.832739 \tValidation Loss: 2.318581\n",
      "Epoch: 12594 \tTraining Loss: 1.851875 \tValidation Loss: 2.318706\n",
      "Epoch: 12595 \tTraining Loss: 1.839802 \tValidation Loss: 2.318753\n",
      "Epoch: 12596 \tTraining Loss: 1.880199 \tValidation Loss: 2.318643\n",
      "Epoch: 12597 \tTraining Loss: 1.789217 \tValidation Loss: 2.318940\n",
      "Epoch: 12598 \tTraining Loss: 1.841059 \tValidation Loss: 2.318739\n",
      "Epoch: 12599 \tTraining Loss: 1.848303 \tValidation Loss: 2.318924\n",
      "Epoch: 12600 \tTraining Loss: 1.831665 \tValidation Loss: 2.318973\n",
      "Epoch: 12601 \tTraining Loss: 1.830759 \tValidation Loss: 2.319264\n",
      "Epoch: 12602 \tTraining Loss: 1.864171 \tValidation Loss: 2.318708\n",
      "Epoch: 12603 \tTraining Loss: 1.821265 \tValidation Loss: 2.318860\n",
      "Epoch: 12604 \tTraining Loss: 1.878328 \tValidation Loss: 2.318603\n",
      "Epoch: 12605 \tTraining Loss: 1.839631 \tValidation Loss: 2.318793\n",
      "Epoch: 12606 \tTraining Loss: 1.840322 \tValidation Loss: 2.318980\n",
      "Epoch: 12607 \tTraining Loss: 1.827178 \tValidation Loss: 2.318843\n",
      "Epoch: 12608 \tTraining Loss: 1.819980 \tValidation Loss: 2.318617\n",
      "Epoch: 12609 \tTraining Loss: 1.852185 \tValidation Loss: 2.318624\n",
      "Epoch: 12610 \tTraining Loss: 1.822281 \tValidation Loss: 2.319033\n",
      "Epoch: 12611 \tTraining Loss: 1.822136 \tValidation Loss: 2.319026\n",
      "Epoch: 12612 \tTraining Loss: 1.811614 \tValidation Loss: 2.318802\n",
      "Epoch: 12613 \tTraining Loss: 1.884181 \tValidation Loss: 2.318737\n",
      "Epoch: 12614 \tTraining Loss: 1.844140 \tValidation Loss: 2.318744\n",
      "Epoch: 12615 \tTraining Loss: 1.810794 \tValidation Loss: 2.319178\n",
      "Epoch: 12616 \tTraining Loss: 1.829918 \tValidation Loss: 2.319210\n",
      "Epoch: 12617 \tTraining Loss: 1.798669 \tValidation Loss: 2.318546\n",
      "Epoch: 12618 \tTraining Loss: 1.842091 \tValidation Loss: 2.319043\n",
      "Epoch: 12619 \tTraining Loss: 1.830254 \tValidation Loss: 2.318969\n",
      "Epoch: 12620 \tTraining Loss: 1.822538 \tValidation Loss: 2.319173\n",
      "Epoch: 12621 \tTraining Loss: 1.832256 \tValidation Loss: 2.318983\n",
      "Epoch: 12622 \tTraining Loss: 1.797741 \tValidation Loss: 2.319071\n",
      "Epoch: 12623 \tTraining Loss: 1.839223 \tValidation Loss: 2.319105\n",
      "Epoch: 12624 \tTraining Loss: 1.841204 \tValidation Loss: 2.318873\n",
      "Epoch: 12625 \tTraining Loss: 1.805902 \tValidation Loss: 2.319127\n",
      "Epoch: 12626 \tTraining Loss: 1.858911 \tValidation Loss: 2.319408\n",
      "Epoch: 12627 \tTraining Loss: 1.825948 \tValidation Loss: 2.319070\n",
      "Epoch: 12628 \tTraining Loss: 1.848989 \tValidation Loss: 2.318878\n",
      "Epoch: 12629 \tTraining Loss: 1.855309 \tValidation Loss: 2.318914\n",
      "Epoch: 12630 \tTraining Loss: 1.857019 \tValidation Loss: 2.319261\n",
      "Epoch: 12631 \tTraining Loss: 1.833090 \tValidation Loss: 2.319825\n",
      "Epoch: 12632 \tTraining Loss: 1.853462 \tValidation Loss: 2.319655\n",
      "Epoch: 12633 \tTraining Loss: 1.805129 \tValidation Loss: 2.319733\n",
      "Epoch: 12634 \tTraining Loss: 1.824532 \tValidation Loss: 2.319928\n",
      "Epoch: 12635 \tTraining Loss: 1.847799 \tValidation Loss: 2.319706\n",
      "Epoch: 12636 \tTraining Loss: 1.869419 \tValidation Loss: 2.319512\n",
      "Epoch: 12637 \tTraining Loss: 1.813081 \tValidation Loss: 2.319854\n",
      "Epoch: 12638 \tTraining Loss: 1.815497 \tValidation Loss: 2.319422\n",
      "Epoch: 12639 \tTraining Loss: 1.803957 \tValidation Loss: 2.319807\n",
      "Epoch: 12640 \tTraining Loss: 1.818926 \tValidation Loss: 2.319669\n",
      "Epoch: 12641 \tTraining Loss: 1.825711 \tValidation Loss: 2.319634\n",
      "Epoch: 12642 \tTraining Loss: 1.814487 \tValidation Loss: 2.319447\n",
      "Epoch: 12643 \tTraining Loss: 1.811515 \tValidation Loss: 2.319366\n",
      "Epoch: 12644 \tTraining Loss: 1.830530 \tValidation Loss: 2.319341\n",
      "Epoch: 12645 \tTraining Loss: 1.815762 \tValidation Loss: 2.319106\n",
      "Epoch: 12646 \tTraining Loss: 1.833269 \tValidation Loss: 2.319141\n",
      "Epoch: 12647 \tTraining Loss: 1.835632 \tValidation Loss: 2.318835\n",
      "Epoch: 12648 \tTraining Loss: 1.837803 \tValidation Loss: 2.318506\n",
      "Epoch: 12649 \tTraining Loss: 1.842329 \tValidation Loss: 2.318861\n",
      "Epoch: 12650 \tTraining Loss: 1.813348 \tValidation Loss: 2.319120\n",
      "Epoch: 12651 \tTraining Loss: 1.855042 \tValidation Loss: 2.318937\n",
      "Epoch: 12652 \tTraining Loss: 1.847346 \tValidation Loss: 2.318926\n",
      "Epoch: 12653 \tTraining Loss: 1.815733 \tValidation Loss: 2.318986\n",
      "Epoch: 12654 \tTraining Loss: 1.853731 \tValidation Loss: 2.319450\n",
      "Epoch: 12655 \tTraining Loss: 1.806638 \tValidation Loss: 2.319716\n",
      "Epoch: 12656 \tTraining Loss: 1.820296 \tValidation Loss: 2.319888\n",
      "Epoch: 12657 \tTraining Loss: 1.861356 \tValidation Loss: 2.320033\n",
      "Epoch: 12658 \tTraining Loss: 1.844327 \tValidation Loss: 2.320138\n",
      "Epoch: 12659 \tTraining Loss: 1.848943 \tValidation Loss: 2.319916\n",
      "Epoch: 12660 \tTraining Loss: 1.826457 \tValidation Loss: 2.319861\n",
      "Epoch: 12661 \tTraining Loss: 1.850852 \tValidation Loss: 2.319586\n",
      "Epoch: 12662 \tTraining Loss: 1.883376 \tValidation Loss: 2.319895\n",
      "Epoch: 12663 \tTraining Loss: 1.824871 \tValidation Loss: 2.320117\n",
      "Epoch: 12664 \tTraining Loss: 1.829912 \tValidation Loss: 2.319925\n",
      "Epoch: 12665 \tTraining Loss: 1.806691 \tValidation Loss: 2.319684\n",
      "Epoch: 12666 \tTraining Loss: 1.840258 \tValidation Loss: 2.319604\n",
      "Epoch: 12667 \tTraining Loss: 1.873950 \tValidation Loss: 2.318827\n",
      "Epoch: 12668 \tTraining Loss: 1.826509 \tValidation Loss: 2.318503\n",
      "Epoch: 12669 \tTraining Loss: 1.847555 \tValidation Loss: 2.318868\n",
      "Epoch: 12670 \tTraining Loss: 1.822830 \tValidation Loss: 2.319039\n",
      "Epoch: 12671 \tTraining Loss: 1.825255 \tValidation Loss: 2.319273\n",
      "Epoch: 12672 \tTraining Loss: 1.836195 \tValidation Loss: 2.319361\n",
      "Epoch: 12673 \tTraining Loss: 1.841397 \tValidation Loss: 2.319747\n",
      "Epoch: 12674 \tTraining Loss: 1.814994 \tValidation Loss: 2.319500\n",
      "Epoch: 12675 \tTraining Loss: 1.818962 \tValidation Loss: 2.319551\n",
      "Epoch: 12676 \tTraining Loss: 1.800658 \tValidation Loss: 2.319761\n",
      "Epoch: 12677 \tTraining Loss: 1.869186 \tValidation Loss: 2.319386\n",
      "Epoch: 12678 \tTraining Loss: 1.849096 \tValidation Loss: 2.319824\n",
      "Epoch: 12679 \tTraining Loss: 1.821717 \tValidation Loss: 2.319725\n",
      "Epoch: 12680 \tTraining Loss: 1.844925 \tValidation Loss: 2.319887\n",
      "Epoch: 12681 \tTraining Loss: 1.862076 \tValidation Loss: 2.319975\n",
      "Epoch: 12682 \tTraining Loss: 1.842779 \tValidation Loss: 2.319752\n",
      "Epoch: 12683 \tTraining Loss: 1.817007 \tValidation Loss: 2.320010\n",
      "Epoch: 12684 \tTraining Loss: 1.826999 \tValidation Loss: 2.320226\n",
      "Epoch: 12685 \tTraining Loss: 1.811409 \tValidation Loss: 2.320416\n",
      "Epoch: 12686 \tTraining Loss: 1.845376 \tValidation Loss: 2.320204\n",
      "Epoch: 12687 \tTraining Loss: 1.823116 \tValidation Loss: 2.319686\n",
      "Epoch: 12688 \tTraining Loss: 1.842523 \tValidation Loss: 2.319942\n",
      "Epoch: 12689 \tTraining Loss: 1.828982 \tValidation Loss: 2.320020\n",
      "Epoch: 12690 \tTraining Loss: 1.828407 \tValidation Loss: 2.320045\n",
      "Epoch: 12691 \tTraining Loss: 1.836864 \tValidation Loss: 2.319915\n",
      "Epoch: 12692 \tTraining Loss: 1.847784 \tValidation Loss: 2.320211\n",
      "Epoch: 12693 \tTraining Loss: 1.823490 \tValidation Loss: 2.320052\n",
      "Epoch: 12694 \tTraining Loss: 1.836100 \tValidation Loss: 2.319961\n",
      "Epoch: 12695 \tTraining Loss: 1.853594 \tValidation Loss: 2.319301\n",
      "Epoch: 12696 \tTraining Loss: 1.860144 \tValidation Loss: 2.319657\n",
      "Epoch: 12697 \tTraining Loss: 1.853747 \tValidation Loss: 2.319149\n",
      "Epoch: 12698 \tTraining Loss: 1.836762 \tValidation Loss: 2.319396\n",
      "Epoch: 12699 \tTraining Loss: 1.861641 \tValidation Loss: 2.319407\n",
      "Epoch: 12700 \tTraining Loss: 1.835660 \tValidation Loss: 2.319765\n",
      "Epoch: 12701 \tTraining Loss: 1.825562 \tValidation Loss: 2.319376\n",
      "Epoch: 12702 \tTraining Loss: 1.808886 \tValidation Loss: 2.319720\n",
      "Epoch: 12703 \tTraining Loss: 1.823591 \tValidation Loss: 2.319485\n",
      "Epoch: 12704 \tTraining Loss: 1.791258 \tValidation Loss: 2.319441\n",
      "Epoch: 12705 \tTraining Loss: 1.845088 \tValidation Loss: 2.319469\n",
      "Epoch: 12706 \tTraining Loss: 1.823671 \tValidation Loss: 2.319087\n",
      "Epoch: 12707 \tTraining Loss: 1.828033 \tValidation Loss: 2.319357\n",
      "Epoch: 12708 \tTraining Loss: 1.834581 \tValidation Loss: 2.319493\n",
      "Epoch: 12709 \tTraining Loss: 1.854903 \tValidation Loss: 2.319950\n",
      "Epoch: 12710 \tTraining Loss: 1.821213 \tValidation Loss: 2.320470\n",
      "Epoch: 12711 \tTraining Loss: 1.820139 \tValidation Loss: 2.320649\n",
      "Epoch: 12712 \tTraining Loss: 1.832248 \tValidation Loss: 2.320335\n",
      "Epoch: 12713 \tTraining Loss: 1.790527 \tValidation Loss: 2.320775\n",
      "Epoch: 12714 \tTraining Loss: 1.800539 \tValidation Loss: 2.320719\n",
      "Epoch: 12715 \tTraining Loss: 1.851871 \tValidation Loss: 2.320253\n",
      "Epoch: 12716 \tTraining Loss: 1.812025 \tValidation Loss: 2.320245\n",
      "Epoch: 12717 \tTraining Loss: 1.831599 \tValidation Loss: 2.319824\n",
      "Epoch: 12718 \tTraining Loss: 1.818140 \tValidation Loss: 2.319577\n",
      "Epoch: 12719 \tTraining Loss: 1.842746 \tValidation Loss: 2.319458\n",
      "Epoch: 12720 \tTraining Loss: 1.852589 \tValidation Loss: 2.319350\n",
      "Epoch: 12721 \tTraining Loss: 1.819703 \tValidation Loss: 2.319357\n",
      "Epoch: 12722 \tTraining Loss: 1.852381 \tValidation Loss: 2.319427\n",
      "Epoch: 12723 \tTraining Loss: 1.855427 \tValidation Loss: 2.319679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12724 \tTraining Loss: 1.820674 \tValidation Loss: 2.319862\n",
      "Epoch: 12725 \tTraining Loss: 1.816071 \tValidation Loss: 2.319771\n",
      "Epoch: 12726 \tTraining Loss: 1.861636 \tValidation Loss: 2.319769\n",
      "Epoch: 12727 \tTraining Loss: 1.831672 \tValidation Loss: 2.320190\n",
      "Epoch: 12728 \tTraining Loss: 1.849771 \tValidation Loss: 2.320330\n",
      "Epoch: 12729 \tTraining Loss: 1.850700 \tValidation Loss: 2.320144\n",
      "Epoch: 12730 \tTraining Loss: 1.856316 \tValidation Loss: 2.320357\n",
      "Epoch: 12731 \tTraining Loss: 1.819569 \tValidation Loss: 2.319834\n",
      "Epoch: 12732 \tTraining Loss: 1.856518 \tValidation Loss: 2.319724\n",
      "Epoch: 12733 \tTraining Loss: 1.817081 \tValidation Loss: 2.319918\n",
      "Epoch: 12734 \tTraining Loss: 1.824740 \tValidation Loss: 2.319634\n",
      "Epoch: 12735 \tTraining Loss: 1.828908 \tValidation Loss: 2.320015\n",
      "Epoch: 12736 \tTraining Loss: 1.812870 \tValidation Loss: 2.319921\n",
      "Epoch: 12737 \tTraining Loss: 1.813364 \tValidation Loss: 2.319676\n",
      "Epoch: 12738 \tTraining Loss: 1.845365 \tValidation Loss: 2.319617\n",
      "Epoch: 12739 \tTraining Loss: 1.838510 \tValidation Loss: 2.320060\n",
      "Epoch: 12740 \tTraining Loss: 1.832037 \tValidation Loss: 2.319679\n",
      "Epoch: 12741 \tTraining Loss: 1.818305 \tValidation Loss: 2.320098\n",
      "Epoch: 12742 \tTraining Loss: 1.841929 \tValidation Loss: 2.320376\n",
      "Epoch: 12743 \tTraining Loss: 1.851867 \tValidation Loss: 2.320115\n",
      "Epoch: 12744 \tTraining Loss: 1.819038 \tValidation Loss: 2.320128\n",
      "Epoch: 12745 \tTraining Loss: 1.824448 \tValidation Loss: 2.319964\n",
      "Epoch: 12746 \tTraining Loss: 1.824574 \tValidation Loss: 2.320144\n",
      "Epoch: 12747 \tTraining Loss: 1.813072 \tValidation Loss: 2.320020\n",
      "Epoch: 12748 \tTraining Loss: 1.825162 \tValidation Loss: 2.320015\n",
      "Epoch: 12749 \tTraining Loss: 1.843934 \tValidation Loss: 2.319858\n",
      "Epoch: 12750 \tTraining Loss: 1.808518 \tValidation Loss: 2.319846\n",
      "Epoch: 12751 \tTraining Loss: 1.843933 \tValidation Loss: 2.320034\n",
      "Epoch: 12752 \tTraining Loss: 1.840488 \tValidation Loss: 2.319842\n",
      "Epoch: 12753 \tTraining Loss: 1.817421 \tValidation Loss: 2.319794\n",
      "Epoch: 12754 \tTraining Loss: 1.835253 \tValidation Loss: 2.319409\n",
      "Epoch: 12755 \tTraining Loss: 1.818305 \tValidation Loss: 2.319448\n",
      "Epoch: 12756 \tTraining Loss: 1.818935 \tValidation Loss: 2.319872\n",
      "Epoch: 12757 \tTraining Loss: 1.863668 \tValidation Loss: 2.319769\n",
      "Epoch: 12758 \tTraining Loss: 1.807651 \tValidation Loss: 2.320249\n",
      "Epoch: 12759 \tTraining Loss: 1.832757 \tValidation Loss: 2.320179\n",
      "Epoch: 12760 \tTraining Loss: 1.827264 \tValidation Loss: 2.320428\n",
      "Epoch: 12761 \tTraining Loss: 1.812642 \tValidation Loss: 2.320461\n",
      "Epoch: 12762 \tTraining Loss: 1.821425 \tValidation Loss: 2.320545\n",
      "Epoch: 12763 \tTraining Loss: 1.835256 \tValidation Loss: 2.320489\n",
      "Epoch: 12764 \tTraining Loss: 1.848155 \tValidation Loss: 2.320608\n",
      "Epoch: 12765 \tTraining Loss: 1.849721 \tValidation Loss: 2.320319\n",
      "Epoch: 12766 \tTraining Loss: 1.874559 \tValidation Loss: 2.320430\n",
      "Epoch: 12767 \tTraining Loss: 1.839827 \tValidation Loss: 2.320646\n",
      "Epoch: 12768 \tTraining Loss: 1.824037 \tValidation Loss: 2.320494\n",
      "Epoch: 12769 \tTraining Loss: 1.870668 \tValidation Loss: 2.320174\n",
      "Epoch: 12770 \tTraining Loss: 1.843113 \tValidation Loss: 2.320265\n",
      "Epoch: 12771 \tTraining Loss: 1.874175 \tValidation Loss: 2.320043\n",
      "Epoch: 12772 \tTraining Loss: 1.810338 \tValidation Loss: 2.320437\n",
      "Epoch: 12773 \tTraining Loss: 1.843246 \tValidation Loss: 2.320577\n",
      "Epoch: 12774 \tTraining Loss: 1.826779 \tValidation Loss: 2.320507\n",
      "Epoch: 12775 \tTraining Loss: 1.816154 \tValidation Loss: 2.320635\n",
      "Epoch: 12776 \tTraining Loss: 1.850649 \tValidation Loss: 2.320716\n",
      "Epoch: 12777 \tTraining Loss: 1.845990 \tValidation Loss: 2.320799\n",
      "Epoch: 12778 \tTraining Loss: 1.825862 \tValidation Loss: 2.320787\n",
      "Epoch: 12779 \tTraining Loss: 1.828144 \tValidation Loss: 2.320691\n",
      "Epoch: 12780 \tTraining Loss: 1.802053 \tValidation Loss: 2.320555\n",
      "Epoch: 12781 \tTraining Loss: 1.799729 \tValidation Loss: 2.321241\n",
      "Epoch: 12782 \tTraining Loss: 1.852180 \tValidation Loss: 2.320846\n",
      "Epoch: 12783 \tTraining Loss: 1.817923 \tValidation Loss: 2.320908\n",
      "Epoch: 12784 \tTraining Loss: 1.849182 \tValidation Loss: 2.320548\n",
      "Epoch: 12785 \tTraining Loss: 1.845595 \tValidation Loss: 2.320684\n",
      "Epoch: 12786 \tTraining Loss: 1.812302 \tValidation Loss: 2.320652\n",
      "Epoch: 12787 \tTraining Loss: 1.807682 \tValidation Loss: 2.320330\n",
      "Epoch: 12788 \tTraining Loss: 1.852945 \tValidation Loss: 2.320326\n",
      "Epoch: 12789 \tTraining Loss: 1.853950 \tValidation Loss: 2.320289\n",
      "Epoch: 12790 \tTraining Loss: 1.827846 \tValidation Loss: 2.320116\n",
      "Epoch: 12791 \tTraining Loss: 1.823801 \tValidation Loss: 2.319687\n",
      "Epoch: 12792 \tTraining Loss: 1.812177 \tValidation Loss: 2.319443\n",
      "Epoch: 12793 \tTraining Loss: 1.800757 \tValidation Loss: 2.319447\n",
      "Epoch: 12794 \tTraining Loss: 1.859396 \tValidation Loss: 2.319699\n",
      "Epoch: 12795 \tTraining Loss: 1.859394 \tValidation Loss: 2.319831\n",
      "Epoch: 12796 \tTraining Loss: 1.838184 \tValidation Loss: 2.319817\n",
      "Epoch: 12797 \tTraining Loss: 1.825637 \tValidation Loss: 2.320059\n",
      "Epoch: 12798 \tTraining Loss: 1.801137 \tValidation Loss: 2.320427\n",
      "Epoch: 12799 \tTraining Loss: 1.839586 \tValidation Loss: 2.320475\n",
      "Epoch: 12800 \tTraining Loss: 1.823770 \tValidation Loss: 2.320783\n",
      "Epoch: 12801 \tTraining Loss: 1.840817 \tValidation Loss: 2.320843\n",
      "Epoch: 12802 \tTraining Loss: 1.838267 \tValidation Loss: 2.320813\n",
      "Epoch: 12803 \tTraining Loss: 1.837474 \tValidation Loss: 2.320511\n",
      "Epoch: 12804 \tTraining Loss: 1.848599 \tValidation Loss: 2.320229\n",
      "Epoch: 12805 \tTraining Loss: 1.837595 \tValidation Loss: 2.320513\n",
      "Epoch: 12806 \tTraining Loss: 1.780454 \tValidation Loss: 2.320392\n",
      "Epoch: 12807 \tTraining Loss: 1.800690 \tValidation Loss: 2.320730\n",
      "Epoch: 12808 \tTraining Loss: 1.817427 \tValidation Loss: 2.321166\n",
      "Epoch: 12809 \tTraining Loss: 1.835984 \tValidation Loss: 2.321064\n",
      "Epoch: 12810 \tTraining Loss: 1.856210 \tValidation Loss: 2.320997\n",
      "Epoch: 12811 \tTraining Loss: 1.843051 \tValidation Loss: 2.320600\n",
      "Epoch: 12812 \tTraining Loss: 1.838249 \tValidation Loss: 2.320541\n",
      "Epoch: 12813 \tTraining Loss: 1.805016 \tValidation Loss: 2.320662\n",
      "Epoch: 12814 \tTraining Loss: 1.830427 \tValidation Loss: 2.320318\n",
      "Epoch: 12815 \tTraining Loss: 1.845961 \tValidation Loss: 2.320220\n",
      "Epoch: 12816 \tTraining Loss: 1.822018 \tValidation Loss: 2.320524\n",
      "Epoch: 12817 \tTraining Loss: 1.844851 \tValidation Loss: 2.320818\n",
      "Epoch: 12818 \tTraining Loss: 1.841967 \tValidation Loss: 2.320721\n",
      "Epoch: 12819 \tTraining Loss: 1.832883 \tValidation Loss: 2.320535\n",
      "Epoch: 12820 \tTraining Loss: 1.881128 \tValidation Loss: 2.320363\n",
      "Epoch: 12821 \tTraining Loss: 1.841562 \tValidation Loss: 2.320942\n",
      "Epoch: 12822 \tTraining Loss: 1.841972 \tValidation Loss: 2.320786\n",
      "Epoch: 12823 \tTraining Loss: 1.830657 \tValidation Loss: 2.320912\n",
      "Epoch: 12824 \tTraining Loss: 1.847279 \tValidation Loss: 2.320420\n",
      "Epoch: 12825 \tTraining Loss: 1.859129 \tValidation Loss: 2.319856\n",
      "Epoch: 12826 \tTraining Loss: 1.832516 \tValidation Loss: 2.319604\n",
      "Epoch: 12827 \tTraining Loss: 1.801866 \tValidation Loss: 2.320005\n",
      "Epoch: 12828 \tTraining Loss: 1.820929 \tValidation Loss: 2.320591\n",
      "Epoch: 12829 \tTraining Loss: 1.837916 \tValidation Loss: 2.320587\n",
      "Epoch: 12830 \tTraining Loss: 1.839709 \tValidation Loss: 2.320187\n",
      "Epoch: 12831 \tTraining Loss: 1.837254 \tValidation Loss: 2.321155\n",
      "Epoch: 12832 \tTraining Loss: 1.835770 \tValidation Loss: 2.321297\n",
      "Epoch: 12833 \tTraining Loss: 1.826942 \tValidation Loss: 2.321301\n",
      "Epoch: 12834 \tTraining Loss: 1.834236 \tValidation Loss: 2.320989\n",
      "Epoch: 12835 \tTraining Loss: 1.827386 \tValidation Loss: 2.321483\n",
      "Epoch: 12836 \tTraining Loss: 1.815549 \tValidation Loss: 2.321339\n",
      "Epoch: 12837 \tTraining Loss: 1.850059 \tValidation Loss: 2.320982\n",
      "Epoch: 12838 \tTraining Loss: 1.804760 \tValidation Loss: 2.321528\n",
      "Epoch: 12839 \tTraining Loss: 1.835799 \tValidation Loss: 2.321484\n",
      "Epoch: 12840 \tTraining Loss: 1.831235 \tValidation Loss: 2.321525\n",
      "Epoch: 12841 \tTraining Loss: 1.835381 \tValidation Loss: 2.320976\n",
      "Epoch: 12842 \tTraining Loss: 1.824772 \tValidation Loss: 2.321198\n",
      "Epoch: 12843 \tTraining Loss: 1.854643 \tValidation Loss: 2.321080\n",
      "Epoch: 12844 \tTraining Loss: 1.819650 \tValidation Loss: 2.320968\n",
      "Epoch: 12845 \tTraining Loss: 1.810166 \tValidation Loss: 2.321237\n",
      "Epoch: 12846 \tTraining Loss: 1.881155 \tValidation Loss: 2.320375\n",
      "Epoch: 12847 \tTraining Loss: 1.830630 \tValidation Loss: 2.320374\n",
      "Epoch: 12848 \tTraining Loss: 1.794869 \tValidation Loss: 2.320259\n",
      "Epoch: 12849 \tTraining Loss: 1.863460 \tValidation Loss: 2.319993\n",
      "Epoch: 12850 \tTraining Loss: 1.842017 \tValidation Loss: 2.320409\n",
      "Epoch: 12851 \tTraining Loss: 1.816305 \tValidation Loss: 2.320573\n",
      "Epoch: 12852 \tTraining Loss: 1.804839 \tValidation Loss: 2.320763\n",
      "Epoch: 12853 \tTraining Loss: 1.813271 \tValidation Loss: 2.320448\n",
      "Epoch: 12854 \tTraining Loss: 1.857895 \tValidation Loss: 2.320440\n",
      "Epoch: 12855 \tTraining Loss: 1.830249 \tValidation Loss: 2.321216\n",
      "Epoch: 12856 \tTraining Loss: 1.829884 \tValidation Loss: 2.320841\n",
      "Epoch: 12857 \tTraining Loss: 1.867043 \tValidation Loss: 2.320443\n",
      "Epoch: 12858 \tTraining Loss: 1.804172 \tValidation Loss: 2.320376\n",
      "Epoch: 12859 \tTraining Loss: 1.786533 \tValidation Loss: 2.320938\n",
      "Epoch: 12860 \tTraining Loss: 1.850113 \tValidation Loss: 2.320626\n",
      "Epoch: 12861 \tTraining Loss: 1.796163 \tValidation Loss: 2.320825\n",
      "Epoch: 12862 \tTraining Loss: 1.840600 \tValidation Loss: 2.320576\n",
      "Epoch: 12863 \tTraining Loss: 1.816817 \tValidation Loss: 2.320744\n",
      "Epoch: 12864 \tTraining Loss: 1.840720 \tValidation Loss: 2.320541\n",
      "Epoch: 12865 \tTraining Loss: 1.843119 \tValidation Loss: 2.320459\n",
      "Epoch: 12866 \tTraining Loss: 1.821454 \tValidation Loss: 2.320580\n",
      "Epoch: 12867 \tTraining Loss: 1.809717 \tValidation Loss: 2.320752\n",
      "Epoch: 12868 \tTraining Loss: 1.809208 \tValidation Loss: 2.320835\n",
      "Epoch: 12869 \tTraining Loss: 1.849548 \tValidation Loss: 2.320853\n",
      "Epoch: 12870 \tTraining Loss: 1.822106 \tValidation Loss: 2.320602\n",
      "Epoch: 12871 \tTraining Loss: 1.829115 \tValidation Loss: 2.320721\n",
      "Epoch: 12872 \tTraining Loss: 1.817225 \tValidation Loss: 2.320488\n",
      "Epoch: 12873 \tTraining Loss: 1.857434 \tValidation Loss: 2.320158\n",
      "Epoch: 12874 \tTraining Loss: 1.848157 \tValidation Loss: 2.320124\n",
      "Epoch: 12875 \tTraining Loss: 1.868600 \tValidation Loss: 2.319829\n",
      "Epoch: 12876 \tTraining Loss: 1.850996 \tValidation Loss: 2.320176\n",
      "Epoch: 12877 \tTraining Loss: 1.835807 \tValidation Loss: 2.320097\n",
      "Epoch: 12878 \tTraining Loss: 1.830683 \tValidation Loss: 2.320258\n",
      "Epoch: 12879 \tTraining Loss: 1.845696 \tValidation Loss: 2.320392\n",
      "Epoch: 12880 \tTraining Loss: 1.824300 \tValidation Loss: 2.320894\n",
      "Epoch: 12881 \tTraining Loss: 1.846649 \tValidation Loss: 2.321059\n",
      "Epoch: 12882 \tTraining Loss: 1.842495 \tValidation Loss: 2.320572\n",
      "Epoch: 12883 \tTraining Loss: 1.814103 \tValidation Loss: 2.320503\n",
      "Epoch: 12884 \tTraining Loss: 1.803441 \tValidation Loss: 2.320754\n",
      "Epoch: 12885 \tTraining Loss: 1.843257 \tValidation Loss: 2.320374\n",
      "Epoch: 12886 \tTraining Loss: 1.852757 \tValidation Loss: 2.321070\n",
      "Epoch: 12887 \tTraining Loss: 1.826615 \tValidation Loss: 2.321077\n",
      "Epoch: 12888 \tTraining Loss: 1.842250 \tValidation Loss: 2.321477\n",
      "Epoch: 12889 \tTraining Loss: 1.845038 \tValidation Loss: 2.321102\n",
      "Epoch: 12890 \tTraining Loss: 1.821927 \tValidation Loss: 2.321101\n",
      "Epoch: 12891 \tTraining Loss: 1.822092 \tValidation Loss: 2.321253\n",
      "Epoch: 12892 \tTraining Loss: 1.829688 \tValidation Loss: 2.321026\n",
      "Epoch: 12893 \tTraining Loss: 1.857116 \tValidation Loss: 2.320625\n",
      "Epoch: 12894 \tTraining Loss: 1.813525 \tValidation Loss: 2.320392\n",
      "Epoch: 12895 \tTraining Loss: 1.834653 \tValidation Loss: 2.320447\n",
      "Epoch: 12896 \tTraining Loss: 1.822230 \tValidation Loss: 2.320677\n",
      "Epoch: 12897 \tTraining Loss: 1.858372 \tValidation Loss: 2.321163\n",
      "Epoch: 12898 \tTraining Loss: 1.837217 \tValidation Loss: 2.321195\n",
      "Epoch: 12899 \tTraining Loss: 1.841045 \tValidation Loss: 2.321343\n",
      "Epoch: 12900 \tTraining Loss: 1.823258 \tValidation Loss: 2.321429\n",
      "Epoch: 12901 \tTraining Loss: 1.823714 \tValidation Loss: 2.321691\n",
      "Epoch: 12902 \tTraining Loss: 1.808126 \tValidation Loss: 2.321331\n",
      "Epoch: 12903 \tTraining Loss: 1.854370 \tValidation Loss: 2.321287\n",
      "Epoch: 12904 \tTraining Loss: 1.818217 \tValidation Loss: 2.321396\n",
      "Epoch: 12905 \tTraining Loss: 1.827893 \tValidation Loss: 2.321248\n",
      "Epoch: 12906 \tTraining Loss: 1.810220 \tValidation Loss: 2.321304\n",
      "Epoch: 12907 \tTraining Loss: 1.813544 \tValidation Loss: 2.321282\n",
      "Epoch: 12908 \tTraining Loss: 1.868405 \tValidation Loss: 2.320860\n",
      "Epoch: 12909 \tTraining Loss: 1.847647 \tValidation Loss: 2.320462\n",
      "Epoch: 12910 \tTraining Loss: 1.830189 \tValidation Loss: 2.321105\n",
      "Epoch: 12911 \tTraining Loss: 1.822480 \tValidation Loss: 2.320514\n",
      "Epoch: 12912 \tTraining Loss: 1.832667 \tValidation Loss: 2.320546\n",
      "Epoch: 12913 \tTraining Loss: 1.835922 \tValidation Loss: 2.320522\n",
      "Epoch: 12914 \tTraining Loss: 1.830411 \tValidation Loss: 2.320697\n",
      "Epoch: 12915 \tTraining Loss: 1.829591 \tValidation Loss: 2.320619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12916 \tTraining Loss: 1.838958 \tValidation Loss: 2.320645\n",
      "Epoch: 12917 \tTraining Loss: 1.838790 \tValidation Loss: 2.320728\n",
      "Epoch: 12918 \tTraining Loss: 1.805077 \tValidation Loss: 2.320652\n",
      "Epoch: 12919 \tTraining Loss: 1.821027 \tValidation Loss: 2.320522\n",
      "Epoch: 12920 \tTraining Loss: 1.812802 \tValidation Loss: 2.320516\n",
      "Epoch: 12921 \tTraining Loss: 1.840726 \tValidation Loss: 2.320516\n",
      "Epoch: 12922 \tTraining Loss: 1.824257 \tValidation Loss: 2.320391\n",
      "Epoch: 12923 \tTraining Loss: 1.822784 \tValidation Loss: 2.320250\n",
      "Epoch: 12924 \tTraining Loss: 1.801431 \tValidation Loss: 2.320475\n",
      "Epoch: 12925 \tTraining Loss: 1.812440 \tValidation Loss: 2.320216\n",
      "Epoch: 12926 \tTraining Loss: 1.796742 \tValidation Loss: 2.320469\n",
      "Epoch: 12927 \tTraining Loss: 1.833536 \tValidation Loss: 2.320578\n",
      "Epoch: 12928 \tTraining Loss: 1.842748 \tValidation Loss: 2.320451\n",
      "Epoch: 12929 \tTraining Loss: 1.829824 \tValidation Loss: 2.321030\n",
      "Epoch: 12930 \tTraining Loss: 1.801741 \tValidation Loss: 2.320840\n",
      "Epoch: 12931 \tTraining Loss: 1.825991 \tValidation Loss: 2.321336\n",
      "Epoch: 12932 \tTraining Loss: 1.804249 \tValidation Loss: 2.321478\n",
      "Epoch: 12933 \tTraining Loss: 1.833745 \tValidation Loss: 2.321127\n",
      "Epoch: 12934 \tTraining Loss: 1.809031 \tValidation Loss: 2.320957\n",
      "Epoch: 12935 \tTraining Loss: 1.818867 \tValidation Loss: 2.321231\n",
      "Epoch: 12936 \tTraining Loss: 1.819799 \tValidation Loss: 2.321009\n",
      "Epoch: 12937 \tTraining Loss: 1.831297 \tValidation Loss: 2.320807\n",
      "Epoch: 12938 \tTraining Loss: 1.850290 \tValidation Loss: 2.320954\n",
      "Epoch: 12939 \tTraining Loss: 1.831077 \tValidation Loss: 2.321249\n",
      "Epoch: 12940 \tTraining Loss: 1.821554 \tValidation Loss: 2.321165\n",
      "Epoch: 12941 \tTraining Loss: 1.829671 \tValidation Loss: 2.321303\n",
      "Epoch: 12942 \tTraining Loss: 1.812243 \tValidation Loss: 2.321612\n",
      "Epoch: 12943 \tTraining Loss: 1.809529 \tValidation Loss: 2.321291\n",
      "Epoch: 12944 \tTraining Loss: 1.835888 \tValidation Loss: 2.321537\n",
      "Epoch: 12945 \tTraining Loss: 1.805410 \tValidation Loss: 2.321746\n",
      "Epoch: 12946 \tTraining Loss: 1.809195 \tValidation Loss: 2.321286\n",
      "Epoch: 12947 \tTraining Loss: 1.831105 \tValidation Loss: 2.321218\n",
      "Epoch: 12948 \tTraining Loss: 1.826867 \tValidation Loss: 2.321695\n",
      "Epoch: 12949 \tTraining Loss: 1.836380 \tValidation Loss: 2.321346\n",
      "Epoch: 12950 \tTraining Loss: 1.798921 \tValidation Loss: 2.320957\n",
      "Epoch: 12951 \tTraining Loss: 1.829320 \tValidation Loss: 2.320875\n",
      "Epoch: 12952 \tTraining Loss: 1.789988 \tValidation Loss: 2.321259\n",
      "Epoch: 12953 \tTraining Loss: 1.836632 \tValidation Loss: 2.320883\n",
      "Epoch: 12954 \tTraining Loss: 1.801717 \tValidation Loss: 2.321053\n",
      "Epoch: 12955 \tTraining Loss: 1.811352 \tValidation Loss: 2.321080\n",
      "Epoch: 12956 \tTraining Loss: 1.820112 \tValidation Loss: 2.321125\n",
      "Epoch: 12957 \tTraining Loss: 1.826077 \tValidation Loss: 2.321802\n",
      "Epoch: 12958 \tTraining Loss: 1.856121 \tValidation Loss: 2.321437\n",
      "Epoch: 12959 \tTraining Loss: 1.827598 \tValidation Loss: 2.321610\n",
      "Epoch: 12960 \tTraining Loss: 1.819401 \tValidation Loss: 2.321717\n",
      "Epoch: 12961 \tTraining Loss: 1.812880 \tValidation Loss: 2.321453\n",
      "Epoch: 12962 \tTraining Loss: 1.786293 \tValidation Loss: 2.321393\n",
      "Epoch: 12963 \tTraining Loss: 1.794391 \tValidation Loss: 2.321295\n",
      "Epoch: 12964 \tTraining Loss: 1.827570 \tValidation Loss: 2.320875\n",
      "Epoch: 12965 \tTraining Loss: 1.803065 \tValidation Loss: 2.321292\n",
      "Epoch: 12966 \tTraining Loss: 1.819094 \tValidation Loss: 2.321405\n",
      "Epoch: 12967 \tTraining Loss: 1.820278 \tValidation Loss: 2.321814\n",
      "Epoch: 12968 \tTraining Loss: 1.820266 \tValidation Loss: 2.321625\n",
      "Epoch: 12969 \tTraining Loss: 1.814828 \tValidation Loss: 2.321335\n",
      "Epoch: 12970 \tTraining Loss: 1.829557 \tValidation Loss: 2.321516\n",
      "Epoch: 12971 \tTraining Loss: 1.819151 \tValidation Loss: 2.321280\n",
      "Epoch: 12972 \tTraining Loss: 1.836390 \tValidation Loss: 2.321694\n",
      "Epoch: 12973 \tTraining Loss: 1.808732 \tValidation Loss: 2.321358\n",
      "Epoch: 12974 \tTraining Loss: 1.822803 \tValidation Loss: 2.321289\n",
      "Epoch: 12975 \tTraining Loss: 1.791817 \tValidation Loss: 2.321789\n",
      "Epoch: 12976 \tTraining Loss: 1.839408 \tValidation Loss: 2.321102\n",
      "Epoch: 12977 \tTraining Loss: 1.799338 \tValidation Loss: 2.321149\n",
      "Epoch: 12978 \tTraining Loss: 1.821057 \tValidation Loss: 2.321330\n",
      "Epoch: 12979 \tTraining Loss: 1.867693 \tValidation Loss: 2.321068\n",
      "Epoch: 12980 \tTraining Loss: 1.811442 \tValidation Loss: 2.321428\n",
      "Epoch: 12981 \tTraining Loss: 1.793176 \tValidation Loss: 2.322005\n",
      "Epoch: 12982 \tTraining Loss: 1.824534 \tValidation Loss: 2.322349\n",
      "Epoch: 12983 \tTraining Loss: 1.822023 \tValidation Loss: 2.322395\n",
      "Epoch: 12984 \tTraining Loss: 1.839610 \tValidation Loss: 2.322111\n",
      "Epoch: 12985 \tTraining Loss: 1.800832 \tValidation Loss: 2.321992\n",
      "Epoch: 12986 \tTraining Loss: 1.844283 \tValidation Loss: 2.321778\n",
      "Epoch: 12987 \tTraining Loss: 1.825742 \tValidation Loss: 2.321546\n",
      "Epoch: 12988 \tTraining Loss: 1.837827 \tValidation Loss: 2.321140\n",
      "Epoch: 12989 \tTraining Loss: 1.844504 \tValidation Loss: 2.321198\n",
      "Epoch: 12990 \tTraining Loss: 1.808952 \tValidation Loss: 2.321165\n",
      "Epoch: 12991 \tTraining Loss: 1.795702 \tValidation Loss: 2.321419\n",
      "Epoch: 12992 \tTraining Loss: 1.843990 \tValidation Loss: 2.321385\n",
      "Epoch: 12993 \tTraining Loss: 1.815392 \tValidation Loss: 2.321522\n",
      "Epoch: 12994 \tTraining Loss: 1.816694 \tValidation Loss: 2.321331\n",
      "Epoch: 12995 \tTraining Loss: 1.773962 \tValidation Loss: 2.321387\n",
      "Epoch: 12996 \tTraining Loss: 1.837285 \tValidation Loss: 2.321667\n",
      "Epoch: 12997 \tTraining Loss: 1.831962 \tValidation Loss: 2.322050\n",
      "Epoch: 12998 \tTraining Loss: 1.832894 \tValidation Loss: 2.321717\n",
      "Epoch: 12999 \tTraining Loss: 1.822050 \tValidation Loss: 2.321945\n",
      "Epoch: 13000 \tTraining Loss: 1.841601 \tValidation Loss: 2.321770\n",
      "Epoch: 13001 \tTraining Loss: 1.836936 \tValidation Loss: 2.322044\n",
      "Epoch: 13002 \tTraining Loss: 1.802622 \tValidation Loss: 2.321861\n",
      "Epoch: 13003 \tTraining Loss: 1.812994 \tValidation Loss: 2.322280\n",
      "Epoch: 13004 \tTraining Loss: 1.840004 \tValidation Loss: 2.321608\n",
      "Epoch: 13005 \tTraining Loss: 1.835108 \tValidation Loss: 2.321659\n",
      "Epoch: 13006 \tTraining Loss: 1.837706 \tValidation Loss: 2.322156\n",
      "Epoch: 13007 \tTraining Loss: 1.781013 \tValidation Loss: 2.322247\n",
      "Epoch: 13008 \tTraining Loss: 1.840865 \tValidation Loss: 2.322231\n",
      "Epoch: 13009 \tTraining Loss: 1.784010 \tValidation Loss: 2.322469\n",
      "Epoch: 13010 \tTraining Loss: 1.807704 \tValidation Loss: 2.322210\n",
      "Epoch: 13011 \tTraining Loss: 1.818982 \tValidation Loss: 2.322116\n",
      "Epoch: 13012 \tTraining Loss: 1.851231 \tValidation Loss: 2.322170\n",
      "Epoch: 13013 \tTraining Loss: 1.816688 \tValidation Loss: 2.322010\n",
      "Epoch: 13014 \tTraining Loss: 1.831611 \tValidation Loss: 2.322186\n",
      "Epoch: 13015 \tTraining Loss: 1.852488 \tValidation Loss: 2.322073\n",
      "Epoch: 13016 \tTraining Loss: 1.788264 \tValidation Loss: 2.322142\n",
      "Epoch: 13017 \tTraining Loss: 1.833956 \tValidation Loss: 2.322274\n",
      "Epoch: 13018 \tTraining Loss: 1.849110 \tValidation Loss: 2.321929\n",
      "Epoch: 13019 \tTraining Loss: 1.816494 \tValidation Loss: 2.322126\n",
      "Epoch: 13020 \tTraining Loss: 1.791790 \tValidation Loss: 2.322400\n",
      "Epoch: 13021 \tTraining Loss: 1.799252 \tValidation Loss: 2.322250\n",
      "Epoch: 13022 \tTraining Loss: 1.802770 \tValidation Loss: 2.322276\n",
      "Epoch: 13023 \tTraining Loss: 1.829516 \tValidation Loss: 2.322117\n",
      "Epoch: 13024 \tTraining Loss: 1.857927 \tValidation Loss: 2.322065\n",
      "Epoch: 13025 \tTraining Loss: 1.834730 \tValidation Loss: 2.322177\n",
      "Epoch: 13026 \tTraining Loss: 1.850149 \tValidation Loss: 2.321911\n",
      "Epoch: 13027 \tTraining Loss: 1.843159 \tValidation Loss: 2.321789\n",
      "Epoch: 13028 \tTraining Loss: 1.788601 \tValidation Loss: 2.321916\n",
      "Epoch: 13029 \tTraining Loss: 1.816255 \tValidation Loss: 2.322193\n",
      "Epoch: 13030 \tTraining Loss: 1.805470 \tValidation Loss: 2.322316\n",
      "Epoch: 13031 \tTraining Loss: 1.848740 \tValidation Loss: 2.322063\n",
      "Epoch: 13032 \tTraining Loss: 1.866312 \tValidation Loss: 2.322288\n",
      "Epoch: 13033 \tTraining Loss: 1.846382 \tValidation Loss: 2.322789\n",
      "Epoch: 13034 \tTraining Loss: 1.835798 \tValidation Loss: 2.322827\n",
      "Epoch: 13035 \tTraining Loss: 1.837987 \tValidation Loss: 2.322733\n",
      "Epoch: 13036 \tTraining Loss: 1.797465 \tValidation Loss: 2.322795\n",
      "Epoch: 13037 \tTraining Loss: 1.790045 \tValidation Loss: 2.323062\n",
      "Epoch: 13038 \tTraining Loss: 1.823672 \tValidation Loss: 2.322337\n",
      "Epoch: 13039 \tTraining Loss: 1.798360 \tValidation Loss: 2.322490\n",
      "Epoch: 13040 \tTraining Loss: 1.793493 \tValidation Loss: 2.322408\n",
      "Epoch: 13041 \tTraining Loss: 1.827906 \tValidation Loss: 2.322751\n",
      "Epoch: 13042 \tTraining Loss: 1.836866 \tValidation Loss: 2.322817\n",
      "Epoch: 13043 \tTraining Loss: 1.846579 \tValidation Loss: 2.322165\n",
      "Epoch: 13044 \tTraining Loss: 1.831834 \tValidation Loss: 2.322432\n",
      "Epoch: 13045 \tTraining Loss: 1.796613 \tValidation Loss: 2.322732\n",
      "Epoch: 13046 \tTraining Loss: 1.814224 \tValidation Loss: 2.322278\n",
      "Epoch: 13047 \tTraining Loss: 1.847645 \tValidation Loss: 2.322130\n",
      "Epoch: 13048 \tTraining Loss: 1.826483 \tValidation Loss: 2.322032\n",
      "Epoch: 13049 \tTraining Loss: 1.849255 \tValidation Loss: 2.322624\n",
      "Epoch: 13050 \tTraining Loss: 1.828271 \tValidation Loss: 2.322173\n",
      "Epoch: 13051 \tTraining Loss: 1.819217 \tValidation Loss: 2.322251\n",
      "Epoch: 13052 \tTraining Loss: 1.838619 \tValidation Loss: 2.322097\n",
      "Epoch: 13053 \tTraining Loss: 1.844286 \tValidation Loss: 2.322929\n",
      "Epoch: 13054 \tTraining Loss: 1.811063 \tValidation Loss: 2.322830\n",
      "Epoch: 13055 \tTraining Loss: 1.849840 \tValidation Loss: 2.322194\n",
      "Epoch: 13056 \tTraining Loss: 1.820857 \tValidation Loss: 2.322194\n",
      "Epoch: 13057 \tTraining Loss: 1.862494 \tValidation Loss: 2.322088\n",
      "Epoch: 13058 \tTraining Loss: 1.833068 \tValidation Loss: 2.321892\n",
      "Epoch: 13059 \tTraining Loss: 1.814670 \tValidation Loss: 2.322183\n",
      "Epoch: 13060 \tTraining Loss: 1.810112 \tValidation Loss: 2.322459\n",
      "Epoch: 13061 \tTraining Loss: 1.839518 \tValidation Loss: 2.322582\n",
      "Epoch: 13062 \tTraining Loss: 1.807948 \tValidation Loss: 2.322622\n",
      "Epoch: 13063 \tTraining Loss: 1.832734 \tValidation Loss: 2.322601\n",
      "Epoch: 13064 \tTraining Loss: 1.805118 \tValidation Loss: 2.322679\n",
      "Epoch: 13065 \tTraining Loss: 1.815833 \tValidation Loss: 2.322680\n",
      "Epoch: 13066 \tTraining Loss: 1.808917 \tValidation Loss: 2.323004\n",
      "Epoch: 13067 \tTraining Loss: 1.853349 \tValidation Loss: 2.322798\n",
      "Epoch: 13068 \tTraining Loss: 1.806347 \tValidation Loss: 2.322644\n",
      "Epoch: 13069 \tTraining Loss: 1.804322 \tValidation Loss: 2.323056\n",
      "Epoch: 13070 \tTraining Loss: 1.793290 \tValidation Loss: 2.323643\n",
      "Epoch: 13071 \tTraining Loss: 1.806035 \tValidation Loss: 2.323616\n",
      "Epoch: 13072 \tTraining Loss: 1.802317 \tValidation Loss: 2.323489\n",
      "Epoch: 13073 \tTraining Loss: 1.808112 \tValidation Loss: 2.323172\n",
      "Epoch: 13074 \tTraining Loss: 1.802010 \tValidation Loss: 2.322795\n",
      "Epoch: 13075 \tTraining Loss: 1.817632 \tValidation Loss: 2.323056\n",
      "Epoch: 13076 \tTraining Loss: 1.827631 \tValidation Loss: 2.322698\n",
      "Epoch: 13077 \tTraining Loss: 1.779517 \tValidation Loss: 2.322779\n",
      "Epoch: 13078 \tTraining Loss: 1.802922 \tValidation Loss: 2.322685\n",
      "Epoch: 13079 \tTraining Loss: 1.821170 \tValidation Loss: 2.322613\n",
      "Epoch: 13080 \tTraining Loss: 1.843585 \tValidation Loss: 2.322845\n",
      "Epoch: 13081 \tTraining Loss: 1.818901 \tValidation Loss: 2.323019\n",
      "Epoch: 13082 \tTraining Loss: 1.807529 \tValidation Loss: 2.323255\n",
      "Epoch: 13083 \tTraining Loss: 1.804015 \tValidation Loss: 2.322533\n",
      "Epoch: 13084 \tTraining Loss: 1.792336 \tValidation Loss: 2.322650\n",
      "Epoch: 13085 \tTraining Loss: 1.838727 \tValidation Loss: 2.322010\n",
      "Epoch: 13086 \tTraining Loss: 1.805872 \tValidation Loss: 2.322343\n",
      "Epoch: 13087 \tTraining Loss: 1.770239 \tValidation Loss: 2.322935\n",
      "Epoch: 13088 \tTraining Loss: 1.834984 \tValidation Loss: 2.322772\n",
      "Epoch: 13089 \tTraining Loss: 1.812303 \tValidation Loss: 2.322829\n",
      "Epoch: 13090 \tTraining Loss: 1.792607 \tValidation Loss: 2.322624\n",
      "Epoch: 13091 \tTraining Loss: 1.826969 \tValidation Loss: 2.322555\n",
      "Epoch: 13092 \tTraining Loss: 1.800756 \tValidation Loss: 2.322378\n",
      "Epoch: 13093 \tTraining Loss: 1.842783 \tValidation Loss: 2.322473\n",
      "Epoch: 13094 \tTraining Loss: 1.812273 \tValidation Loss: 2.322663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13095 \tTraining Loss: 1.828734 \tValidation Loss: 2.323063\n",
      "Epoch: 13096 \tTraining Loss: 1.806613 \tValidation Loss: 2.323253\n",
      "Epoch: 13097 \tTraining Loss: 1.819392 \tValidation Loss: 2.322829\n",
      "Epoch: 13098 \tTraining Loss: 1.839233 \tValidation Loss: 2.322649\n",
      "Epoch: 13099 \tTraining Loss: 1.820327 \tValidation Loss: 2.322546\n",
      "Epoch: 13100 \tTraining Loss: 1.812438 \tValidation Loss: 2.322101\n",
      "Epoch: 13101 \tTraining Loss: 1.819291 \tValidation Loss: 2.322205\n",
      "Epoch: 13102 \tTraining Loss: 1.768182 \tValidation Loss: 2.322116\n",
      "Epoch: 13103 \tTraining Loss: 1.821438 \tValidation Loss: 2.322036\n",
      "Epoch: 13104 \tTraining Loss: 1.814564 \tValidation Loss: 2.322229\n",
      "Epoch: 13105 \tTraining Loss: 1.819819 \tValidation Loss: 2.322567\n",
      "Epoch: 13106 \tTraining Loss: 1.816167 \tValidation Loss: 2.322107\n",
      "Epoch: 13107 \tTraining Loss: 1.816973 \tValidation Loss: 2.322264\n",
      "Epoch: 13108 \tTraining Loss: 1.801930 \tValidation Loss: 2.322654\n",
      "Epoch: 13109 \tTraining Loss: 1.826973 \tValidation Loss: 2.322826\n",
      "Epoch: 13110 \tTraining Loss: 1.777208 \tValidation Loss: 2.323258\n",
      "Epoch: 13111 \tTraining Loss: 1.783004 \tValidation Loss: 2.322963\n",
      "Epoch: 13112 \tTraining Loss: 1.839836 \tValidation Loss: 2.322704\n",
      "Epoch: 13113 \tTraining Loss: 1.792630 \tValidation Loss: 2.322994\n",
      "Epoch: 13114 \tTraining Loss: 1.818683 \tValidation Loss: 2.323114\n",
      "Epoch: 13115 \tTraining Loss: 1.822353 \tValidation Loss: 2.323283\n",
      "Epoch: 13116 \tTraining Loss: 1.796479 \tValidation Loss: 2.322723\n",
      "Epoch: 13117 \tTraining Loss: 1.833995 \tValidation Loss: 2.322772\n",
      "Epoch: 13118 \tTraining Loss: 1.794260 \tValidation Loss: 2.322478\n",
      "Epoch: 13119 \tTraining Loss: 1.805276 \tValidation Loss: 2.322732\n",
      "Epoch: 13120 \tTraining Loss: 1.785311 \tValidation Loss: 2.322846\n",
      "Epoch: 13121 \tTraining Loss: 1.835167 \tValidation Loss: 2.322768\n",
      "Epoch: 13122 \tTraining Loss: 1.821083 \tValidation Loss: 2.322869\n",
      "Epoch: 13123 \tTraining Loss: 1.809739 \tValidation Loss: 2.322653\n",
      "Epoch: 13124 \tTraining Loss: 1.819392 \tValidation Loss: 2.322716\n",
      "Epoch: 13125 \tTraining Loss: 1.796508 \tValidation Loss: 2.322735\n",
      "Epoch: 13126 \tTraining Loss: 1.816088 \tValidation Loss: 2.322599\n",
      "Epoch: 13127 \tTraining Loss: 1.856998 \tValidation Loss: 2.322641\n",
      "Epoch: 13128 \tTraining Loss: 1.832121 \tValidation Loss: 2.323038\n",
      "Epoch: 13129 \tTraining Loss: 1.808095 \tValidation Loss: 2.322934\n",
      "Epoch: 13130 \tTraining Loss: 1.858108 \tValidation Loss: 2.322460\n",
      "Epoch: 13131 \tTraining Loss: 1.804986 \tValidation Loss: 2.322459\n",
      "Epoch: 13132 \tTraining Loss: 1.793054 \tValidation Loss: 2.322878\n",
      "Epoch: 13133 \tTraining Loss: 1.772488 \tValidation Loss: 2.322803\n",
      "Epoch: 13134 \tTraining Loss: 1.825241 \tValidation Loss: 2.322910\n",
      "Epoch: 13135 \tTraining Loss: 1.847692 \tValidation Loss: 2.323239\n",
      "Epoch: 13136 \tTraining Loss: 1.813786 \tValidation Loss: 2.322982\n",
      "Epoch: 13137 \tTraining Loss: 1.806096 \tValidation Loss: 2.322793\n",
      "Epoch: 13138 \tTraining Loss: 1.783019 \tValidation Loss: 2.322797\n",
      "Epoch: 13139 \tTraining Loss: 1.817815 \tValidation Loss: 2.322904\n",
      "Epoch: 13140 \tTraining Loss: 1.792334 \tValidation Loss: 2.322620\n",
      "Epoch: 13141 \tTraining Loss: 1.829750 \tValidation Loss: 2.322616\n",
      "Epoch: 13142 \tTraining Loss: 1.811233 \tValidation Loss: 2.323047\n",
      "Epoch: 13143 \tTraining Loss: 1.820962 \tValidation Loss: 2.322635\n",
      "Epoch: 13144 \tTraining Loss: 1.869444 \tValidation Loss: 2.322640\n",
      "Epoch: 13145 \tTraining Loss: 1.814201 \tValidation Loss: 2.322896\n",
      "Epoch: 13146 \tTraining Loss: 1.798781 \tValidation Loss: 2.322982\n",
      "Epoch: 13147 \tTraining Loss: 1.778072 \tValidation Loss: 2.323120\n",
      "Epoch: 13148 \tTraining Loss: 1.841354 \tValidation Loss: 2.323207\n",
      "Epoch: 13149 \tTraining Loss: 1.809037 \tValidation Loss: 2.323516\n",
      "Epoch: 13150 \tTraining Loss: 1.815303 \tValidation Loss: 2.323286\n",
      "Epoch: 13151 \tTraining Loss: 1.831269 \tValidation Loss: 2.323372\n",
      "Epoch: 13152 \tTraining Loss: 1.789213 \tValidation Loss: 2.323161\n",
      "Epoch: 13153 \tTraining Loss: 1.789539 \tValidation Loss: 2.323223\n",
      "Epoch: 13154 \tTraining Loss: 1.814470 \tValidation Loss: 2.323608\n",
      "Epoch: 13155 \tTraining Loss: 1.829173 \tValidation Loss: 2.323154\n",
      "Epoch: 13156 \tTraining Loss: 1.827144 \tValidation Loss: 2.322977\n",
      "Epoch: 13157 \tTraining Loss: 1.807069 \tValidation Loss: 2.323064\n",
      "Epoch: 13158 \tTraining Loss: 1.825837 \tValidation Loss: 2.323361\n",
      "Epoch: 13159 \tTraining Loss: 1.777139 \tValidation Loss: 2.323747\n",
      "Epoch: 13160 \tTraining Loss: 1.807213 \tValidation Loss: 2.323465\n",
      "Epoch: 13161 \tTraining Loss: 1.806574 \tValidation Loss: 2.323115\n",
      "Epoch: 13162 \tTraining Loss: 1.810799 \tValidation Loss: 2.323570\n",
      "Epoch: 13163 \tTraining Loss: 1.806085 \tValidation Loss: 2.323513\n",
      "Epoch: 13164 \tTraining Loss: 1.825149 \tValidation Loss: 2.323641\n",
      "Epoch: 13165 \tTraining Loss: 1.834377 \tValidation Loss: 2.323423\n",
      "Epoch: 13166 \tTraining Loss: 1.859405 \tValidation Loss: 2.323246\n",
      "Epoch: 13167 \tTraining Loss: 1.799781 \tValidation Loss: 2.323110\n",
      "Epoch: 13168 \tTraining Loss: 1.805490 \tValidation Loss: 2.323143\n",
      "Epoch: 13169 \tTraining Loss: 1.849164 \tValidation Loss: 2.323330\n",
      "Epoch: 13170 \tTraining Loss: 1.818593 \tValidation Loss: 2.322646\n",
      "Epoch: 13171 \tTraining Loss: 1.831010 \tValidation Loss: 2.323042\n",
      "Epoch: 13172 \tTraining Loss: 1.797834 \tValidation Loss: 2.323025\n",
      "Epoch: 13173 \tTraining Loss: 1.840023 \tValidation Loss: 2.323189\n",
      "Epoch: 13174 \tTraining Loss: 1.826783 \tValidation Loss: 2.323115\n",
      "Epoch: 13175 \tTraining Loss: 1.843925 \tValidation Loss: 2.323141\n",
      "Epoch: 13176 \tTraining Loss: 1.809827 \tValidation Loss: 2.322994\n",
      "Epoch: 13177 \tTraining Loss: 1.841168 \tValidation Loss: 2.322769\n",
      "Epoch: 13178 \tTraining Loss: 1.780351 \tValidation Loss: 2.323123\n",
      "Epoch: 13179 \tTraining Loss: 1.827376 \tValidation Loss: 2.323336\n",
      "Epoch: 13180 \tTraining Loss: 1.805698 \tValidation Loss: 2.322639\n",
      "Epoch: 13181 \tTraining Loss: 1.821708 \tValidation Loss: 2.322968\n",
      "Epoch: 13182 \tTraining Loss: 1.795592 \tValidation Loss: 2.323436\n",
      "Epoch: 13183 \tTraining Loss: 1.806704 \tValidation Loss: 2.323380\n",
      "Epoch: 13184 \tTraining Loss: 1.787155 \tValidation Loss: 2.323813\n",
      "Epoch: 13185 \tTraining Loss: 1.782607 \tValidation Loss: 2.323891\n",
      "Epoch: 13186 \tTraining Loss: 1.793425 \tValidation Loss: 2.323498\n",
      "Epoch: 13187 \tTraining Loss: 1.797273 \tValidation Loss: 2.323019\n",
      "Epoch: 13188 \tTraining Loss: 1.841231 \tValidation Loss: 2.322972\n",
      "Epoch: 13189 \tTraining Loss: 1.803240 \tValidation Loss: 2.323238\n",
      "Epoch: 13190 \tTraining Loss: 1.778571 \tValidation Loss: 2.323545\n",
      "Epoch: 13191 \tTraining Loss: 1.826206 \tValidation Loss: 2.323740\n",
      "Epoch: 13192 \tTraining Loss: 1.796117 \tValidation Loss: 2.323417\n",
      "Epoch: 13193 \tTraining Loss: 1.807343 \tValidation Loss: 2.323473\n",
      "Epoch: 13194 \tTraining Loss: 1.787753 \tValidation Loss: 2.323145\n",
      "Epoch: 13195 \tTraining Loss: 1.798995 \tValidation Loss: 2.323395\n",
      "Epoch: 13196 \tTraining Loss: 1.834894 \tValidation Loss: 2.323367\n",
      "Epoch: 13197 \tTraining Loss: 1.816854 \tValidation Loss: 2.323544\n",
      "Epoch: 13198 \tTraining Loss: 1.831139 \tValidation Loss: 2.323217\n",
      "Epoch: 13199 \tTraining Loss: 1.851052 \tValidation Loss: 2.323127\n",
      "Epoch: 13200 \tTraining Loss: 1.849960 \tValidation Loss: 2.323222\n",
      "Epoch: 13201 \tTraining Loss: 1.803815 \tValidation Loss: 2.323095\n",
      "Epoch: 13202 \tTraining Loss: 1.811209 \tValidation Loss: 2.323222\n",
      "Epoch: 13203 \tTraining Loss: 1.822576 \tValidation Loss: 2.323264\n",
      "Epoch: 13204 \tTraining Loss: 1.793314 \tValidation Loss: 2.323215\n",
      "Epoch: 13205 \tTraining Loss: 1.780130 \tValidation Loss: 2.323749\n",
      "Epoch: 13206 \tTraining Loss: 1.802093 \tValidation Loss: 2.323517\n",
      "Epoch: 13207 \tTraining Loss: 1.802513 \tValidation Loss: 2.323417\n",
      "Epoch: 13208 \tTraining Loss: 1.811386 \tValidation Loss: 2.323140\n",
      "Epoch: 13209 \tTraining Loss: 1.830520 \tValidation Loss: 2.323438\n",
      "Epoch: 13210 \tTraining Loss: 1.805473 \tValidation Loss: 2.323524\n",
      "Epoch: 13211 \tTraining Loss: 1.799411 \tValidation Loss: 2.323487\n",
      "Epoch: 13212 \tTraining Loss: 1.826141 \tValidation Loss: 2.323566\n",
      "Epoch: 13213 \tTraining Loss: 1.792637 \tValidation Loss: 2.323333\n",
      "Epoch: 13214 \tTraining Loss: 1.819834 \tValidation Loss: 2.322886\n",
      "Epoch: 13215 \tTraining Loss: 1.820777 \tValidation Loss: 2.323260\n",
      "Epoch: 13216 \tTraining Loss: 1.832318 \tValidation Loss: 2.323518\n",
      "Epoch: 13217 \tTraining Loss: 1.784147 \tValidation Loss: 2.323264\n",
      "Epoch: 13218 \tTraining Loss: 1.810781 \tValidation Loss: 2.323320\n",
      "Epoch: 13219 \tTraining Loss: 1.815993 \tValidation Loss: 2.323042\n",
      "Epoch: 13220 \tTraining Loss: 1.790029 \tValidation Loss: 2.323407\n",
      "Epoch: 13221 \tTraining Loss: 1.826862 \tValidation Loss: 2.323816\n",
      "Epoch: 13222 \tTraining Loss: 1.835945 \tValidation Loss: 2.323415\n",
      "Epoch: 13223 \tTraining Loss: 1.826664 \tValidation Loss: 2.323709\n",
      "Epoch: 13224 \tTraining Loss: 1.826900 \tValidation Loss: 2.323359\n",
      "Epoch: 13225 \tTraining Loss: 1.779144 \tValidation Loss: 2.323624\n",
      "Epoch: 13226 \tTraining Loss: 1.783882 \tValidation Loss: 2.323545\n",
      "Epoch: 13227 \tTraining Loss: 1.789323 \tValidation Loss: 2.323929\n",
      "Epoch: 13228 \tTraining Loss: 1.826418 \tValidation Loss: 2.323964\n",
      "Epoch: 13229 \tTraining Loss: 1.816621 \tValidation Loss: 2.323929\n",
      "Epoch: 13230 \tTraining Loss: 1.862821 \tValidation Loss: 2.323771\n",
      "Epoch: 13231 \tTraining Loss: 1.813375 \tValidation Loss: 2.323540\n",
      "Epoch: 13232 \tTraining Loss: 1.805882 \tValidation Loss: 2.323544\n",
      "Epoch: 13233 \tTraining Loss: 1.792673 \tValidation Loss: 2.323407\n",
      "Epoch: 13234 \tTraining Loss: 1.817728 \tValidation Loss: 2.323421\n",
      "Epoch: 13235 \tTraining Loss: 1.798719 \tValidation Loss: 2.323445\n",
      "Epoch: 13236 \tTraining Loss: 1.810222 \tValidation Loss: 2.323740\n",
      "Epoch: 13237 \tTraining Loss: 1.819534 \tValidation Loss: 2.324269\n",
      "Epoch: 13238 \tTraining Loss: 1.811830 \tValidation Loss: 2.324016\n",
      "Epoch: 13239 \tTraining Loss: 1.848263 \tValidation Loss: 2.324013\n",
      "Epoch: 13240 \tTraining Loss: 1.798374 \tValidation Loss: 2.324051\n",
      "Epoch: 13241 \tTraining Loss: 1.780501 \tValidation Loss: 2.324237\n",
      "Epoch: 13242 \tTraining Loss: 1.822347 \tValidation Loss: 2.324153\n",
      "Epoch: 13243 \tTraining Loss: 1.771655 \tValidation Loss: 2.324768\n",
      "Epoch: 13244 \tTraining Loss: 1.812250 \tValidation Loss: 2.324812\n",
      "Epoch: 13245 \tTraining Loss: 1.795356 \tValidation Loss: 2.324394\n",
      "Epoch: 13246 \tTraining Loss: 1.793942 \tValidation Loss: 2.324059\n",
      "Epoch: 13247 \tTraining Loss: 1.780299 \tValidation Loss: 2.324227\n",
      "Epoch: 13248 \tTraining Loss: 1.803611 \tValidation Loss: 2.323965\n",
      "Epoch: 13249 \tTraining Loss: 1.829576 \tValidation Loss: 2.324394\n",
      "Epoch: 13250 \tTraining Loss: 1.772949 \tValidation Loss: 2.323964\n",
      "Epoch: 13251 \tTraining Loss: 1.825209 \tValidation Loss: 2.324298\n",
      "Epoch: 13252 \tTraining Loss: 1.774966 \tValidation Loss: 2.324157\n",
      "Epoch: 13253 \tTraining Loss: 1.822049 \tValidation Loss: 2.324309\n",
      "Epoch: 13254 \tTraining Loss: 1.801135 \tValidation Loss: 2.324643\n",
      "Epoch: 13255 \tTraining Loss: 1.798850 \tValidation Loss: 2.324111\n",
      "Epoch: 13256 \tTraining Loss: 1.814773 \tValidation Loss: 2.324337\n",
      "Epoch: 13257 \tTraining Loss: 1.835532 \tValidation Loss: 2.324136\n",
      "Epoch: 13258 \tTraining Loss: 1.799243 \tValidation Loss: 2.323350\n",
      "Epoch: 13259 \tTraining Loss: 1.836161 \tValidation Loss: 2.323289\n",
      "Epoch: 13260 \tTraining Loss: 1.820907 \tValidation Loss: 2.323471\n",
      "Epoch: 13261 \tTraining Loss: 1.750659 \tValidation Loss: 2.324108\n",
      "Epoch: 13262 \tTraining Loss: 1.817625 \tValidation Loss: 2.324467\n",
      "Epoch: 13263 \tTraining Loss: 1.792272 \tValidation Loss: 2.324419\n",
      "Epoch: 13264 \tTraining Loss: 1.771221 \tValidation Loss: 2.324385\n",
      "Epoch: 13265 \tTraining Loss: 1.808719 \tValidation Loss: 2.324356\n",
      "Epoch: 13266 \tTraining Loss: 1.807233 \tValidation Loss: 2.324437\n",
      "Epoch: 13267 \tTraining Loss: 1.766631 \tValidation Loss: 2.324803\n",
      "Epoch: 13268 \tTraining Loss: 1.796664 \tValidation Loss: 2.324616\n",
      "Epoch: 13269 \tTraining Loss: 1.820247 \tValidation Loss: 2.324250\n",
      "Epoch: 13270 \tTraining Loss: 1.827061 \tValidation Loss: 2.324402\n",
      "Epoch: 13271 \tTraining Loss: 1.836279 \tValidation Loss: 2.324032\n",
      "Epoch: 13272 \tTraining Loss: 1.825596 \tValidation Loss: 2.323746\n",
      "Epoch: 13273 \tTraining Loss: 1.786076 \tValidation Loss: 2.324271\n",
      "Epoch: 13274 \tTraining Loss: 1.824663 \tValidation Loss: 2.324460\n",
      "Epoch: 13275 \tTraining Loss: 1.824114 \tValidation Loss: 2.324022\n",
      "Epoch: 13276 \tTraining Loss: 1.802347 \tValidation Loss: 2.323940\n",
      "Epoch: 13277 \tTraining Loss: 1.823417 \tValidation Loss: 2.324403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13278 \tTraining Loss: 1.813966 \tValidation Loss: 2.324183\n",
      "Epoch: 13279 \tTraining Loss: 1.808497 \tValidation Loss: 2.324027\n",
      "Epoch: 13280 \tTraining Loss: 1.812077 \tValidation Loss: 2.324322\n",
      "Epoch: 13281 \tTraining Loss: 1.804997 \tValidation Loss: 2.324305\n",
      "Epoch: 13282 \tTraining Loss: 1.800246 \tValidation Loss: 2.324432\n",
      "Epoch: 13283 \tTraining Loss: 1.817167 \tValidation Loss: 2.324037\n",
      "Epoch: 13284 \tTraining Loss: 1.789416 \tValidation Loss: 2.324070\n",
      "Epoch: 13285 \tTraining Loss: 1.801263 \tValidation Loss: 2.323903\n",
      "Epoch: 13286 \tTraining Loss: 1.799289 \tValidation Loss: 2.324632\n",
      "Epoch: 13287 \tTraining Loss: 1.783190 \tValidation Loss: 2.324535\n",
      "Epoch: 13288 \tTraining Loss: 1.816944 \tValidation Loss: 2.324278\n",
      "Epoch: 13289 \tTraining Loss: 1.821805 \tValidation Loss: 2.324474\n",
      "Epoch: 13290 \tTraining Loss: 1.796960 \tValidation Loss: 2.324610\n",
      "Epoch: 13291 \tTraining Loss: 1.798774 \tValidation Loss: 2.324650\n",
      "Epoch: 13292 \tTraining Loss: 1.798978 \tValidation Loss: 2.325074\n",
      "Epoch: 13293 \tTraining Loss: 1.799948 \tValidation Loss: 2.324910\n",
      "Epoch: 13294 \tTraining Loss: 1.827045 \tValidation Loss: 2.324663\n",
      "Epoch: 13295 \tTraining Loss: 1.830728 \tValidation Loss: 2.325191\n",
      "Epoch: 13296 \tTraining Loss: 1.821073 \tValidation Loss: 2.324497\n",
      "Epoch: 13297 \tTraining Loss: 1.811678 \tValidation Loss: 2.324229\n",
      "Epoch: 13298 \tTraining Loss: 1.789629 \tValidation Loss: 2.324300\n",
      "Epoch: 13299 \tTraining Loss: 1.790755 \tValidation Loss: 2.324922\n",
      "Epoch: 13300 \tTraining Loss: 1.836287 \tValidation Loss: 2.324658\n",
      "Epoch: 13301 \tTraining Loss: 1.803717 \tValidation Loss: 2.324669\n",
      "Epoch: 13302 \tTraining Loss: 1.824682 \tValidation Loss: 2.324677\n",
      "Epoch: 13303 \tTraining Loss: 1.814531 \tValidation Loss: 2.324690\n",
      "Epoch: 13304 \tTraining Loss: 1.800828 \tValidation Loss: 2.324966\n",
      "Epoch: 13305 \tTraining Loss: 1.840972 \tValidation Loss: 2.325130\n",
      "Epoch: 13306 \tTraining Loss: 1.793950 \tValidation Loss: 2.325414\n",
      "Epoch: 13307 \tTraining Loss: 1.794595 \tValidation Loss: 2.324964\n",
      "Epoch: 13308 \tTraining Loss: 1.813882 \tValidation Loss: 2.324836\n",
      "Epoch: 13309 \tTraining Loss: 1.836914 \tValidation Loss: 2.324720\n",
      "Epoch: 13310 \tTraining Loss: 1.826504 \tValidation Loss: 2.324471\n",
      "Epoch: 13311 \tTraining Loss: 1.849006 \tValidation Loss: 2.324389\n",
      "Epoch: 13312 \tTraining Loss: 1.815991 \tValidation Loss: 2.324347\n",
      "Epoch: 13313 \tTraining Loss: 1.785335 \tValidation Loss: 2.324338\n",
      "Epoch: 13314 \tTraining Loss: 1.832055 \tValidation Loss: 2.324347\n",
      "Epoch: 13315 \tTraining Loss: 1.779535 \tValidation Loss: 2.323875\n",
      "Epoch: 13316 \tTraining Loss: 1.811457 \tValidation Loss: 2.324006\n",
      "Epoch: 13317 \tTraining Loss: 1.811683 \tValidation Loss: 2.324568\n",
      "Epoch: 13318 \tTraining Loss: 1.803686 \tValidation Loss: 2.324559\n",
      "Epoch: 13319 \tTraining Loss: 1.831302 \tValidation Loss: 2.324871\n",
      "Epoch: 13320 \tTraining Loss: 1.805748 \tValidation Loss: 2.325310\n",
      "Epoch: 13321 \tTraining Loss: 1.818310 \tValidation Loss: 2.325517\n",
      "Epoch: 13322 \tTraining Loss: 1.816421 \tValidation Loss: 2.325562\n",
      "Epoch: 13323 \tTraining Loss: 1.811294 \tValidation Loss: 2.324902\n",
      "Epoch: 13324 \tTraining Loss: 1.830058 \tValidation Loss: 2.324799\n",
      "Epoch: 13325 \tTraining Loss: 1.804021 \tValidation Loss: 2.325138\n",
      "Epoch: 13326 \tTraining Loss: 1.826601 \tValidation Loss: 2.324928\n",
      "Epoch: 13327 \tTraining Loss: 1.799190 \tValidation Loss: 2.325171\n",
      "Epoch: 13328 \tTraining Loss: 1.780020 \tValidation Loss: 2.325570\n",
      "Epoch: 13329 \tTraining Loss: 1.803581 \tValidation Loss: 2.325517\n",
      "Epoch: 13330 \tTraining Loss: 1.787078 \tValidation Loss: 2.325435\n",
      "Epoch: 13331 \tTraining Loss: 1.794970 \tValidation Loss: 2.325837\n",
      "Epoch: 13332 \tTraining Loss: 1.787876 \tValidation Loss: 2.325181\n",
      "Epoch: 13333 \tTraining Loss: 1.813211 \tValidation Loss: 2.324781\n",
      "Epoch: 13334 \tTraining Loss: 1.768247 \tValidation Loss: 2.325209\n",
      "Epoch: 13335 \tTraining Loss: 1.794657 \tValidation Loss: 2.325116\n",
      "Epoch: 13336 \tTraining Loss: 1.771851 \tValidation Loss: 2.325474\n",
      "Epoch: 13337 \tTraining Loss: 1.806222 \tValidation Loss: 2.324923\n",
      "Epoch: 13338 \tTraining Loss: 1.807386 \tValidation Loss: 2.324850\n",
      "Epoch: 13339 \tTraining Loss: 1.861337 \tValidation Loss: 2.324829\n",
      "Epoch: 13340 \tTraining Loss: 1.814845 \tValidation Loss: 2.324629\n",
      "Epoch: 13341 \tTraining Loss: 1.825407 \tValidation Loss: 2.324171\n",
      "Epoch: 13342 \tTraining Loss: 1.755346 \tValidation Loss: 2.324337\n",
      "Epoch: 13343 \tTraining Loss: 1.843839 \tValidation Loss: 2.324178\n",
      "Epoch: 13344 \tTraining Loss: 1.820661 \tValidation Loss: 2.323961\n",
      "Epoch: 13345 \tTraining Loss: 1.826962 \tValidation Loss: 2.323838\n",
      "Epoch: 13346 \tTraining Loss: 1.808281 \tValidation Loss: 2.324482\n",
      "Epoch: 13347 \tTraining Loss: 1.830237 \tValidation Loss: 2.324392\n",
      "Epoch: 13348 \tTraining Loss: 1.821265 \tValidation Loss: 2.324386\n",
      "Epoch: 13349 \tTraining Loss: 1.793336 \tValidation Loss: 2.324962\n",
      "Epoch: 13350 \tTraining Loss: 1.807277 \tValidation Loss: 2.325144\n",
      "Epoch: 13351 \tTraining Loss: 1.834648 \tValidation Loss: 2.325222\n",
      "Epoch: 13352 \tTraining Loss: 1.802179 \tValidation Loss: 2.325228\n",
      "Epoch: 13353 \tTraining Loss: 1.778307 \tValidation Loss: 2.324780\n",
      "Epoch: 13354 \tTraining Loss: 1.790622 \tValidation Loss: 2.325124\n",
      "Epoch: 13355 \tTraining Loss: 1.808158 \tValidation Loss: 2.324808\n",
      "Epoch: 13356 \tTraining Loss: 1.747566 \tValidation Loss: 2.325307\n",
      "Epoch: 13357 \tTraining Loss: 1.816066 \tValidation Loss: 2.325363\n",
      "Epoch: 13358 \tTraining Loss: 1.792230 \tValidation Loss: 2.325327\n",
      "Epoch: 13359 \tTraining Loss: 1.828750 \tValidation Loss: 2.324726\n",
      "Epoch: 13360 \tTraining Loss: 1.839623 \tValidation Loss: 2.324923\n",
      "Epoch: 13361 \tTraining Loss: 1.809954 \tValidation Loss: 2.325251\n",
      "Epoch: 13362 \tTraining Loss: 1.820980 \tValidation Loss: 2.325060\n",
      "Epoch: 13363 \tTraining Loss: 1.780226 \tValidation Loss: 2.324893\n",
      "Epoch: 13364 \tTraining Loss: 1.812935 \tValidation Loss: 2.324816\n",
      "Epoch: 13365 \tTraining Loss: 1.772080 \tValidation Loss: 2.325312\n",
      "Epoch: 13366 \tTraining Loss: 1.760448 \tValidation Loss: 2.325566\n",
      "Epoch: 13367 \tTraining Loss: 1.788167 \tValidation Loss: 2.325641\n",
      "Epoch: 13368 \tTraining Loss: 1.780024 \tValidation Loss: 2.325016\n",
      "Epoch: 13369 \tTraining Loss: 1.823436 \tValidation Loss: 2.325232\n",
      "Epoch: 13370 \tTraining Loss: 1.803587 \tValidation Loss: 2.325097\n",
      "Epoch: 13371 \tTraining Loss: 1.805973 \tValidation Loss: 2.324821\n",
      "Epoch: 13372 \tTraining Loss: 1.825404 \tValidation Loss: 2.324978\n",
      "Epoch: 13373 \tTraining Loss: 1.817365 \tValidation Loss: 2.325194\n",
      "Epoch: 13374 \tTraining Loss: 1.791233 \tValidation Loss: 2.325278\n",
      "Epoch: 13375 \tTraining Loss: 1.793677 \tValidation Loss: 2.325650\n",
      "Epoch: 13376 \tTraining Loss: 1.787608 \tValidation Loss: 2.325114\n",
      "Epoch: 13377 \tTraining Loss: 1.799057 \tValidation Loss: 2.325303\n",
      "Epoch: 13378 \tTraining Loss: 1.814547 \tValidation Loss: 2.325320\n",
      "Epoch: 13379 \tTraining Loss: 1.831483 \tValidation Loss: 2.324972\n",
      "Epoch: 13380 \tTraining Loss: 1.804216 \tValidation Loss: 2.325343\n",
      "Epoch: 13381 \tTraining Loss: 1.789157 \tValidation Loss: 2.325618\n",
      "Epoch: 13382 \tTraining Loss: 1.768884 \tValidation Loss: 2.325129\n",
      "Epoch: 13383 \tTraining Loss: 1.790705 \tValidation Loss: 2.325371\n",
      "Epoch: 13384 \tTraining Loss: 1.838676 \tValidation Loss: 2.325036\n",
      "Epoch: 13385 \tTraining Loss: 1.825594 \tValidation Loss: 2.325293\n",
      "Epoch: 13386 \tTraining Loss: 1.793559 \tValidation Loss: 2.325279\n",
      "Epoch: 13387 \tTraining Loss: 1.824100 \tValidation Loss: 2.325253\n",
      "Epoch: 13388 \tTraining Loss: 1.761259 \tValidation Loss: 2.325431\n",
      "Epoch: 13389 \tTraining Loss: 1.849000 \tValidation Loss: 2.325103\n",
      "Epoch: 13390 \tTraining Loss: 1.811021 \tValidation Loss: 2.324950\n",
      "Epoch: 13391 \tTraining Loss: 1.771318 \tValidation Loss: 2.325583\n",
      "Epoch: 13392 \tTraining Loss: 1.811514 \tValidation Loss: 2.325964\n",
      "Epoch: 13393 \tTraining Loss: 1.793925 \tValidation Loss: 2.326237\n",
      "Epoch: 13394 \tTraining Loss: 1.807026 \tValidation Loss: 2.325824\n",
      "Epoch: 13395 \tTraining Loss: 1.807078 \tValidation Loss: 2.325474\n",
      "Epoch: 13396 \tTraining Loss: 1.806440 \tValidation Loss: 2.326112\n",
      "Epoch: 13397 \tTraining Loss: 1.844860 \tValidation Loss: 2.325596\n",
      "Epoch: 13398 \tTraining Loss: 1.777330 \tValidation Loss: 2.325800\n",
      "Epoch: 13399 \tTraining Loss: 1.785649 \tValidation Loss: 2.325549\n",
      "Epoch: 13400 \tTraining Loss: 1.773036 \tValidation Loss: 2.325475\n",
      "Epoch: 13401 \tTraining Loss: 1.796806 \tValidation Loss: 2.325596\n",
      "Epoch: 13402 \tTraining Loss: 1.794108 \tValidation Loss: 2.325618\n",
      "Epoch: 13403 \tTraining Loss: 1.808455 \tValidation Loss: 2.325552\n",
      "Epoch: 13404 \tTraining Loss: 1.831511 \tValidation Loss: 2.325113\n",
      "Epoch: 13405 \tTraining Loss: 1.793257 \tValidation Loss: 2.325027\n",
      "Epoch: 13406 \tTraining Loss: 1.810405 \tValidation Loss: 2.324922\n",
      "Epoch: 13407 \tTraining Loss: 1.806832 \tValidation Loss: 2.325258\n",
      "Epoch: 13408 \tTraining Loss: 1.836252 \tValidation Loss: 2.325300\n",
      "Epoch: 13409 \tTraining Loss: 1.784984 \tValidation Loss: 2.325858\n",
      "Epoch: 13410 \tTraining Loss: 1.812755 \tValidation Loss: 2.325695\n",
      "Epoch: 13411 \tTraining Loss: 1.816744 \tValidation Loss: 2.325451\n",
      "Epoch: 13412 \tTraining Loss: 1.804685 \tValidation Loss: 2.325838\n",
      "Epoch: 13413 \tTraining Loss: 1.838595 \tValidation Loss: 2.325232\n",
      "Epoch: 13414 \tTraining Loss: 1.793250 \tValidation Loss: 2.324914\n",
      "Epoch: 13415 \tTraining Loss: 1.827205 \tValidation Loss: 2.324915\n",
      "Epoch: 13416 \tTraining Loss: 1.814198 \tValidation Loss: 2.325684\n",
      "Epoch: 13417 \tTraining Loss: 1.815443 \tValidation Loss: 2.325567\n",
      "Epoch: 13418 \tTraining Loss: 1.780365 \tValidation Loss: 2.325629\n",
      "Epoch: 13419 \tTraining Loss: 1.790660 \tValidation Loss: 2.325455\n",
      "Epoch: 13420 \tTraining Loss: 1.797204 \tValidation Loss: 2.325414\n",
      "Epoch: 13421 \tTraining Loss: 1.777983 \tValidation Loss: 2.325716\n",
      "Epoch: 13422 \tTraining Loss: 1.805513 \tValidation Loss: 2.325217\n",
      "Epoch: 13423 \tTraining Loss: 1.829195 \tValidation Loss: 2.325432\n",
      "Epoch: 13424 \tTraining Loss: 1.793475 \tValidation Loss: 2.325714\n",
      "Epoch: 13425 \tTraining Loss: 1.817696 \tValidation Loss: 2.325375\n",
      "Epoch: 13426 \tTraining Loss: 1.784159 \tValidation Loss: 2.324910\n",
      "Epoch: 13427 \tTraining Loss: 1.805473 \tValidation Loss: 2.325445\n",
      "Epoch: 13428 \tTraining Loss: 1.810360 \tValidation Loss: 2.325352\n",
      "Epoch: 13429 \tTraining Loss: 1.782005 \tValidation Loss: 2.325630\n",
      "Epoch: 13430 \tTraining Loss: 1.804185 \tValidation Loss: 2.325718\n",
      "Epoch: 13431 \tTraining Loss: 1.808998 \tValidation Loss: 2.325316\n",
      "Epoch: 13432 \tTraining Loss: 1.797750 \tValidation Loss: 2.325395\n",
      "Epoch: 13433 \tTraining Loss: 1.823472 \tValidation Loss: 2.325258\n",
      "Epoch: 13434 \tTraining Loss: 1.835344 \tValidation Loss: 2.325536\n",
      "Epoch: 13435 \tTraining Loss: 1.792117 \tValidation Loss: 2.325866\n",
      "Epoch: 13436 \tTraining Loss: 1.802557 \tValidation Loss: 2.325784\n",
      "Epoch: 13437 \tTraining Loss: 1.793409 \tValidation Loss: 2.326011\n",
      "Epoch: 13438 \tTraining Loss: 1.809426 \tValidation Loss: 2.325666\n",
      "Epoch: 13439 \tTraining Loss: 1.799981 \tValidation Loss: 2.325675\n",
      "Epoch: 13440 \tTraining Loss: 1.808185 \tValidation Loss: 2.325884\n",
      "Epoch: 13441 \tTraining Loss: 1.828367 \tValidation Loss: 2.325461\n",
      "Epoch: 13442 \tTraining Loss: 1.815083 \tValidation Loss: 2.325553\n",
      "Epoch: 13443 \tTraining Loss: 1.794495 \tValidation Loss: 2.325472\n",
      "Epoch: 13444 \tTraining Loss: 1.803692 \tValidation Loss: 2.325886\n",
      "Epoch: 13445 \tTraining Loss: 1.786011 \tValidation Loss: 2.325524\n",
      "Epoch: 13446 \tTraining Loss: 1.774623 \tValidation Loss: 2.325382\n",
      "Epoch: 13447 \tTraining Loss: 1.816113 \tValidation Loss: 2.325793\n",
      "Epoch: 13448 \tTraining Loss: 1.785993 \tValidation Loss: 2.325654\n",
      "Epoch: 13449 \tTraining Loss: 1.816281 \tValidation Loss: 2.325518\n",
      "Epoch: 13450 \tTraining Loss: 1.822501 \tValidation Loss: 2.325807\n",
      "Epoch: 13451 \tTraining Loss: 1.826017 \tValidation Loss: 2.325748\n",
      "Epoch: 13452 \tTraining Loss: 1.805188 \tValidation Loss: 2.325925\n",
      "Epoch: 13453 \tTraining Loss: 1.816419 \tValidation Loss: 2.325848\n",
      "Epoch: 13454 \tTraining Loss: 1.814237 \tValidation Loss: 2.326208\n",
      "Epoch: 13455 \tTraining Loss: 1.813861 \tValidation Loss: 2.325938\n",
      "Epoch: 13456 \tTraining Loss: 1.806938 \tValidation Loss: 2.325750\n",
      "Epoch: 13457 \tTraining Loss: 1.802544 \tValidation Loss: 2.325524\n",
      "Epoch: 13458 \tTraining Loss: 1.783562 \tValidation Loss: 2.325458\n",
      "Epoch: 13459 \tTraining Loss: 1.819637 \tValidation Loss: 2.325660\n",
      "Epoch: 13460 \tTraining Loss: 1.803704 \tValidation Loss: 2.325398\n",
      "Epoch: 13461 \tTraining Loss: 1.808084 \tValidation Loss: 2.325092\n",
      "Epoch: 13462 \tTraining Loss: 1.817501 \tValidation Loss: 2.325343\n",
      "Epoch: 13463 \tTraining Loss: 1.810368 \tValidation Loss: 2.325426\n",
      "Epoch: 13464 \tTraining Loss: 1.815228 \tValidation Loss: 2.325117\n",
      "Epoch: 13465 \tTraining Loss: 1.789405 \tValidation Loss: 2.325099\n",
      "Epoch: 13466 \tTraining Loss: 1.798675 \tValidation Loss: 2.325663\n",
      "Epoch: 13467 \tTraining Loss: 1.798226 \tValidation Loss: 2.326115\n",
      "Epoch: 13468 \tTraining Loss: 1.785874 \tValidation Loss: 2.325857\n",
      "Epoch: 13469 \tTraining Loss: 1.823002 \tValidation Loss: 2.325706\n",
      "Epoch: 13470 \tTraining Loss: 1.829698 \tValidation Loss: 2.325684\n",
      "Epoch: 13471 \tTraining Loss: 1.827301 \tValidation Loss: 2.325876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13472 \tTraining Loss: 1.785583 \tValidation Loss: 2.326093\n",
      "Epoch: 13473 \tTraining Loss: 1.763554 \tValidation Loss: 2.325891\n",
      "Epoch: 13474 \tTraining Loss: 1.818547 \tValidation Loss: 2.325949\n",
      "Epoch: 13475 \tTraining Loss: 1.795318 \tValidation Loss: 2.325800\n",
      "Epoch: 13476 \tTraining Loss: 1.782268 \tValidation Loss: 2.325988\n",
      "Epoch: 13477 \tTraining Loss: 1.759981 \tValidation Loss: 2.326243\n",
      "Epoch: 13478 \tTraining Loss: 1.807394 \tValidation Loss: 2.325911\n",
      "Epoch: 13479 \tTraining Loss: 1.765421 \tValidation Loss: 2.325879\n",
      "Epoch: 13480 \tTraining Loss: 1.771252 \tValidation Loss: 2.326038\n",
      "Epoch: 13481 \tTraining Loss: 1.809324 \tValidation Loss: 2.326223\n",
      "Epoch: 13482 \tTraining Loss: 1.787886 \tValidation Loss: 2.326171\n",
      "Epoch: 13483 \tTraining Loss: 1.799875 \tValidation Loss: 2.326142\n",
      "Epoch: 13484 \tTraining Loss: 1.803595 \tValidation Loss: 2.326837\n",
      "Epoch: 13485 \tTraining Loss: 1.827294 \tValidation Loss: 2.326363\n",
      "Epoch: 13486 \tTraining Loss: 1.851061 \tValidation Loss: 2.326376\n",
      "Epoch: 13487 \tTraining Loss: 1.799892 \tValidation Loss: 2.326472\n",
      "Epoch: 13488 \tTraining Loss: 1.802316 \tValidation Loss: 2.326295\n",
      "Epoch: 13489 \tTraining Loss: 1.768812 \tValidation Loss: 2.326672\n",
      "Epoch: 13490 \tTraining Loss: 1.814700 \tValidation Loss: 2.326689\n",
      "Epoch: 13491 \tTraining Loss: 1.803490 \tValidation Loss: 2.326932\n",
      "Epoch: 13492 \tTraining Loss: 1.803295 \tValidation Loss: 2.326595\n",
      "Epoch: 13493 \tTraining Loss: 1.816247 \tValidation Loss: 2.326674\n",
      "Epoch: 13494 \tTraining Loss: 1.816730 \tValidation Loss: 2.326411\n",
      "Epoch: 13495 \tTraining Loss: 1.794706 \tValidation Loss: 2.326218\n",
      "Epoch: 13496 \tTraining Loss: 1.819598 \tValidation Loss: 2.325994\n",
      "Epoch: 13497 \tTraining Loss: 1.770007 \tValidation Loss: 2.326390\n",
      "Epoch: 13498 \tTraining Loss: 1.773010 \tValidation Loss: 2.326554\n",
      "Epoch: 13499 \tTraining Loss: 1.825036 \tValidation Loss: 2.326404\n",
      "Epoch: 13500 \tTraining Loss: 1.766677 \tValidation Loss: 2.326305\n",
      "Epoch: 13501 \tTraining Loss: 1.779313 \tValidation Loss: 2.326221\n",
      "Epoch: 13502 \tTraining Loss: 1.775993 \tValidation Loss: 2.326290\n",
      "Epoch: 13503 \tTraining Loss: 1.817765 \tValidation Loss: 2.326521\n",
      "Epoch: 13504 \tTraining Loss: 1.794920 \tValidation Loss: 2.326516\n",
      "Epoch: 13505 \tTraining Loss: 1.818039 \tValidation Loss: 2.326642\n",
      "Epoch: 13506 \tTraining Loss: 1.775537 \tValidation Loss: 2.325967\n",
      "Epoch: 13507 \tTraining Loss: 1.796472 \tValidation Loss: 2.326411\n",
      "Epoch: 13508 \tTraining Loss: 1.801782 \tValidation Loss: 2.326131\n",
      "Epoch: 13509 \tTraining Loss: 1.800442 \tValidation Loss: 2.325999\n",
      "Epoch: 13510 \tTraining Loss: 1.804805 \tValidation Loss: 2.325833\n",
      "Epoch: 13511 \tTraining Loss: 1.771456 \tValidation Loss: 2.326179\n",
      "Epoch: 13512 \tTraining Loss: 1.812242 \tValidation Loss: 2.326337\n",
      "Epoch: 13513 \tTraining Loss: 1.812129 \tValidation Loss: 2.326445\n",
      "Epoch: 13514 \tTraining Loss: 1.807202 \tValidation Loss: 2.326314\n",
      "Epoch: 13515 \tTraining Loss: 1.804891 \tValidation Loss: 2.326719\n",
      "Epoch: 13516 \tTraining Loss: 1.797992 \tValidation Loss: 2.326895\n",
      "Epoch: 13517 \tTraining Loss: 1.821787 \tValidation Loss: 2.327352\n",
      "Epoch: 13518 \tTraining Loss: 1.825887 \tValidation Loss: 2.326938\n",
      "Epoch: 13519 \tTraining Loss: 1.732127 \tValidation Loss: 2.326696\n",
      "Epoch: 13520 \tTraining Loss: 1.798077 \tValidation Loss: 2.326171\n",
      "Epoch: 13521 \tTraining Loss: 1.831411 \tValidation Loss: 2.326123\n",
      "Epoch: 13522 \tTraining Loss: 1.799140 \tValidation Loss: 2.325845\n",
      "Epoch: 13523 \tTraining Loss: 1.798440 \tValidation Loss: 2.326037\n",
      "Epoch: 13524 \tTraining Loss: 1.778600 \tValidation Loss: 2.326061\n",
      "Epoch: 13525 \tTraining Loss: 1.812959 \tValidation Loss: 2.326255\n",
      "Epoch: 13526 \tTraining Loss: 1.807167 \tValidation Loss: 2.326262\n",
      "Epoch: 13527 \tTraining Loss: 1.779420 \tValidation Loss: 2.326578\n",
      "Epoch: 13528 \tTraining Loss: 1.791977 \tValidation Loss: 2.326387\n",
      "Epoch: 13529 \tTraining Loss: 1.787008 \tValidation Loss: 2.326262\n",
      "Epoch: 13530 \tTraining Loss: 1.863905 \tValidation Loss: 2.326044\n",
      "Epoch: 13531 \tTraining Loss: 1.766411 \tValidation Loss: 2.325841\n",
      "Epoch: 13532 \tTraining Loss: 1.795857 \tValidation Loss: 2.325400\n",
      "Epoch: 13533 \tTraining Loss: 1.821119 \tValidation Loss: 2.325530\n",
      "Epoch: 13534 \tTraining Loss: 1.804020 \tValidation Loss: 2.325657\n",
      "Epoch: 13535 \tTraining Loss: 1.846437 \tValidation Loss: 2.325569\n",
      "Epoch: 13536 \tTraining Loss: 1.810888 \tValidation Loss: 2.325497\n",
      "Epoch: 13537 \tTraining Loss: 1.802165 \tValidation Loss: 2.325870\n",
      "Epoch: 13538 \tTraining Loss: 1.819026 \tValidation Loss: 2.326093\n",
      "Epoch: 13539 \tTraining Loss: 1.811841 \tValidation Loss: 2.326567\n",
      "Epoch: 13540 \tTraining Loss: 1.793022 \tValidation Loss: 2.326735\n",
      "Epoch: 13541 \tTraining Loss: 1.805850 \tValidation Loss: 2.326685\n",
      "Epoch: 13542 \tTraining Loss: 1.769280 \tValidation Loss: 2.327036\n",
      "Epoch: 13543 \tTraining Loss: 1.779894 \tValidation Loss: 2.326726\n",
      "Epoch: 13544 \tTraining Loss: 1.786287 \tValidation Loss: 2.326282\n",
      "Epoch: 13545 \tTraining Loss: 1.836533 \tValidation Loss: 2.326326\n",
      "Epoch: 13546 \tTraining Loss: 1.834046 \tValidation Loss: 2.326120\n",
      "Epoch: 13547 \tTraining Loss: 1.793962 \tValidation Loss: 2.326202\n",
      "Epoch: 13548 \tTraining Loss: 1.791624 \tValidation Loss: 2.326499\n",
      "Epoch: 13549 \tTraining Loss: 1.806772 \tValidation Loss: 2.326382\n",
      "Epoch: 13550 \tTraining Loss: 1.799563 \tValidation Loss: 2.326158\n",
      "Epoch: 13551 \tTraining Loss: 1.786295 \tValidation Loss: 2.326659\n",
      "Epoch: 13552 \tTraining Loss: 1.806606 \tValidation Loss: 2.326578\n",
      "Epoch: 13553 \tTraining Loss: 1.772040 \tValidation Loss: 2.326960\n",
      "Epoch: 13554 \tTraining Loss: 1.802705 \tValidation Loss: 2.326676\n",
      "Epoch: 13555 \tTraining Loss: 1.770457 \tValidation Loss: 2.326741\n",
      "Epoch: 13556 \tTraining Loss: 1.813045 \tValidation Loss: 2.326496\n",
      "Epoch: 13557 \tTraining Loss: 1.811966 \tValidation Loss: 2.326563\n",
      "Epoch: 13558 \tTraining Loss: 1.797776 \tValidation Loss: 2.326479\n",
      "Epoch: 13559 \tTraining Loss: 1.764958 \tValidation Loss: 2.326473\n",
      "Epoch: 13560 \tTraining Loss: 1.820585 \tValidation Loss: 2.326385\n",
      "Epoch: 13561 \tTraining Loss: 1.757530 \tValidation Loss: 2.326888\n",
      "Epoch: 13562 \tTraining Loss: 1.804164 \tValidation Loss: 2.327116\n",
      "Epoch: 13563 \tTraining Loss: 1.788734 \tValidation Loss: 2.326902\n",
      "Epoch: 13564 \tTraining Loss: 1.788502 \tValidation Loss: 2.326846\n",
      "Epoch: 13565 \tTraining Loss: 1.794158 \tValidation Loss: 2.327220\n",
      "Epoch: 13566 \tTraining Loss: 1.784023 \tValidation Loss: 2.327101\n",
      "Epoch: 13567 \tTraining Loss: 1.816004 \tValidation Loss: 2.327081\n",
      "Epoch: 13568 \tTraining Loss: 1.803146 \tValidation Loss: 2.327050\n",
      "Epoch: 13569 \tTraining Loss: 1.783638 \tValidation Loss: 2.326677\n",
      "Epoch: 13570 \tTraining Loss: 1.795724 \tValidation Loss: 2.326558\n",
      "Epoch: 13571 \tTraining Loss: 1.783300 \tValidation Loss: 2.326934\n",
      "Epoch: 13572 \tTraining Loss: 1.757523 \tValidation Loss: 2.326953\n",
      "Epoch: 13573 \tTraining Loss: 1.816230 \tValidation Loss: 2.327254\n",
      "Epoch: 13574 \tTraining Loss: 1.822499 \tValidation Loss: 2.327193\n",
      "Epoch: 13575 \tTraining Loss: 1.803440 \tValidation Loss: 2.326673\n",
      "Epoch: 13576 \tTraining Loss: 1.804546 \tValidation Loss: 2.326214\n",
      "Epoch: 13577 \tTraining Loss: 1.777652 \tValidation Loss: 2.326141\n",
      "Epoch: 13578 \tTraining Loss: 1.818944 \tValidation Loss: 2.326423\n",
      "Epoch: 13579 \tTraining Loss: 1.798741 \tValidation Loss: 2.326576\n",
      "Epoch: 13580 \tTraining Loss: 1.792151 \tValidation Loss: 2.326626\n",
      "Epoch: 13581 \tTraining Loss: 1.765508 \tValidation Loss: 2.326551\n",
      "Epoch: 13582 \tTraining Loss: 1.782989 \tValidation Loss: 2.326771\n",
      "Epoch: 13583 \tTraining Loss: 1.759843 \tValidation Loss: 2.326462\n",
      "Epoch: 13584 \tTraining Loss: 1.793661 \tValidation Loss: 2.326388\n",
      "Epoch: 13585 \tTraining Loss: 1.774927 \tValidation Loss: 2.326282\n",
      "Epoch: 13586 \tTraining Loss: 1.798493 \tValidation Loss: 2.326095\n",
      "Epoch: 13587 \tTraining Loss: 1.775450 \tValidation Loss: 2.325951\n",
      "Epoch: 13588 \tTraining Loss: 1.804313 \tValidation Loss: 2.326152\n",
      "Epoch: 13589 \tTraining Loss: 1.786656 \tValidation Loss: 2.326509\n",
      "Epoch: 13590 \tTraining Loss: 1.795607 \tValidation Loss: 2.326590\n",
      "Epoch: 13591 \tTraining Loss: 1.780161 \tValidation Loss: 2.326555\n",
      "Epoch: 13592 \tTraining Loss: 1.786443 \tValidation Loss: 2.326752\n",
      "Epoch: 13593 \tTraining Loss: 1.793376 \tValidation Loss: 2.326856\n",
      "Epoch: 13594 \tTraining Loss: 1.820833 \tValidation Loss: 2.326963\n",
      "Epoch: 13595 \tTraining Loss: 1.807858 \tValidation Loss: 2.327263\n",
      "Epoch: 13596 \tTraining Loss: 1.778414 \tValidation Loss: 2.327156\n",
      "Epoch: 13597 \tTraining Loss: 1.787618 \tValidation Loss: 2.326913\n",
      "Epoch: 13598 \tTraining Loss: 1.808107 \tValidation Loss: 2.326854\n",
      "Epoch: 13599 \tTraining Loss: 1.784703 \tValidation Loss: 2.326627\n",
      "Epoch: 13600 \tTraining Loss: 1.813207 \tValidation Loss: 2.326842\n",
      "Epoch: 13601 \tTraining Loss: 1.784171 \tValidation Loss: 2.327218\n",
      "Epoch: 13602 \tTraining Loss: 1.799268 \tValidation Loss: 2.327190\n",
      "Epoch: 13603 \tTraining Loss: 1.778768 \tValidation Loss: 2.327350\n",
      "Epoch: 13604 \tTraining Loss: 1.788363 \tValidation Loss: 2.326858\n",
      "Epoch: 13605 \tTraining Loss: 1.810147 \tValidation Loss: 2.326872\n",
      "Epoch: 13606 \tTraining Loss: 1.792946 \tValidation Loss: 2.326519\n",
      "Epoch: 13607 \tTraining Loss: 1.770866 \tValidation Loss: 2.326341\n",
      "Epoch: 13608 \tTraining Loss: 1.789549 \tValidation Loss: 2.326511\n",
      "Epoch: 13609 \tTraining Loss: 1.803989 \tValidation Loss: 2.326598\n",
      "Epoch: 13610 \tTraining Loss: 1.804096 \tValidation Loss: 2.326600\n",
      "Epoch: 13611 \tTraining Loss: 1.795570 \tValidation Loss: 2.326577\n",
      "Epoch: 13612 \tTraining Loss: 1.801617 \tValidation Loss: 2.326645\n",
      "Epoch: 13613 \tTraining Loss: 1.800498 \tValidation Loss: 2.326368\n",
      "Epoch: 13614 \tTraining Loss: 1.793685 \tValidation Loss: 2.326444\n",
      "Epoch: 13615 \tTraining Loss: 1.789060 \tValidation Loss: 2.326818\n",
      "Epoch: 13616 \tTraining Loss: 1.768676 \tValidation Loss: 2.327008\n",
      "Epoch: 13617 \tTraining Loss: 1.782457 \tValidation Loss: 2.326762\n",
      "Epoch: 13618 \tTraining Loss: 1.779246 \tValidation Loss: 2.326978\n",
      "Epoch: 13619 \tTraining Loss: 1.797346 \tValidation Loss: 2.327324\n",
      "Epoch: 13620 \tTraining Loss: 1.828143 \tValidation Loss: 2.326771\n",
      "Epoch: 13621 \tTraining Loss: 1.821037 \tValidation Loss: 2.326633\n",
      "Epoch: 13622 \tTraining Loss: 1.785879 \tValidation Loss: 2.326628\n",
      "Epoch: 13623 \tTraining Loss: 1.786983 \tValidation Loss: 2.326982\n",
      "Epoch: 13624 \tTraining Loss: 1.775661 \tValidation Loss: 2.327275\n",
      "Epoch: 13625 \tTraining Loss: 1.808146 \tValidation Loss: 2.326646\n",
      "Epoch: 13626 \tTraining Loss: 1.798718 \tValidation Loss: 2.326483\n",
      "Epoch: 13627 \tTraining Loss: 1.794589 \tValidation Loss: 2.326603\n",
      "Epoch: 13628 \tTraining Loss: 1.835244 \tValidation Loss: 2.326654\n",
      "Epoch: 13629 \tTraining Loss: 1.793857 \tValidation Loss: 2.326753\n",
      "Epoch: 13630 \tTraining Loss: 1.809939 \tValidation Loss: 2.326622\n",
      "Epoch: 13631 \tTraining Loss: 1.798500 \tValidation Loss: 2.327031\n",
      "Epoch: 13632 \tTraining Loss: 1.793702 \tValidation Loss: 2.326946\n",
      "Epoch: 13633 \tTraining Loss: 1.813864 \tValidation Loss: 2.327033\n",
      "Epoch: 13634 \tTraining Loss: 1.794837 \tValidation Loss: 2.327123\n",
      "Epoch: 13635 \tTraining Loss: 1.795507 \tValidation Loss: 2.327420\n",
      "Epoch: 13636 \tTraining Loss: 1.795424 \tValidation Loss: 2.327112\n",
      "Epoch: 13637 \tTraining Loss: 1.769109 \tValidation Loss: 2.327407\n",
      "Epoch: 13638 \tTraining Loss: 1.794730 \tValidation Loss: 2.327284\n",
      "Epoch: 13639 \tTraining Loss: 1.810011 \tValidation Loss: 2.327379\n",
      "Epoch: 13640 \tTraining Loss: 1.790254 \tValidation Loss: 2.327358\n",
      "Epoch: 13641 \tTraining Loss: 1.820300 \tValidation Loss: 2.327167\n",
      "Epoch: 13642 \tTraining Loss: 1.793174 \tValidation Loss: 2.326982\n",
      "Epoch: 13643 \tTraining Loss: 1.793447 \tValidation Loss: 2.327142\n",
      "Epoch: 13644 \tTraining Loss: 1.776878 \tValidation Loss: 2.327334\n",
      "Epoch: 13645 \tTraining Loss: 1.810053 \tValidation Loss: 2.327456\n",
      "Epoch: 13646 \tTraining Loss: 1.793208 \tValidation Loss: 2.327107\n",
      "Epoch: 13647 \tTraining Loss: 1.772117 \tValidation Loss: 2.327047\n",
      "Epoch: 13648 \tTraining Loss: 1.778720 \tValidation Loss: 2.326662\n",
      "Epoch: 13649 \tTraining Loss: 1.772521 \tValidation Loss: 2.326873\n",
      "Epoch: 13650 \tTraining Loss: 1.822134 \tValidation Loss: 2.326523\n",
      "Epoch: 13651 \tTraining Loss: 1.827553 \tValidation Loss: 2.326364\n",
      "Epoch: 13652 \tTraining Loss: 1.802722 \tValidation Loss: 2.326148\n",
      "Epoch: 13653 \tTraining Loss: 1.805773 \tValidation Loss: 2.326448\n",
      "Epoch: 13654 \tTraining Loss: 1.768180 \tValidation Loss: 2.326552\n",
      "Epoch: 13655 \tTraining Loss: 1.796195 \tValidation Loss: 2.326706\n",
      "Epoch: 13656 \tTraining Loss: 1.761052 \tValidation Loss: 2.327095\n",
      "Epoch: 13657 \tTraining Loss: 1.806405 \tValidation Loss: 2.327142\n",
      "Epoch: 13658 \tTraining Loss: 1.783265 \tValidation Loss: 2.326852\n",
      "Epoch: 13659 \tTraining Loss: 1.801821 \tValidation Loss: 2.327117\n",
      "Epoch: 13660 \tTraining Loss: 1.795032 \tValidation Loss: 2.327879\n",
      "Epoch: 13661 \tTraining Loss: 1.812742 \tValidation Loss: 2.326988\n",
      "Epoch: 13662 \tTraining Loss: 1.800492 \tValidation Loss: 2.327150\n",
      "Epoch: 13663 \tTraining Loss: 1.792593 \tValidation Loss: 2.327118\n",
      "Epoch: 13664 \tTraining Loss: 1.781545 \tValidation Loss: 2.326690\n",
      "Epoch: 13665 \tTraining Loss: 1.776268 \tValidation Loss: 2.326979\n",
      "Epoch: 13666 \tTraining Loss: 1.841947 \tValidation Loss: 2.326940\n",
      "Epoch: 13667 \tTraining Loss: 1.785854 \tValidation Loss: 2.326771\n",
      "Epoch: 13668 \tTraining Loss: 1.784281 \tValidation Loss: 2.327336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13669 \tTraining Loss: 1.822765 \tValidation Loss: 2.327290\n",
      "Epoch: 13670 \tTraining Loss: 1.841923 \tValidation Loss: 2.327294\n",
      "Epoch: 13671 \tTraining Loss: 1.775869 \tValidation Loss: 2.327768\n",
      "Epoch: 13672 \tTraining Loss: 1.794973 \tValidation Loss: 2.327816\n",
      "Epoch: 13673 \tTraining Loss: 1.797104 \tValidation Loss: 2.327898\n",
      "Epoch: 13674 \tTraining Loss: 1.806587 \tValidation Loss: 2.327748\n",
      "Epoch: 13675 \tTraining Loss: 1.773022 \tValidation Loss: 2.327339\n",
      "Epoch: 13676 \tTraining Loss: 1.795933 \tValidation Loss: 2.327608\n",
      "Epoch: 13677 \tTraining Loss: 1.815026 \tValidation Loss: 2.327388\n",
      "Epoch: 13678 \tTraining Loss: 1.760404 \tValidation Loss: 2.327463\n",
      "Epoch: 13679 \tTraining Loss: 1.799834 \tValidation Loss: 2.328187\n",
      "Epoch: 13680 \tTraining Loss: 1.808435 \tValidation Loss: 2.327610\n",
      "Epoch: 13681 \tTraining Loss: 1.809977 \tValidation Loss: 2.327846\n",
      "Epoch: 13682 \tTraining Loss: 1.781410 \tValidation Loss: 2.327600\n",
      "Epoch: 13683 \tTraining Loss: 1.803210 \tValidation Loss: 2.327513\n",
      "Epoch: 13684 \tTraining Loss: 1.794868 \tValidation Loss: 2.327552\n",
      "Epoch: 13685 \tTraining Loss: 1.772560 \tValidation Loss: 2.327975\n",
      "Epoch: 13686 \tTraining Loss: 1.789754 \tValidation Loss: 2.327838\n",
      "Epoch: 13687 \tTraining Loss: 1.818230 \tValidation Loss: 2.327461\n",
      "Epoch: 13688 \tTraining Loss: 1.777431 \tValidation Loss: 2.327142\n",
      "Epoch: 13689 \tTraining Loss: 1.806675 \tValidation Loss: 2.327123\n",
      "Epoch: 13690 \tTraining Loss: 1.776848 \tValidation Loss: 2.327041\n",
      "Epoch: 13691 \tTraining Loss: 1.802492 \tValidation Loss: 2.326913\n",
      "Epoch: 13692 \tTraining Loss: 1.783449 \tValidation Loss: 2.326946\n",
      "Epoch: 13693 \tTraining Loss: 1.793509 \tValidation Loss: 2.327283\n",
      "Epoch: 13694 \tTraining Loss: 1.835953 \tValidation Loss: 2.327721\n",
      "Epoch: 13695 \tTraining Loss: 1.785010 \tValidation Loss: 2.327640\n",
      "Epoch: 13696 \tTraining Loss: 1.820601 \tValidation Loss: 2.327770\n",
      "Epoch: 13697 \tTraining Loss: 1.786878 \tValidation Loss: 2.328235\n",
      "Epoch: 13698 \tTraining Loss: 1.774216 \tValidation Loss: 2.328214\n",
      "Epoch: 13699 \tTraining Loss: 1.794566 \tValidation Loss: 2.327978\n",
      "Epoch: 13700 \tTraining Loss: 1.776036 \tValidation Loss: 2.328522\n",
      "Epoch: 13701 \tTraining Loss: 1.781894 \tValidation Loss: 2.328715\n",
      "Epoch: 13702 \tTraining Loss: 1.826257 \tValidation Loss: 2.328289\n",
      "Epoch: 13703 \tTraining Loss: 1.777996 \tValidation Loss: 2.328316\n",
      "Epoch: 13704 \tTraining Loss: 1.797766 \tValidation Loss: 2.327932\n",
      "Epoch: 13705 \tTraining Loss: 1.790837 \tValidation Loss: 2.327538\n",
      "Epoch: 13706 \tTraining Loss: 1.800836 \tValidation Loss: 2.327894\n",
      "Epoch: 13707 \tTraining Loss: 1.777245 \tValidation Loss: 2.327808\n",
      "Epoch: 13708 \tTraining Loss: 1.764583 \tValidation Loss: 2.327801\n",
      "Epoch: 13709 \tTraining Loss: 1.786513 \tValidation Loss: 2.327982\n",
      "Epoch: 13710 \tTraining Loss: 1.793085 \tValidation Loss: 2.327784\n",
      "Epoch: 13711 \tTraining Loss: 1.805921 \tValidation Loss: 2.327640\n",
      "Epoch: 13712 \tTraining Loss: 1.789356 \tValidation Loss: 2.327776\n",
      "Epoch: 13713 \tTraining Loss: 1.783132 \tValidation Loss: 2.327806\n",
      "Epoch: 13714 \tTraining Loss: 1.793025 \tValidation Loss: 2.328167\n",
      "Epoch: 13715 \tTraining Loss: 1.810799 \tValidation Loss: 2.328010\n",
      "Epoch: 13716 \tTraining Loss: 1.776591 \tValidation Loss: 2.328240\n",
      "Epoch: 13717 \tTraining Loss: 1.801461 \tValidation Loss: 2.328202\n",
      "Epoch: 13718 \tTraining Loss: 1.755213 \tValidation Loss: 2.328208\n",
      "Epoch: 13719 \tTraining Loss: 1.747788 \tValidation Loss: 2.328295\n",
      "Epoch: 13720 \tTraining Loss: 1.780084 \tValidation Loss: 2.328507\n",
      "Epoch: 13721 \tTraining Loss: 1.773646 \tValidation Loss: 2.328477\n",
      "Epoch: 13722 \tTraining Loss: 1.775710 \tValidation Loss: 2.329006\n",
      "Epoch: 13723 \tTraining Loss: 1.777324 \tValidation Loss: 2.329036\n",
      "Epoch: 13724 \tTraining Loss: 1.774765 \tValidation Loss: 2.328903\n",
      "Epoch: 13725 \tTraining Loss: 1.784782 \tValidation Loss: 2.328587\n",
      "Epoch: 13726 \tTraining Loss: 1.787579 \tValidation Loss: 2.328616\n",
      "Epoch: 13727 \tTraining Loss: 1.791505 \tValidation Loss: 2.328650\n",
      "Epoch: 13728 \tTraining Loss: 1.812709 \tValidation Loss: 2.328570\n",
      "Epoch: 13729 \tTraining Loss: 1.804983 \tValidation Loss: 2.328890\n",
      "Epoch: 13730 \tTraining Loss: 1.809717 \tValidation Loss: 2.328972\n",
      "Epoch: 13731 \tTraining Loss: 1.794670 \tValidation Loss: 2.328387\n",
      "Epoch: 13732 \tTraining Loss: 1.768160 \tValidation Loss: 2.328411\n",
      "Epoch: 13733 \tTraining Loss: 1.806264 \tValidation Loss: 2.328554\n",
      "Epoch: 13734 \tTraining Loss: 1.775344 \tValidation Loss: 2.328536\n",
      "Epoch: 13735 \tTraining Loss: 1.793456 \tValidation Loss: 2.328252\n",
      "Epoch: 13736 \tTraining Loss: 1.800163 \tValidation Loss: 2.328625\n",
      "Epoch: 13737 \tTraining Loss: 1.781380 \tValidation Loss: 2.328480\n",
      "Epoch: 13738 \tTraining Loss: 1.807042 \tValidation Loss: 2.328491\n",
      "Epoch: 13739 \tTraining Loss: 1.774184 \tValidation Loss: 2.328603\n",
      "Epoch: 13740 \tTraining Loss: 1.792047 \tValidation Loss: 2.328477\n",
      "Epoch: 13741 \tTraining Loss: 1.768740 \tValidation Loss: 2.328688\n",
      "Epoch: 13742 \tTraining Loss: 1.752004 \tValidation Loss: 2.328727\n",
      "Epoch: 13743 \tTraining Loss: 1.772603 \tValidation Loss: 2.328426\n",
      "Epoch: 13744 \tTraining Loss: 1.795688 \tValidation Loss: 2.328254\n",
      "Epoch: 13745 \tTraining Loss: 1.788612 \tValidation Loss: 2.328387\n",
      "Epoch: 13746 \tTraining Loss: 1.769625 \tValidation Loss: 2.329015\n",
      "Epoch: 13747 \tTraining Loss: 1.783012 \tValidation Loss: 2.328720\n",
      "Epoch: 13748 \tTraining Loss: 1.792842 \tValidation Loss: 2.328385\n",
      "Epoch: 13749 \tTraining Loss: 1.820082 \tValidation Loss: 2.328559\n",
      "Epoch: 13750 \tTraining Loss: 1.777530 \tValidation Loss: 2.328682\n",
      "Epoch: 13751 \tTraining Loss: 1.831230 \tValidation Loss: 2.328406\n",
      "Epoch: 13752 \tTraining Loss: 1.779217 \tValidation Loss: 2.327811\n",
      "Epoch: 13753 \tTraining Loss: 1.811075 \tValidation Loss: 2.328149\n",
      "Epoch: 13754 \tTraining Loss: 1.778387 \tValidation Loss: 2.327998\n",
      "Epoch: 13755 \tTraining Loss: 1.804451 \tValidation Loss: 2.328252\n",
      "Epoch: 13756 \tTraining Loss: 1.791193 \tValidation Loss: 2.327921\n",
      "Epoch: 13757 \tTraining Loss: 1.821131 \tValidation Loss: 2.327971\n",
      "Epoch: 13758 \tTraining Loss: 1.784804 \tValidation Loss: 2.327844\n",
      "Epoch: 13759 \tTraining Loss: 1.776859 \tValidation Loss: 2.327712\n",
      "Epoch: 13760 \tTraining Loss: 1.801982 \tValidation Loss: 2.327985\n",
      "Epoch: 13761 \tTraining Loss: 1.808191 \tValidation Loss: 2.328138\n",
      "Epoch: 13762 \tTraining Loss: 1.797813 \tValidation Loss: 2.327961\n",
      "Epoch: 13763 \tTraining Loss: 1.785077 \tValidation Loss: 2.328632\n",
      "Epoch: 13764 \tTraining Loss: 1.800117 \tValidation Loss: 2.328552\n",
      "Epoch: 13765 \tTraining Loss: 1.775568 \tValidation Loss: 2.328693\n",
      "Epoch: 13766 \tTraining Loss: 1.802606 \tValidation Loss: 2.328450\n",
      "Epoch: 13767 \tTraining Loss: 1.780901 \tValidation Loss: 2.328648\n",
      "Epoch: 13768 \tTraining Loss: 1.773285 \tValidation Loss: 2.328703\n",
      "Epoch: 13769 \tTraining Loss: 1.794655 \tValidation Loss: 2.328480\n",
      "Epoch: 13770 \tTraining Loss: 1.771267 \tValidation Loss: 2.328690\n",
      "Epoch: 13771 \tTraining Loss: 1.806210 \tValidation Loss: 2.328390\n",
      "Epoch: 13772 \tTraining Loss: 1.779698 \tValidation Loss: 2.328409\n",
      "Epoch: 13773 \tTraining Loss: 1.764697 \tValidation Loss: 2.328324\n",
      "Epoch: 13774 \tTraining Loss: 1.805924 \tValidation Loss: 2.328430\n",
      "Epoch: 13775 \tTraining Loss: 1.786783 \tValidation Loss: 2.328260\n",
      "Epoch: 13776 \tTraining Loss: 1.803115 \tValidation Loss: 2.327870\n",
      "Epoch: 13777 \tTraining Loss: 1.784878 \tValidation Loss: 2.327768\n",
      "Epoch: 13778 \tTraining Loss: 1.783949 \tValidation Loss: 2.328495\n",
      "Epoch: 13779 \tTraining Loss: 1.818891 \tValidation Loss: 2.328362\n",
      "Epoch: 13780 \tTraining Loss: 1.785095 \tValidation Loss: 2.328299\n",
      "Epoch: 13781 \tTraining Loss: 1.802173 \tValidation Loss: 2.328551\n",
      "Epoch: 13782 \tTraining Loss: 1.759133 \tValidation Loss: 2.328233\n",
      "Epoch: 13783 \tTraining Loss: 1.782916 \tValidation Loss: 2.328269\n",
      "Epoch: 13784 \tTraining Loss: 1.780836 \tValidation Loss: 2.328123\n",
      "Epoch: 13785 \tTraining Loss: 1.751839 \tValidation Loss: 2.328226\n",
      "Epoch: 13786 \tTraining Loss: 1.803959 \tValidation Loss: 2.327971\n",
      "Epoch: 13787 \tTraining Loss: 1.809584 \tValidation Loss: 2.328448\n",
      "Epoch: 13788 \tTraining Loss: 1.753006 \tValidation Loss: 2.328514\n",
      "Epoch: 13789 \tTraining Loss: 1.830851 \tValidation Loss: 2.328608\n",
      "Epoch: 13790 \tTraining Loss: 1.768050 \tValidation Loss: 2.328330\n",
      "Epoch: 13791 \tTraining Loss: 1.780887 \tValidation Loss: 2.328767\n",
      "Epoch: 13792 \tTraining Loss: 1.799798 \tValidation Loss: 2.329525\n",
      "Epoch: 13793 \tTraining Loss: 1.777925 \tValidation Loss: 2.329140\n",
      "Epoch: 13794 \tTraining Loss: 1.807291 \tValidation Loss: 2.329423\n",
      "Epoch: 13795 \tTraining Loss: 1.766123 \tValidation Loss: 2.329125\n",
      "Epoch: 13796 \tTraining Loss: 1.782242 \tValidation Loss: 2.329161\n",
      "Epoch: 13797 \tTraining Loss: 1.783509 \tValidation Loss: 2.329139\n",
      "Epoch: 13798 \tTraining Loss: 1.814190 \tValidation Loss: 2.328678\n",
      "Epoch: 13799 \tTraining Loss: 1.785752 \tValidation Loss: 2.328334\n",
      "Epoch: 13800 \tTraining Loss: 1.783698 \tValidation Loss: 2.328552\n",
      "Epoch: 13801 \tTraining Loss: 1.790596 \tValidation Loss: 2.328624\n",
      "Epoch: 13802 \tTraining Loss: 1.772602 \tValidation Loss: 2.328619\n",
      "Epoch: 13803 \tTraining Loss: 1.766110 \tValidation Loss: 2.328701\n",
      "Epoch: 13804 \tTraining Loss: 1.796083 \tValidation Loss: 2.329152\n",
      "Epoch: 13805 \tTraining Loss: 1.774255 \tValidation Loss: 2.329465\n",
      "Epoch: 13806 \tTraining Loss: 1.803634 \tValidation Loss: 2.329318\n",
      "Epoch: 13807 \tTraining Loss: 1.777160 \tValidation Loss: 2.329355\n",
      "Epoch: 13808 \tTraining Loss: 1.796243 \tValidation Loss: 2.329512\n",
      "Epoch: 13809 \tTraining Loss: 1.807849 \tValidation Loss: 2.329419\n",
      "Epoch: 13810 \tTraining Loss: 1.793183 \tValidation Loss: 2.329277\n",
      "Epoch: 13811 \tTraining Loss: 1.802388 \tValidation Loss: 2.329418\n",
      "Epoch: 13812 \tTraining Loss: 1.785104 \tValidation Loss: 2.329468\n",
      "Epoch: 13813 \tTraining Loss: 1.794040 \tValidation Loss: 2.329375\n",
      "Epoch: 13814 \tTraining Loss: 1.782834 \tValidation Loss: 2.329906\n",
      "Epoch: 13815 \tTraining Loss: 1.780637 \tValidation Loss: 2.329828\n",
      "Epoch: 13816 \tTraining Loss: 1.821224 \tValidation Loss: 2.329176\n",
      "Epoch: 13817 \tTraining Loss: 1.805950 \tValidation Loss: 2.329442\n",
      "Epoch: 13818 \tTraining Loss: 1.779854 \tValidation Loss: 2.329146\n",
      "Epoch: 13819 \tTraining Loss: 1.790423 \tValidation Loss: 2.329017\n",
      "Epoch: 13820 \tTraining Loss: 1.784030 \tValidation Loss: 2.329416\n",
      "Epoch: 13821 \tTraining Loss: 1.770467 \tValidation Loss: 2.329253\n",
      "Epoch: 13822 \tTraining Loss: 1.799611 \tValidation Loss: 2.329026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13823 \tTraining Loss: 1.789076 \tValidation Loss: 2.329239\n",
      "Epoch: 13824 \tTraining Loss: 1.764767 \tValidation Loss: 2.329453\n",
      "Epoch: 13825 \tTraining Loss: 1.824487 \tValidation Loss: 2.329476\n",
      "Epoch: 13826 \tTraining Loss: 1.772130 \tValidation Loss: 2.329786\n",
      "Epoch: 13827 \tTraining Loss: 1.784098 \tValidation Loss: 2.330035\n",
      "Epoch: 13828 \tTraining Loss: 1.809859 \tValidation Loss: 2.329609\n",
      "Epoch: 13829 \tTraining Loss: 1.776572 \tValidation Loss: 2.329415\n",
      "Epoch: 13830 \tTraining Loss: 1.770444 \tValidation Loss: 2.329379\n",
      "Epoch: 13831 \tTraining Loss: 1.800417 \tValidation Loss: 2.328951\n",
      "Epoch: 13832 \tTraining Loss: 1.799771 \tValidation Loss: 2.328350\n",
      "Epoch: 13833 \tTraining Loss: 1.812550 \tValidation Loss: 2.328224\n",
      "Epoch: 13834 \tTraining Loss: 1.753669 \tValidation Loss: 2.328634\n",
      "Epoch: 13835 \tTraining Loss: 1.775306 \tValidation Loss: 2.328319\n",
      "Epoch: 13836 \tTraining Loss: 1.787075 \tValidation Loss: 2.328472\n",
      "Epoch: 13837 \tTraining Loss: 1.780809 \tValidation Loss: 2.328951\n",
      "Epoch: 13838 \tTraining Loss: 1.812171 \tValidation Loss: 2.328501\n",
      "Epoch: 13839 \tTraining Loss: 1.821296 \tValidation Loss: 2.328435\n",
      "Epoch: 13840 \tTraining Loss: 1.742030 \tValidation Loss: 2.328480\n",
      "Epoch: 13841 \tTraining Loss: 1.798297 \tValidation Loss: 2.328657\n",
      "Epoch: 13842 \tTraining Loss: 1.748573 \tValidation Loss: 2.329225\n",
      "Epoch: 13843 \tTraining Loss: 1.811439 \tValidation Loss: 2.329374\n",
      "Epoch: 13844 \tTraining Loss: 1.755384 \tValidation Loss: 2.329201\n",
      "Epoch: 13845 \tTraining Loss: 1.801798 \tValidation Loss: 2.329172\n",
      "Epoch: 13846 \tTraining Loss: 1.737635 \tValidation Loss: 2.329671\n",
      "Epoch: 13847 \tTraining Loss: 1.777547 \tValidation Loss: 2.330032\n",
      "Epoch: 13848 \tTraining Loss: 1.786207 \tValidation Loss: 2.329342\n",
      "Epoch: 13849 \tTraining Loss: 1.793696 \tValidation Loss: 2.328959\n",
      "Epoch: 13850 \tTraining Loss: 1.778747 \tValidation Loss: 2.328833\n",
      "Epoch: 13851 \tTraining Loss: 1.788983 \tValidation Loss: 2.329012\n",
      "Epoch: 13852 \tTraining Loss: 1.810939 \tValidation Loss: 2.328892\n",
      "Epoch: 13853 \tTraining Loss: 1.781333 \tValidation Loss: 2.329509\n",
      "Epoch: 13854 \tTraining Loss: 1.747976 \tValidation Loss: 2.329315\n",
      "Epoch: 13855 \tTraining Loss: 1.811470 \tValidation Loss: 2.329483\n",
      "Epoch: 13856 \tTraining Loss: 1.782726 \tValidation Loss: 2.329230\n",
      "Epoch: 13857 \tTraining Loss: 1.771219 \tValidation Loss: 2.329470\n",
      "Epoch: 13858 \tTraining Loss: 1.761648 \tValidation Loss: 2.329675\n",
      "Epoch: 13859 \tTraining Loss: 1.815268 \tValidation Loss: 2.329594\n",
      "Epoch: 13860 \tTraining Loss: 1.783048 \tValidation Loss: 2.329563\n",
      "Epoch: 13861 \tTraining Loss: 1.822684 \tValidation Loss: 2.329332\n",
      "Epoch: 13862 \tTraining Loss: 1.789478 \tValidation Loss: 2.329244\n",
      "Epoch: 13863 \tTraining Loss: 1.769544 \tValidation Loss: 2.329836\n",
      "Epoch: 13864 \tTraining Loss: 1.769754 \tValidation Loss: 2.329907\n",
      "Epoch: 13865 \tTraining Loss: 1.792680 \tValidation Loss: 2.329761\n",
      "Epoch: 13866 \tTraining Loss: 1.766274 \tValidation Loss: 2.329826\n",
      "Epoch: 13867 \tTraining Loss: 1.784975 \tValidation Loss: 2.329295\n",
      "Epoch: 13868 \tTraining Loss: 1.782303 \tValidation Loss: 2.329060\n",
      "Epoch: 13869 \tTraining Loss: 1.793022 \tValidation Loss: 2.329242\n",
      "Epoch: 13870 \tTraining Loss: 1.768533 \tValidation Loss: 2.329825\n",
      "Epoch: 13871 \tTraining Loss: 1.769765 \tValidation Loss: 2.330031\n",
      "Epoch: 13872 \tTraining Loss: 1.766877 \tValidation Loss: 2.330251\n",
      "Epoch: 13873 \tTraining Loss: 1.806501 \tValidation Loss: 2.329632\n",
      "Epoch: 13874 \tTraining Loss: 1.781345 \tValidation Loss: 2.329418\n",
      "Epoch: 13875 \tTraining Loss: 1.749738 \tValidation Loss: 2.329276\n",
      "Epoch: 13876 \tTraining Loss: 1.771466 \tValidation Loss: 2.329961\n",
      "Epoch: 13877 \tTraining Loss: 1.800924 \tValidation Loss: 2.329922\n",
      "Epoch: 13878 \tTraining Loss: 1.777041 \tValidation Loss: 2.329831\n",
      "Epoch: 13879 \tTraining Loss: 1.785585 \tValidation Loss: 2.329116\n",
      "Epoch: 13880 \tTraining Loss: 1.804722 \tValidation Loss: 2.329653\n",
      "Epoch: 13881 \tTraining Loss: 1.811947 \tValidation Loss: 2.328904\n",
      "Epoch: 13882 \tTraining Loss: 1.806397 \tValidation Loss: 2.329179\n",
      "Epoch: 13883 \tTraining Loss: 1.765643 \tValidation Loss: 2.329483\n",
      "Epoch: 13884 \tTraining Loss: 1.779013 \tValidation Loss: 2.328807\n",
      "Epoch: 13885 \tTraining Loss: 1.801131 \tValidation Loss: 2.328733\n",
      "Epoch: 13886 \tTraining Loss: 1.800502 \tValidation Loss: 2.329191\n",
      "Epoch: 13887 \tTraining Loss: 1.796878 \tValidation Loss: 2.329232\n",
      "Epoch: 13888 \tTraining Loss: 1.765479 \tValidation Loss: 2.329587\n",
      "Epoch: 13889 \tTraining Loss: 1.773899 \tValidation Loss: 2.329185\n",
      "Epoch: 13890 \tTraining Loss: 1.781874 \tValidation Loss: 2.329210\n",
      "Epoch: 13891 \tTraining Loss: 1.803955 \tValidation Loss: 2.329319\n",
      "Epoch: 13892 \tTraining Loss: 1.796762 \tValidation Loss: 2.329683\n",
      "Epoch: 13893 \tTraining Loss: 1.802626 \tValidation Loss: 2.329509\n",
      "Epoch: 13894 \tTraining Loss: 1.752060 \tValidation Loss: 2.330118\n",
      "Epoch: 13895 \tTraining Loss: 1.809335 \tValidation Loss: 2.329908\n",
      "Epoch: 13896 \tTraining Loss: 1.751098 \tValidation Loss: 2.329601\n",
      "Epoch: 13897 \tTraining Loss: 1.772473 \tValidation Loss: 2.329033\n",
      "Epoch: 13898 \tTraining Loss: 1.770406 \tValidation Loss: 2.329171\n",
      "Epoch: 13899 \tTraining Loss: 1.792973 \tValidation Loss: 2.329176\n",
      "Epoch: 13900 \tTraining Loss: 1.735373 \tValidation Loss: 2.329710\n",
      "Epoch: 13901 \tTraining Loss: 1.827112 \tValidation Loss: 2.330122\n",
      "Epoch: 13902 \tTraining Loss: 1.761093 \tValidation Loss: 2.329661\n",
      "Epoch: 13903 \tTraining Loss: 1.780309 \tValidation Loss: 2.329902\n",
      "Epoch: 13904 \tTraining Loss: 1.786251 \tValidation Loss: 2.330328\n",
      "Epoch: 13905 \tTraining Loss: 1.786682 \tValidation Loss: 2.329501\n",
      "Epoch: 13906 \tTraining Loss: 1.778897 \tValidation Loss: 2.329699\n",
      "Epoch: 13907 \tTraining Loss: 1.790893 \tValidation Loss: 2.329597\n",
      "Epoch: 13908 \tTraining Loss: 1.799852 \tValidation Loss: 2.329240\n",
      "Epoch: 13909 \tTraining Loss: 1.795174 \tValidation Loss: 2.329537\n",
      "Epoch: 13910 \tTraining Loss: 1.783693 \tValidation Loss: 2.330096\n",
      "Epoch: 13911 \tTraining Loss: 1.743627 \tValidation Loss: 2.330233\n",
      "Epoch: 13912 \tTraining Loss: 1.764973 \tValidation Loss: 2.330065\n",
      "Epoch: 13913 \tTraining Loss: 1.783282 \tValidation Loss: 2.330142\n",
      "Epoch: 13914 \tTraining Loss: 1.744032 \tValidation Loss: 2.330181\n",
      "Epoch: 13915 \tTraining Loss: 1.748639 \tValidation Loss: 2.330439\n",
      "Epoch: 13916 \tTraining Loss: 1.761223 \tValidation Loss: 2.330486\n",
      "Epoch: 13917 \tTraining Loss: 1.807066 \tValidation Loss: 2.330379\n",
      "Epoch: 13918 \tTraining Loss: 1.759872 \tValidation Loss: 2.330602\n",
      "Epoch: 13919 \tTraining Loss: 1.788800 \tValidation Loss: 2.330460\n",
      "Epoch: 13920 \tTraining Loss: 1.802454 \tValidation Loss: 2.330661\n",
      "Epoch: 13921 \tTraining Loss: 1.803748 \tValidation Loss: 2.330753\n",
      "Epoch: 13922 \tTraining Loss: 1.794010 \tValidation Loss: 2.330372\n",
      "Epoch: 13923 \tTraining Loss: 1.778110 \tValidation Loss: 2.330310\n",
      "Epoch: 13924 \tTraining Loss: 1.774459 \tValidation Loss: 2.330148\n",
      "Epoch: 13925 \tTraining Loss: 1.787318 \tValidation Loss: 2.330016\n",
      "Epoch: 13926 \tTraining Loss: 1.758941 \tValidation Loss: 2.330524\n",
      "Epoch: 13927 \tTraining Loss: 1.764913 \tValidation Loss: 2.330203\n",
      "Epoch: 13928 \tTraining Loss: 1.795213 \tValidation Loss: 2.329923\n",
      "Epoch: 13929 \tTraining Loss: 1.769571 \tValidation Loss: 2.329561\n",
      "Epoch: 13930 \tTraining Loss: 1.745164 \tValidation Loss: 2.329847\n",
      "Epoch: 13931 \tTraining Loss: 1.796483 \tValidation Loss: 2.329748\n",
      "Epoch: 13932 \tTraining Loss: 1.771372 \tValidation Loss: 2.329673\n",
      "Epoch: 13933 \tTraining Loss: 1.802853 \tValidation Loss: 2.329859\n",
      "Epoch: 13934 \tTraining Loss: 1.764665 \tValidation Loss: 2.330146\n",
      "Epoch: 13935 \tTraining Loss: 1.805414 \tValidation Loss: 2.330103\n",
      "Epoch: 13936 \tTraining Loss: 1.795555 \tValidation Loss: 2.329718\n",
      "Epoch: 13937 \tTraining Loss: 1.760442 \tValidation Loss: 2.330010\n",
      "Epoch: 13938 \tTraining Loss: 1.795835 \tValidation Loss: 2.329778\n",
      "Epoch: 13939 \tTraining Loss: 1.768448 \tValidation Loss: 2.329848\n",
      "Epoch: 13940 \tTraining Loss: 1.799920 \tValidation Loss: 2.329409\n",
      "Epoch: 13941 \tTraining Loss: 1.800324 \tValidation Loss: 2.329694\n",
      "Epoch: 13942 \tTraining Loss: 1.800211 \tValidation Loss: 2.329553\n",
      "Epoch: 13943 \tTraining Loss: 1.772444 \tValidation Loss: 2.329853\n",
      "Epoch: 13944 \tTraining Loss: 1.757430 \tValidation Loss: 2.329825\n",
      "Epoch: 13945 \tTraining Loss: 1.784580 \tValidation Loss: 2.329855\n",
      "Epoch: 13946 \tTraining Loss: 1.811747 \tValidation Loss: 2.329346\n",
      "Epoch: 13947 \tTraining Loss: 1.735843 \tValidation Loss: 2.329621\n",
      "Epoch: 13948 \tTraining Loss: 1.754748 \tValidation Loss: 2.330043\n",
      "Epoch: 13949 \tTraining Loss: 1.777277 \tValidation Loss: 2.329965\n",
      "Epoch: 13950 \tTraining Loss: 1.794247 \tValidation Loss: 2.329868\n",
      "Epoch: 13951 \tTraining Loss: 1.792252 \tValidation Loss: 2.329671\n",
      "Epoch: 13952 \tTraining Loss: 1.776135 \tValidation Loss: 2.329695\n",
      "Epoch: 13953 \tTraining Loss: 1.741883 \tValidation Loss: 2.330281\n",
      "Epoch: 13954 \tTraining Loss: 1.760915 \tValidation Loss: 2.330139\n",
      "Epoch: 13955 \tTraining Loss: 1.822568 \tValidation Loss: 2.330006\n",
      "Epoch: 13956 \tTraining Loss: 1.756862 \tValidation Loss: 2.330828\n",
      "Epoch: 13957 \tTraining Loss: 1.777099 \tValidation Loss: 2.330423\n",
      "Epoch: 13958 \tTraining Loss: 1.758876 \tValidation Loss: 2.330755\n",
      "Epoch: 13959 \tTraining Loss: 1.762216 \tValidation Loss: 2.330715\n",
      "Epoch: 13960 \tTraining Loss: 1.750666 \tValidation Loss: 2.330650\n",
      "Epoch: 13961 \tTraining Loss: 1.772899 \tValidation Loss: 2.330676\n",
      "Epoch: 13962 \tTraining Loss: 1.814433 \tValidation Loss: 2.330275\n",
      "Epoch: 13963 \tTraining Loss: 1.794724 \tValidation Loss: 2.330261\n",
      "Epoch: 13964 \tTraining Loss: 1.765293 \tValidation Loss: 2.330206\n",
      "Epoch: 13965 \tTraining Loss: 1.800629 \tValidation Loss: 2.330614\n",
      "Epoch: 13966 \tTraining Loss: 1.816701 \tValidation Loss: 2.330612\n",
      "Epoch: 13967 \tTraining Loss: 1.778406 \tValidation Loss: 2.330374\n",
      "Epoch: 13968 \tTraining Loss: 1.748380 \tValidation Loss: 2.330656\n",
      "Epoch: 13969 \tTraining Loss: 1.793332 \tValidation Loss: 2.330173\n",
      "Epoch: 13970 \tTraining Loss: 1.801538 \tValidation Loss: 2.329987\n",
      "Epoch: 13971 \tTraining Loss: 1.761131 \tValidation Loss: 2.330082\n",
      "Epoch: 13972 \tTraining Loss: 1.769099 \tValidation Loss: 2.329808\n",
      "Epoch: 13973 \tTraining Loss: 1.770082 \tValidation Loss: 2.329419\n",
      "Epoch: 13974 \tTraining Loss: 1.763496 \tValidation Loss: 2.329885\n",
      "Epoch: 13975 \tTraining Loss: 1.786069 \tValidation Loss: 2.330043\n",
      "Epoch: 13976 \tTraining Loss: 1.748202 \tValidation Loss: 2.329857\n",
      "Epoch: 13977 \tTraining Loss: 1.780196 \tValidation Loss: 2.330059\n",
      "Epoch: 13978 \tTraining Loss: 1.789734 \tValidation Loss: 2.330226\n",
      "Epoch: 13979 \tTraining Loss: 1.799594 \tValidation Loss: 2.330789\n",
      "Epoch: 13980 \tTraining Loss: 1.756809 \tValidation Loss: 2.330821\n",
      "Epoch: 13981 \tTraining Loss: 1.748779 \tValidation Loss: 2.331166\n",
      "Epoch: 13982 \tTraining Loss: 1.787918 \tValidation Loss: 2.331002\n",
      "Epoch: 13983 \tTraining Loss: 1.788182 \tValidation Loss: 2.330834\n",
      "Epoch: 13984 \tTraining Loss: 1.775854 \tValidation Loss: 2.330663\n",
      "Epoch: 13985 \tTraining Loss: 1.807011 \tValidation Loss: 2.330413\n",
      "Epoch: 13986 \tTraining Loss: 1.794274 \tValidation Loss: 2.330423\n",
      "Epoch: 13987 \tTraining Loss: 1.782667 \tValidation Loss: 2.330692\n",
      "Epoch: 13988 \tTraining Loss: 1.804781 \tValidation Loss: 2.330382\n",
      "Epoch: 13989 \tTraining Loss: 1.758218 \tValidation Loss: 2.330614\n",
      "Epoch: 13990 \tTraining Loss: 1.818408 \tValidation Loss: 2.329921\n",
      "Epoch: 13991 \tTraining Loss: 1.801083 \tValidation Loss: 2.329893\n",
      "Epoch: 13992 \tTraining Loss: 1.754280 \tValidation Loss: 2.330328\n",
      "Epoch: 13993 \tTraining Loss: 1.747465 \tValidation Loss: 2.330743\n",
      "Epoch: 13994 \tTraining Loss: 1.773605 \tValidation Loss: 2.330422\n",
      "Epoch: 13995 \tTraining Loss: 1.771682 \tValidation Loss: 2.330430\n",
      "Epoch: 13996 \tTraining Loss: 1.783808 \tValidation Loss: 2.330910\n",
      "Epoch: 13997 \tTraining Loss: 1.775207 \tValidation Loss: 2.330596\n",
      "Epoch: 13998 \tTraining Loss: 1.782553 \tValidation Loss: 2.330506\n",
      "Epoch: 13999 \tTraining Loss: 1.780879 \tValidation Loss: 2.330804\n",
      "Epoch: 14000 \tTraining Loss: 1.814513 \tValidation Loss: 2.330776\n",
      "Epoch: 14001 \tTraining Loss: 1.769749 \tValidation Loss: 2.330557\n",
      "Epoch: 14002 \tTraining Loss: 1.787764 \tValidation Loss: 2.330699\n",
      "Epoch: 14003 \tTraining Loss: 1.785313 \tValidation Loss: 2.330258\n",
      "Epoch: 14004 \tTraining Loss: 1.777680 \tValidation Loss: 2.330790\n",
      "Epoch: 14005 \tTraining Loss: 1.779217 \tValidation Loss: 2.330948\n",
      "Epoch: 14006 \tTraining Loss: 1.792638 \tValidation Loss: 2.331070\n",
      "Epoch: 14007 \tTraining Loss: 1.795236 \tValidation Loss: 2.330399\n",
      "Epoch: 14008 \tTraining Loss: 1.731310 \tValidation Loss: 2.330716\n",
      "Epoch: 14009 \tTraining Loss: 1.763463 \tValidation Loss: 2.330760\n",
      "Epoch: 14010 \tTraining Loss: 1.756913 \tValidation Loss: 2.330523\n",
      "Epoch: 14011 \tTraining Loss: 1.780083 \tValidation Loss: 2.330282\n",
      "Epoch: 14012 \tTraining Loss: 1.759004 \tValidation Loss: 2.330160\n",
      "Epoch: 14013 \tTraining Loss: 1.789090 \tValidation Loss: 2.330191\n",
      "Epoch: 14014 \tTraining Loss: 1.805596 \tValidation Loss: 2.330620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14015 \tTraining Loss: 1.782076 \tValidation Loss: 2.330363\n",
      "Epoch: 14016 \tTraining Loss: 1.778724 \tValidation Loss: 2.330525\n",
      "Epoch: 14017 \tTraining Loss: 1.789232 \tValidation Loss: 2.330741\n",
      "Epoch: 14018 \tTraining Loss: 1.767020 \tValidation Loss: 2.331444\n",
      "Epoch: 14019 \tTraining Loss: 1.764958 \tValidation Loss: 2.331345\n",
      "Epoch: 14020 \tTraining Loss: 1.781964 \tValidation Loss: 2.331363\n",
      "Epoch: 14021 \tTraining Loss: 1.778413 \tValidation Loss: 2.331560\n",
      "Epoch: 14022 \tTraining Loss: 1.780745 \tValidation Loss: 2.331328\n",
      "Epoch: 14023 \tTraining Loss: 1.781606 \tValidation Loss: 2.331483\n",
      "Epoch: 14024 \tTraining Loss: 1.801219 \tValidation Loss: 2.330456\n",
      "Epoch: 14025 \tTraining Loss: 1.768860 \tValidation Loss: 2.330643\n",
      "Epoch: 14026 \tTraining Loss: 1.796918 \tValidation Loss: 2.330792\n",
      "Epoch: 14027 \tTraining Loss: 1.799328 \tValidation Loss: 2.330611\n",
      "Epoch: 14028 \tTraining Loss: 1.756913 \tValidation Loss: 2.330978\n",
      "Epoch: 14029 \tTraining Loss: 1.778941 \tValidation Loss: 2.331195\n",
      "Epoch: 14030 \tTraining Loss: 1.778118 \tValidation Loss: 2.331073\n",
      "Epoch: 14031 \tTraining Loss: 1.793852 \tValidation Loss: 2.330982\n",
      "Epoch: 14032 \tTraining Loss: 1.753228 \tValidation Loss: 2.331279\n",
      "Epoch: 14033 \tTraining Loss: 1.773263 \tValidation Loss: 2.331459\n",
      "Epoch: 14034 \tTraining Loss: 1.780960 \tValidation Loss: 2.331790\n",
      "Epoch: 14035 \tTraining Loss: 1.781174 \tValidation Loss: 2.331182\n",
      "Epoch: 14036 \tTraining Loss: 1.755811 \tValidation Loss: 2.331084\n",
      "Epoch: 14037 \tTraining Loss: 1.791700 \tValidation Loss: 2.331479\n",
      "Epoch: 14038 \tTraining Loss: 1.797065 \tValidation Loss: 2.331210\n",
      "Epoch: 14039 \tTraining Loss: 1.803915 \tValidation Loss: 2.330590\n",
      "Epoch: 14040 \tTraining Loss: 1.819508 \tValidation Loss: 2.330306\n",
      "Epoch: 14041 \tTraining Loss: 1.778738 \tValidation Loss: 2.330180\n",
      "Epoch: 14042 \tTraining Loss: 1.757773 \tValidation Loss: 2.330813\n",
      "Epoch: 14043 \tTraining Loss: 1.770801 \tValidation Loss: 2.331210\n",
      "Epoch: 14044 \tTraining Loss: 1.822447 \tValidation Loss: 2.330511\n",
      "Epoch: 14045 \tTraining Loss: 1.800316 \tValidation Loss: 2.330803\n",
      "Epoch: 14046 \tTraining Loss: 1.798209 \tValidation Loss: 2.331165\n",
      "Epoch: 14047 \tTraining Loss: 1.771604 \tValidation Loss: 2.331393\n",
      "Epoch: 14048 \tTraining Loss: 1.777113 \tValidation Loss: 2.331303\n",
      "Epoch: 14049 \tTraining Loss: 1.765941 \tValidation Loss: 2.331332\n",
      "Epoch: 14050 \tTraining Loss: 1.796990 \tValidation Loss: 2.330584\n",
      "Epoch: 14051 \tTraining Loss: 1.763648 \tValidation Loss: 2.330556\n",
      "Epoch: 14052 \tTraining Loss: 1.771876 \tValidation Loss: 2.330839\n",
      "Epoch: 14053 \tTraining Loss: 1.811965 \tValidation Loss: 2.330719\n",
      "Epoch: 14054 \tTraining Loss: 1.789062 \tValidation Loss: 2.330860\n",
      "Epoch: 14055 \tTraining Loss: 1.762102 \tValidation Loss: 2.330791\n",
      "Epoch: 14056 \tTraining Loss: 1.754746 \tValidation Loss: 2.330820\n",
      "Epoch: 14057 \tTraining Loss: 1.796227 \tValidation Loss: 2.330673\n",
      "Epoch: 14058 \tTraining Loss: 1.759648 \tValidation Loss: 2.331094\n",
      "Epoch: 14059 \tTraining Loss: 1.763272 \tValidation Loss: 2.331262\n",
      "Epoch: 14060 \tTraining Loss: 1.784439 \tValidation Loss: 2.331153\n",
      "Epoch: 14061 \tTraining Loss: 1.759188 \tValidation Loss: 2.331485\n",
      "Epoch: 14062 \tTraining Loss: 1.764448 \tValidation Loss: 2.331486\n",
      "Epoch: 14063 \tTraining Loss: 1.781911 \tValidation Loss: 2.331538\n",
      "Epoch: 14064 \tTraining Loss: 1.765741 \tValidation Loss: 2.331302\n",
      "Epoch: 14065 \tTraining Loss: 1.747474 \tValidation Loss: 2.331603\n",
      "Epoch: 14066 \tTraining Loss: 1.775425 \tValidation Loss: 2.331497\n",
      "Epoch: 14067 \tTraining Loss: 1.798262 \tValidation Loss: 2.331357\n",
      "Epoch: 14068 \tTraining Loss: 1.790167 \tValidation Loss: 2.331709\n",
      "Epoch: 14069 \tTraining Loss: 1.786957 \tValidation Loss: 2.331679\n",
      "Epoch: 14070 \tTraining Loss: 1.775069 \tValidation Loss: 2.331631\n",
      "Epoch: 14071 \tTraining Loss: 1.765907 \tValidation Loss: 2.331702\n",
      "Epoch: 14072 \tTraining Loss: 1.802711 \tValidation Loss: 2.331764\n",
      "Epoch: 14073 \tTraining Loss: 1.778465 \tValidation Loss: 2.331923\n",
      "Epoch: 14074 \tTraining Loss: 1.768035 \tValidation Loss: 2.331797\n",
      "Epoch: 14075 \tTraining Loss: 1.766538 \tValidation Loss: 2.331782\n",
      "Epoch: 14076 \tTraining Loss: 1.814506 \tValidation Loss: 2.331669\n",
      "Epoch: 14077 \tTraining Loss: 1.789680 \tValidation Loss: 2.331518\n",
      "Epoch: 14078 \tTraining Loss: 1.822487 \tValidation Loss: 2.331624\n",
      "Epoch: 14079 \tTraining Loss: 1.779246 \tValidation Loss: 2.331959\n",
      "Epoch: 14080 \tTraining Loss: 1.777096 \tValidation Loss: 2.331902\n",
      "Epoch: 14081 \tTraining Loss: 1.750647 \tValidation Loss: 2.331903\n",
      "Epoch: 14082 \tTraining Loss: 1.792238 \tValidation Loss: 2.331605\n",
      "Epoch: 14083 \tTraining Loss: 1.787255 \tValidation Loss: 2.332051\n",
      "Epoch: 14084 \tTraining Loss: 1.760054 \tValidation Loss: 2.332371\n",
      "Epoch: 14085 \tTraining Loss: 1.772385 \tValidation Loss: 2.331755\n",
      "Epoch: 14086 \tTraining Loss: 1.816837 \tValidation Loss: 2.331539\n",
      "Epoch: 14087 \tTraining Loss: 1.778260 \tValidation Loss: 2.331812\n",
      "Epoch: 14088 \tTraining Loss: 1.777155 \tValidation Loss: 2.332012\n",
      "Epoch: 14089 \tTraining Loss: 1.767741 \tValidation Loss: 2.331782\n",
      "Epoch: 14090 \tTraining Loss: 1.754684 \tValidation Loss: 2.331848\n",
      "Epoch: 14091 \tTraining Loss: 1.796497 \tValidation Loss: 2.331691\n",
      "Epoch: 14092 \tTraining Loss: 1.768345 \tValidation Loss: 2.331517\n",
      "Epoch: 14093 \tTraining Loss: 1.778165 \tValidation Loss: 2.331811\n",
      "Epoch: 14094 \tTraining Loss: 1.788111 \tValidation Loss: 2.331402\n",
      "Epoch: 14095 \tTraining Loss: 1.772051 \tValidation Loss: 2.331461\n",
      "Epoch: 14096 \tTraining Loss: 1.771705 \tValidation Loss: 2.331460\n",
      "Epoch: 14097 \tTraining Loss: 1.790961 \tValidation Loss: 2.331387\n",
      "Epoch: 14098 \tTraining Loss: 1.774134 \tValidation Loss: 2.331646\n",
      "Epoch: 14099 \tTraining Loss: 1.788109 \tValidation Loss: 2.331531\n",
      "Epoch: 14100 \tTraining Loss: 1.771793 \tValidation Loss: 2.331607\n",
      "Epoch: 14101 \tTraining Loss: 1.801120 \tValidation Loss: 2.331783\n",
      "Epoch: 14102 \tTraining Loss: 1.769283 \tValidation Loss: 2.331857\n",
      "Epoch: 14103 \tTraining Loss: 1.774379 \tValidation Loss: 2.332086\n",
      "Epoch: 14104 \tTraining Loss: 1.770025 \tValidation Loss: 2.332153\n",
      "Epoch: 14105 \tTraining Loss: 1.768341 \tValidation Loss: 2.332077\n",
      "Epoch: 14106 \tTraining Loss: 1.754723 \tValidation Loss: 2.332134\n",
      "Epoch: 14107 \tTraining Loss: 1.805725 \tValidation Loss: 2.332369\n",
      "Epoch: 14108 \tTraining Loss: 1.790287 \tValidation Loss: 2.332587\n",
      "Epoch: 14109 \tTraining Loss: 1.748727 \tValidation Loss: 2.332580\n",
      "Epoch: 14110 \tTraining Loss: 1.779310 \tValidation Loss: 2.332694\n",
      "Epoch: 14111 \tTraining Loss: 1.753459 \tValidation Loss: 2.332997\n",
      "Epoch: 14112 \tTraining Loss: 1.783305 \tValidation Loss: 2.332808\n",
      "Epoch: 14113 \tTraining Loss: 1.774981 \tValidation Loss: 2.332400\n",
      "Epoch: 14114 \tTraining Loss: 1.753699 \tValidation Loss: 2.332482\n",
      "Epoch: 14115 \tTraining Loss: 1.756371 \tValidation Loss: 2.332526\n",
      "Epoch: 14116 \tTraining Loss: 1.791479 \tValidation Loss: 2.331854\n",
      "Epoch: 14117 \tTraining Loss: 1.741553 \tValidation Loss: 2.332251\n",
      "Epoch: 14118 \tTraining Loss: 1.742386 \tValidation Loss: 2.332428\n",
      "Epoch: 14119 \tTraining Loss: 1.763851 \tValidation Loss: 2.332656\n",
      "Epoch: 14120 \tTraining Loss: 1.783411 \tValidation Loss: 2.332909\n",
      "Epoch: 14121 \tTraining Loss: 1.760365 \tValidation Loss: 2.332242\n",
      "Epoch: 14122 \tTraining Loss: 1.794655 \tValidation Loss: 2.332452\n",
      "Epoch: 14123 \tTraining Loss: 1.748075 \tValidation Loss: 2.332587\n",
      "Epoch: 14124 \tTraining Loss: 1.754345 \tValidation Loss: 2.332292\n",
      "Epoch: 14125 \tTraining Loss: 1.778663 \tValidation Loss: 2.331767\n",
      "Epoch: 14126 \tTraining Loss: 1.793686 \tValidation Loss: 2.332031\n",
      "Epoch: 14127 \tTraining Loss: 1.795179 \tValidation Loss: 2.331947\n",
      "Epoch: 14128 \tTraining Loss: 1.745268 \tValidation Loss: 2.332227\n",
      "Epoch: 14129 \tTraining Loss: 1.762177 \tValidation Loss: 2.331989\n",
      "Epoch: 14130 \tTraining Loss: 1.751617 \tValidation Loss: 2.332357\n",
      "Epoch: 14131 \tTraining Loss: 1.751524 \tValidation Loss: 2.332803\n",
      "Epoch: 14132 \tTraining Loss: 1.746142 \tValidation Loss: 2.332447\n",
      "Epoch: 14133 \tTraining Loss: 1.744107 \tValidation Loss: 2.332401\n",
      "Epoch: 14134 \tTraining Loss: 1.781356 \tValidation Loss: 2.332833\n",
      "Epoch: 14135 \tTraining Loss: 1.809944 \tValidation Loss: 2.332426\n",
      "Epoch: 14136 \tTraining Loss: 1.774708 \tValidation Loss: 2.332188\n",
      "Epoch: 14137 \tTraining Loss: 1.785693 \tValidation Loss: 2.332230\n",
      "Epoch: 14138 \tTraining Loss: 1.783301 \tValidation Loss: 2.332118\n",
      "Epoch: 14139 \tTraining Loss: 1.764755 \tValidation Loss: 2.332124\n",
      "Epoch: 14140 \tTraining Loss: 1.765719 \tValidation Loss: 2.332438\n",
      "Epoch: 14141 \tTraining Loss: 1.766957 \tValidation Loss: 2.332424\n",
      "Epoch: 14142 \tTraining Loss: 1.763148 \tValidation Loss: 2.332669\n",
      "Epoch: 14143 \tTraining Loss: 1.773552 \tValidation Loss: 2.332653\n",
      "Epoch: 14144 \tTraining Loss: 1.789983 \tValidation Loss: 2.332432\n",
      "Epoch: 14145 \tTraining Loss: 1.723372 \tValidation Loss: 2.332323\n",
      "Epoch: 14146 \tTraining Loss: 1.811337 \tValidation Loss: 2.331668\n",
      "Epoch: 14147 \tTraining Loss: 1.760518 \tValidation Loss: 2.331697\n",
      "Epoch: 14148 \tTraining Loss: 1.764567 \tValidation Loss: 2.331607\n",
      "Epoch: 14149 \tTraining Loss: 1.778298 \tValidation Loss: 2.332287\n",
      "Epoch: 14150 \tTraining Loss: 1.760039 \tValidation Loss: 2.332338\n",
      "Epoch: 14151 \tTraining Loss: 1.834734 \tValidation Loss: 2.332271\n",
      "Epoch: 14152 \tTraining Loss: 1.760507 \tValidation Loss: 2.332240\n",
      "Epoch: 14153 \tTraining Loss: 1.779979 \tValidation Loss: 2.332277\n",
      "Epoch: 14154 \tTraining Loss: 1.786345 \tValidation Loss: 2.332058\n",
      "Epoch: 14155 \tTraining Loss: 1.795690 \tValidation Loss: 2.332329\n",
      "Epoch: 14156 \tTraining Loss: 1.780594 \tValidation Loss: 2.332578\n",
      "Epoch: 14157 \tTraining Loss: 1.772008 \tValidation Loss: 2.332151\n",
      "Epoch: 14158 \tTraining Loss: 1.775368 \tValidation Loss: 2.332035\n",
      "Epoch: 14159 \tTraining Loss: 1.791064 \tValidation Loss: 2.331982\n",
      "Epoch: 14160 \tTraining Loss: 1.766405 \tValidation Loss: 2.332086\n",
      "Epoch: 14161 \tTraining Loss: 1.766485 \tValidation Loss: 2.332152\n",
      "Epoch: 14162 \tTraining Loss: 1.794191 \tValidation Loss: 2.332447\n",
      "Epoch: 14163 \tTraining Loss: 1.755478 \tValidation Loss: 2.332605\n",
      "Epoch: 14164 \tTraining Loss: 1.784617 \tValidation Loss: 2.332987\n",
      "Epoch: 14165 \tTraining Loss: 1.782117 \tValidation Loss: 2.332455\n",
      "Epoch: 14166 \tTraining Loss: 1.768927 \tValidation Loss: 2.332484\n",
      "Epoch: 14167 \tTraining Loss: 1.767000 \tValidation Loss: 2.332067\n",
      "Epoch: 14168 \tTraining Loss: 1.776833 \tValidation Loss: 2.332486\n",
      "Epoch: 14169 \tTraining Loss: 1.746410 \tValidation Loss: 2.332127\n",
      "Epoch: 14170 \tTraining Loss: 1.817838 \tValidation Loss: 2.332129\n",
      "Epoch: 14171 \tTraining Loss: 1.766065 \tValidation Loss: 2.331550\n",
      "Epoch: 14172 \tTraining Loss: 1.762305 \tValidation Loss: 2.332115\n",
      "Epoch: 14173 \tTraining Loss: 1.790554 \tValidation Loss: 2.332024\n",
      "Epoch: 14174 \tTraining Loss: 1.745897 \tValidation Loss: 2.331795\n",
      "Epoch: 14175 \tTraining Loss: 1.767211 \tValidation Loss: 2.331989\n",
      "Epoch: 14176 \tTraining Loss: 1.779689 \tValidation Loss: 2.332400\n",
      "Epoch: 14177 \tTraining Loss: 1.769620 \tValidation Loss: 2.332161\n",
      "Epoch: 14178 \tTraining Loss: 1.786861 \tValidation Loss: 2.332211\n",
      "Epoch: 14179 \tTraining Loss: 1.781920 \tValidation Loss: 2.332237\n",
      "Epoch: 14180 \tTraining Loss: 1.789768 \tValidation Loss: 2.332059\n",
      "Epoch: 14181 \tTraining Loss: 1.764610 \tValidation Loss: 2.332246\n",
      "Epoch: 14182 \tTraining Loss: 1.748975 \tValidation Loss: 2.332041\n",
      "Epoch: 14183 \tTraining Loss: 1.787802 \tValidation Loss: 2.332667\n",
      "Epoch: 14184 \tTraining Loss: 1.750996 \tValidation Loss: 2.332517\n",
      "Epoch: 14185 \tTraining Loss: 1.772807 \tValidation Loss: 2.332341\n",
      "Epoch: 14186 \tTraining Loss: 1.786099 \tValidation Loss: 2.332618\n",
      "Epoch: 14187 \tTraining Loss: 1.782877 \tValidation Loss: 2.332311\n",
      "Epoch: 14188 \tTraining Loss: 1.809455 \tValidation Loss: 2.332205\n",
      "Epoch: 14189 \tTraining Loss: 1.810981 \tValidation Loss: 2.331959\n",
      "Epoch: 14190 \tTraining Loss: 1.759440 \tValidation Loss: 2.332187\n",
      "Epoch: 14191 \tTraining Loss: 1.780551 \tValidation Loss: 2.332178\n",
      "Epoch: 14192 \tTraining Loss: 1.770759 \tValidation Loss: 2.332158\n",
      "Epoch: 14193 \tTraining Loss: 1.773160 \tValidation Loss: 2.332054\n",
      "Epoch: 14194 \tTraining Loss: 1.762695 \tValidation Loss: 2.332274\n",
      "Epoch: 14195 \tTraining Loss: 1.749148 \tValidation Loss: 2.332660\n",
      "Epoch: 14196 \tTraining Loss: 1.761138 \tValidation Loss: 2.332577\n",
      "Epoch: 14197 \tTraining Loss: 1.782010 \tValidation Loss: 2.332427\n",
      "Epoch: 14198 \tTraining Loss: 1.784071 \tValidation Loss: 2.332283\n",
      "Epoch: 14199 \tTraining Loss: 1.783344 \tValidation Loss: 2.332629\n",
      "Epoch: 14200 \tTraining Loss: 1.747700 \tValidation Loss: 2.332852\n",
      "Epoch: 14201 \tTraining Loss: 1.764613 \tValidation Loss: 2.332398\n",
      "Epoch: 14202 \tTraining Loss: 1.741086 \tValidation Loss: 2.332821\n",
      "Epoch: 14203 \tTraining Loss: 1.773815 \tValidation Loss: 2.333016\n",
      "Epoch: 14204 \tTraining Loss: 1.774141 \tValidation Loss: 2.332920\n",
      "Epoch: 14205 \tTraining Loss: 1.745615 \tValidation Loss: 2.332773\n",
      "Epoch: 14206 \tTraining Loss: 1.753492 \tValidation Loss: 2.332721\n",
      "Epoch: 14207 \tTraining Loss: 1.748103 \tValidation Loss: 2.332939\n",
      "Epoch: 14208 \tTraining Loss: 1.786117 \tValidation Loss: 2.333173\n",
      "Epoch: 14209 \tTraining Loss: 1.777386 \tValidation Loss: 2.333217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14210 \tTraining Loss: 1.755012 \tValidation Loss: 2.333124\n",
      "Epoch: 14211 \tTraining Loss: 1.773573 \tValidation Loss: 2.333065\n",
      "Epoch: 14212 \tTraining Loss: 1.781315 \tValidation Loss: 2.332695\n",
      "Epoch: 14213 \tTraining Loss: 1.747452 \tValidation Loss: 2.333149\n",
      "Epoch: 14214 \tTraining Loss: 1.747519 \tValidation Loss: 2.333406\n",
      "Epoch: 14215 \tTraining Loss: 1.740817 \tValidation Loss: 2.332656\n",
      "Epoch: 14216 \tTraining Loss: 1.741722 \tValidation Loss: 2.332847\n",
      "Epoch: 14217 \tTraining Loss: 1.756264 \tValidation Loss: 2.333312\n",
      "Epoch: 14218 \tTraining Loss: 1.767208 \tValidation Loss: 2.333615\n",
      "Epoch: 14219 \tTraining Loss: 1.741989 \tValidation Loss: 2.333602\n",
      "Epoch: 14220 \tTraining Loss: 1.781675 \tValidation Loss: 2.333134\n",
      "Epoch: 14221 \tTraining Loss: 1.738891 \tValidation Loss: 2.333088\n",
      "Epoch: 14222 \tTraining Loss: 1.745411 \tValidation Loss: 2.333277\n",
      "Epoch: 14223 \tTraining Loss: 1.740592 \tValidation Loss: 2.333275\n",
      "Epoch: 14224 \tTraining Loss: 1.751717 \tValidation Loss: 2.333046\n",
      "Epoch: 14225 \tTraining Loss: 1.773019 \tValidation Loss: 2.332582\n",
      "Epoch: 14226 \tTraining Loss: 1.762891 \tValidation Loss: 2.332572\n",
      "Epoch: 14227 \tTraining Loss: 1.761516 \tValidation Loss: 2.333035\n",
      "Epoch: 14228 \tTraining Loss: 1.736556 \tValidation Loss: 2.333444\n",
      "Epoch: 14229 \tTraining Loss: 1.769622 \tValidation Loss: 2.332724\n",
      "Epoch: 14230 \tTraining Loss: 1.768384 \tValidation Loss: 2.331958\n",
      "Epoch: 14231 \tTraining Loss: 1.749463 \tValidation Loss: 2.332669\n",
      "Epoch: 14232 \tTraining Loss: 1.790653 \tValidation Loss: 2.332337\n",
      "Epoch: 14233 \tTraining Loss: 1.759362 \tValidation Loss: 2.332401\n",
      "Epoch: 14234 \tTraining Loss: 1.760860 \tValidation Loss: 2.332695\n",
      "Epoch: 14235 \tTraining Loss: 1.784707 \tValidation Loss: 2.332193\n",
      "Epoch: 14236 \tTraining Loss: 1.762879 \tValidation Loss: 2.332129\n",
      "Epoch: 14237 \tTraining Loss: 1.789025 \tValidation Loss: 2.332517\n",
      "Epoch: 14238 \tTraining Loss: 1.731230 \tValidation Loss: 2.332948\n",
      "Epoch: 14239 \tTraining Loss: 1.801003 \tValidation Loss: 2.332920\n",
      "Epoch: 14240 \tTraining Loss: 1.766625 \tValidation Loss: 2.332735\n",
      "Epoch: 14241 \tTraining Loss: 1.712774 \tValidation Loss: 2.333030\n",
      "Epoch: 14242 \tTraining Loss: 1.739222 \tValidation Loss: 2.332728\n",
      "Epoch: 14243 \tTraining Loss: 1.772165 \tValidation Loss: 2.332720\n",
      "Epoch: 14244 \tTraining Loss: 1.764273 \tValidation Loss: 2.332318\n",
      "Epoch: 14245 \tTraining Loss: 1.775731 \tValidation Loss: 2.332542\n",
      "Epoch: 14246 \tTraining Loss: 1.771013 \tValidation Loss: 2.332638\n",
      "Epoch: 14247 \tTraining Loss: 1.740939 \tValidation Loss: 2.332500\n",
      "Epoch: 14248 \tTraining Loss: 1.778366 \tValidation Loss: 2.332489\n",
      "Epoch: 14249 \tTraining Loss: 1.764188 \tValidation Loss: 2.332511\n",
      "Epoch: 14250 \tTraining Loss: 1.791843 \tValidation Loss: 2.332433\n",
      "Epoch: 14251 \tTraining Loss: 1.775953 \tValidation Loss: 2.332866\n",
      "Epoch: 14252 \tTraining Loss: 1.732907 \tValidation Loss: 2.332762\n",
      "Epoch: 14253 \tTraining Loss: 1.746786 \tValidation Loss: 2.333047\n",
      "Epoch: 14254 \tTraining Loss: 1.734933 \tValidation Loss: 2.332909\n",
      "Epoch: 14255 \tTraining Loss: 1.751287 \tValidation Loss: 2.333388\n",
      "Epoch: 14256 \tTraining Loss: 1.781040 \tValidation Loss: 2.333163\n",
      "Epoch: 14257 \tTraining Loss: 1.777203 \tValidation Loss: 2.332773\n",
      "Epoch: 14258 \tTraining Loss: 1.767092 \tValidation Loss: 2.333330\n",
      "Epoch: 14259 \tTraining Loss: 1.754163 \tValidation Loss: 2.333137\n",
      "Epoch: 14260 \tTraining Loss: 1.752944 \tValidation Loss: 2.332940\n",
      "Epoch: 14261 \tTraining Loss: 1.763757 \tValidation Loss: 2.332838\n",
      "Epoch: 14262 \tTraining Loss: 1.762644 \tValidation Loss: 2.332771\n",
      "Epoch: 14263 \tTraining Loss: 1.786669 \tValidation Loss: 2.333112\n",
      "Epoch: 14264 \tTraining Loss: 1.767088 \tValidation Loss: 2.333678\n",
      "Epoch: 14265 \tTraining Loss: 1.737595 \tValidation Loss: 2.333311\n",
      "Epoch: 14266 \tTraining Loss: 1.786444 \tValidation Loss: 2.333118\n",
      "Epoch: 14267 \tTraining Loss: 1.749250 \tValidation Loss: 2.333140\n",
      "Epoch: 14268 \tTraining Loss: 1.762022 \tValidation Loss: 2.332971\n",
      "Epoch: 14269 \tTraining Loss: 1.749380 \tValidation Loss: 2.333218\n",
      "Epoch: 14270 \tTraining Loss: 1.786098 \tValidation Loss: 2.332967\n",
      "Epoch: 14271 \tTraining Loss: 1.784251 \tValidation Loss: 2.333168\n",
      "Epoch: 14272 \tTraining Loss: 1.758613 \tValidation Loss: 2.333341\n",
      "Epoch: 14273 \tTraining Loss: 1.808092 \tValidation Loss: 2.333328\n",
      "Epoch: 14274 \tTraining Loss: 1.768041 \tValidation Loss: 2.333216\n",
      "Epoch: 14275 \tTraining Loss: 1.797503 \tValidation Loss: 2.333101\n",
      "Epoch: 14276 \tTraining Loss: 1.797300 \tValidation Loss: 2.332390\n",
      "Epoch: 14277 \tTraining Loss: 1.759590 \tValidation Loss: 2.332444\n",
      "Epoch: 14278 \tTraining Loss: 1.758651 \tValidation Loss: 2.332860\n",
      "Epoch: 14279 \tTraining Loss: 1.738690 \tValidation Loss: 2.332868\n",
      "Epoch: 14280 \tTraining Loss: 1.740080 \tValidation Loss: 2.333065\n",
      "Epoch: 14281 \tTraining Loss: 1.747908 \tValidation Loss: 2.333282\n",
      "Epoch: 14282 \tTraining Loss: 1.739746 \tValidation Loss: 2.333339\n",
      "Epoch: 14283 \tTraining Loss: 1.735141 \tValidation Loss: 2.333687\n",
      "Epoch: 14284 \tTraining Loss: 1.751727 \tValidation Loss: 2.333825\n",
      "Epoch: 14285 \tTraining Loss: 1.739887 \tValidation Loss: 2.333823\n",
      "Epoch: 14286 \tTraining Loss: 1.730133 \tValidation Loss: 2.333886\n",
      "Epoch: 14287 \tTraining Loss: 1.751977 \tValidation Loss: 2.333485\n",
      "Epoch: 14288 \tTraining Loss: 1.781505 \tValidation Loss: 2.333654\n",
      "Epoch: 14289 \tTraining Loss: 1.769124 \tValidation Loss: 2.333846\n",
      "Epoch: 14290 \tTraining Loss: 1.775264 \tValidation Loss: 2.333823\n",
      "Epoch: 14291 \tTraining Loss: 1.745600 \tValidation Loss: 2.333904\n",
      "Epoch: 14292 \tTraining Loss: 1.761899 \tValidation Loss: 2.333835\n",
      "Epoch: 14293 \tTraining Loss: 1.788434 \tValidation Loss: 2.334243\n",
      "Epoch: 14294 \tTraining Loss: 1.762118 \tValidation Loss: 2.333882\n",
      "Epoch: 14295 \tTraining Loss: 1.786213 \tValidation Loss: 2.333634\n",
      "Epoch: 14296 \tTraining Loss: 1.770002 \tValidation Loss: 2.334063\n",
      "Epoch: 14297 \tTraining Loss: 1.761872 \tValidation Loss: 2.333236\n",
      "Epoch: 14298 \tTraining Loss: 1.776893 \tValidation Loss: 2.333191\n",
      "Epoch: 14299 \tTraining Loss: 1.788592 \tValidation Loss: 2.333023\n",
      "Epoch: 14300 \tTraining Loss: 1.784773 \tValidation Loss: 2.333281\n",
      "Epoch: 14301 \tTraining Loss: 1.761964 \tValidation Loss: 2.333516\n",
      "Epoch: 14302 \tTraining Loss: 1.783712 \tValidation Loss: 2.333700\n",
      "Epoch: 14303 \tTraining Loss: 1.769065 \tValidation Loss: 2.333796\n",
      "Epoch: 14304 \tTraining Loss: 1.798772 \tValidation Loss: 2.333559\n",
      "Epoch: 14305 \tTraining Loss: 1.789829 \tValidation Loss: 2.333471\n",
      "Epoch: 14306 \tTraining Loss: 1.763693 \tValidation Loss: 2.333884\n",
      "Epoch: 14307 \tTraining Loss: 1.758341 \tValidation Loss: 2.333954\n",
      "Epoch: 14308 \tTraining Loss: 1.781784 \tValidation Loss: 2.333416\n",
      "Epoch: 14309 \tTraining Loss: 1.726668 \tValidation Loss: 2.333276\n",
      "Epoch: 14310 \tTraining Loss: 1.767052 \tValidation Loss: 2.333274\n",
      "Epoch: 14311 \tTraining Loss: 1.756682 \tValidation Loss: 2.333300\n",
      "Epoch: 14312 \tTraining Loss: 1.756584 \tValidation Loss: 2.333588\n",
      "Epoch: 14313 \tTraining Loss: 1.769262 \tValidation Loss: 2.333709\n",
      "Epoch: 14314 \tTraining Loss: 1.810858 \tValidation Loss: 2.333446\n",
      "Epoch: 14315 \tTraining Loss: 1.763279 \tValidation Loss: 2.333456\n",
      "Epoch: 14316 \tTraining Loss: 1.743926 \tValidation Loss: 2.334054\n",
      "Epoch: 14317 \tTraining Loss: 1.754566 \tValidation Loss: 2.333500\n",
      "Epoch: 14318 \tTraining Loss: 1.784726 \tValidation Loss: 2.333550\n",
      "Epoch: 14319 \tTraining Loss: 1.758735 \tValidation Loss: 2.334105\n",
      "Epoch: 14320 \tTraining Loss: 1.784038 \tValidation Loss: 2.333789\n",
      "Epoch: 14321 \tTraining Loss: 1.758071 \tValidation Loss: 2.334034\n",
      "Epoch: 14322 \tTraining Loss: 1.761770 \tValidation Loss: 2.333763\n",
      "Epoch: 14323 \tTraining Loss: 1.753398 \tValidation Loss: 2.333904\n",
      "Epoch: 14324 \tTraining Loss: 1.705145 \tValidation Loss: 2.333823\n",
      "Epoch: 14325 \tTraining Loss: 1.782211 \tValidation Loss: 2.333522\n",
      "Epoch: 14326 \tTraining Loss: 1.780334 \tValidation Loss: 2.333980\n",
      "Epoch: 14327 \tTraining Loss: 1.734056 \tValidation Loss: 2.334153\n",
      "Epoch: 14328 \tTraining Loss: 1.769868 \tValidation Loss: 2.334091\n",
      "Epoch: 14329 \tTraining Loss: 1.789416 \tValidation Loss: 2.334268\n",
      "Epoch: 14330 \tTraining Loss: 1.773092 \tValidation Loss: 2.334553\n",
      "Epoch: 14331 \tTraining Loss: 1.722925 \tValidation Loss: 2.334545\n",
      "Epoch: 14332 \tTraining Loss: 1.776890 \tValidation Loss: 2.333934\n",
      "Epoch: 14333 \tTraining Loss: 1.763113 \tValidation Loss: 2.333940\n",
      "Epoch: 14334 \tTraining Loss: 1.770079 \tValidation Loss: 2.334089\n",
      "Epoch: 14335 \tTraining Loss: 1.759933 \tValidation Loss: 2.334540\n",
      "Epoch: 14336 \tTraining Loss: 1.775706 \tValidation Loss: 2.334552\n",
      "Epoch: 14337 \tTraining Loss: 1.765235 \tValidation Loss: 2.334144\n",
      "Epoch: 14338 \tTraining Loss: 1.739918 \tValidation Loss: 2.334502\n",
      "Epoch: 14339 \tTraining Loss: 1.779127 \tValidation Loss: 2.334553\n",
      "Epoch: 14340 \tTraining Loss: 1.796183 \tValidation Loss: 2.334642\n",
      "Epoch: 14341 \tTraining Loss: 1.771108 \tValidation Loss: 2.334336\n",
      "Epoch: 14342 \tTraining Loss: 1.751482 \tValidation Loss: 2.334294\n",
      "Epoch: 14343 \tTraining Loss: 1.748368 \tValidation Loss: 2.334323\n",
      "Epoch: 14344 \tTraining Loss: 1.808661 \tValidation Loss: 2.333762\n",
      "Epoch: 14345 \tTraining Loss: 1.776374 \tValidation Loss: 2.333526\n",
      "Epoch: 14346 \tTraining Loss: 1.749552 \tValidation Loss: 2.333901\n",
      "Epoch: 14347 \tTraining Loss: 1.731722 \tValidation Loss: 2.334102\n",
      "Epoch: 14348 \tTraining Loss: 1.811427 \tValidation Loss: 2.334123\n",
      "Epoch: 14349 \tTraining Loss: 1.720109 \tValidation Loss: 2.334044\n",
      "Epoch: 14350 \tTraining Loss: 1.760320 \tValidation Loss: 2.334478\n",
      "Epoch: 14351 \tTraining Loss: 1.729900 \tValidation Loss: 2.334246\n",
      "Epoch: 14352 \tTraining Loss: 1.766196 \tValidation Loss: 2.333974\n",
      "Epoch: 14353 \tTraining Loss: 1.750593 \tValidation Loss: 2.334144\n",
      "Epoch: 14354 \tTraining Loss: 1.770393 \tValidation Loss: 2.334213\n",
      "Epoch: 14355 \tTraining Loss: 1.756014 \tValidation Loss: 2.334064\n",
      "Epoch: 14356 \tTraining Loss: 1.786772 \tValidation Loss: 2.333743\n",
      "Epoch: 14357 \tTraining Loss: 1.767782 \tValidation Loss: 2.333635\n",
      "Epoch: 14358 \tTraining Loss: 1.738763 \tValidation Loss: 2.333890\n",
      "Epoch: 14359 \tTraining Loss: 1.754927 \tValidation Loss: 2.333897\n",
      "Epoch: 14360 \tTraining Loss: 1.753199 \tValidation Loss: 2.334153\n",
      "Epoch: 14361 \tTraining Loss: 1.758212 \tValidation Loss: 2.333875\n",
      "Epoch: 14362 \tTraining Loss: 1.716447 \tValidation Loss: 2.334474\n",
      "Epoch: 14363 \tTraining Loss: 1.772917 \tValidation Loss: 2.334530\n",
      "Epoch: 14364 \tTraining Loss: 1.788035 \tValidation Loss: 2.334524\n",
      "Epoch: 14365 \tTraining Loss: 1.752095 \tValidation Loss: 2.334515\n",
      "Epoch: 14366 \tTraining Loss: 1.766630 \tValidation Loss: 2.334319\n",
      "Epoch: 14367 \tTraining Loss: 1.729213 \tValidation Loss: 2.334821\n",
      "Epoch: 14368 \tTraining Loss: 1.747020 \tValidation Loss: 2.334878\n",
      "Epoch: 14369 \tTraining Loss: 1.737863 \tValidation Loss: 2.334947\n",
      "Epoch: 14370 \tTraining Loss: 1.779807 \tValidation Loss: 2.334692\n",
      "Epoch: 14371 \tTraining Loss: 1.782901 \tValidation Loss: 2.334029\n",
      "Epoch: 14372 \tTraining Loss: 1.814726 \tValidation Loss: 2.334400\n",
      "Epoch: 14373 \tTraining Loss: 1.758512 \tValidation Loss: 2.333729\n",
      "Epoch: 14374 \tTraining Loss: 1.731512 \tValidation Loss: 2.334259\n",
      "Epoch: 14375 \tTraining Loss: 1.767311 \tValidation Loss: 2.334180\n",
      "Epoch: 14376 \tTraining Loss: 1.786026 \tValidation Loss: 2.333941\n",
      "Epoch: 14377 \tTraining Loss: 1.763956 \tValidation Loss: 2.334219\n",
      "Epoch: 14378 \tTraining Loss: 1.750524 \tValidation Loss: 2.334960\n",
      "Epoch: 14379 \tTraining Loss: 1.779416 \tValidation Loss: 2.335253\n",
      "Epoch: 14380 \tTraining Loss: 1.773935 \tValidation Loss: 2.334445\n",
      "Epoch: 14381 \tTraining Loss: 1.736320 \tValidation Loss: 2.334359\n",
      "Epoch: 14382 \tTraining Loss: 1.765322 \tValidation Loss: 2.334177\n",
      "Epoch: 14383 \tTraining Loss: 1.750707 \tValidation Loss: 2.334641\n",
      "Epoch: 14384 \tTraining Loss: 1.744695 \tValidation Loss: 2.334752\n",
      "Epoch: 14385 \tTraining Loss: 1.762506 \tValidation Loss: 2.334706\n",
      "Epoch: 14386 \tTraining Loss: 1.757808 \tValidation Loss: 2.334979\n",
      "Epoch: 14387 \tTraining Loss: 1.798065 \tValidation Loss: 2.334854\n",
      "Epoch: 14388 \tTraining Loss: 1.748108 \tValidation Loss: 2.334947\n",
      "Epoch: 14389 \tTraining Loss: 1.734880 \tValidation Loss: 2.335366\n",
      "Epoch: 14390 \tTraining Loss: 1.785406 \tValidation Loss: 2.335055\n",
      "Epoch: 14391 \tTraining Loss: 1.768685 \tValidation Loss: 2.335215\n",
      "Epoch: 14392 \tTraining Loss: 1.785963 \tValidation Loss: 2.334837\n",
      "Epoch: 14393 \tTraining Loss: 1.759799 \tValidation Loss: 2.335032\n",
      "Epoch: 14394 \tTraining Loss: 1.767256 \tValidation Loss: 2.334491\n",
      "Epoch: 14395 \tTraining Loss: 1.777528 \tValidation Loss: 2.334511\n",
      "Epoch: 14396 \tTraining Loss: 1.780941 \tValidation Loss: 2.333762\n",
      "Epoch: 14397 \tTraining Loss: 1.767301 \tValidation Loss: 2.334358\n",
      "Epoch: 14398 \tTraining Loss: 1.765720 \tValidation Loss: 2.334460\n",
      "Epoch: 14399 \tTraining Loss: 1.755642 \tValidation Loss: 2.334519\n",
      "Epoch: 14400 \tTraining Loss: 1.754764 \tValidation Loss: 2.334870\n",
      "Epoch: 14401 \tTraining Loss: 1.755805 \tValidation Loss: 2.335175\n",
      "Epoch: 14402 \tTraining Loss: 1.808149 \tValidation Loss: 2.334524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14403 \tTraining Loss: 1.756220 \tValidation Loss: 2.334575\n",
      "Epoch: 14404 \tTraining Loss: 1.726807 \tValidation Loss: 2.334913\n",
      "Epoch: 14405 \tTraining Loss: 1.791746 \tValidation Loss: 2.334829\n",
      "Epoch: 14406 \tTraining Loss: 1.764827 \tValidation Loss: 2.334216\n",
      "Epoch: 14407 \tTraining Loss: 1.751650 \tValidation Loss: 2.334286\n",
      "Epoch: 14408 \tTraining Loss: 1.742442 \tValidation Loss: 2.334358\n",
      "Epoch: 14409 \tTraining Loss: 1.747022 \tValidation Loss: 2.334948\n",
      "Epoch: 14410 \tTraining Loss: 1.790939 \tValidation Loss: 2.335005\n",
      "Epoch: 14411 \tTraining Loss: 1.769292 \tValidation Loss: 2.334695\n",
      "Epoch: 14412 \tTraining Loss: 1.748446 \tValidation Loss: 2.334747\n",
      "Epoch: 14413 \tTraining Loss: 1.771587 \tValidation Loss: 2.334539\n",
      "Epoch: 14414 \tTraining Loss: 1.746910 \tValidation Loss: 2.335236\n",
      "Epoch: 14415 \tTraining Loss: 1.775779 \tValidation Loss: 2.334917\n",
      "Epoch: 14416 \tTraining Loss: 1.761114 \tValidation Loss: 2.335241\n",
      "Epoch: 14417 \tTraining Loss: 1.764187 \tValidation Loss: 2.334857\n",
      "Epoch: 14418 \tTraining Loss: 1.781764 \tValidation Loss: 2.334496\n",
      "Epoch: 14419 \tTraining Loss: 1.771483 \tValidation Loss: 2.334614\n",
      "Epoch: 14420 \tTraining Loss: 1.758391 \tValidation Loss: 2.334663\n",
      "Epoch: 14421 \tTraining Loss: 1.734921 \tValidation Loss: 2.334649\n",
      "Epoch: 14422 \tTraining Loss: 1.752253 \tValidation Loss: 2.334733\n",
      "Epoch: 14423 \tTraining Loss: 1.755743 \tValidation Loss: 2.335042\n",
      "Epoch: 14424 \tTraining Loss: 1.759795 \tValidation Loss: 2.335052\n",
      "Epoch: 14425 \tTraining Loss: 1.750981 \tValidation Loss: 2.335621\n",
      "Epoch: 14426 \tTraining Loss: 1.740136 \tValidation Loss: 2.335695\n",
      "Epoch: 14427 \tTraining Loss: 1.757911 \tValidation Loss: 2.335537\n",
      "Epoch: 14428 \tTraining Loss: 1.767434 \tValidation Loss: 2.335147\n",
      "Epoch: 14429 \tTraining Loss: 1.762630 \tValidation Loss: 2.335332\n",
      "Epoch: 14430 \tTraining Loss: 1.769896 \tValidation Loss: 2.334921\n",
      "Epoch: 14431 \tTraining Loss: 1.788121 \tValidation Loss: 2.334738\n",
      "Epoch: 14432 \tTraining Loss: 1.768201 \tValidation Loss: 2.334550\n",
      "Epoch: 14433 \tTraining Loss: 1.745634 \tValidation Loss: 2.334762\n",
      "Epoch: 14434 \tTraining Loss: 1.749966 \tValidation Loss: 2.335092\n",
      "Epoch: 14435 \tTraining Loss: 1.784685 \tValidation Loss: 2.335143\n",
      "Epoch: 14436 \tTraining Loss: 1.754128 \tValidation Loss: 2.334946\n",
      "Epoch: 14437 \tTraining Loss: 1.767010 \tValidation Loss: 2.335076\n",
      "Epoch: 14438 \tTraining Loss: 1.751903 \tValidation Loss: 2.335757\n",
      "Epoch: 14439 \tTraining Loss: 1.775884 \tValidation Loss: 2.335357\n",
      "Epoch: 14440 \tTraining Loss: 1.738189 \tValidation Loss: 2.335546\n",
      "Epoch: 14441 \tTraining Loss: 1.792168 \tValidation Loss: 2.335197\n",
      "Epoch: 14442 \tTraining Loss: 1.726393 \tValidation Loss: 2.335418\n",
      "Epoch: 14443 \tTraining Loss: 1.747398 \tValidation Loss: 2.335674\n",
      "Epoch: 14444 \tTraining Loss: 1.739837 \tValidation Loss: 2.335728\n",
      "Epoch: 14445 \tTraining Loss: 1.761526 \tValidation Loss: 2.336061\n",
      "Epoch: 14446 \tTraining Loss: 1.747299 \tValidation Loss: 2.335921\n",
      "Epoch: 14447 \tTraining Loss: 1.753350 \tValidation Loss: 2.335723\n",
      "Epoch: 14448 \tTraining Loss: 1.753928 \tValidation Loss: 2.336133\n",
      "Epoch: 14449 \tTraining Loss: 1.749834 \tValidation Loss: 2.335742\n",
      "Epoch: 14450 \tTraining Loss: 1.774667 \tValidation Loss: 2.335288\n",
      "Epoch: 14451 \tTraining Loss: 1.803756 \tValidation Loss: 2.335257\n",
      "Epoch: 14452 \tTraining Loss: 1.773766 \tValidation Loss: 2.334818\n",
      "Epoch: 14453 \tTraining Loss: 1.743375 \tValidation Loss: 2.334963\n",
      "Epoch: 14454 \tTraining Loss: 1.776398 \tValidation Loss: 2.335186\n",
      "Epoch: 14455 \tTraining Loss: 1.764108 \tValidation Loss: 2.335524\n",
      "Epoch: 14456 \tTraining Loss: 1.778548 \tValidation Loss: 2.335361\n",
      "Epoch: 14457 \tTraining Loss: 1.770337 \tValidation Loss: 2.335531\n",
      "Epoch: 14458 \tTraining Loss: 1.794950 \tValidation Loss: 2.335166\n",
      "Epoch: 14459 \tTraining Loss: 1.786795 \tValidation Loss: 2.335321\n",
      "Epoch: 14460 \tTraining Loss: 1.791833 \tValidation Loss: 2.334868\n",
      "Epoch: 14461 \tTraining Loss: 1.770835 \tValidation Loss: 2.335048\n",
      "Epoch: 14462 \tTraining Loss: 1.729039 \tValidation Loss: 2.335089\n",
      "Epoch: 14463 \tTraining Loss: 1.744280 \tValidation Loss: 2.335238\n",
      "Epoch: 14464 \tTraining Loss: 1.712013 \tValidation Loss: 2.335113\n",
      "Epoch: 14465 \tTraining Loss: 1.753286 \tValidation Loss: 2.334980\n",
      "Epoch: 14466 \tTraining Loss: 1.747623 \tValidation Loss: 2.335149\n",
      "Epoch: 14467 \tTraining Loss: 1.738485 \tValidation Loss: 2.335104\n",
      "Epoch: 14468 \tTraining Loss: 1.749125 \tValidation Loss: 2.335359\n",
      "Epoch: 14469 \tTraining Loss: 1.772688 \tValidation Loss: 2.335341\n",
      "Epoch: 14470 \tTraining Loss: 1.783898 \tValidation Loss: 2.335474\n",
      "Epoch: 14471 \tTraining Loss: 1.801853 \tValidation Loss: 2.335280\n",
      "Epoch: 14472 \tTraining Loss: 1.785372 \tValidation Loss: 2.335281\n",
      "Epoch: 14473 \tTraining Loss: 1.778962 \tValidation Loss: 2.335270\n",
      "Epoch: 14474 \tTraining Loss: 1.739854 \tValidation Loss: 2.335917\n",
      "Epoch: 14475 \tTraining Loss: 1.788312 \tValidation Loss: 2.335964\n",
      "Epoch: 14476 \tTraining Loss: 1.786574 \tValidation Loss: 2.334838\n",
      "Epoch: 14477 \tTraining Loss: 1.758325 \tValidation Loss: 2.335021\n",
      "Epoch: 14478 \tTraining Loss: 1.773583 \tValidation Loss: 2.334938\n",
      "Epoch: 14479 \tTraining Loss: 1.755303 \tValidation Loss: 2.335047\n",
      "Epoch: 14480 \tTraining Loss: 1.732020 \tValidation Loss: 2.335443\n",
      "Epoch: 14481 \tTraining Loss: 1.805265 \tValidation Loss: 2.335163\n",
      "Epoch: 14482 \tTraining Loss: 1.757646 \tValidation Loss: 2.334964\n",
      "Epoch: 14483 \tTraining Loss: 1.741977 \tValidation Loss: 2.334807\n",
      "Epoch: 14484 \tTraining Loss: 1.753304 \tValidation Loss: 2.334857\n",
      "Epoch: 14485 \tTraining Loss: 1.796376 \tValidation Loss: 2.334905\n",
      "Epoch: 14486 \tTraining Loss: 1.721472 \tValidation Loss: 2.335256\n",
      "Epoch: 14487 \tTraining Loss: 1.735404 \tValidation Loss: 2.335294\n",
      "Epoch: 14488 \tTraining Loss: 1.727452 \tValidation Loss: 2.335265\n",
      "Epoch: 14489 \tTraining Loss: 1.756326 \tValidation Loss: 2.334966\n",
      "Epoch: 14490 \tTraining Loss: 1.753810 \tValidation Loss: 2.335512\n",
      "Epoch: 14491 \tTraining Loss: 1.785544 \tValidation Loss: 2.335512\n",
      "Epoch: 14492 \tTraining Loss: 1.755814 \tValidation Loss: 2.335724\n",
      "Epoch: 14493 \tTraining Loss: 1.758861 \tValidation Loss: 2.335922\n",
      "Epoch: 14494 \tTraining Loss: 1.770257 \tValidation Loss: 2.335520\n",
      "Epoch: 14495 \tTraining Loss: 1.777878 \tValidation Loss: 2.335670\n",
      "Epoch: 14496 \tTraining Loss: 1.770434 \tValidation Loss: 2.335663\n",
      "Epoch: 14497 \tTraining Loss: 1.792376 \tValidation Loss: 2.335506\n",
      "Epoch: 14498 \tTraining Loss: 1.765420 \tValidation Loss: 2.335677\n",
      "Epoch: 14499 \tTraining Loss: 1.770355 \tValidation Loss: 2.335876\n",
      "Epoch: 14500 \tTraining Loss: 1.777858 \tValidation Loss: 2.335998\n",
      "Epoch: 14501 \tTraining Loss: 1.751360 \tValidation Loss: 2.335905\n",
      "Epoch: 14502 \tTraining Loss: 1.747442 \tValidation Loss: 2.335985\n",
      "Epoch: 14503 \tTraining Loss: 1.798720 \tValidation Loss: 2.335670\n",
      "Epoch: 14504 \tTraining Loss: 1.748187 \tValidation Loss: 2.335648\n",
      "Epoch: 14505 \tTraining Loss: 1.789463 \tValidation Loss: 2.335570\n",
      "Epoch: 14506 \tTraining Loss: 1.745711 \tValidation Loss: 2.336068\n",
      "Epoch: 14507 \tTraining Loss: 1.753996 \tValidation Loss: 2.336051\n",
      "Epoch: 14508 \tTraining Loss: 1.738511 \tValidation Loss: 2.335895\n",
      "Epoch: 14509 \tTraining Loss: 1.744804 \tValidation Loss: 2.335694\n",
      "Epoch: 14510 \tTraining Loss: 1.760725 \tValidation Loss: 2.335484\n",
      "Epoch: 14511 \tTraining Loss: 1.732368 \tValidation Loss: 2.335737\n",
      "Epoch: 14512 \tTraining Loss: 1.745209 \tValidation Loss: 2.335705\n",
      "Epoch: 14513 \tTraining Loss: 1.787993 \tValidation Loss: 2.335664\n",
      "Epoch: 14514 \tTraining Loss: 1.786456 \tValidation Loss: 2.335688\n",
      "Epoch: 14515 \tTraining Loss: 1.753986 \tValidation Loss: 2.335881\n",
      "Epoch: 14516 \tTraining Loss: 1.743467 \tValidation Loss: 2.335960\n",
      "Epoch: 14517 \tTraining Loss: 1.731476 \tValidation Loss: 2.336204\n",
      "Epoch: 14518 \tTraining Loss: 1.778320 \tValidation Loss: 2.336298\n",
      "Epoch: 14519 \tTraining Loss: 1.771448 \tValidation Loss: 2.335619\n",
      "Epoch: 14520 \tTraining Loss: 1.785763 \tValidation Loss: 2.335433\n",
      "Epoch: 14521 \tTraining Loss: 1.780837 \tValidation Loss: 2.335114\n",
      "Epoch: 14522 \tTraining Loss: 1.765115 \tValidation Loss: 2.335485\n",
      "Epoch: 14523 \tTraining Loss: 1.783137 \tValidation Loss: 2.335086\n",
      "Epoch: 14524 \tTraining Loss: 1.750019 \tValidation Loss: 2.335376\n",
      "Epoch: 14525 \tTraining Loss: 1.779375 \tValidation Loss: 2.335354\n",
      "Epoch: 14526 \tTraining Loss: 1.732332 \tValidation Loss: 2.335771\n",
      "Epoch: 14527 \tTraining Loss: 1.785604 \tValidation Loss: 2.335250\n",
      "Epoch: 14528 \tTraining Loss: 1.740586 \tValidation Loss: 2.335507\n",
      "Epoch: 14529 \tTraining Loss: 1.761960 \tValidation Loss: 2.335132\n",
      "Epoch: 14530 \tTraining Loss: 1.760412 \tValidation Loss: 2.335228\n",
      "Epoch: 14531 \tTraining Loss: 1.738268 \tValidation Loss: 2.335508\n",
      "Epoch: 14532 \tTraining Loss: 1.765490 \tValidation Loss: 2.335253\n",
      "Epoch: 14533 \tTraining Loss: 1.784552 \tValidation Loss: 2.335224\n",
      "Epoch: 14534 \tTraining Loss: 1.781739 \tValidation Loss: 2.335141\n",
      "Epoch: 14535 \tTraining Loss: 1.798602 \tValidation Loss: 2.335450\n",
      "Epoch: 14536 \tTraining Loss: 1.790416 \tValidation Loss: 2.335504\n",
      "Epoch: 14537 \tTraining Loss: 1.763448 \tValidation Loss: 2.335135\n",
      "Epoch: 14538 \tTraining Loss: 1.764403 \tValidation Loss: 2.334855\n",
      "Epoch: 14539 \tTraining Loss: 1.773994 \tValidation Loss: 2.334990\n",
      "Epoch: 14540 \tTraining Loss: 1.760403 \tValidation Loss: 2.334968\n",
      "Epoch: 14541 \tTraining Loss: 1.762790 \tValidation Loss: 2.335752\n",
      "Epoch: 14542 \tTraining Loss: 1.785357 \tValidation Loss: 2.335363\n",
      "Epoch: 14543 \tTraining Loss: 1.747639 \tValidation Loss: 2.335428\n",
      "Epoch: 14544 \tTraining Loss: 1.719128 \tValidation Loss: 2.335664\n",
      "Epoch: 14545 \tTraining Loss: 1.761370 \tValidation Loss: 2.335951\n",
      "Epoch: 14546 \tTraining Loss: 1.765544 \tValidation Loss: 2.336170\n",
      "Epoch: 14547 \tTraining Loss: 1.758721 \tValidation Loss: 2.336182\n",
      "Epoch: 14548 \tTraining Loss: 1.780827 \tValidation Loss: 2.335907\n",
      "Epoch: 14549 \tTraining Loss: 1.732222 \tValidation Loss: 2.336181\n",
      "Epoch: 14550 \tTraining Loss: 1.744961 \tValidation Loss: 2.336248\n",
      "Epoch: 14551 \tTraining Loss: 1.738734 \tValidation Loss: 2.335915\n",
      "Epoch: 14552 \tTraining Loss: 1.710139 \tValidation Loss: 2.336022\n",
      "Epoch: 14553 \tTraining Loss: 1.747685 \tValidation Loss: 2.335766\n",
      "Epoch: 14554 \tTraining Loss: 1.792876 \tValidation Loss: 2.335449\n",
      "Epoch: 14555 \tTraining Loss: 1.753126 \tValidation Loss: 2.335414\n",
      "Epoch: 14556 \tTraining Loss: 1.749033 \tValidation Loss: 2.334871\n",
      "Epoch: 14557 \tTraining Loss: 1.765627 \tValidation Loss: 2.335259\n",
      "Epoch: 14558 \tTraining Loss: 1.766373 \tValidation Loss: 2.335553\n",
      "Epoch: 14559 \tTraining Loss: 1.742048 \tValidation Loss: 2.335537\n",
      "Epoch: 14560 \tTraining Loss: 1.744342 \tValidation Loss: 2.335682\n",
      "Epoch: 14561 \tTraining Loss: 1.733618 \tValidation Loss: 2.335995\n",
      "Epoch: 14562 \tTraining Loss: 1.743606 \tValidation Loss: 2.335763\n",
      "Epoch: 14563 \tTraining Loss: 1.750580 \tValidation Loss: 2.335930\n",
      "Epoch: 14564 \tTraining Loss: 1.748647 \tValidation Loss: 2.335826\n",
      "Epoch: 14565 \tTraining Loss: 1.735165 \tValidation Loss: 2.336244\n",
      "Epoch: 14566 \tTraining Loss: 1.783993 \tValidation Loss: 2.335929\n",
      "Epoch: 14567 \tTraining Loss: 1.765311 \tValidation Loss: 2.335623\n",
      "Epoch: 14568 \tTraining Loss: 1.767628 \tValidation Loss: 2.336085\n",
      "Epoch: 14569 \tTraining Loss: 1.741650 \tValidation Loss: 2.335766\n",
      "Epoch: 14570 \tTraining Loss: 1.774193 \tValidation Loss: 2.336057\n",
      "Epoch: 14571 \tTraining Loss: 1.761997 \tValidation Loss: 2.336073\n",
      "Epoch: 14572 \tTraining Loss: 1.763049 \tValidation Loss: 2.335725\n",
      "Epoch: 14573 \tTraining Loss: 1.769781 \tValidation Loss: 2.335577\n",
      "Epoch: 14574 \tTraining Loss: 1.704864 \tValidation Loss: 2.336035\n",
      "Epoch: 14575 \tTraining Loss: 1.753781 \tValidation Loss: 2.335855\n",
      "Epoch: 14576 \tTraining Loss: 1.751971 \tValidation Loss: 2.336140\n",
      "Epoch: 14577 \tTraining Loss: 1.721161 \tValidation Loss: 2.336085\n",
      "Epoch: 14578 \tTraining Loss: 1.746026 \tValidation Loss: 2.335863\n",
      "Epoch: 14579 \tTraining Loss: 1.758221 \tValidation Loss: 2.335859\n",
      "Epoch: 14580 \tTraining Loss: 1.759216 \tValidation Loss: 2.335755\n",
      "Epoch: 14581 \tTraining Loss: 1.798622 \tValidation Loss: 2.335419\n",
      "Epoch: 14582 \tTraining Loss: 1.744964 \tValidation Loss: 2.335657\n",
      "Epoch: 14583 \tTraining Loss: 1.712832 \tValidation Loss: 2.336111\n",
      "Epoch: 14584 \tTraining Loss: 1.793149 \tValidation Loss: 2.335925\n",
      "Epoch: 14585 \tTraining Loss: 1.773728 \tValidation Loss: 2.335518\n",
      "Epoch: 14586 \tTraining Loss: 1.788682 \tValidation Loss: 2.335611\n",
      "Epoch: 14587 \tTraining Loss: 1.773237 \tValidation Loss: 2.335464\n",
      "Epoch: 14588 \tTraining Loss: 1.728258 \tValidation Loss: 2.335776\n",
      "Epoch: 14589 \tTraining Loss: 1.761162 \tValidation Loss: 2.335986\n",
      "Epoch: 14590 \tTraining Loss: 1.743211 \tValidation Loss: 2.336195\n",
      "Epoch: 14591 \tTraining Loss: 1.708252 \tValidation Loss: 2.336093\n",
      "Epoch: 14592 \tTraining Loss: 1.754695 \tValidation Loss: 2.335701\n",
      "Epoch: 14593 \tTraining Loss: 1.742325 \tValidation Loss: 2.335360\n",
      "Epoch: 14594 \tTraining Loss: 1.781403 \tValidation Loss: 2.335450\n",
      "Epoch: 14595 \tTraining Loss: 1.750407 \tValidation Loss: 2.335552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14596 \tTraining Loss: 1.746448 \tValidation Loss: 2.335600\n",
      "Epoch: 14597 \tTraining Loss: 1.719959 \tValidation Loss: 2.335721\n",
      "Epoch: 14598 \tTraining Loss: 1.752229 \tValidation Loss: 2.335957\n",
      "Epoch: 14599 \tTraining Loss: 1.774472 \tValidation Loss: 2.335339\n",
      "Epoch: 14600 \tTraining Loss: 1.797449 \tValidation Loss: 2.335583\n",
      "Epoch: 14601 \tTraining Loss: 1.760001 \tValidation Loss: 2.335788\n",
      "Epoch: 14602 \tTraining Loss: 1.714699 \tValidation Loss: 2.336275\n",
      "Epoch: 14603 \tTraining Loss: 1.786551 \tValidation Loss: 2.336191\n",
      "Epoch: 14604 \tTraining Loss: 1.702354 \tValidation Loss: 2.336441\n",
      "Epoch: 14605 \tTraining Loss: 1.768600 \tValidation Loss: 2.336358\n",
      "Epoch: 14606 \tTraining Loss: 1.745122 \tValidation Loss: 2.336303\n",
      "Epoch: 14607 \tTraining Loss: 1.725661 \tValidation Loss: 2.336109\n",
      "Epoch: 14608 \tTraining Loss: 1.747897 \tValidation Loss: 2.335825\n",
      "Epoch: 14609 \tTraining Loss: 1.762555 \tValidation Loss: 2.335785\n",
      "Epoch: 14610 \tTraining Loss: 1.736961 \tValidation Loss: 2.335848\n",
      "Epoch: 14611 \tTraining Loss: 1.715727 \tValidation Loss: 2.335891\n",
      "Epoch: 14612 \tTraining Loss: 1.754299 \tValidation Loss: 2.335881\n",
      "Epoch: 14613 \tTraining Loss: 1.701898 \tValidation Loss: 2.336385\n",
      "Epoch: 14614 \tTraining Loss: 1.782661 \tValidation Loss: 2.336232\n",
      "Epoch: 14615 \tTraining Loss: 1.769078 \tValidation Loss: 2.336347\n",
      "Epoch: 14616 \tTraining Loss: 1.757009 \tValidation Loss: 2.336397\n",
      "Epoch: 14617 \tTraining Loss: 1.783853 \tValidation Loss: 2.336427\n",
      "Epoch: 14618 \tTraining Loss: 1.762465 \tValidation Loss: 2.336347\n",
      "Epoch: 14619 \tTraining Loss: 1.764918 \tValidation Loss: 2.336101\n",
      "Epoch: 14620 \tTraining Loss: 1.761107 \tValidation Loss: 2.335963\n",
      "Epoch: 14621 \tTraining Loss: 1.745353 \tValidation Loss: 2.336067\n",
      "Epoch: 14622 \tTraining Loss: 1.774964 \tValidation Loss: 2.336096\n",
      "Epoch: 14623 \tTraining Loss: 1.745560 \tValidation Loss: 2.336136\n",
      "Epoch: 14624 \tTraining Loss: 1.762751 \tValidation Loss: 2.335853\n",
      "Epoch: 14625 \tTraining Loss: 1.749568 \tValidation Loss: 2.335901\n",
      "Epoch: 14626 \tTraining Loss: 1.737978 \tValidation Loss: 2.335794\n",
      "Epoch: 14627 \tTraining Loss: 1.729480 \tValidation Loss: 2.335836\n",
      "Epoch: 14628 \tTraining Loss: 1.762104 \tValidation Loss: 2.335872\n",
      "Epoch: 14629 \tTraining Loss: 1.751817 \tValidation Loss: 2.336460\n",
      "Epoch: 14630 \tTraining Loss: 1.772282 \tValidation Loss: 2.335822\n",
      "Epoch: 14631 \tTraining Loss: 1.754448 \tValidation Loss: 2.336059\n",
      "Epoch: 14632 \tTraining Loss: 1.767712 \tValidation Loss: 2.336334\n",
      "Epoch: 14633 \tTraining Loss: 1.764992 \tValidation Loss: 2.336704\n",
      "Epoch: 14634 \tTraining Loss: 1.797166 \tValidation Loss: 2.336519\n",
      "Epoch: 14635 \tTraining Loss: 1.735999 \tValidation Loss: 2.336951\n",
      "Epoch: 14636 \tTraining Loss: 1.776069 \tValidation Loss: 2.336596\n",
      "Epoch: 14637 \tTraining Loss: 1.706205 \tValidation Loss: 2.336966\n",
      "Epoch: 14638 \tTraining Loss: 1.753177 \tValidation Loss: 2.336779\n",
      "Epoch: 14639 \tTraining Loss: 1.727811 \tValidation Loss: 2.336999\n",
      "Epoch: 14640 \tTraining Loss: 1.715432 \tValidation Loss: 2.336711\n",
      "Epoch: 14641 \tTraining Loss: 1.719030 \tValidation Loss: 2.336709\n",
      "Epoch: 14642 \tTraining Loss: 1.755645 \tValidation Loss: 2.336551\n",
      "Epoch: 14643 \tTraining Loss: 1.711255 \tValidation Loss: 2.336969\n",
      "Epoch: 14644 \tTraining Loss: 1.782527 \tValidation Loss: 2.336600\n",
      "Epoch: 14645 \tTraining Loss: 1.760690 \tValidation Loss: 2.336609\n",
      "Epoch: 14646 \tTraining Loss: 1.753649 \tValidation Loss: 2.336580\n",
      "Epoch: 14647 \tTraining Loss: 1.747400 \tValidation Loss: 2.336564\n",
      "Epoch: 14648 \tTraining Loss: 1.766515 \tValidation Loss: 2.336096\n",
      "Epoch: 14649 \tTraining Loss: 1.774948 \tValidation Loss: 2.336437\n",
      "Epoch: 14650 \tTraining Loss: 1.740258 \tValidation Loss: 2.336905\n",
      "Epoch: 14651 \tTraining Loss: 1.742614 \tValidation Loss: 2.337040\n",
      "Epoch: 14652 \tTraining Loss: 1.757413 \tValidation Loss: 2.336617\n",
      "Epoch: 14653 \tTraining Loss: 1.748175 \tValidation Loss: 2.336638\n",
      "Epoch: 14654 \tTraining Loss: 1.782734 \tValidation Loss: 2.336847\n",
      "Epoch: 14655 \tTraining Loss: 1.770451 \tValidation Loss: 2.336482\n",
      "Epoch: 14656 \tTraining Loss: 1.748095 \tValidation Loss: 2.336837\n",
      "Epoch: 14657 \tTraining Loss: 1.791816 \tValidation Loss: 2.337037\n",
      "Epoch: 14658 \tTraining Loss: 1.738412 \tValidation Loss: 2.337046\n",
      "Epoch: 14659 \tTraining Loss: 1.763465 \tValidation Loss: 2.337238\n",
      "Epoch: 14660 \tTraining Loss: 1.735966 \tValidation Loss: 2.337126\n",
      "Epoch: 14661 \tTraining Loss: 1.738263 \tValidation Loss: 2.336590\n",
      "Epoch: 14662 \tTraining Loss: 1.771426 \tValidation Loss: 2.336303\n",
      "Epoch: 14663 \tTraining Loss: 1.719754 \tValidation Loss: 2.336729\n",
      "Epoch: 14664 \tTraining Loss: 1.744127 \tValidation Loss: 2.336772\n",
      "Epoch: 14665 \tTraining Loss: 1.731959 \tValidation Loss: 2.336428\n",
      "Epoch: 14666 \tTraining Loss: 1.754289 \tValidation Loss: 2.337107\n",
      "Epoch: 14667 \tTraining Loss: 1.752160 \tValidation Loss: 2.337183\n",
      "Epoch: 14668 \tTraining Loss: 1.746209 \tValidation Loss: 2.337214\n",
      "Epoch: 14669 \tTraining Loss: 1.772469 \tValidation Loss: 2.336712\n",
      "Epoch: 14670 \tTraining Loss: 1.738441 \tValidation Loss: 2.336828\n",
      "Epoch: 14671 \tTraining Loss: 1.770501 \tValidation Loss: 2.336680\n",
      "Epoch: 14672 \tTraining Loss: 1.752797 \tValidation Loss: 2.336943\n",
      "Epoch: 14673 \tTraining Loss: 1.784199 \tValidation Loss: 2.336560\n",
      "Epoch: 14674 \tTraining Loss: 1.758555 \tValidation Loss: 2.336815\n",
      "Epoch: 14675 \tTraining Loss: 1.754717 \tValidation Loss: 2.337291\n",
      "Epoch: 14676 \tTraining Loss: 1.731211 \tValidation Loss: 2.337452\n",
      "Epoch: 14677 \tTraining Loss: 1.725813 \tValidation Loss: 2.337121\n",
      "Epoch: 14678 \tTraining Loss: 1.722040 \tValidation Loss: 2.337084\n",
      "Epoch: 14679 \tTraining Loss: 1.761928 \tValidation Loss: 2.336764\n",
      "Epoch: 14680 \tTraining Loss: 1.728408 \tValidation Loss: 2.336807\n",
      "Epoch: 14681 \tTraining Loss: 1.718107 \tValidation Loss: 2.336946\n",
      "Epoch: 14682 \tTraining Loss: 1.763556 \tValidation Loss: 2.336790\n",
      "Epoch: 14683 \tTraining Loss: 1.740152 \tValidation Loss: 2.337210\n",
      "Epoch: 14684 \tTraining Loss: 1.724857 \tValidation Loss: 2.337632\n",
      "Epoch: 14685 \tTraining Loss: 1.757685 \tValidation Loss: 2.336987\n",
      "Epoch: 14686 \tTraining Loss: 1.726661 \tValidation Loss: 2.337416\n",
      "Epoch: 14687 \tTraining Loss: 1.736821 \tValidation Loss: 2.337638\n",
      "Epoch: 14688 \tTraining Loss: 1.716857 \tValidation Loss: 2.337590\n",
      "Epoch: 14689 \tTraining Loss: 1.775184 \tValidation Loss: 2.337587\n",
      "Epoch: 14690 \tTraining Loss: 1.735843 \tValidation Loss: 2.337355\n",
      "Epoch: 14691 \tTraining Loss: 1.781963 \tValidation Loss: 2.336677\n",
      "Epoch: 14692 \tTraining Loss: 1.708593 \tValidation Loss: 2.337088\n",
      "Epoch: 14693 \tTraining Loss: 1.742921 \tValidation Loss: 2.337046\n",
      "Epoch: 14694 \tTraining Loss: 1.734876 \tValidation Loss: 2.336611\n",
      "Epoch: 14695 \tTraining Loss: 1.738288 \tValidation Loss: 2.336832\n",
      "Epoch: 14696 \tTraining Loss: 1.731049 \tValidation Loss: 2.336541\n",
      "Epoch: 14697 \tTraining Loss: 1.755156 \tValidation Loss: 2.336262\n",
      "Epoch: 14698 \tTraining Loss: 1.741017 \tValidation Loss: 2.336599\n",
      "Epoch: 14699 \tTraining Loss: 1.752771 \tValidation Loss: 2.336587\n",
      "Epoch: 14700 \tTraining Loss: 1.738650 \tValidation Loss: 2.336654\n",
      "Epoch: 14701 \tTraining Loss: 1.761347 \tValidation Loss: 2.336731\n",
      "Epoch: 14702 \tTraining Loss: 1.723544 \tValidation Loss: 2.336923\n",
      "Epoch: 14703 \tTraining Loss: 1.770139 \tValidation Loss: 2.337200\n",
      "Epoch: 14704 \tTraining Loss: 1.702451 \tValidation Loss: 2.336957\n",
      "Epoch: 14705 \tTraining Loss: 1.753210 \tValidation Loss: 2.337210\n",
      "Epoch: 14706 \tTraining Loss: 1.755356 \tValidation Loss: 2.337708\n",
      "Epoch: 14707 \tTraining Loss: 1.778932 \tValidation Loss: 2.337199\n",
      "Epoch: 14708 \tTraining Loss: 1.737682 \tValidation Loss: 2.337513\n",
      "Epoch: 14709 \tTraining Loss: 1.734863 \tValidation Loss: 2.337302\n",
      "Epoch: 14710 \tTraining Loss: 1.729215 \tValidation Loss: 2.337485\n",
      "Epoch: 14711 \tTraining Loss: 1.773676 \tValidation Loss: 2.337683\n",
      "Epoch: 14712 \tTraining Loss: 1.756631 \tValidation Loss: 2.337643\n",
      "Epoch: 14713 \tTraining Loss: 1.777207 \tValidation Loss: 2.336493\n",
      "Epoch: 14714 \tTraining Loss: 1.781159 \tValidation Loss: 2.336520\n",
      "Epoch: 14715 \tTraining Loss: 1.754910 \tValidation Loss: 2.336384\n",
      "Epoch: 14716 \tTraining Loss: 1.754825 \tValidation Loss: 2.336538\n",
      "Epoch: 14717 \tTraining Loss: 1.763962 \tValidation Loss: 2.336555\n",
      "Epoch: 14718 \tTraining Loss: 1.734663 \tValidation Loss: 2.336820\n",
      "Epoch: 14719 \tTraining Loss: 1.763230 \tValidation Loss: 2.336753\n",
      "Epoch: 14720 \tTraining Loss: 1.754126 \tValidation Loss: 2.336946\n",
      "Epoch: 14721 \tTraining Loss: 1.746749 \tValidation Loss: 2.336732\n",
      "Epoch: 14722 \tTraining Loss: 1.798275 \tValidation Loss: 2.336806\n",
      "Epoch: 14723 \tTraining Loss: 1.775021 \tValidation Loss: 2.336960\n",
      "Epoch: 14724 \tTraining Loss: 1.741935 \tValidation Loss: 2.337271\n",
      "Epoch: 14725 \tTraining Loss: 1.734939 \tValidation Loss: 2.337109\n",
      "Epoch: 14726 \tTraining Loss: 1.773751 \tValidation Loss: 2.337048\n",
      "Epoch: 14727 \tTraining Loss: 1.741715 \tValidation Loss: 2.337281\n",
      "Epoch: 14728 \tTraining Loss: 1.746934 \tValidation Loss: 2.337075\n",
      "Epoch: 14729 \tTraining Loss: 1.719344 \tValidation Loss: 2.337315\n",
      "Epoch: 14730 \tTraining Loss: 1.733111 \tValidation Loss: 2.336929\n",
      "Epoch: 14731 \tTraining Loss: 1.743505 \tValidation Loss: 2.337233\n",
      "Epoch: 14732 \tTraining Loss: 1.758061 \tValidation Loss: 2.336961\n",
      "Epoch: 14733 \tTraining Loss: 1.744739 \tValidation Loss: 2.337066\n",
      "Epoch: 14734 \tTraining Loss: 1.758453 \tValidation Loss: 2.336903\n",
      "Epoch: 14735 \tTraining Loss: 1.758337 \tValidation Loss: 2.337302\n",
      "Epoch: 14736 \tTraining Loss: 1.723758 \tValidation Loss: 2.337805\n",
      "Epoch: 14737 \tTraining Loss: 1.738360 \tValidation Loss: 2.337652\n",
      "Epoch: 14738 \tTraining Loss: 1.745114 \tValidation Loss: 2.337591\n",
      "Epoch: 14739 \tTraining Loss: 1.728326 \tValidation Loss: 2.337378\n",
      "Epoch: 14740 \tTraining Loss: 1.701729 \tValidation Loss: 2.337443\n",
      "Epoch: 14741 \tTraining Loss: 1.773398 \tValidation Loss: 2.337136\n",
      "Epoch: 14742 \tTraining Loss: 1.714859 \tValidation Loss: 2.337125\n",
      "Epoch: 14743 \tTraining Loss: 1.730570 \tValidation Loss: 2.337515\n",
      "Epoch: 14744 \tTraining Loss: 1.718316 \tValidation Loss: 2.337551\n",
      "Epoch: 14745 \tTraining Loss: 1.747925 \tValidation Loss: 2.337235\n",
      "Epoch: 14746 \tTraining Loss: 1.750029 \tValidation Loss: 2.336983\n",
      "Epoch: 14747 \tTraining Loss: 1.746333 \tValidation Loss: 2.337626\n",
      "Epoch: 14748 \tTraining Loss: 1.759257 \tValidation Loss: 2.337516\n",
      "Epoch: 14749 \tTraining Loss: 1.715515 \tValidation Loss: 2.337496\n",
      "Epoch: 14750 \tTraining Loss: 1.734118 \tValidation Loss: 2.337335\n",
      "Epoch: 14751 \tTraining Loss: 1.749737 \tValidation Loss: 2.337612\n",
      "Epoch: 14752 \tTraining Loss: 1.750663 \tValidation Loss: 2.337246\n",
      "Epoch: 14753 \tTraining Loss: 1.753500 \tValidation Loss: 2.337304\n",
      "Epoch: 14754 \tTraining Loss: 1.731653 \tValidation Loss: 2.337289\n",
      "Epoch: 14755 \tTraining Loss: 1.755313 \tValidation Loss: 2.337070\n",
      "Epoch: 14756 \tTraining Loss: 1.745832 \tValidation Loss: 2.337038\n",
      "Epoch: 14757 \tTraining Loss: 1.756046 \tValidation Loss: 2.337526\n",
      "Epoch: 14758 \tTraining Loss: 1.730976 \tValidation Loss: 2.337563\n",
      "Epoch: 14759 \tTraining Loss: 1.724880 \tValidation Loss: 2.337922\n",
      "Epoch: 14760 \tTraining Loss: 1.748139 \tValidation Loss: 2.338073\n",
      "Epoch: 14761 \tTraining Loss: 1.725522 \tValidation Loss: 2.337853\n",
      "Epoch: 14762 \tTraining Loss: 1.727250 \tValidation Loss: 2.338401\n",
      "Epoch: 14763 \tTraining Loss: 1.758099 \tValidation Loss: 2.338156\n",
      "Epoch: 14764 \tTraining Loss: 1.741039 \tValidation Loss: 2.338215\n",
      "Epoch: 14765 \tTraining Loss: 1.770013 \tValidation Loss: 2.337742\n",
      "Epoch: 14766 \tTraining Loss: 1.756751 \tValidation Loss: 2.337815\n",
      "Epoch: 14767 \tTraining Loss: 1.770979 \tValidation Loss: 2.337482\n",
      "Epoch: 14768 \tTraining Loss: 1.737502 \tValidation Loss: 2.337644\n",
      "Epoch: 14769 \tTraining Loss: 1.768456 \tValidation Loss: 2.337331\n",
      "Epoch: 14770 \tTraining Loss: 1.774169 \tValidation Loss: 2.337304\n",
      "Epoch: 14771 \tTraining Loss: 1.746333 \tValidation Loss: 2.337629\n",
      "Epoch: 14772 \tTraining Loss: 1.753136 \tValidation Loss: 2.337638\n",
      "Epoch: 14773 \tTraining Loss: 1.769091 \tValidation Loss: 2.337762\n",
      "Epoch: 14774 \tTraining Loss: 1.747586 \tValidation Loss: 2.337912\n",
      "Epoch: 14775 \tTraining Loss: 1.756276 \tValidation Loss: 2.338055\n",
      "Epoch: 14776 \tTraining Loss: 1.731314 \tValidation Loss: 2.337977\n",
      "Epoch: 14777 \tTraining Loss: 1.709029 \tValidation Loss: 2.337680\n",
      "Epoch: 14778 \tTraining Loss: 1.734912 \tValidation Loss: 2.337930\n",
      "Epoch: 14779 \tTraining Loss: 1.768945 \tValidation Loss: 2.338158\n",
      "Epoch: 14780 \tTraining Loss: 1.736060 \tValidation Loss: 2.338168\n",
      "Epoch: 14781 \tTraining Loss: 1.792052 \tValidation Loss: 2.337801\n",
      "Epoch: 14782 \tTraining Loss: 1.741763 \tValidation Loss: 2.337891\n",
      "Epoch: 14783 \tTraining Loss: 1.762383 \tValidation Loss: 2.337879\n",
      "Epoch: 14784 \tTraining Loss: 1.767434 \tValidation Loss: 2.337990\n",
      "Epoch: 14785 \tTraining Loss: 1.751886 \tValidation Loss: 2.338220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14786 \tTraining Loss: 1.739378 \tValidation Loss: 2.338153\n",
      "Epoch: 14787 \tTraining Loss: 1.737869 \tValidation Loss: 2.338493\n",
      "Epoch: 14788 \tTraining Loss: 1.727508 \tValidation Loss: 2.338247\n",
      "Epoch: 14789 \tTraining Loss: 1.733474 \tValidation Loss: 2.338203\n",
      "Epoch: 14790 \tTraining Loss: 1.757598 \tValidation Loss: 2.338259\n",
      "Epoch: 14791 \tTraining Loss: 1.784479 \tValidation Loss: 2.338266\n",
      "Epoch: 14792 \tTraining Loss: 1.703548 \tValidation Loss: 2.338459\n",
      "Epoch: 14793 \tTraining Loss: 1.775522 \tValidation Loss: 2.338112\n",
      "Epoch: 14794 \tTraining Loss: 1.766183 \tValidation Loss: 2.338219\n",
      "Epoch: 14795 \tTraining Loss: 1.724670 \tValidation Loss: 2.338246\n",
      "Epoch: 14796 \tTraining Loss: 1.720455 \tValidation Loss: 2.338712\n",
      "Epoch: 14797 \tTraining Loss: 1.760656 \tValidation Loss: 2.337962\n",
      "Epoch: 14798 \tTraining Loss: 1.699637 \tValidation Loss: 2.337831\n",
      "Epoch: 14799 \tTraining Loss: 1.745319 \tValidation Loss: 2.337959\n",
      "Epoch: 14800 \tTraining Loss: 1.738995 \tValidation Loss: 2.338407\n",
      "Epoch: 14801 \tTraining Loss: 1.755636 \tValidation Loss: 2.338550\n",
      "Epoch: 14802 \tTraining Loss: 1.745317 \tValidation Loss: 2.338114\n",
      "Epoch: 14803 \tTraining Loss: 1.726270 \tValidation Loss: 2.338312\n",
      "Epoch: 14804 \tTraining Loss: 1.772812 \tValidation Loss: 2.337902\n",
      "Epoch: 14805 \tTraining Loss: 1.754518 \tValidation Loss: 2.337903\n",
      "Epoch: 14806 \tTraining Loss: 1.741250 \tValidation Loss: 2.337700\n",
      "Epoch: 14807 \tTraining Loss: 1.750713 \tValidation Loss: 2.337893\n",
      "Epoch: 14808 \tTraining Loss: 1.785708 \tValidation Loss: 2.337958\n",
      "Epoch: 14809 \tTraining Loss: 1.763606 \tValidation Loss: 2.338039\n",
      "Epoch: 14810 \tTraining Loss: 1.745669 \tValidation Loss: 2.337661\n",
      "Epoch: 14811 \tTraining Loss: 1.769502 \tValidation Loss: 2.337534\n",
      "Epoch: 14812 \tTraining Loss: 1.750680 \tValidation Loss: 2.338024\n",
      "Epoch: 14813 \tTraining Loss: 1.759862 \tValidation Loss: 2.337989\n",
      "Epoch: 14814 \tTraining Loss: 1.753152 \tValidation Loss: 2.338198\n",
      "Epoch: 14815 \tTraining Loss: 1.730919 \tValidation Loss: 2.338071\n",
      "Epoch: 14816 \tTraining Loss: 1.711669 \tValidation Loss: 2.338393\n",
      "Epoch: 14817 \tTraining Loss: 1.757681 \tValidation Loss: 2.338325\n",
      "Epoch: 14818 \tTraining Loss: 1.760724 \tValidation Loss: 2.338058\n",
      "Epoch: 14819 \tTraining Loss: 1.713878 \tValidation Loss: 2.338997\n",
      "Epoch: 14820 \tTraining Loss: 1.751027 \tValidation Loss: 2.339176\n",
      "Epoch: 14821 \tTraining Loss: 1.730155 \tValidation Loss: 2.339303\n",
      "Epoch: 14822 \tTraining Loss: 1.737651 \tValidation Loss: 2.338818\n",
      "Epoch: 14823 \tTraining Loss: 1.701541 \tValidation Loss: 2.339196\n",
      "Epoch: 14824 \tTraining Loss: 1.740515 \tValidation Loss: 2.339064\n",
      "Epoch: 14825 \tTraining Loss: 1.766686 \tValidation Loss: 2.338954\n",
      "Epoch: 14826 \tTraining Loss: 1.771065 \tValidation Loss: 2.338616\n",
      "Epoch: 14827 \tTraining Loss: 1.756323 \tValidation Loss: 2.338631\n",
      "Epoch: 14828 \tTraining Loss: 1.737604 \tValidation Loss: 2.338545\n",
      "Epoch: 14829 \tTraining Loss: 1.717673 \tValidation Loss: 2.338732\n",
      "Epoch: 14830 \tTraining Loss: 1.749175 \tValidation Loss: 2.338429\n",
      "Epoch: 14831 \tTraining Loss: 1.739450 \tValidation Loss: 2.338359\n",
      "Epoch: 14832 \tTraining Loss: 1.737917 \tValidation Loss: 2.338050\n",
      "Epoch: 14833 \tTraining Loss: 1.750781 \tValidation Loss: 2.337927\n",
      "Epoch: 14834 \tTraining Loss: 1.753302 \tValidation Loss: 2.338411\n",
      "Epoch: 14835 \tTraining Loss: 1.734383 \tValidation Loss: 2.338442\n",
      "Epoch: 14836 \tTraining Loss: 1.724609 \tValidation Loss: 2.338557\n",
      "Epoch: 14837 \tTraining Loss: 1.765954 \tValidation Loss: 2.338446\n",
      "Epoch: 14838 \tTraining Loss: 1.765336 \tValidation Loss: 2.338222\n",
      "Epoch: 14839 \tTraining Loss: 1.773720 \tValidation Loss: 2.338196\n",
      "Epoch: 14840 \tTraining Loss: 1.760681 \tValidation Loss: 2.338011\n",
      "Epoch: 14841 \tTraining Loss: 1.734983 \tValidation Loss: 2.338311\n",
      "Epoch: 14842 \tTraining Loss: 1.749186 \tValidation Loss: 2.338289\n",
      "Epoch: 14843 \tTraining Loss: 1.742337 \tValidation Loss: 2.338145\n",
      "Epoch: 14844 \tTraining Loss: 1.706143 \tValidation Loss: 2.338585\n",
      "Epoch: 14845 \tTraining Loss: 1.756502 \tValidation Loss: 2.338653\n",
      "Epoch: 14846 \tTraining Loss: 1.758322 \tValidation Loss: 2.338834\n",
      "Epoch: 14847 \tTraining Loss: 1.751781 \tValidation Loss: 2.338794\n",
      "Epoch: 14848 \tTraining Loss: 1.756530 \tValidation Loss: 2.339262\n",
      "Epoch: 14849 \tTraining Loss: 1.716717 \tValidation Loss: 2.339009\n",
      "Epoch: 14850 \tTraining Loss: 1.763429 \tValidation Loss: 2.339042\n",
      "Epoch: 14851 \tTraining Loss: 1.721209 \tValidation Loss: 2.339129\n",
      "Epoch: 14852 \tTraining Loss: 1.746102 \tValidation Loss: 2.339192\n",
      "Epoch: 14853 \tTraining Loss: 1.748848 \tValidation Loss: 2.339407\n",
      "Epoch: 14854 \tTraining Loss: 1.748328 \tValidation Loss: 2.339124\n",
      "Epoch: 14855 \tTraining Loss: 1.742315 \tValidation Loss: 2.339076\n",
      "Epoch: 14856 \tTraining Loss: 1.705986 \tValidation Loss: 2.338969\n",
      "Epoch: 14857 \tTraining Loss: 1.728083 \tValidation Loss: 2.338698\n",
      "Epoch: 14858 \tTraining Loss: 1.761923 \tValidation Loss: 2.338990\n",
      "Epoch: 14859 \tTraining Loss: 1.719346 \tValidation Loss: 2.338923\n",
      "Epoch: 14860 \tTraining Loss: 1.774843 \tValidation Loss: 2.338802\n",
      "Epoch: 14861 \tTraining Loss: 1.764795 \tValidation Loss: 2.338581\n",
      "Epoch: 14862 \tTraining Loss: 1.740216 \tValidation Loss: 2.338478\n",
      "Epoch: 14863 \tTraining Loss: 1.751318 \tValidation Loss: 2.338562\n",
      "Epoch: 14864 \tTraining Loss: 1.763067 \tValidation Loss: 2.338616\n",
      "Epoch: 14865 \tTraining Loss: 1.748637 \tValidation Loss: 2.339124\n",
      "Epoch: 14866 \tTraining Loss: 1.735385 \tValidation Loss: 2.339154\n",
      "Epoch: 14867 \tTraining Loss: 1.750203 \tValidation Loss: 2.339009\n",
      "Epoch: 14868 \tTraining Loss: 1.747923 \tValidation Loss: 2.338807\n",
      "Epoch: 14869 \tTraining Loss: 1.726219 \tValidation Loss: 2.339229\n",
      "Epoch: 14870 \tTraining Loss: 1.741140 \tValidation Loss: 2.339244\n",
      "Epoch: 14871 \tTraining Loss: 1.769561 \tValidation Loss: 2.339573\n",
      "Epoch: 14872 \tTraining Loss: 1.755421 \tValidation Loss: 2.339279\n",
      "Epoch: 14873 \tTraining Loss: 1.745564 \tValidation Loss: 2.338957\n",
      "Epoch: 14874 \tTraining Loss: 1.728424 \tValidation Loss: 2.338913\n",
      "Epoch: 14875 \tTraining Loss: 1.749366 \tValidation Loss: 2.338793\n",
      "Epoch: 14876 \tTraining Loss: 1.711396 \tValidation Loss: 2.339185\n",
      "Epoch: 14877 \tTraining Loss: 1.742375 \tValidation Loss: 2.339052\n",
      "Epoch: 14878 \tTraining Loss: 1.745864 \tValidation Loss: 2.339026\n",
      "Epoch: 14879 \tTraining Loss: 1.751019 \tValidation Loss: 2.338692\n",
      "Epoch: 14880 \tTraining Loss: 1.752185 \tValidation Loss: 2.339354\n",
      "Epoch: 14881 \tTraining Loss: 1.707048 \tValidation Loss: 2.339439\n",
      "Epoch: 14882 \tTraining Loss: 1.739011 \tValidation Loss: 2.338870\n",
      "Epoch: 14883 \tTraining Loss: 1.742491 \tValidation Loss: 2.338948\n",
      "Epoch: 14884 \tTraining Loss: 1.738308 \tValidation Loss: 2.338865\n",
      "Epoch: 14885 \tTraining Loss: 1.759824 \tValidation Loss: 2.339128\n",
      "Epoch: 14886 \tTraining Loss: 1.730580 \tValidation Loss: 2.339331\n",
      "Epoch: 14887 \tTraining Loss: 1.769712 \tValidation Loss: 2.338963\n",
      "Epoch: 14888 \tTraining Loss: 1.763646 \tValidation Loss: 2.338931\n",
      "Epoch: 14889 \tTraining Loss: 1.745620 \tValidation Loss: 2.338832\n",
      "Epoch: 14890 \tTraining Loss: 1.730633 \tValidation Loss: 2.339034\n",
      "Epoch: 14891 \tTraining Loss: 1.715604 \tValidation Loss: 2.338855\n",
      "Epoch: 14892 \tTraining Loss: 1.746546 \tValidation Loss: 2.339052\n",
      "Epoch: 14893 \tTraining Loss: 1.723955 \tValidation Loss: 2.339096\n",
      "Epoch: 14894 \tTraining Loss: 1.725409 \tValidation Loss: 2.338812\n",
      "Epoch: 14895 \tTraining Loss: 1.756647 \tValidation Loss: 2.338922\n",
      "Epoch: 14896 \tTraining Loss: 1.728490 \tValidation Loss: 2.339148\n",
      "Epoch: 14897 \tTraining Loss: 1.748901 \tValidation Loss: 2.338866\n",
      "Epoch: 14898 \tTraining Loss: 1.749603 \tValidation Loss: 2.338582\n",
      "Epoch: 14899 \tTraining Loss: 1.744543 \tValidation Loss: 2.338506\n",
      "Epoch: 14900 \tTraining Loss: 1.759882 \tValidation Loss: 2.338903\n",
      "Epoch: 14901 \tTraining Loss: 1.719765 \tValidation Loss: 2.339200\n",
      "Epoch: 14902 \tTraining Loss: 1.726179 \tValidation Loss: 2.339260\n",
      "Epoch: 14903 \tTraining Loss: 1.775397 \tValidation Loss: 2.339472\n",
      "Epoch: 14904 \tTraining Loss: 1.775863 \tValidation Loss: 2.339110\n",
      "Epoch: 14905 \tTraining Loss: 1.762528 \tValidation Loss: 2.338523\n",
      "Epoch: 14906 \tTraining Loss: 1.779972 \tValidation Loss: 2.338542\n",
      "Epoch: 14907 \tTraining Loss: 1.739219 \tValidation Loss: 2.338763\n",
      "Epoch: 14908 \tTraining Loss: 1.769076 \tValidation Loss: 2.338666\n",
      "Epoch: 14909 \tTraining Loss: 1.755127 \tValidation Loss: 2.338880\n",
      "Epoch: 14910 \tTraining Loss: 1.761508 \tValidation Loss: 2.338706\n",
      "Epoch: 14911 \tTraining Loss: 1.801896 \tValidation Loss: 2.338827\n",
      "Epoch: 14912 \tTraining Loss: 1.750027 \tValidation Loss: 2.338573\n",
      "Epoch: 14913 \tTraining Loss: 1.750832 \tValidation Loss: 2.338362\n",
      "Epoch: 14914 \tTraining Loss: 1.741829 \tValidation Loss: 2.338745\n",
      "Epoch: 14915 \tTraining Loss: 1.723277 \tValidation Loss: 2.338631\n",
      "Epoch: 14916 \tTraining Loss: 1.742123 \tValidation Loss: 2.338428\n",
      "Epoch: 14917 \tTraining Loss: 1.743348 \tValidation Loss: 2.338812\n",
      "Epoch: 14918 \tTraining Loss: 1.740562 \tValidation Loss: 2.338450\n",
      "Epoch: 14919 \tTraining Loss: 1.720998 \tValidation Loss: 2.338777\n",
      "Epoch: 14920 \tTraining Loss: 1.745067 \tValidation Loss: 2.338763\n",
      "Epoch: 14921 \tTraining Loss: 1.725278 \tValidation Loss: 2.338347\n",
      "Epoch: 14922 \tTraining Loss: 1.719812 \tValidation Loss: 2.338485\n",
      "Epoch: 14923 \tTraining Loss: 1.731513 \tValidation Loss: 2.338899\n",
      "Epoch: 14924 \tTraining Loss: 1.739419 \tValidation Loss: 2.338560\n",
      "Epoch: 14925 \tTraining Loss: 1.766949 \tValidation Loss: 2.338336\n",
      "Epoch: 14926 \tTraining Loss: 1.730354 \tValidation Loss: 2.338483\n",
      "Epoch: 14927 \tTraining Loss: 1.780321 \tValidation Loss: 2.338093\n",
      "Epoch: 14928 \tTraining Loss: 1.705891 \tValidation Loss: 2.338022\n",
      "Epoch: 14929 \tTraining Loss: 1.752192 \tValidation Loss: 2.338280\n",
      "Epoch: 14930 \tTraining Loss: 1.725913 \tValidation Loss: 2.338738\n",
      "Epoch: 14931 \tTraining Loss: 1.738529 \tValidation Loss: 2.338803\n",
      "Epoch: 14932 \tTraining Loss: 1.740687 \tValidation Loss: 2.338739\n",
      "Epoch: 14933 \tTraining Loss: 1.735679 \tValidation Loss: 2.338980\n",
      "Epoch: 14934 \tTraining Loss: 1.736634 \tValidation Loss: 2.338993\n",
      "Epoch: 14935 \tTraining Loss: 1.742765 \tValidation Loss: 2.338941\n",
      "Epoch: 14936 \tTraining Loss: 1.723593 \tValidation Loss: 2.338964\n",
      "Epoch: 14937 \tTraining Loss: 1.754465 \tValidation Loss: 2.339137\n",
      "Epoch: 14938 \tTraining Loss: 1.730477 \tValidation Loss: 2.339205\n",
      "Epoch: 14939 \tTraining Loss: 1.721591 \tValidation Loss: 2.339057\n",
      "Epoch: 14940 \tTraining Loss: 1.721190 \tValidation Loss: 2.339052\n",
      "Epoch: 14941 \tTraining Loss: 1.759124 \tValidation Loss: 2.339151\n",
      "Epoch: 14942 \tTraining Loss: 1.760667 \tValidation Loss: 2.339210\n",
      "Epoch: 14943 \tTraining Loss: 1.746470 \tValidation Loss: 2.339350\n",
      "Epoch: 14944 \tTraining Loss: 1.748007 \tValidation Loss: 2.339211\n",
      "Epoch: 14945 \tTraining Loss: 1.752768 \tValidation Loss: 2.338930\n",
      "Epoch: 14946 \tTraining Loss: 1.701585 \tValidation Loss: 2.339150\n",
      "Epoch: 14947 \tTraining Loss: 1.724428 \tValidation Loss: 2.338877\n",
      "Epoch: 14948 \tTraining Loss: 1.753132 \tValidation Loss: 2.339176\n",
      "Epoch: 14949 \tTraining Loss: 1.773906 \tValidation Loss: 2.339009\n",
      "Epoch: 14950 \tTraining Loss: 1.733846 \tValidation Loss: 2.338935\n",
      "Epoch: 14951 \tTraining Loss: 1.707067 \tValidation Loss: 2.338984\n",
      "Epoch: 14952 \tTraining Loss: 1.751798 \tValidation Loss: 2.339592\n",
      "Epoch: 14953 \tTraining Loss: 1.750726 \tValidation Loss: 2.339693\n",
      "Epoch: 14954 \tTraining Loss: 1.737281 \tValidation Loss: 2.339356\n",
      "Epoch: 14955 \tTraining Loss: 1.753322 \tValidation Loss: 2.339161\n",
      "Epoch: 14956 \tTraining Loss: 1.705601 \tValidation Loss: 2.339332\n",
      "Epoch: 14957 \tTraining Loss: 1.721653 \tValidation Loss: 2.339860\n",
      "Epoch: 14958 \tTraining Loss: 1.742370 \tValidation Loss: 2.339840\n",
      "Epoch: 14959 \tTraining Loss: 1.725852 \tValidation Loss: 2.339619\n",
      "Epoch: 14960 \tTraining Loss: 1.748212 \tValidation Loss: 2.339006\n",
      "Epoch: 14961 \tTraining Loss: 1.726461 \tValidation Loss: 2.339377\n",
      "Epoch: 14962 \tTraining Loss: 1.764508 \tValidation Loss: 2.339285\n",
      "Epoch: 14963 \tTraining Loss: 1.726633 \tValidation Loss: 2.339556\n",
      "Epoch: 14964 \tTraining Loss: 1.767727 \tValidation Loss: 2.339037\n",
      "Epoch: 14965 \tTraining Loss: 1.749829 \tValidation Loss: 2.339312\n",
      "Epoch: 14966 \tTraining Loss: 1.798108 \tValidation Loss: 2.339243\n",
      "Epoch: 14967 \tTraining Loss: 1.774612 \tValidation Loss: 2.338719\n",
      "Epoch: 14968 \tTraining Loss: 1.691078 \tValidation Loss: 2.339041\n",
      "Epoch: 14969 \tTraining Loss: 1.743078 \tValidation Loss: 2.338901\n",
      "Epoch: 14970 \tTraining Loss: 1.772510 \tValidation Loss: 2.338994\n",
      "Epoch: 14971 \tTraining Loss: 1.730466 \tValidation Loss: 2.339752\n",
      "Epoch: 14972 \tTraining Loss: 1.769581 \tValidation Loss: 2.339910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14973 \tTraining Loss: 1.747384 \tValidation Loss: 2.339838\n",
      "Epoch: 14974 \tTraining Loss: 1.703367 \tValidation Loss: 2.339989\n",
      "Epoch: 14975 \tTraining Loss: 1.719994 \tValidation Loss: 2.340242\n",
      "Epoch: 14976 \tTraining Loss: 1.748353 \tValidation Loss: 2.340026\n",
      "Epoch: 14977 \tTraining Loss: 1.724236 \tValidation Loss: 2.339801\n",
      "Epoch: 14978 \tTraining Loss: 1.756230 \tValidation Loss: 2.339731\n",
      "Epoch: 14979 \tTraining Loss: 1.743856 \tValidation Loss: 2.339959\n",
      "Epoch: 14980 \tTraining Loss: 1.743294 \tValidation Loss: 2.340031\n",
      "Epoch: 14981 \tTraining Loss: 1.757223 \tValidation Loss: 2.339727\n",
      "Epoch: 14982 \tTraining Loss: 1.747066 \tValidation Loss: 2.339461\n",
      "Epoch: 14983 \tTraining Loss: 1.747707 \tValidation Loss: 2.338967\n",
      "Epoch: 14984 \tTraining Loss: 1.749488 \tValidation Loss: 2.338533\n",
      "Epoch: 14985 \tTraining Loss: 1.740583 \tValidation Loss: 2.339081\n",
      "Epoch: 14986 \tTraining Loss: 1.738836 \tValidation Loss: 2.339465\n",
      "Epoch: 14987 \tTraining Loss: 1.720331 \tValidation Loss: 2.339309\n",
      "Epoch: 14988 \tTraining Loss: 1.727816 \tValidation Loss: 2.339410\n",
      "Epoch: 14989 \tTraining Loss: 1.713241 \tValidation Loss: 2.339502\n",
      "Epoch: 14990 \tTraining Loss: 1.734490 \tValidation Loss: 2.339568\n",
      "Epoch: 14991 \tTraining Loss: 1.762909 \tValidation Loss: 2.339798\n",
      "Epoch: 14992 \tTraining Loss: 1.723572 \tValidation Loss: 2.339327\n",
      "Epoch: 14993 \tTraining Loss: 1.723792 \tValidation Loss: 2.339383\n",
      "Epoch: 14994 \tTraining Loss: 1.732505 \tValidation Loss: 2.339695\n",
      "Epoch: 14995 \tTraining Loss: 1.737979 \tValidation Loss: 2.339398\n",
      "Epoch: 14996 \tTraining Loss: 1.738452 \tValidation Loss: 2.339329\n",
      "Epoch: 14997 \tTraining Loss: 1.708834 \tValidation Loss: 2.339653\n",
      "Epoch: 14998 \tTraining Loss: 1.732925 \tValidation Loss: 2.339327\n",
      "Epoch: 14999 \tTraining Loss: 1.742852 \tValidation Loss: 2.339275\n",
      "Epoch: 15000 \tTraining Loss: 1.736611 \tValidation Loss: 2.339164\n",
      "Epoch: 15001 \tTraining Loss: 1.756625 \tValidation Loss: 2.339231\n",
      "Epoch: 15002 \tTraining Loss: 1.735360 \tValidation Loss: 2.338721\n",
      "Epoch: 15003 \tTraining Loss: 1.720979 \tValidation Loss: 2.339048\n",
      "Epoch: 15004 \tTraining Loss: 1.761693 \tValidation Loss: 2.339142\n",
      "Epoch: 15005 \tTraining Loss: 1.745928 \tValidation Loss: 2.339092\n",
      "Epoch: 15006 \tTraining Loss: 1.733840 \tValidation Loss: 2.339051\n",
      "Epoch: 15007 \tTraining Loss: 1.730267 \tValidation Loss: 2.339315\n",
      "Epoch: 15008 \tTraining Loss: 1.712159 \tValidation Loss: 2.339630\n",
      "Epoch: 15009 \tTraining Loss: 1.729845 \tValidation Loss: 2.339441\n",
      "Epoch: 15010 \tTraining Loss: 1.721472 \tValidation Loss: 2.340001\n",
      "Epoch: 15011 \tTraining Loss: 1.748839 \tValidation Loss: 2.339350\n",
      "Epoch: 15012 \tTraining Loss: 1.748862 \tValidation Loss: 2.339709\n",
      "Epoch: 15013 \tTraining Loss: 1.739348 \tValidation Loss: 2.339647\n",
      "Epoch: 15014 \tTraining Loss: 1.718509 \tValidation Loss: 2.339903\n",
      "Epoch: 15015 \tTraining Loss: 1.703599 \tValidation Loss: 2.339802\n",
      "Epoch: 15016 \tTraining Loss: 1.750192 \tValidation Loss: 2.339434\n",
      "Epoch: 15017 \tTraining Loss: 1.760249 \tValidation Loss: 2.339541\n",
      "Epoch: 15018 \tTraining Loss: 1.743317 \tValidation Loss: 2.339659\n",
      "Epoch: 15019 \tTraining Loss: 1.735819 \tValidation Loss: 2.339737\n",
      "Epoch: 15020 \tTraining Loss: 1.701259 \tValidation Loss: 2.339776\n",
      "Epoch: 15021 \tTraining Loss: 1.751944 \tValidation Loss: 2.339529\n",
      "Epoch: 15022 \tTraining Loss: 1.736131 \tValidation Loss: 2.340065\n",
      "Epoch: 15023 \tTraining Loss: 1.766278 \tValidation Loss: 2.339932\n",
      "Epoch: 15024 \tTraining Loss: 1.727949 \tValidation Loss: 2.339967\n",
      "Epoch: 15025 \tTraining Loss: 1.720595 \tValidation Loss: 2.339518\n",
      "Epoch: 15026 \tTraining Loss: 1.746292 \tValidation Loss: 2.339533\n",
      "Epoch: 15027 \tTraining Loss: 1.735826 \tValidation Loss: 2.339573\n",
      "Epoch: 15028 \tTraining Loss: 1.749553 \tValidation Loss: 2.339466\n",
      "Epoch: 15029 \tTraining Loss: 1.722774 \tValidation Loss: 2.339147\n",
      "Epoch: 15030 \tTraining Loss: 1.764640 \tValidation Loss: 2.339355\n",
      "Epoch: 15031 \tTraining Loss: 1.748105 \tValidation Loss: 2.339696\n",
      "Epoch: 15032 \tTraining Loss: 1.738153 \tValidation Loss: 2.340038\n",
      "Epoch: 15033 \tTraining Loss: 1.748571 \tValidation Loss: 2.339584\n",
      "Epoch: 15034 \tTraining Loss: 1.742899 \tValidation Loss: 2.339672\n",
      "Epoch: 15035 \tTraining Loss: 1.738900 \tValidation Loss: 2.340260\n",
      "Epoch: 15036 \tTraining Loss: 1.754747 \tValidation Loss: 2.339882\n",
      "Epoch: 15037 \tTraining Loss: 1.770165 \tValidation Loss: 2.339693\n",
      "Epoch: 15038 \tTraining Loss: 1.726579 \tValidation Loss: 2.339905\n",
      "Epoch: 15039 \tTraining Loss: 1.751694 \tValidation Loss: 2.340092\n",
      "Epoch: 15040 \tTraining Loss: 1.744704 \tValidation Loss: 2.340268\n",
      "Epoch: 15041 \tTraining Loss: 1.756824 \tValidation Loss: 2.340071\n",
      "Epoch: 15042 \tTraining Loss: 1.733863 \tValidation Loss: 2.339621\n",
      "Epoch: 15043 \tTraining Loss: 1.751015 \tValidation Loss: 2.339612\n",
      "Epoch: 15044 \tTraining Loss: 1.752304 \tValidation Loss: 2.339632\n",
      "Epoch: 15045 \tTraining Loss: 1.741959 \tValidation Loss: 2.339970\n",
      "Epoch: 15046 \tTraining Loss: 1.741087 \tValidation Loss: 2.340199\n",
      "Epoch: 15047 \tTraining Loss: 1.756270 \tValidation Loss: 2.339677\n",
      "Epoch: 15048 \tTraining Loss: 1.733575 \tValidation Loss: 2.339763\n",
      "Epoch: 15049 \tTraining Loss: 1.695388 \tValidation Loss: 2.340245\n",
      "Epoch: 15050 \tTraining Loss: 1.724129 \tValidation Loss: 2.340195\n",
      "Epoch: 15051 \tTraining Loss: 1.742543 \tValidation Loss: 2.340157\n",
      "Epoch: 15052 \tTraining Loss: 1.727580 \tValidation Loss: 2.340544\n",
      "Epoch: 15053 \tTraining Loss: 1.758560 \tValidation Loss: 2.340011\n",
      "Epoch: 15054 \tTraining Loss: 1.753454 \tValidation Loss: 2.339879\n",
      "Epoch: 15055 \tTraining Loss: 1.713322 \tValidation Loss: 2.339833\n",
      "Epoch: 15056 \tTraining Loss: 1.732707 \tValidation Loss: 2.340158\n",
      "Epoch: 15057 \tTraining Loss: 1.744473 \tValidation Loss: 2.339649\n",
      "Epoch: 15058 \tTraining Loss: 1.755827 \tValidation Loss: 2.339603\n",
      "Epoch: 15059 \tTraining Loss: 1.734657 \tValidation Loss: 2.339901\n",
      "Epoch: 15060 \tTraining Loss: 1.730324 \tValidation Loss: 2.340288\n",
      "Epoch: 15061 \tTraining Loss: 1.729509 \tValidation Loss: 2.339838\n",
      "Epoch: 15062 \tTraining Loss: 1.768785 \tValidation Loss: 2.340009\n",
      "Epoch: 15063 \tTraining Loss: 1.722917 \tValidation Loss: 2.340479\n",
      "Epoch: 15064 \tTraining Loss: 1.709135 \tValidation Loss: 2.340353\n",
      "Epoch: 15065 \tTraining Loss: 1.728682 \tValidation Loss: 2.340385\n",
      "Epoch: 15066 \tTraining Loss: 1.738903 \tValidation Loss: 2.340119\n",
      "Epoch: 15067 \tTraining Loss: 1.704861 \tValidation Loss: 2.340426\n",
      "Epoch: 15068 \tTraining Loss: 1.754046 \tValidation Loss: 2.340321\n",
      "Epoch: 15069 \tTraining Loss: 1.720622 \tValidation Loss: 2.340716\n",
      "Epoch: 15070 \tTraining Loss: 1.723333 \tValidation Loss: 2.340600\n",
      "Epoch: 15071 \tTraining Loss: 1.761916 \tValidation Loss: 2.339946\n",
      "Epoch: 15072 \tTraining Loss: 1.740908 \tValidation Loss: 2.339864\n",
      "Epoch: 15073 \tTraining Loss: 1.747263 \tValidation Loss: 2.339736\n",
      "Epoch: 15074 \tTraining Loss: 1.727502 \tValidation Loss: 2.339787\n",
      "Epoch: 15075 \tTraining Loss: 1.744567 \tValidation Loss: 2.339721\n",
      "Epoch: 15076 \tTraining Loss: 1.728295 \tValidation Loss: 2.339937\n",
      "Epoch: 15077 \tTraining Loss: 1.714575 \tValidation Loss: 2.339977\n",
      "Epoch: 15078 \tTraining Loss: 1.689648 \tValidation Loss: 2.340248\n",
      "Epoch: 15079 \tTraining Loss: 1.714872 \tValidation Loss: 2.340270\n",
      "Epoch: 15080 \tTraining Loss: 1.721264 \tValidation Loss: 2.340434\n",
      "Epoch: 15081 \tTraining Loss: 1.730683 \tValidation Loss: 2.340257\n",
      "Epoch: 15082 \tTraining Loss: 1.754839 \tValidation Loss: 2.340453\n",
      "Epoch: 15083 \tTraining Loss: 1.768180 \tValidation Loss: 2.340080\n",
      "Epoch: 15084 \tTraining Loss: 1.717659 \tValidation Loss: 2.340358\n",
      "Epoch: 15085 \tTraining Loss: 1.725558 \tValidation Loss: 2.340121\n",
      "Epoch: 15086 \tTraining Loss: 1.702326 \tValidation Loss: 2.340281\n",
      "Epoch: 15087 \tTraining Loss: 1.742252 \tValidation Loss: 2.340114\n",
      "Epoch: 15088 \tTraining Loss: 1.770728 \tValidation Loss: 2.340024\n",
      "Epoch: 15089 \tTraining Loss: 1.736094 \tValidation Loss: 2.340458\n",
      "Epoch: 15090 \tTraining Loss: 1.730463 \tValidation Loss: 2.340465\n",
      "Epoch: 15091 \tTraining Loss: 1.752182 \tValidation Loss: 2.340318\n",
      "Epoch: 15092 \tTraining Loss: 1.732158 \tValidation Loss: 2.340122\n",
      "Epoch: 15093 \tTraining Loss: 1.730825 \tValidation Loss: 2.340543\n",
      "Epoch: 15094 \tTraining Loss: 1.716868 \tValidation Loss: 2.340369\n",
      "Epoch: 15095 \tTraining Loss: 1.733295 \tValidation Loss: 2.340478\n",
      "Epoch: 15096 \tTraining Loss: 1.733219 \tValidation Loss: 2.340560\n",
      "Epoch: 15097 \tTraining Loss: 1.768331 \tValidation Loss: 2.339892\n",
      "Epoch: 15098 \tTraining Loss: 1.740661 \tValidation Loss: 2.340128\n",
      "Epoch: 15099 \tTraining Loss: 1.728872 \tValidation Loss: 2.340344\n",
      "Epoch: 15100 \tTraining Loss: 1.765592 \tValidation Loss: 2.340700\n",
      "Epoch: 15101 \tTraining Loss: 1.748253 \tValidation Loss: 2.340573\n",
      "Epoch: 15102 \tTraining Loss: 1.742339 \tValidation Loss: 2.340755\n",
      "Epoch: 15103 \tTraining Loss: 1.761790 \tValidation Loss: 2.340303\n",
      "Epoch: 15104 \tTraining Loss: 1.702828 \tValidation Loss: 2.340408\n",
      "Epoch: 15105 \tTraining Loss: 1.755087 \tValidation Loss: 2.340315\n",
      "Epoch: 15106 \tTraining Loss: 1.740462 \tValidation Loss: 2.339914\n",
      "Epoch: 15107 \tTraining Loss: 1.717988 \tValidation Loss: 2.339757\n",
      "Epoch: 15108 \tTraining Loss: 1.724120 \tValidation Loss: 2.339857\n",
      "Epoch: 15109 \tTraining Loss: 1.760411 \tValidation Loss: 2.339841\n",
      "Epoch: 15110 \tTraining Loss: 1.722603 \tValidation Loss: 2.340055\n",
      "Epoch: 15111 \tTraining Loss: 1.737872 \tValidation Loss: 2.340042\n",
      "Epoch: 15112 \tTraining Loss: 1.735442 \tValidation Loss: 2.340242\n",
      "Epoch: 15113 \tTraining Loss: 1.763650 \tValidation Loss: 2.339990\n",
      "Epoch: 15114 \tTraining Loss: 1.742293 \tValidation Loss: 2.339724\n",
      "Epoch: 15115 \tTraining Loss: 1.710160 \tValidation Loss: 2.340520\n",
      "Epoch: 15116 \tTraining Loss: 1.720195 \tValidation Loss: 2.340396\n",
      "Epoch: 15117 \tTraining Loss: 1.730361 \tValidation Loss: 2.340450\n",
      "Epoch: 15118 \tTraining Loss: 1.704983 \tValidation Loss: 2.340443\n",
      "Epoch: 15119 \tTraining Loss: 1.721176 \tValidation Loss: 2.340922\n",
      "Epoch: 15120 \tTraining Loss: 1.737501 \tValidation Loss: 2.340130\n",
      "Epoch: 15121 \tTraining Loss: 1.731692 \tValidation Loss: 2.340793\n",
      "Epoch: 15122 \tTraining Loss: 1.719898 \tValidation Loss: 2.340996\n",
      "Epoch: 15123 \tTraining Loss: 1.763625 \tValidation Loss: 2.341063\n",
      "Epoch: 15124 \tTraining Loss: 1.731763 \tValidation Loss: 2.340612\n",
      "Epoch: 15125 \tTraining Loss: 1.716544 \tValidation Loss: 2.340511\n",
      "Epoch: 15126 \tTraining Loss: 1.737601 \tValidation Loss: 2.340750\n",
      "Epoch: 15127 \tTraining Loss: 1.707902 \tValidation Loss: 2.340760\n",
      "Epoch: 15128 \tTraining Loss: 1.704894 \tValidation Loss: 2.340668\n",
      "Epoch: 15129 \tTraining Loss: 1.764943 \tValidation Loss: 2.340718\n",
      "Epoch: 15130 \tTraining Loss: 1.741053 \tValidation Loss: 2.340517\n",
      "Epoch: 15131 \tTraining Loss: 1.728943 \tValidation Loss: 2.340629\n",
      "Epoch: 15132 \tTraining Loss: 1.729164 \tValidation Loss: 2.340422\n",
      "Epoch: 15133 \tTraining Loss: 1.758530 \tValidation Loss: 2.340672\n",
      "Epoch: 15134 \tTraining Loss: 1.735828 \tValidation Loss: 2.341131\n",
      "Epoch: 15135 \tTraining Loss: 1.794586 \tValidation Loss: 2.340605\n",
      "Epoch: 15136 \tTraining Loss: 1.768347 \tValidation Loss: 2.340535\n",
      "Epoch: 15137 \tTraining Loss: 1.726641 \tValidation Loss: 2.340548\n",
      "Epoch: 15138 \tTraining Loss: 1.733058 \tValidation Loss: 2.340939\n",
      "Epoch: 15139 \tTraining Loss: 1.729123 \tValidation Loss: 2.340662\n",
      "Epoch: 15140 \tTraining Loss: 1.765956 \tValidation Loss: 2.340887\n",
      "Epoch: 15141 \tTraining Loss: 1.718022 \tValidation Loss: 2.341051\n",
      "Epoch: 15142 \tTraining Loss: 1.736709 \tValidation Loss: 2.340801\n",
      "Epoch: 15143 \tTraining Loss: 1.727938 \tValidation Loss: 2.340897\n",
      "Epoch: 15144 \tTraining Loss: 1.732878 \tValidation Loss: 2.340764\n",
      "Epoch: 15145 \tTraining Loss: 1.724692 \tValidation Loss: 2.341182\n",
      "Epoch: 15146 \tTraining Loss: 1.697883 \tValidation Loss: 2.340795\n",
      "Epoch: 15147 \tTraining Loss: 1.739072 \tValidation Loss: 2.341286\n",
      "Epoch: 15148 \tTraining Loss: 1.734426 \tValidation Loss: 2.341221\n",
      "Epoch: 15149 \tTraining Loss: 1.728970 \tValidation Loss: 2.341348\n",
      "Epoch: 15150 \tTraining Loss: 1.705671 \tValidation Loss: 2.341238\n",
      "Epoch: 15151 \tTraining Loss: 1.764112 \tValidation Loss: 2.341243\n",
      "Epoch: 15152 \tTraining Loss: 1.702847 \tValidation Loss: 2.341541\n",
      "Epoch: 15153 \tTraining Loss: 1.763003 \tValidation Loss: 2.341382\n",
      "Epoch: 15154 \tTraining Loss: 1.716866 \tValidation Loss: 2.341074\n",
      "Epoch: 15155 \tTraining Loss: 1.719810 \tValidation Loss: 2.341455\n",
      "Epoch: 15156 \tTraining Loss: 1.710269 \tValidation Loss: 2.341197\n",
      "Epoch: 15157 \tTraining Loss: 1.706414 \tValidation Loss: 2.341327\n",
      "Epoch: 15158 \tTraining Loss: 1.717230 \tValidation Loss: 2.341406\n",
      "Epoch: 15159 \tTraining Loss: 1.733098 \tValidation Loss: 2.341205\n",
      "Epoch: 15160 \tTraining Loss: 1.737723 \tValidation Loss: 2.341058\n",
      "Epoch: 15161 \tTraining Loss: 1.716166 \tValidation Loss: 2.341186\n",
      "Epoch: 15162 \tTraining Loss: 1.727111 \tValidation Loss: 2.341236\n",
      "Epoch: 15163 \tTraining Loss: 1.702952 \tValidation Loss: 2.341475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15164 \tTraining Loss: 1.681036 \tValidation Loss: 2.341981\n",
      "Epoch: 15165 \tTraining Loss: 1.767842 \tValidation Loss: 2.341736\n",
      "Epoch: 15166 \tTraining Loss: 1.756917 \tValidation Loss: 2.341488\n",
      "Epoch: 15167 \tTraining Loss: 1.715683 \tValidation Loss: 2.341575\n",
      "Epoch: 15168 \tTraining Loss: 1.730430 \tValidation Loss: 2.341596\n",
      "Epoch: 15169 \tTraining Loss: 1.719812 \tValidation Loss: 2.341575\n",
      "Epoch: 15170 \tTraining Loss: 1.697555 \tValidation Loss: 2.341738\n",
      "Epoch: 15171 \tTraining Loss: 1.746943 \tValidation Loss: 2.341625\n",
      "Epoch: 15172 \tTraining Loss: 1.709686 \tValidation Loss: 2.341498\n",
      "Epoch: 15173 \tTraining Loss: 1.764680 \tValidation Loss: 2.341763\n",
      "Epoch: 15174 \tTraining Loss: 1.734575 \tValidation Loss: 2.341747\n",
      "Epoch: 15175 \tTraining Loss: 1.750738 \tValidation Loss: 2.341038\n",
      "Epoch: 15176 \tTraining Loss: 1.740980 \tValidation Loss: 2.341753\n",
      "Epoch: 15177 \tTraining Loss: 1.720996 \tValidation Loss: 2.341584\n",
      "Epoch: 15178 \tTraining Loss: 1.735528 \tValidation Loss: 2.341532\n",
      "Epoch: 15179 \tTraining Loss: 1.705412 \tValidation Loss: 2.341203\n",
      "Epoch: 15180 \tTraining Loss: 1.719381 \tValidation Loss: 2.341389\n",
      "Epoch: 15181 \tTraining Loss: 1.736003 \tValidation Loss: 2.341388\n",
      "Epoch: 15182 \tTraining Loss: 1.743005 \tValidation Loss: 2.340917\n",
      "Epoch: 15183 \tTraining Loss: 1.709231 \tValidation Loss: 2.341306\n",
      "Epoch: 15184 \tTraining Loss: 1.750384 \tValidation Loss: 2.341173\n",
      "Epoch: 15185 \tTraining Loss: 1.738067 \tValidation Loss: 2.341024\n",
      "Epoch: 15186 \tTraining Loss: 1.741605 \tValidation Loss: 2.341353\n",
      "Epoch: 15187 \tTraining Loss: 1.695471 \tValidation Loss: 2.341327\n",
      "Epoch: 15188 \tTraining Loss: 1.739766 \tValidation Loss: 2.341787\n",
      "Epoch: 15189 \tTraining Loss: 1.741265 \tValidation Loss: 2.341542\n",
      "Epoch: 15190 \tTraining Loss: 1.791138 \tValidation Loss: 2.341398\n",
      "Epoch: 15191 \tTraining Loss: 1.728177 \tValidation Loss: 2.341435\n",
      "Epoch: 15192 \tTraining Loss: 1.748575 \tValidation Loss: 2.341664\n",
      "Epoch: 15193 \tTraining Loss: 1.733582 \tValidation Loss: 2.341517\n",
      "Epoch: 15194 \tTraining Loss: 1.723233 \tValidation Loss: 2.341732\n",
      "Epoch: 15195 \tTraining Loss: 1.731154 \tValidation Loss: 2.341341\n",
      "Epoch: 15196 \tTraining Loss: 1.734749 \tValidation Loss: 2.341310\n",
      "Epoch: 15197 \tTraining Loss: 1.752893 \tValidation Loss: 2.341728\n",
      "Epoch: 15198 \tTraining Loss: 1.718852 \tValidation Loss: 2.341777\n",
      "Epoch: 15199 \tTraining Loss: 1.735361 \tValidation Loss: 2.341540\n",
      "Epoch: 15200 \tTraining Loss: 1.710571 \tValidation Loss: 2.341706\n",
      "Epoch: 15201 \tTraining Loss: 1.695575 \tValidation Loss: 2.341822\n",
      "Epoch: 15202 \tTraining Loss: 1.754600 \tValidation Loss: 2.341584\n",
      "Epoch: 15203 \tTraining Loss: 1.752248 \tValidation Loss: 2.341200\n",
      "Epoch: 15204 \tTraining Loss: 1.719925 \tValidation Loss: 2.341202\n",
      "Epoch: 15205 \tTraining Loss: 1.715033 \tValidation Loss: 2.341430\n",
      "Epoch: 15206 \tTraining Loss: 1.766917 \tValidation Loss: 2.341563\n",
      "Epoch: 15207 \tTraining Loss: 1.705546 \tValidation Loss: 2.341392\n",
      "Epoch: 15208 \tTraining Loss: 1.743188 \tValidation Loss: 2.341253\n",
      "Epoch: 15209 \tTraining Loss: 1.705575 \tValidation Loss: 2.341751\n",
      "Epoch: 15210 \tTraining Loss: 1.712123 \tValidation Loss: 2.342413\n",
      "Epoch: 15211 \tTraining Loss: 1.754014 \tValidation Loss: 2.342138\n",
      "Epoch: 15212 \tTraining Loss: 1.758931 \tValidation Loss: 2.342165\n",
      "Epoch: 15213 \tTraining Loss: 1.723596 \tValidation Loss: 2.341832\n",
      "Epoch: 15214 \tTraining Loss: 1.754198 \tValidation Loss: 2.341633\n",
      "Epoch: 15215 \tTraining Loss: 1.747877 \tValidation Loss: 2.341831\n",
      "Epoch: 15216 \tTraining Loss: 1.708996 \tValidation Loss: 2.341888\n",
      "Epoch: 15217 \tTraining Loss: 1.743166 \tValidation Loss: 2.341773\n",
      "Epoch: 15218 \tTraining Loss: 1.729435 \tValidation Loss: 2.341731\n",
      "Epoch: 15219 \tTraining Loss: 1.711689 \tValidation Loss: 2.341869\n",
      "Epoch: 15220 \tTraining Loss: 1.736365 \tValidation Loss: 2.342185\n",
      "Epoch: 15221 \tTraining Loss: 1.755832 \tValidation Loss: 2.341726\n",
      "Epoch: 15222 \tTraining Loss: 1.705534 \tValidation Loss: 2.342260\n",
      "Epoch: 15223 \tTraining Loss: 1.720292 \tValidation Loss: 2.342200\n",
      "Epoch: 15224 \tTraining Loss: 1.732235 \tValidation Loss: 2.341989\n",
      "Epoch: 15225 \tTraining Loss: 1.695598 \tValidation Loss: 2.341870\n",
      "Epoch: 15226 \tTraining Loss: 1.726388 \tValidation Loss: 2.341343\n",
      "Epoch: 15227 \tTraining Loss: 1.709260 \tValidation Loss: 2.341666\n",
      "Epoch: 15228 \tTraining Loss: 1.758123 \tValidation Loss: 2.341288\n",
      "Epoch: 15229 \tTraining Loss: 1.732548 \tValidation Loss: 2.341472\n",
      "Epoch: 15230 \tTraining Loss: 1.763093 \tValidation Loss: 2.341315\n",
      "Epoch: 15231 \tTraining Loss: 1.714800 \tValidation Loss: 2.341548\n",
      "Epoch: 15232 \tTraining Loss: 1.719987 \tValidation Loss: 2.341688\n",
      "Epoch: 15233 \tTraining Loss: 1.725034 \tValidation Loss: 2.341645\n",
      "Epoch: 15234 \tTraining Loss: 1.759332 \tValidation Loss: 2.341655\n",
      "Epoch: 15235 \tTraining Loss: 1.727709 \tValidation Loss: 2.341694\n",
      "Epoch: 15236 \tTraining Loss: 1.739376 \tValidation Loss: 2.341373\n",
      "Epoch: 15237 \tTraining Loss: 1.747149 \tValidation Loss: 2.341541\n",
      "Epoch: 15238 \tTraining Loss: 1.764447 \tValidation Loss: 2.341118\n",
      "Epoch: 15239 \tTraining Loss: 1.791040 \tValidation Loss: 2.341457\n",
      "Epoch: 15240 \tTraining Loss: 1.751950 \tValidation Loss: 2.341726\n",
      "Epoch: 15241 \tTraining Loss: 1.692266 \tValidation Loss: 2.341984\n",
      "Epoch: 15242 \tTraining Loss: 1.717009 \tValidation Loss: 2.342003\n",
      "Epoch: 15243 \tTraining Loss: 1.734648 \tValidation Loss: 2.342106\n",
      "Epoch: 15244 \tTraining Loss: 1.717916 \tValidation Loss: 2.342052\n",
      "Epoch: 15245 \tTraining Loss: 1.711616 \tValidation Loss: 2.341997\n",
      "Epoch: 15246 \tTraining Loss: 1.709655 \tValidation Loss: 2.341892\n",
      "Epoch: 15247 \tTraining Loss: 1.683665 \tValidation Loss: 2.341485\n",
      "Epoch: 15248 \tTraining Loss: 1.773081 \tValidation Loss: 2.341716\n",
      "Epoch: 15249 \tTraining Loss: 1.693884 \tValidation Loss: 2.341987\n",
      "Epoch: 15250 \tTraining Loss: 1.747615 \tValidation Loss: 2.341534\n",
      "Epoch: 15251 \tTraining Loss: 1.705029 \tValidation Loss: 2.341791\n",
      "Epoch: 15252 \tTraining Loss: 1.750448 \tValidation Loss: 2.342093\n",
      "Epoch: 15253 \tTraining Loss: 1.690445 \tValidation Loss: 2.342489\n",
      "Epoch: 15254 \tTraining Loss: 1.699115 \tValidation Loss: 2.342653\n",
      "Epoch: 15255 \tTraining Loss: 1.739436 \tValidation Loss: 2.342476\n",
      "Epoch: 15256 \tTraining Loss: 1.750132 \tValidation Loss: 2.342680\n",
      "Epoch: 15257 \tTraining Loss: 1.716802 \tValidation Loss: 2.343150\n",
      "Epoch: 15258 \tTraining Loss: 1.718104 \tValidation Loss: 2.342834\n",
      "Epoch: 15259 \tTraining Loss: 1.752826 \tValidation Loss: 2.342427\n",
      "Epoch: 15260 \tTraining Loss: 1.717589 \tValidation Loss: 2.342752\n",
      "Epoch: 15261 \tTraining Loss: 1.708874 \tValidation Loss: 2.342510\n",
      "Epoch: 15262 \tTraining Loss: 1.701481 \tValidation Loss: 2.342342\n",
      "Epoch: 15263 \tTraining Loss: 1.751147 \tValidation Loss: 2.342007\n",
      "Epoch: 15264 \tTraining Loss: 1.720914 \tValidation Loss: 2.342515\n",
      "Epoch: 15265 \tTraining Loss: 1.743921 \tValidation Loss: 2.342496\n",
      "Epoch: 15266 \tTraining Loss: 1.713769 \tValidation Loss: 2.342487\n",
      "Epoch: 15267 \tTraining Loss: 1.753663 \tValidation Loss: 2.342346\n",
      "Epoch: 15268 \tTraining Loss: 1.753180 \tValidation Loss: 2.342124\n",
      "Epoch: 15269 \tTraining Loss: 1.721211 \tValidation Loss: 2.342007\n",
      "Epoch: 15270 \tTraining Loss: 1.735100 \tValidation Loss: 2.341669\n",
      "Epoch: 15271 \tTraining Loss: 1.712546 \tValidation Loss: 2.341600\n",
      "Epoch: 15272 \tTraining Loss: 1.756981 \tValidation Loss: 2.341671\n",
      "Epoch: 15273 \tTraining Loss: 1.754004 \tValidation Loss: 2.341627\n",
      "Epoch: 15274 \tTraining Loss: 1.739515 \tValidation Loss: 2.341496\n",
      "Epoch: 15275 \tTraining Loss: 1.735549 \tValidation Loss: 2.341503\n",
      "Epoch: 15276 \tTraining Loss: 1.690921 \tValidation Loss: 2.341539\n",
      "Epoch: 15277 \tTraining Loss: 1.686849 \tValidation Loss: 2.341816\n",
      "Epoch: 15278 \tTraining Loss: 1.710996 \tValidation Loss: 2.342061\n",
      "Epoch: 15279 \tTraining Loss: 1.678256 \tValidation Loss: 2.342533\n",
      "Epoch: 15280 \tTraining Loss: 1.732558 \tValidation Loss: 2.342446\n",
      "Epoch: 15281 \tTraining Loss: 1.701903 \tValidation Loss: 2.342409\n",
      "Epoch: 15282 \tTraining Loss: 1.728704 \tValidation Loss: 2.342141\n",
      "Epoch: 15283 \tTraining Loss: 1.727341 \tValidation Loss: 2.342009\n",
      "Epoch: 15284 \tTraining Loss: 1.733685 \tValidation Loss: 2.342274\n",
      "Epoch: 15285 \tTraining Loss: 1.744413 \tValidation Loss: 2.342192\n",
      "Epoch: 15286 \tTraining Loss: 1.691197 \tValidation Loss: 2.342447\n",
      "Epoch: 15287 \tTraining Loss: 1.724224 \tValidation Loss: 2.342169\n",
      "Epoch: 15288 \tTraining Loss: 1.767407 \tValidation Loss: 2.342159\n",
      "Epoch: 15289 \tTraining Loss: 1.704871 \tValidation Loss: 2.342126\n",
      "Epoch: 15290 \tTraining Loss: 1.735525 \tValidation Loss: 2.342145\n",
      "Epoch: 15291 \tTraining Loss: 1.736309 \tValidation Loss: 2.342186\n",
      "Epoch: 15292 \tTraining Loss: 1.783725 \tValidation Loss: 2.341206\n",
      "Epoch: 15293 \tTraining Loss: 1.725556 \tValidation Loss: 2.341144\n",
      "Epoch: 15294 \tTraining Loss: 1.701650 \tValidation Loss: 2.341502\n",
      "Epoch: 15295 \tTraining Loss: 1.749786 \tValidation Loss: 2.341248\n",
      "Epoch: 15296 \tTraining Loss: 1.714191 \tValidation Loss: 2.341828\n",
      "Epoch: 15297 \tTraining Loss: 1.734755 \tValidation Loss: 2.341756\n",
      "Epoch: 15298 \tTraining Loss: 1.722323 \tValidation Loss: 2.341877\n",
      "Epoch: 15299 \tTraining Loss: 1.710948 \tValidation Loss: 2.341925\n",
      "Epoch: 15300 \tTraining Loss: 1.740149 \tValidation Loss: 2.342037\n",
      "Epoch: 15301 \tTraining Loss: 1.734165 \tValidation Loss: 2.342337\n",
      "Epoch: 15302 \tTraining Loss: 1.734080 \tValidation Loss: 2.342264\n",
      "Epoch: 15303 \tTraining Loss: 1.735177 \tValidation Loss: 2.342731\n",
      "Epoch: 15304 \tTraining Loss: 1.736143 \tValidation Loss: 2.341720\n",
      "Epoch: 15305 \tTraining Loss: 1.711704 \tValidation Loss: 2.342130\n",
      "Epoch: 15306 \tTraining Loss: 1.738443 \tValidation Loss: 2.342174\n",
      "Epoch: 15307 \tTraining Loss: 1.715037 \tValidation Loss: 2.342079\n",
      "Epoch: 15308 \tTraining Loss: 1.728053 \tValidation Loss: 2.342284\n",
      "Epoch: 15309 \tTraining Loss: 1.728742 \tValidation Loss: 2.342251\n",
      "Epoch: 15310 \tTraining Loss: 1.730892 \tValidation Loss: 2.342194\n",
      "Epoch: 15311 \tTraining Loss: 1.749948 \tValidation Loss: 2.342165\n",
      "Epoch: 15312 \tTraining Loss: 1.725684 \tValidation Loss: 2.342309\n",
      "Epoch: 15313 \tTraining Loss: 1.713212 \tValidation Loss: 2.342875\n",
      "Epoch: 15314 \tTraining Loss: 1.735896 \tValidation Loss: 2.342734\n",
      "Epoch: 15315 \tTraining Loss: 1.736355 \tValidation Loss: 2.342752\n",
      "Epoch: 15316 \tTraining Loss: 1.777463 \tValidation Loss: 2.342604\n",
      "Epoch: 15317 \tTraining Loss: 1.731329 \tValidation Loss: 2.343316\n",
      "Epoch: 15318 \tTraining Loss: 1.714826 \tValidation Loss: 2.343261\n",
      "Epoch: 15319 \tTraining Loss: 1.699133 \tValidation Loss: 2.343863\n",
      "Epoch: 15320 \tTraining Loss: 1.735860 \tValidation Loss: 2.343287\n",
      "Epoch: 15321 \tTraining Loss: 1.727214 \tValidation Loss: 2.343355\n",
      "Epoch: 15322 \tTraining Loss: 1.720703 \tValidation Loss: 2.342995\n",
      "Epoch: 15323 \tTraining Loss: 1.743546 \tValidation Loss: 2.342896\n",
      "Epoch: 15324 \tTraining Loss: 1.686942 \tValidation Loss: 2.342882\n",
      "Epoch: 15325 \tTraining Loss: 1.709512 \tValidation Loss: 2.342699\n",
      "Epoch: 15326 \tTraining Loss: 1.694515 \tValidation Loss: 2.342535\n",
      "Epoch: 15327 \tTraining Loss: 1.727198 \tValidation Loss: 2.342606\n",
      "Epoch: 15328 \tTraining Loss: 1.701336 \tValidation Loss: 2.342509\n",
      "Epoch: 15329 \tTraining Loss: 1.733794 \tValidation Loss: 2.342225\n",
      "Epoch: 15330 \tTraining Loss: 1.721260 \tValidation Loss: 2.342318\n",
      "Epoch: 15331 \tTraining Loss: 1.718486 \tValidation Loss: 2.342692\n",
      "Epoch: 15332 \tTraining Loss: 1.726439 \tValidation Loss: 2.342933\n",
      "Epoch: 15333 \tTraining Loss: 1.717735 \tValidation Loss: 2.342727\n",
      "Epoch: 15334 \tTraining Loss: 1.744369 \tValidation Loss: 2.342466\n",
      "Epoch: 15335 \tTraining Loss: 1.714861 \tValidation Loss: 2.342441\n",
      "Epoch: 15336 \tTraining Loss: 1.730120 \tValidation Loss: 2.342649\n",
      "Epoch: 15337 \tTraining Loss: 1.731609 \tValidation Loss: 2.342752\n",
      "Epoch: 15338 \tTraining Loss: 1.729366 \tValidation Loss: 2.342097\n",
      "Epoch: 15339 \tTraining Loss: 1.776159 \tValidation Loss: 2.342289\n",
      "Epoch: 15340 \tTraining Loss: 1.706472 \tValidation Loss: 2.342415\n",
      "Epoch: 15341 \tTraining Loss: 1.695483 \tValidation Loss: 2.342298\n",
      "Epoch: 15342 \tTraining Loss: 1.717293 \tValidation Loss: 2.342563\n",
      "Epoch: 15343 \tTraining Loss: 1.757596 \tValidation Loss: 2.342422\n",
      "Epoch: 15344 \tTraining Loss: 1.698280 \tValidation Loss: 2.342931\n",
      "Epoch: 15345 \tTraining Loss: 1.711445 \tValidation Loss: 2.342315\n",
      "Epoch: 15346 \tTraining Loss: 1.700330 \tValidation Loss: 2.342550\n",
      "Epoch: 15347 \tTraining Loss: 1.714205 \tValidation Loss: 2.342385\n",
      "Epoch: 15348 \tTraining Loss: 1.752197 \tValidation Loss: 2.342403\n",
      "Epoch: 15349 \tTraining Loss: 1.707569 \tValidation Loss: 2.342444\n",
      "Epoch: 15350 \tTraining Loss: 1.740721 \tValidation Loss: 2.342481\n",
      "Epoch: 15351 \tTraining Loss: 1.708185 \tValidation Loss: 2.342574\n",
      "Epoch: 15352 \tTraining Loss: 1.728947 \tValidation Loss: 2.342487\n",
      "Epoch: 15353 \tTraining Loss: 1.749009 \tValidation Loss: 2.342487\n",
      "Epoch: 15354 \tTraining Loss: 1.713137 \tValidation Loss: 2.342325\n",
      "Epoch: 15355 \tTraining Loss: 1.743915 \tValidation Loss: 2.342404\n",
      "Epoch: 15356 \tTraining Loss: 1.725153 \tValidation Loss: 2.342654\n",
      "Epoch: 15357 \tTraining Loss: 1.723201 \tValidation Loss: 2.342190\n",
      "Epoch: 15358 \tTraining Loss: 1.717727 \tValidation Loss: 2.342488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15359 \tTraining Loss: 1.727203 \tValidation Loss: 2.342078\n",
      "Epoch: 15360 \tTraining Loss: 1.728362 \tValidation Loss: 2.341653\n",
      "Epoch: 15361 \tTraining Loss: 1.709969 \tValidation Loss: 2.341839\n",
      "Epoch: 15362 \tTraining Loss: 1.743688 \tValidation Loss: 2.341801\n",
      "Epoch: 15363 \tTraining Loss: 1.729548 \tValidation Loss: 2.342227\n",
      "Epoch: 15364 \tTraining Loss: 1.736370 \tValidation Loss: 2.342382\n",
      "Epoch: 15365 \tTraining Loss: 1.740462 \tValidation Loss: 2.342547\n",
      "Epoch: 15366 \tTraining Loss: 1.696549 \tValidation Loss: 2.343105\n",
      "Epoch: 15367 \tTraining Loss: 1.737235 \tValidation Loss: 2.342971\n",
      "Epoch: 15368 \tTraining Loss: 1.737700 \tValidation Loss: 2.342904\n",
      "Epoch: 15369 \tTraining Loss: 1.759010 \tValidation Loss: 2.343034\n",
      "Epoch: 15370 \tTraining Loss: 1.747271 \tValidation Loss: 2.342205\n",
      "Epoch: 15371 \tTraining Loss: 1.697535 \tValidation Loss: 2.342113\n",
      "Epoch: 15372 \tTraining Loss: 1.713171 \tValidation Loss: 2.342073\n",
      "Epoch: 15373 \tTraining Loss: 1.729167 \tValidation Loss: 2.342042\n",
      "Epoch: 15374 \tTraining Loss: 1.729480 \tValidation Loss: 2.342124\n",
      "Epoch: 15375 \tTraining Loss: 1.739905 \tValidation Loss: 2.342256\n",
      "Epoch: 15376 \tTraining Loss: 1.727079 \tValidation Loss: 2.342575\n",
      "Epoch: 15377 \tTraining Loss: 1.716090 \tValidation Loss: 2.342450\n",
      "Epoch: 15378 \tTraining Loss: 1.718419 \tValidation Loss: 2.342877\n",
      "Epoch: 15379 \tTraining Loss: 1.730818 \tValidation Loss: 2.342681\n",
      "Epoch: 15380 \tTraining Loss: 1.728287 \tValidation Loss: 2.342779\n",
      "Epoch: 15381 \tTraining Loss: 1.723279 \tValidation Loss: 2.342307\n",
      "Epoch: 15382 \tTraining Loss: 1.708323 \tValidation Loss: 2.342397\n",
      "Epoch: 15383 \tTraining Loss: 1.735540 \tValidation Loss: 2.342828\n",
      "Epoch: 15384 \tTraining Loss: 1.707275 \tValidation Loss: 2.342402\n",
      "Epoch: 15385 \tTraining Loss: 1.730658 \tValidation Loss: 2.342305\n",
      "Epoch: 15386 \tTraining Loss: 1.739747 \tValidation Loss: 2.342557\n",
      "Epoch: 15387 \tTraining Loss: 1.723665 \tValidation Loss: 2.342630\n",
      "Epoch: 15388 \tTraining Loss: 1.724438 \tValidation Loss: 2.342465\n",
      "Epoch: 15389 \tTraining Loss: 1.700068 \tValidation Loss: 2.343003\n",
      "Epoch: 15390 \tTraining Loss: 1.714737 \tValidation Loss: 2.342895\n",
      "Epoch: 15391 \tTraining Loss: 1.737576 \tValidation Loss: 2.342594\n",
      "Epoch: 15392 \tTraining Loss: 1.701269 \tValidation Loss: 2.342635\n",
      "Epoch: 15393 \tTraining Loss: 1.718887 \tValidation Loss: 2.343034\n",
      "Epoch: 15394 \tTraining Loss: 1.731823 \tValidation Loss: 2.343203\n",
      "Epoch: 15395 \tTraining Loss: 1.740456 \tValidation Loss: 2.342762\n",
      "Epoch: 15396 \tTraining Loss: 1.724596 \tValidation Loss: 2.343111\n",
      "Epoch: 15397 \tTraining Loss: 1.705516 \tValidation Loss: 2.343459\n",
      "Epoch: 15398 \tTraining Loss: 1.721244 \tValidation Loss: 2.343659\n",
      "Epoch: 15399 \tTraining Loss: 1.756315 \tValidation Loss: 2.343463\n",
      "Epoch: 15400 \tTraining Loss: 1.774791 \tValidation Loss: 2.343112\n",
      "Epoch: 15401 \tTraining Loss: 1.739377 \tValidation Loss: 2.343380\n",
      "Epoch: 15402 \tTraining Loss: 1.733537 \tValidation Loss: 2.342575\n",
      "Epoch: 15403 \tTraining Loss: 1.730913 \tValidation Loss: 2.342691\n",
      "Epoch: 15404 \tTraining Loss: 1.745907 \tValidation Loss: 2.343022\n",
      "Epoch: 15405 \tTraining Loss: 1.743461 \tValidation Loss: 2.342981\n",
      "Epoch: 15406 \tTraining Loss: 1.740679 \tValidation Loss: 2.342687\n",
      "Epoch: 15407 \tTraining Loss: 1.739431 \tValidation Loss: 2.342828\n",
      "Epoch: 15408 \tTraining Loss: 1.715609 \tValidation Loss: 2.342899\n",
      "Epoch: 15409 \tTraining Loss: 1.695429 \tValidation Loss: 2.343043\n",
      "Epoch: 15410 \tTraining Loss: 1.691514 \tValidation Loss: 2.342855\n",
      "Epoch: 15411 \tTraining Loss: 1.741848 \tValidation Loss: 2.342846\n",
      "Epoch: 15412 \tTraining Loss: 1.716032 \tValidation Loss: 2.342592\n",
      "Epoch: 15413 \tTraining Loss: 1.737042 \tValidation Loss: 2.343072\n",
      "Epoch: 15414 \tTraining Loss: 1.723281 \tValidation Loss: 2.343309\n",
      "Epoch: 15415 \tTraining Loss: 1.725058 \tValidation Loss: 2.342876\n",
      "Epoch: 15416 \tTraining Loss: 1.762510 \tValidation Loss: 2.342942\n",
      "Epoch: 15417 \tTraining Loss: 1.696516 \tValidation Loss: 2.342786\n",
      "Epoch: 15418 \tTraining Loss: 1.740275 \tValidation Loss: 2.342891\n",
      "Epoch: 15419 \tTraining Loss: 1.727790 \tValidation Loss: 2.342811\n",
      "Epoch: 15420 \tTraining Loss: 1.687842 \tValidation Loss: 2.342834\n",
      "Epoch: 15421 \tTraining Loss: 1.720742 \tValidation Loss: 2.342893\n",
      "Epoch: 15422 \tTraining Loss: 1.736759 \tValidation Loss: 2.342827\n",
      "Epoch: 15423 \tTraining Loss: 1.730901 \tValidation Loss: 2.342724\n",
      "Epoch: 15424 \tTraining Loss: 1.709056 \tValidation Loss: 2.342394\n",
      "Epoch: 15425 \tTraining Loss: 1.698291 \tValidation Loss: 2.342791\n",
      "Epoch: 15426 \tTraining Loss: 1.768065 \tValidation Loss: 2.342885\n",
      "Epoch: 15427 \tTraining Loss: 1.750945 \tValidation Loss: 2.342660\n",
      "Epoch: 15428 \tTraining Loss: 1.671989 \tValidation Loss: 2.342613\n",
      "Epoch: 15429 \tTraining Loss: 1.733811 \tValidation Loss: 2.342811\n",
      "Epoch: 15430 \tTraining Loss: 1.767009 \tValidation Loss: 2.342572\n",
      "Epoch: 15431 \tTraining Loss: 1.734227 \tValidation Loss: 2.342938\n",
      "Epoch: 15432 \tTraining Loss: 1.712374 \tValidation Loss: 2.342678\n",
      "Epoch: 15433 \tTraining Loss: 1.754266 \tValidation Loss: 2.342601\n",
      "Epoch: 15434 \tTraining Loss: 1.737503 \tValidation Loss: 2.342565\n",
      "Epoch: 15435 \tTraining Loss: 1.746989 \tValidation Loss: 2.343025\n",
      "Epoch: 15436 \tTraining Loss: 1.681589 \tValidation Loss: 2.343232\n",
      "Epoch: 15437 \tTraining Loss: 1.751447 \tValidation Loss: 2.343019\n",
      "Epoch: 15438 \tTraining Loss: 1.714722 \tValidation Loss: 2.342537\n",
      "Epoch: 15439 \tTraining Loss: 1.735433 \tValidation Loss: 2.342534\n",
      "Epoch: 15440 \tTraining Loss: 1.742011 \tValidation Loss: 2.343101\n",
      "Epoch: 15441 \tTraining Loss: 1.695305 \tValidation Loss: 2.342954\n",
      "Epoch: 15442 \tTraining Loss: 1.737071 \tValidation Loss: 2.342809\n",
      "Epoch: 15443 \tTraining Loss: 1.736546 \tValidation Loss: 2.342896\n",
      "Epoch: 15444 \tTraining Loss: 1.758204 \tValidation Loss: 2.342731\n",
      "Epoch: 15445 \tTraining Loss: 1.720551 \tValidation Loss: 2.343066\n",
      "Epoch: 15446 \tTraining Loss: 1.750098 \tValidation Loss: 2.343182\n",
      "Epoch: 15447 \tTraining Loss: 1.756140 \tValidation Loss: 2.342927\n",
      "Epoch: 15448 \tTraining Loss: 1.715775 \tValidation Loss: 2.343087\n",
      "Epoch: 15449 \tTraining Loss: 1.725023 \tValidation Loss: 2.342919\n",
      "Epoch: 15450 \tTraining Loss: 1.732674 \tValidation Loss: 2.342967\n",
      "Epoch: 15451 \tTraining Loss: 1.708840 \tValidation Loss: 2.343307\n",
      "Epoch: 15452 \tTraining Loss: 1.714922 \tValidation Loss: 2.342876\n",
      "Epoch: 15453 \tTraining Loss: 1.711926 \tValidation Loss: 2.342975\n",
      "Epoch: 15454 \tTraining Loss: 1.751084 \tValidation Loss: 2.342417\n",
      "Epoch: 15455 \tTraining Loss: 1.764629 \tValidation Loss: 2.342811\n",
      "Epoch: 15456 \tTraining Loss: 1.723574 \tValidation Loss: 2.342891\n",
      "Epoch: 15457 \tTraining Loss: 1.708618 \tValidation Loss: 2.343215\n",
      "Epoch: 15458 \tTraining Loss: 1.769989 \tValidation Loss: 2.343229\n",
      "Epoch: 15459 \tTraining Loss: 1.717315 \tValidation Loss: 2.342726\n",
      "Epoch: 15460 \tTraining Loss: 1.735479 \tValidation Loss: 2.342463\n",
      "Epoch: 15461 \tTraining Loss: 1.730036 \tValidation Loss: 2.342806\n",
      "Epoch: 15462 \tTraining Loss: 1.719522 \tValidation Loss: 2.342828\n",
      "Epoch: 15463 \tTraining Loss: 1.725265 \tValidation Loss: 2.342648\n",
      "Epoch: 15464 \tTraining Loss: 1.722123 \tValidation Loss: 2.342762\n",
      "Epoch: 15465 \tTraining Loss: 1.748346 \tValidation Loss: 2.342937\n",
      "Epoch: 15466 \tTraining Loss: 1.713795 \tValidation Loss: 2.343366\n",
      "Epoch: 15467 \tTraining Loss: 1.726766 \tValidation Loss: 2.343389\n",
      "Epoch: 15468 \tTraining Loss: 1.741459 \tValidation Loss: 2.343014\n",
      "Epoch: 15469 \tTraining Loss: 1.727635 \tValidation Loss: 2.342653\n",
      "Epoch: 15470 \tTraining Loss: 1.700190 \tValidation Loss: 2.343104\n",
      "Epoch: 15471 \tTraining Loss: 1.711461 \tValidation Loss: 2.343099\n",
      "Epoch: 15472 \tTraining Loss: 1.705819 \tValidation Loss: 2.343539\n",
      "Epoch: 15473 \tTraining Loss: 1.724260 \tValidation Loss: 2.343648\n",
      "Epoch: 15474 \tTraining Loss: 1.732140 \tValidation Loss: 2.343725\n",
      "Epoch: 15475 \tTraining Loss: 1.714223 \tValidation Loss: 2.343591\n",
      "Epoch: 15476 \tTraining Loss: 1.724116 \tValidation Loss: 2.344185\n",
      "Epoch: 15477 \tTraining Loss: 1.740307 \tValidation Loss: 2.343904\n",
      "Epoch: 15478 \tTraining Loss: 1.723620 \tValidation Loss: 2.343587\n",
      "Epoch: 15479 \tTraining Loss: 1.719551 \tValidation Loss: 2.343769\n",
      "Epoch: 15480 \tTraining Loss: 1.728671 \tValidation Loss: 2.343699\n",
      "Epoch: 15481 \tTraining Loss: 1.745949 \tValidation Loss: 2.343749\n",
      "Epoch: 15482 \tTraining Loss: 1.706795 \tValidation Loss: 2.343529\n",
      "Epoch: 15483 \tTraining Loss: 1.726599 \tValidation Loss: 2.343404\n",
      "Epoch: 15484 \tTraining Loss: 1.725029 \tValidation Loss: 2.343816\n",
      "Epoch: 15485 \tTraining Loss: 1.660423 \tValidation Loss: 2.344290\n",
      "Epoch: 15486 \tTraining Loss: 1.741723 \tValidation Loss: 2.344172\n",
      "Epoch: 15487 \tTraining Loss: 1.713161 \tValidation Loss: 2.343770\n",
      "Epoch: 15488 \tTraining Loss: 1.683392 \tValidation Loss: 2.343894\n",
      "Epoch: 15489 \tTraining Loss: 1.701832 \tValidation Loss: 2.343657\n",
      "Epoch: 15490 \tTraining Loss: 1.721082 \tValidation Loss: 2.343933\n",
      "Epoch: 15491 \tTraining Loss: 1.700012 \tValidation Loss: 2.343740\n",
      "Epoch: 15492 \tTraining Loss: 1.697018 \tValidation Loss: 2.344331\n",
      "Epoch: 15493 \tTraining Loss: 1.743138 \tValidation Loss: 2.344255\n",
      "Epoch: 15494 \tTraining Loss: 1.728800 \tValidation Loss: 2.343702\n",
      "Epoch: 15495 \tTraining Loss: 1.726730 \tValidation Loss: 2.343596\n",
      "Epoch: 15496 \tTraining Loss: 1.685143 \tValidation Loss: 2.344055\n",
      "Epoch: 15497 \tTraining Loss: 1.717999 \tValidation Loss: 2.343774\n",
      "Epoch: 15498 \tTraining Loss: 1.725231 \tValidation Loss: 2.344003\n",
      "Epoch: 15499 \tTraining Loss: 1.739755 \tValidation Loss: 2.343823\n",
      "Epoch: 15500 \tTraining Loss: 1.753557 \tValidation Loss: 2.343620\n",
      "Epoch: 15501 \tTraining Loss: 1.720275 \tValidation Loss: 2.343727\n",
      "Epoch: 15502 \tTraining Loss: 1.711940 \tValidation Loss: 2.343390\n",
      "Epoch: 15503 \tTraining Loss: 1.699543 \tValidation Loss: 2.343712\n",
      "Epoch: 15504 \tTraining Loss: 1.719768 \tValidation Loss: 2.343460\n",
      "Epoch: 15505 \tTraining Loss: 1.701368 \tValidation Loss: 2.343952\n",
      "Epoch: 15506 \tTraining Loss: 1.730750 \tValidation Loss: 2.343815\n",
      "Epoch: 15507 \tTraining Loss: 1.717055 \tValidation Loss: 2.343498\n",
      "Epoch: 15508 \tTraining Loss: 1.720766 \tValidation Loss: 2.343293\n",
      "Epoch: 15509 \tTraining Loss: 1.728853 \tValidation Loss: 2.343416\n",
      "Epoch: 15510 \tTraining Loss: 1.671495 \tValidation Loss: 2.343546\n",
      "Epoch: 15511 \tTraining Loss: 1.730004 \tValidation Loss: 2.343546\n",
      "Epoch: 15512 \tTraining Loss: 1.713631 \tValidation Loss: 2.343672\n",
      "Epoch: 15513 \tTraining Loss: 1.741605 \tValidation Loss: 2.343291\n",
      "Epoch: 15514 \tTraining Loss: 1.743688 \tValidation Loss: 2.343410\n",
      "Epoch: 15515 \tTraining Loss: 1.733696 \tValidation Loss: 2.343525\n",
      "Epoch: 15516 \tTraining Loss: 1.722602 \tValidation Loss: 2.343565\n",
      "Epoch: 15517 \tTraining Loss: 1.699141 \tValidation Loss: 2.343787\n",
      "Epoch: 15518 \tTraining Loss: 1.653845 \tValidation Loss: 2.343960\n",
      "Epoch: 15519 \tTraining Loss: 1.747285 \tValidation Loss: 2.343713\n",
      "Epoch: 15520 \tTraining Loss: 1.705895 \tValidation Loss: 2.343578\n",
      "Epoch: 15521 \tTraining Loss: 1.709808 \tValidation Loss: 2.343376\n",
      "Epoch: 15522 \tTraining Loss: 1.721691 \tValidation Loss: 2.343135\n",
      "Epoch: 15523 \tTraining Loss: 1.679812 \tValidation Loss: 2.343717\n",
      "Epoch: 15524 \tTraining Loss: 1.740306 \tValidation Loss: 2.343936\n",
      "Epoch: 15525 \tTraining Loss: 1.716051 \tValidation Loss: 2.344248\n",
      "Epoch: 15526 \tTraining Loss: 1.701133 \tValidation Loss: 2.344193\n",
      "Epoch: 15527 \tTraining Loss: 1.696969 \tValidation Loss: 2.343815\n",
      "Epoch: 15528 \tTraining Loss: 1.684523 \tValidation Loss: 2.344280\n",
      "Epoch: 15529 \tTraining Loss: 1.693321 \tValidation Loss: 2.344259\n",
      "Epoch: 15530 \tTraining Loss: 1.717504 \tValidation Loss: 2.344094\n",
      "Epoch: 15531 \tTraining Loss: 1.756704 \tValidation Loss: 2.344084\n",
      "Epoch: 15532 \tTraining Loss: 1.734893 \tValidation Loss: 2.344336\n",
      "Epoch: 15533 \tTraining Loss: 1.725981 \tValidation Loss: 2.344429\n",
      "Epoch: 15534 \tTraining Loss: 1.682179 \tValidation Loss: 2.344383\n",
      "Epoch: 15535 \tTraining Loss: 1.728899 \tValidation Loss: 2.344086\n",
      "Epoch: 15536 \tTraining Loss: 1.701194 \tValidation Loss: 2.344217\n",
      "Epoch: 15537 \tTraining Loss: 1.742325 \tValidation Loss: 2.344194\n",
      "Epoch: 15538 \tTraining Loss: 1.682312 \tValidation Loss: 2.344604\n",
      "Epoch: 15539 \tTraining Loss: 1.717209 \tValidation Loss: 2.344365\n",
      "Epoch: 15540 \tTraining Loss: 1.719801 \tValidation Loss: 2.343950\n",
      "Epoch: 15541 \tTraining Loss: 1.747945 \tValidation Loss: 2.343870\n",
      "Epoch: 15542 \tTraining Loss: 1.722204 \tValidation Loss: 2.343773\n",
      "Epoch: 15543 \tTraining Loss: 1.698151 \tValidation Loss: 2.344412\n",
      "Epoch: 15544 \tTraining Loss: 1.717079 \tValidation Loss: 2.344064\n",
      "Epoch: 15545 \tTraining Loss: 1.692365 \tValidation Loss: 2.344048\n",
      "Epoch: 15546 \tTraining Loss: 1.679601 \tValidation Loss: 2.343793\n",
      "Epoch: 15547 \tTraining Loss: 1.694660 \tValidation Loss: 2.343882\n",
      "Epoch: 15548 \tTraining Loss: 1.674602 \tValidation Loss: 2.343965\n",
      "Epoch: 15549 \tTraining Loss: 1.732193 \tValidation Loss: 2.343906\n",
      "Epoch: 15550 \tTraining Loss: 1.732779 \tValidation Loss: 2.344389\n",
      "Epoch: 15551 \tTraining Loss: 1.712324 \tValidation Loss: 2.344548\n",
      "Epoch: 15552 \tTraining Loss: 1.719921 \tValidation Loss: 2.344798\n",
      "Epoch: 15553 \tTraining Loss: 1.715449 \tValidation Loss: 2.344401\n",
      "Epoch: 15554 \tTraining Loss: 1.687931 \tValidation Loss: 2.344606\n",
      "Epoch: 15555 \tTraining Loss: 1.694769 \tValidation Loss: 2.344668\n",
      "Epoch: 15556 \tTraining Loss: 1.705967 \tValidation Loss: 2.344718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15557 \tTraining Loss: 1.707646 \tValidation Loss: 2.344667\n",
      "Epoch: 15558 \tTraining Loss: 1.704705 \tValidation Loss: 2.344433\n",
      "Epoch: 15559 \tTraining Loss: 1.701826 \tValidation Loss: 2.344118\n",
      "Epoch: 15560 \tTraining Loss: 1.732504 \tValidation Loss: 2.344406\n",
      "Epoch: 15561 \tTraining Loss: 1.706984 \tValidation Loss: 2.344212\n",
      "Epoch: 15562 \tTraining Loss: 1.706538 \tValidation Loss: 2.344120\n",
      "Epoch: 15563 \tTraining Loss: 1.690294 \tValidation Loss: 2.344384\n",
      "Epoch: 15564 \tTraining Loss: 1.715445 \tValidation Loss: 2.344541\n",
      "Epoch: 15565 \tTraining Loss: 1.748186 \tValidation Loss: 2.344126\n",
      "Epoch: 15566 \tTraining Loss: 1.715835 \tValidation Loss: 2.343753\n",
      "Epoch: 15567 \tTraining Loss: 1.714700 \tValidation Loss: 2.343717\n",
      "Epoch: 15568 \tTraining Loss: 1.711268 \tValidation Loss: 2.344248\n",
      "Epoch: 15569 \tTraining Loss: 1.720706 \tValidation Loss: 2.343954\n",
      "Epoch: 15570 \tTraining Loss: 1.703035 \tValidation Loss: 2.344220\n",
      "Epoch: 15571 \tTraining Loss: 1.713823 \tValidation Loss: 2.344484\n",
      "Epoch: 15572 \tTraining Loss: 1.724426 \tValidation Loss: 2.343632\n",
      "Epoch: 15573 \tTraining Loss: 1.725933 \tValidation Loss: 2.344230\n",
      "Epoch: 15574 \tTraining Loss: 1.721090 \tValidation Loss: 2.344402\n",
      "Epoch: 15575 \tTraining Loss: 1.731212 \tValidation Loss: 2.344386\n",
      "Epoch: 15576 \tTraining Loss: 1.707959 \tValidation Loss: 2.344744\n",
      "Epoch: 15577 \tTraining Loss: 1.757083 \tValidation Loss: 2.344515\n",
      "Epoch: 15578 \tTraining Loss: 1.714357 \tValidation Loss: 2.344912\n",
      "Epoch: 15579 \tTraining Loss: 1.698349 \tValidation Loss: 2.344739\n",
      "Epoch: 15580 \tTraining Loss: 1.736467 \tValidation Loss: 2.344187\n",
      "Epoch: 15581 \tTraining Loss: 1.736838 \tValidation Loss: 2.344214\n",
      "Epoch: 15582 \tTraining Loss: 1.720635 \tValidation Loss: 2.344625\n",
      "Epoch: 15583 \tTraining Loss: 1.724490 \tValidation Loss: 2.344231\n",
      "Epoch: 15584 \tTraining Loss: 1.713036 \tValidation Loss: 2.344393\n",
      "Epoch: 15585 \tTraining Loss: 1.701200 \tValidation Loss: 2.344052\n",
      "Epoch: 15586 \tTraining Loss: 1.684758 \tValidation Loss: 2.344748\n",
      "Epoch: 15587 \tTraining Loss: 1.684212 \tValidation Loss: 2.344654\n",
      "Epoch: 15588 \tTraining Loss: 1.729312 \tValidation Loss: 2.344208\n",
      "Epoch: 15589 \tTraining Loss: 1.685311 \tValidation Loss: 2.344561\n",
      "Epoch: 15590 \tTraining Loss: 1.734189 \tValidation Loss: 2.344921\n",
      "Epoch: 15591 \tTraining Loss: 1.693807 \tValidation Loss: 2.344442\n",
      "Epoch: 15592 \tTraining Loss: 1.749005 \tValidation Loss: 2.344425\n",
      "Epoch: 15593 \tTraining Loss: 1.709170 \tValidation Loss: 2.344380\n",
      "Epoch: 15594 \tTraining Loss: 1.697181 \tValidation Loss: 2.344752\n",
      "Epoch: 15595 \tTraining Loss: 1.713678 \tValidation Loss: 2.344709\n",
      "Epoch: 15596 \tTraining Loss: 1.697612 \tValidation Loss: 2.344837\n",
      "Epoch: 15597 \tTraining Loss: 1.693140 \tValidation Loss: 2.344761\n",
      "Epoch: 15598 \tTraining Loss: 1.714499 \tValidation Loss: 2.344581\n",
      "Epoch: 15599 \tTraining Loss: 1.704249 \tValidation Loss: 2.344806\n",
      "Epoch: 15600 \tTraining Loss: 1.730880 \tValidation Loss: 2.344364\n",
      "Epoch: 15601 \tTraining Loss: 1.725624 \tValidation Loss: 2.344273\n",
      "Epoch: 15602 \tTraining Loss: 1.723804 \tValidation Loss: 2.344513\n",
      "Epoch: 15603 \tTraining Loss: 1.725312 \tValidation Loss: 2.344395\n",
      "Epoch: 15604 \tTraining Loss: 1.719255 \tValidation Loss: 2.344263\n",
      "Epoch: 15605 \tTraining Loss: 1.690522 \tValidation Loss: 2.344574\n",
      "Epoch: 15606 \tTraining Loss: 1.679063 \tValidation Loss: 2.344584\n",
      "Epoch: 15607 \tTraining Loss: 1.649311 \tValidation Loss: 2.344308\n",
      "Epoch: 15608 \tTraining Loss: 1.707152 \tValidation Loss: 2.344751\n",
      "Epoch: 15609 \tTraining Loss: 1.721790 \tValidation Loss: 2.344417\n",
      "Epoch: 15610 \tTraining Loss: 1.678022 \tValidation Loss: 2.344439\n",
      "Epoch: 15611 \tTraining Loss: 1.734115 \tValidation Loss: 2.344779\n",
      "Epoch: 15612 \tTraining Loss: 1.701490 \tValidation Loss: 2.344825\n",
      "Epoch: 15613 \tTraining Loss: 1.709730 \tValidation Loss: 2.344645\n",
      "Epoch: 15614 \tTraining Loss: 1.743609 \tValidation Loss: 2.344851\n",
      "Epoch: 15615 \tTraining Loss: 1.723285 \tValidation Loss: 2.345199\n",
      "Epoch: 15616 \tTraining Loss: 1.696171 \tValidation Loss: 2.345452\n",
      "Epoch: 15617 \tTraining Loss: 1.732064 \tValidation Loss: 2.344950\n",
      "Epoch: 15618 \tTraining Loss: 1.709830 \tValidation Loss: 2.344885\n",
      "Epoch: 15619 \tTraining Loss: 1.723394 \tValidation Loss: 2.344602\n",
      "Epoch: 15620 \tTraining Loss: 1.695325 \tValidation Loss: 2.344897\n",
      "Epoch: 15621 \tTraining Loss: 1.687712 \tValidation Loss: 2.344926\n",
      "Epoch: 15622 \tTraining Loss: 1.719484 \tValidation Loss: 2.345022\n",
      "Epoch: 15623 \tTraining Loss: 1.717966 \tValidation Loss: 2.344949\n",
      "Epoch: 15624 \tTraining Loss: 1.677199 \tValidation Loss: 2.345105\n",
      "Epoch: 15625 \tTraining Loss: 1.719539 \tValidation Loss: 2.345077\n",
      "Epoch: 15626 \tTraining Loss: 1.738980 \tValidation Loss: 2.344851\n",
      "Epoch: 15627 \tTraining Loss: 1.689860 \tValidation Loss: 2.344443\n",
      "Epoch: 15628 \tTraining Loss: 1.731040 \tValidation Loss: 2.344599\n",
      "Epoch: 15629 \tTraining Loss: 1.735072 \tValidation Loss: 2.344632\n",
      "Epoch: 15630 \tTraining Loss: 1.760960 \tValidation Loss: 2.344604\n",
      "Epoch: 15631 \tTraining Loss: 1.710926 \tValidation Loss: 2.344554\n",
      "Epoch: 15632 \tTraining Loss: 1.716943 \tValidation Loss: 2.344823\n",
      "Epoch: 15633 \tTraining Loss: 1.704042 \tValidation Loss: 2.344665\n",
      "Epoch: 15634 \tTraining Loss: 1.728959 \tValidation Loss: 2.344401\n",
      "Epoch: 15635 \tTraining Loss: 1.733026 \tValidation Loss: 2.344408\n",
      "Epoch: 15636 \tTraining Loss: 1.719950 \tValidation Loss: 2.344647\n",
      "Epoch: 15637 \tTraining Loss: 1.719601 \tValidation Loss: 2.344801\n",
      "Epoch: 15638 \tTraining Loss: 1.689342 \tValidation Loss: 2.344803\n",
      "Epoch: 15639 \tTraining Loss: 1.715581 \tValidation Loss: 2.345147\n",
      "Epoch: 15640 \tTraining Loss: 1.704515 \tValidation Loss: 2.345452\n",
      "Epoch: 15641 \tTraining Loss: 1.700161 \tValidation Loss: 2.345201\n",
      "Epoch: 15642 \tTraining Loss: 1.678612 \tValidation Loss: 2.345410\n",
      "Epoch: 15643 \tTraining Loss: 1.667567 \tValidation Loss: 2.345434\n",
      "Epoch: 15644 \tTraining Loss: 1.745625 \tValidation Loss: 2.345638\n",
      "Epoch: 15645 \tTraining Loss: 1.689353 \tValidation Loss: 2.345179\n",
      "Epoch: 15646 \tTraining Loss: 1.696872 \tValidation Loss: 2.345448\n",
      "Epoch: 15647 \tTraining Loss: 1.744146 \tValidation Loss: 2.344857\n",
      "Epoch: 15648 \tTraining Loss: 1.713893 \tValidation Loss: 2.345112\n",
      "Epoch: 15649 \tTraining Loss: 1.698056 \tValidation Loss: 2.345395\n",
      "Epoch: 15650 \tTraining Loss: 1.730289 \tValidation Loss: 2.345237\n",
      "Epoch: 15651 \tTraining Loss: 1.708471 \tValidation Loss: 2.345396\n",
      "Epoch: 15652 \tTraining Loss: 1.700944 \tValidation Loss: 2.345572\n",
      "Epoch: 15653 \tTraining Loss: 1.722573 \tValidation Loss: 2.345155\n",
      "Epoch: 15654 \tTraining Loss: 1.702176 \tValidation Loss: 2.344872\n",
      "Epoch: 15655 \tTraining Loss: 1.723106 \tValidation Loss: 2.345101\n",
      "Epoch: 15656 \tTraining Loss: 1.719700 \tValidation Loss: 2.345385\n",
      "Epoch: 15657 \tTraining Loss: 1.713430 \tValidation Loss: 2.344887\n",
      "Epoch: 15658 \tTraining Loss: 1.735491 \tValidation Loss: 2.345012\n",
      "Epoch: 15659 \tTraining Loss: 1.712479 \tValidation Loss: 2.345449\n",
      "Epoch: 15660 \tTraining Loss: 1.710221 \tValidation Loss: 2.345391\n",
      "Epoch: 15661 \tTraining Loss: 1.709949 \tValidation Loss: 2.345252\n",
      "Epoch: 15662 \tTraining Loss: 1.691278 \tValidation Loss: 2.345712\n",
      "Epoch: 15663 \tTraining Loss: 1.710409 \tValidation Loss: 2.345480\n",
      "Epoch: 15664 \tTraining Loss: 1.718904 \tValidation Loss: 2.345569\n",
      "Epoch: 15665 \tTraining Loss: 1.728643 \tValidation Loss: 2.345390\n",
      "Epoch: 15666 \tTraining Loss: 1.728421 \tValidation Loss: 2.345224\n",
      "Epoch: 15667 \tTraining Loss: 1.700203 \tValidation Loss: 2.345167\n",
      "Epoch: 15668 \tTraining Loss: 1.706852 \tValidation Loss: 2.345295\n",
      "Epoch: 15669 \tTraining Loss: 1.723829 \tValidation Loss: 2.345484\n",
      "Epoch: 15670 \tTraining Loss: 1.714049 \tValidation Loss: 2.345608\n",
      "Epoch: 15671 \tTraining Loss: 1.737971 \tValidation Loss: 2.345236\n",
      "Epoch: 15672 \tTraining Loss: 1.724998 \tValidation Loss: 2.345406\n",
      "Epoch: 15673 \tTraining Loss: 1.735684 \tValidation Loss: 2.345166\n",
      "Epoch: 15674 \tTraining Loss: 1.723728 \tValidation Loss: 2.345107\n",
      "Epoch: 15675 \tTraining Loss: 1.739146 \tValidation Loss: 2.345276\n",
      "Epoch: 15676 \tTraining Loss: 1.703437 \tValidation Loss: 2.345249\n",
      "Epoch: 15677 \tTraining Loss: 1.692121 \tValidation Loss: 2.344745\n",
      "Epoch: 15678 \tTraining Loss: 1.724194 \tValidation Loss: 2.345142\n",
      "Epoch: 15679 \tTraining Loss: 1.698764 \tValidation Loss: 2.345243\n",
      "Epoch: 15680 \tTraining Loss: 1.727255 \tValidation Loss: 2.345457\n",
      "Epoch: 15681 \tTraining Loss: 1.742350 \tValidation Loss: 2.345738\n",
      "Epoch: 15682 \tTraining Loss: 1.722998 \tValidation Loss: 2.345703\n",
      "Epoch: 15683 \tTraining Loss: 1.696584 \tValidation Loss: 2.345914\n",
      "Epoch: 15684 \tTraining Loss: 1.720431 \tValidation Loss: 2.345425\n",
      "Epoch: 15685 \tTraining Loss: 1.732091 \tValidation Loss: 2.345294\n",
      "Epoch: 15686 \tTraining Loss: 1.720337 \tValidation Loss: 2.344657\n",
      "Epoch: 15687 \tTraining Loss: 1.699559 \tValidation Loss: 2.345092\n",
      "Epoch: 15688 \tTraining Loss: 1.747804 \tValidation Loss: 2.345021\n",
      "Epoch: 15689 \tTraining Loss: 1.701067 \tValidation Loss: 2.345364\n",
      "Epoch: 15690 \tTraining Loss: 1.697701 \tValidation Loss: 2.345403\n",
      "Epoch: 15691 \tTraining Loss: 1.724744 \tValidation Loss: 2.345663\n",
      "Epoch: 15692 \tTraining Loss: 1.706829 \tValidation Loss: 2.345399\n",
      "Epoch: 15693 \tTraining Loss: 1.730917 \tValidation Loss: 2.345597\n",
      "Epoch: 15694 \tTraining Loss: 1.678681 \tValidation Loss: 2.346155\n",
      "Epoch: 15695 \tTraining Loss: 1.701845 \tValidation Loss: 2.345890\n",
      "Epoch: 15696 \tTraining Loss: 1.731524 \tValidation Loss: 2.344965\n",
      "Epoch: 15697 \tTraining Loss: 1.688030 \tValidation Loss: 2.345361\n",
      "Epoch: 15698 \tTraining Loss: 1.698507 \tValidation Loss: 2.345896\n",
      "Epoch: 15699 \tTraining Loss: 1.694282 \tValidation Loss: 2.346358\n",
      "Epoch: 15700 \tTraining Loss: 1.716948 \tValidation Loss: 2.346084\n",
      "Epoch: 15701 \tTraining Loss: 1.694718 \tValidation Loss: 2.345815\n",
      "Epoch: 15702 \tTraining Loss: 1.711650 \tValidation Loss: 2.346150\n",
      "Epoch: 15703 \tTraining Loss: 1.749813 \tValidation Loss: 2.345649\n",
      "Epoch: 15704 \tTraining Loss: 1.695676 \tValidation Loss: 2.346027\n",
      "Epoch: 15705 \tTraining Loss: 1.714083 \tValidation Loss: 2.345733\n",
      "Epoch: 15706 \tTraining Loss: 1.710377 \tValidation Loss: 2.345867\n",
      "Epoch: 15707 \tTraining Loss: 1.697188 \tValidation Loss: 2.345776\n",
      "Epoch: 15708 \tTraining Loss: 1.696829 \tValidation Loss: 2.345842\n",
      "Epoch: 15709 \tTraining Loss: 1.692496 \tValidation Loss: 2.345871\n",
      "Epoch: 15710 \tTraining Loss: 1.703442 \tValidation Loss: 2.346465\n",
      "Epoch: 15711 \tTraining Loss: 1.706657 \tValidation Loss: 2.345972\n",
      "Epoch: 15712 \tTraining Loss: 1.731476 \tValidation Loss: 2.345809\n",
      "Epoch: 15713 \tTraining Loss: 1.742084 \tValidation Loss: 2.345624\n",
      "Epoch: 15714 \tTraining Loss: 1.716557 \tValidation Loss: 2.345995\n",
      "Epoch: 15715 \tTraining Loss: 1.701969 \tValidation Loss: 2.345973\n",
      "Epoch: 15716 \tTraining Loss: 1.708080 \tValidation Loss: 2.345827\n",
      "Epoch: 15717 \tTraining Loss: 1.695185 \tValidation Loss: 2.345783\n",
      "Epoch: 15718 \tTraining Loss: 1.715084 \tValidation Loss: 2.346184\n",
      "Epoch: 15719 \tTraining Loss: 1.716494 \tValidation Loss: 2.346014\n",
      "Epoch: 15720 \tTraining Loss: 1.702025 \tValidation Loss: 2.345989\n",
      "Epoch: 15721 \tTraining Loss: 1.712159 \tValidation Loss: 2.346508\n",
      "Epoch: 15722 \tTraining Loss: 1.709096 \tValidation Loss: 2.345998\n",
      "Epoch: 15723 \tTraining Loss: 1.680152 \tValidation Loss: 2.346392\n",
      "Epoch: 15724 \tTraining Loss: 1.714135 \tValidation Loss: 2.346381\n",
      "Epoch: 15725 \tTraining Loss: 1.712474 \tValidation Loss: 2.345846\n",
      "Epoch: 15726 \tTraining Loss: 1.710656 \tValidation Loss: 2.345853\n",
      "Epoch: 15727 \tTraining Loss: 1.717004 \tValidation Loss: 2.345707\n",
      "Epoch: 15728 \tTraining Loss: 1.709870 \tValidation Loss: 2.345313\n",
      "Epoch: 15729 \tTraining Loss: 1.683031 \tValidation Loss: 2.345775\n",
      "Epoch: 15730 \tTraining Loss: 1.695616 \tValidation Loss: 2.345875\n",
      "Epoch: 15731 \tTraining Loss: 1.719427 \tValidation Loss: 2.345733\n",
      "Epoch: 15732 \tTraining Loss: 1.702641 \tValidation Loss: 2.345346\n",
      "Epoch: 15733 \tTraining Loss: 1.688627 \tValidation Loss: 2.345920\n",
      "Epoch: 15734 \tTraining Loss: 1.731565 \tValidation Loss: 2.346102\n",
      "Epoch: 15735 \tTraining Loss: 1.748933 \tValidation Loss: 2.345694\n",
      "Epoch: 15736 \tTraining Loss: 1.756793 \tValidation Loss: 2.346010\n",
      "Epoch: 15737 \tTraining Loss: 1.716449 \tValidation Loss: 2.346288\n",
      "Epoch: 15738 \tTraining Loss: 1.741123 \tValidation Loss: 2.346098\n",
      "Epoch: 15739 \tTraining Loss: 1.719248 \tValidation Loss: 2.346346\n",
      "Epoch: 15740 \tTraining Loss: 1.717838 \tValidation Loss: 2.346039\n",
      "Epoch: 15741 \tTraining Loss: 1.693376 \tValidation Loss: 2.346298\n",
      "Epoch: 15742 \tTraining Loss: 1.711938 \tValidation Loss: 2.345984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15743 \tTraining Loss: 1.741960 \tValidation Loss: 2.345742\n",
      "Epoch: 15744 \tTraining Loss: 1.714815 \tValidation Loss: 2.345554\n",
      "Epoch: 15745 \tTraining Loss: 1.697876 \tValidation Loss: 2.345295\n",
      "Epoch: 15746 \tTraining Loss: 1.701653 \tValidation Loss: 2.345486\n",
      "Epoch: 15747 \tTraining Loss: 1.708126 \tValidation Loss: 2.345844\n",
      "Epoch: 15748 \tTraining Loss: 1.716854 \tValidation Loss: 2.345574\n",
      "Epoch: 15749 \tTraining Loss: 1.699411 \tValidation Loss: 2.345812\n",
      "Epoch: 15750 \tTraining Loss: 1.701956 \tValidation Loss: 2.345861\n",
      "Epoch: 15751 \tTraining Loss: 1.712712 \tValidation Loss: 2.345637\n",
      "Epoch: 15752 \tTraining Loss: 1.713769 \tValidation Loss: 2.345896\n",
      "Epoch: 15753 \tTraining Loss: 1.682508 \tValidation Loss: 2.345960\n",
      "Epoch: 15754 \tTraining Loss: 1.664563 \tValidation Loss: 2.346101\n",
      "Epoch: 15755 \tTraining Loss: 1.724861 \tValidation Loss: 2.346067\n",
      "Epoch: 15756 \tTraining Loss: 1.709031 \tValidation Loss: 2.346106\n",
      "Epoch: 15757 \tTraining Loss: 1.712637 \tValidation Loss: 2.346066\n",
      "Epoch: 15758 \tTraining Loss: 1.693095 \tValidation Loss: 2.346285\n",
      "Epoch: 15759 \tTraining Loss: 1.696886 \tValidation Loss: 2.346318\n",
      "Epoch: 15760 \tTraining Loss: 1.687280 \tValidation Loss: 2.346414\n",
      "Epoch: 15761 \tTraining Loss: 1.725128 \tValidation Loss: 2.346580\n",
      "Epoch: 15762 \tTraining Loss: 1.707946 \tValidation Loss: 2.346825\n",
      "Epoch: 15763 \tTraining Loss: 1.719528 \tValidation Loss: 2.346447\n",
      "Epoch: 15764 \tTraining Loss: 1.705811 \tValidation Loss: 2.346411\n",
      "Epoch: 15765 \tTraining Loss: 1.709929 \tValidation Loss: 2.346211\n",
      "Epoch: 15766 \tTraining Loss: 1.691909 \tValidation Loss: 2.346043\n",
      "Epoch: 15767 \tTraining Loss: 1.688721 \tValidation Loss: 2.345388\n",
      "Epoch: 15768 \tTraining Loss: 1.710702 \tValidation Loss: 2.345854\n",
      "Epoch: 15769 \tTraining Loss: 1.701445 \tValidation Loss: 2.346615\n",
      "Epoch: 15770 \tTraining Loss: 1.704279 \tValidation Loss: 2.346323\n",
      "Epoch: 15771 \tTraining Loss: 1.729316 \tValidation Loss: 2.346296\n",
      "Epoch: 15772 \tTraining Loss: 1.681816 \tValidation Loss: 2.346561\n",
      "Epoch: 15773 \tTraining Loss: 1.676186 \tValidation Loss: 2.346798\n",
      "Epoch: 15774 \tTraining Loss: 1.711866 \tValidation Loss: 2.346502\n",
      "Epoch: 15775 \tTraining Loss: 1.732437 \tValidation Loss: 2.346129\n",
      "Epoch: 15776 \tTraining Loss: 1.728277 \tValidation Loss: 2.346328\n",
      "Epoch: 15777 \tTraining Loss: 1.687633 \tValidation Loss: 2.346224\n",
      "Epoch: 15778 \tTraining Loss: 1.692645 \tValidation Loss: 2.346591\n",
      "Epoch: 15779 \tTraining Loss: 1.674004 \tValidation Loss: 2.346367\n",
      "Epoch: 15780 \tTraining Loss: 1.714033 \tValidation Loss: 2.346410\n",
      "Epoch: 15781 \tTraining Loss: 1.724313 \tValidation Loss: 2.346237\n",
      "Epoch: 15782 \tTraining Loss: 1.687679 \tValidation Loss: 2.346487\n",
      "Epoch: 15783 \tTraining Loss: 1.694491 \tValidation Loss: 2.346625\n",
      "Epoch: 15784 \tTraining Loss: 1.709394 \tValidation Loss: 2.346688\n",
      "Epoch: 15785 \tTraining Loss: 1.756169 \tValidation Loss: 2.346399\n",
      "Epoch: 15786 \tTraining Loss: 1.743520 \tValidation Loss: 2.345816\n",
      "Epoch: 15787 \tTraining Loss: 1.722213 \tValidation Loss: 2.346161\n",
      "Epoch: 15788 \tTraining Loss: 1.716343 \tValidation Loss: 2.346302\n",
      "Epoch: 15789 \tTraining Loss: 1.725048 \tValidation Loss: 2.346639\n",
      "Epoch: 15790 \tTraining Loss: 1.694225 \tValidation Loss: 2.346864\n",
      "Epoch: 15791 \tTraining Loss: 1.701924 \tValidation Loss: 2.346198\n",
      "Epoch: 15792 \tTraining Loss: 1.700796 \tValidation Loss: 2.346398\n",
      "Epoch: 15793 \tTraining Loss: 1.707802 \tValidation Loss: 2.346583\n",
      "Epoch: 15794 \tTraining Loss: 1.720293 \tValidation Loss: 2.346663\n",
      "Epoch: 15795 \tTraining Loss: 1.717142 \tValidation Loss: 2.346577\n",
      "Epoch: 15796 \tTraining Loss: 1.740510 \tValidation Loss: 2.346161\n",
      "Epoch: 15797 \tTraining Loss: 1.722194 \tValidation Loss: 2.346778\n",
      "Epoch: 15798 \tTraining Loss: 1.705926 \tValidation Loss: 2.346851\n",
      "Epoch: 15799 \tTraining Loss: 1.681754 \tValidation Loss: 2.346732\n",
      "Epoch: 15800 \tTraining Loss: 1.722799 \tValidation Loss: 2.346457\n",
      "Epoch: 15801 \tTraining Loss: 1.707932 \tValidation Loss: 2.346674\n",
      "Epoch: 15802 \tTraining Loss: 1.736024 \tValidation Loss: 2.346118\n",
      "Epoch: 15803 \tTraining Loss: 1.709746 \tValidation Loss: 2.346890\n",
      "Epoch: 15804 \tTraining Loss: 1.691000 \tValidation Loss: 2.347489\n",
      "Epoch: 15805 \tTraining Loss: 1.722164 \tValidation Loss: 2.347530\n",
      "Epoch: 15806 \tTraining Loss: 1.727340 \tValidation Loss: 2.346771\n",
      "Epoch: 15807 \tTraining Loss: 1.685224 \tValidation Loss: 2.346382\n",
      "Epoch: 15808 \tTraining Loss: 1.704936 \tValidation Loss: 2.346029\n",
      "Epoch: 15809 \tTraining Loss: 1.710549 \tValidation Loss: 2.346700\n",
      "Epoch: 15810 \tTraining Loss: 1.714218 \tValidation Loss: 2.346691\n",
      "Epoch: 15811 \tTraining Loss: 1.712133 \tValidation Loss: 2.346592\n",
      "Epoch: 15812 \tTraining Loss: 1.708129 \tValidation Loss: 2.346355\n",
      "Epoch: 15813 \tTraining Loss: 1.717853 \tValidation Loss: 2.346152\n",
      "Epoch: 15814 \tTraining Loss: 1.682282 \tValidation Loss: 2.346302\n",
      "Epoch: 15815 \tTraining Loss: 1.724965 \tValidation Loss: 2.346619\n",
      "Epoch: 15816 \tTraining Loss: 1.707931 \tValidation Loss: 2.346385\n",
      "Epoch: 15817 \tTraining Loss: 1.688234 \tValidation Loss: 2.346294\n",
      "Epoch: 15818 \tTraining Loss: 1.691230 \tValidation Loss: 2.346847\n",
      "Epoch: 15819 \tTraining Loss: 1.704311 \tValidation Loss: 2.346898\n",
      "Epoch: 15820 \tTraining Loss: 1.712568 \tValidation Loss: 2.346647\n",
      "Epoch: 15821 \tTraining Loss: 1.715873 \tValidation Loss: 2.346688\n",
      "Epoch: 15822 \tTraining Loss: 1.680477 \tValidation Loss: 2.347072\n",
      "Epoch: 15823 \tTraining Loss: 1.732715 \tValidation Loss: 2.347370\n",
      "Epoch: 15824 \tTraining Loss: 1.711365 \tValidation Loss: 2.347390\n",
      "Epoch: 15825 \tTraining Loss: 1.701509 \tValidation Loss: 2.347045\n",
      "Epoch: 15826 \tTraining Loss: 1.704188 \tValidation Loss: 2.346729\n",
      "Epoch: 15827 \tTraining Loss: 1.713648 \tValidation Loss: 2.346320\n",
      "Epoch: 15828 \tTraining Loss: 1.670175 \tValidation Loss: 2.347079\n",
      "Epoch: 15829 \tTraining Loss: 1.718017 \tValidation Loss: 2.347023\n",
      "Epoch: 15830 \tTraining Loss: 1.689340 \tValidation Loss: 2.346890\n",
      "Epoch: 15831 \tTraining Loss: 1.689890 \tValidation Loss: 2.347246\n",
      "Epoch: 15832 \tTraining Loss: 1.690602 \tValidation Loss: 2.346866\n",
      "Epoch: 15833 \tTraining Loss: 1.707399 \tValidation Loss: 2.346646\n",
      "Epoch: 15834 \tTraining Loss: 1.721669 \tValidation Loss: 2.346447\n",
      "Epoch: 15835 \tTraining Loss: 1.708211 \tValidation Loss: 2.347037\n",
      "Epoch: 15836 \tTraining Loss: 1.719234 \tValidation Loss: 2.347359\n",
      "Epoch: 15837 \tTraining Loss: 1.705328 \tValidation Loss: 2.347028\n",
      "Epoch: 15838 \tTraining Loss: 1.691864 \tValidation Loss: 2.347209\n",
      "Epoch: 15839 \tTraining Loss: 1.711870 \tValidation Loss: 2.346745\n",
      "Epoch: 15840 \tTraining Loss: 1.692281 \tValidation Loss: 2.347136\n",
      "Epoch: 15841 \tTraining Loss: 1.757420 \tValidation Loss: 2.347062\n",
      "Epoch: 15842 \tTraining Loss: 1.700751 \tValidation Loss: 2.347283\n",
      "Epoch: 15843 \tTraining Loss: 1.755949 \tValidation Loss: 2.347228\n",
      "Epoch: 15844 \tTraining Loss: 1.733765 \tValidation Loss: 2.347435\n",
      "Epoch: 15845 \tTraining Loss: 1.735548 \tValidation Loss: 2.346624\n",
      "Epoch: 15846 \tTraining Loss: 1.714233 \tValidation Loss: 2.347054\n",
      "Epoch: 15847 \tTraining Loss: 1.684397 \tValidation Loss: 2.347057\n",
      "Epoch: 15848 \tTraining Loss: 1.692685 \tValidation Loss: 2.347627\n",
      "Epoch: 15849 \tTraining Loss: 1.723652 \tValidation Loss: 2.347755\n",
      "Epoch: 15850 \tTraining Loss: 1.679924 \tValidation Loss: 2.347233\n",
      "Epoch: 15851 \tTraining Loss: 1.721871 \tValidation Loss: 2.347169\n",
      "Epoch: 15852 \tTraining Loss: 1.693596 \tValidation Loss: 2.347399\n",
      "Epoch: 15853 \tTraining Loss: 1.679545 \tValidation Loss: 2.347249\n",
      "Epoch: 15854 \tTraining Loss: 1.713497 \tValidation Loss: 2.346929\n",
      "Epoch: 15855 \tTraining Loss: 1.690884 \tValidation Loss: 2.346957\n",
      "Epoch: 15856 \tTraining Loss: 1.710802 \tValidation Loss: 2.347141\n",
      "Epoch: 15857 \tTraining Loss: 1.705457 \tValidation Loss: 2.347089\n",
      "Epoch: 15858 \tTraining Loss: 1.687001 \tValidation Loss: 2.346683\n",
      "Epoch: 15859 \tTraining Loss: 1.750858 \tValidation Loss: 2.346571\n",
      "Epoch: 15860 \tTraining Loss: 1.687117 \tValidation Loss: 2.346807\n",
      "Epoch: 15861 \tTraining Loss: 1.689365 \tValidation Loss: 2.346607\n",
      "Epoch: 15862 \tTraining Loss: 1.702273 \tValidation Loss: 2.346573\n",
      "Epoch: 15863 \tTraining Loss: 1.703141 \tValidation Loss: 2.346932\n",
      "Epoch: 15864 \tTraining Loss: 1.733093 \tValidation Loss: 2.347121\n",
      "Epoch: 15865 \tTraining Loss: 1.697924 \tValidation Loss: 2.346774\n",
      "Epoch: 15866 \tTraining Loss: 1.692380 \tValidation Loss: 2.347217\n",
      "Epoch: 15867 \tTraining Loss: 1.689574 \tValidation Loss: 2.346972\n",
      "Epoch: 15868 \tTraining Loss: 1.698301 \tValidation Loss: 2.347141\n",
      "Epoch: 15869 \tTraining Loss: 1.674803 \tValidation Loss: 2.347512\n",
      "Epoch: 15870 \tTraining Loss: 1.684981 \tValidation Loss: 2.347219\n",
      "Epoch: 15871 \tTraining Loss: 1.696793 \tValidation Loss: 2.347615\n",
      "Epoch: 15872 \tTraining Loss: 1.719295 \tValidation Loss: 2.346967\n",
      "Epoch: 15873 \tTraining Loss: 1.694275 \tValidation Loss: 2.347396\n",
      "Epoch: 15874 \tTraining Loss: 1.719262 \tValidation Loss: 2.347180\n",
      "Epoch: 15875 \tTraining Loss: 1.724981 \tValidation Loss: 2.347697\n",
      "Epoch: 15876 \tTraining Loss: 1.708437 \tValidation Loss: 2.347967\n",
      "Epoch: 15877 \tTraining Loss: 1.706016 \tValidation Loss: 2.347086\n",
      "Epoch: 15878 \tTraining Loss: 1.710179 \tValidation Loss: 2.347186\n",
      "Epoch: 15879 \tTraining Loss: 1.676456 \tValidation Loss: 2.347189\n",
      "Epoch: 15880 \tTraining Loss: 1.700892 \tValidation Loss: 2.347220\n",
      "Epoch: 15881 \tTraining Loss: 1.728716 \tValidation Loss: 2.346561\n",
      "Epoch: 15882 \tTraining Loss: 1.700156 \tValidation Loss: 2.347090\n",
      "Epoch: 15883 \tTraining Loss: 1.703863 \tValidation Loss: 2.347774\n",
      "Epoch: 15884 \tTraining Loss: 1.706903 \tValidation Loss: 2.347776\n",
      "Epoch: 15885 \tTraining Loss: 1.711302 \tValidation Loss: 2.348027\n",
      "Epoch: 15886 \tTraining Loss: 1.718150 \tValidation Loss: 2.347915\n",
      "Epoch: 15887 \tTraining Loss: 1.717142 \tValidation Loss: 2.347393\n",
      "Epoch: 15888 \tTraining Loss: 1.696223 \tValidation Loss: 2.347114\n",
      "Epoch: 15889 \tTraining Loss: 1.705301 \tValidation Loss: 2.347131\n",
      "Epoch: 15890 \tTraining Loss: 1.682921 \tValidation Loss: 2.347070\n",
      "Epoch: 15891 \tTraining Loss: 1.693597 \tValidation Loss: 2.347854\n",
      "Epoch: 15892 \tTraining Loss: 1.710551 \tValidation Loss: 2.347195\n",
      "Epoch: 15893 \tTraining Loss: 1.659039 \tValidation Loss: 2.347301\n",
      "Epoch: 15894 \tTraining Loss: 1.704892 \tValidation Loss: 2.347421\n",
      "Epoch: 15895 \tTraining Loss: 1.692457 \tValidation Loss: 2.347671\n",
      "Epoch: 15896 \tTraining Loss: 1.696493 \tValidation Loss: 2.347481\n",
      "Epoch: 15897 \tTraining Loss: 1.686839 \tValidation Loss: 2.346954\n",
      "Epoch: 15898 \tTraining Loss: 1.719432 \tValidation Loss: 2.347248\n",
      "Epoch: 15899 \tTraining Loss: 1.692747 \tValidation Loss: 2.347044\n",
      "Epoch: 15900 \tTraining Loss: 1.668820 \tValidation Loss: 2.347070\n",
      "Epoch: 15901 \tTraining Loss: 1.706235 \tValidation Loss: 2.347509\n",
      "Epoch: 15902 \tTraining Loss: 1.688390 \tValidation Loss: 2.347306\n",
      "Epoch: 15903 \tTraining Loss: 1.697380 \tValidation Loss: 2.346713\n",
      "Epoch: 15904 \tTraining Loss: 1.756837 \tValidation Loss: 2.346675\n",
      "Epoch: 15905 \tTraining Loss: 1.719992 \tValidation Loss: 2.346574\n",
      "Epoch: 15906 \tTraining Loss: 1.728915 \tValidation Loss: 2.346673\n",
      "Epoch: 15907 \tTraining Loss: 1.701795 \tValidation Loss: 2.346874\n",
      "Epoch: 15908 \tTraining Loss: 1.689690 \tValidation Loss: 2.347229\n",
      "Epoch: 15909 \tTraining Loss: 1.686295 \tValidation Loss: 2.347374\n",
      "Epoch: 15910 \tTraining Loss: 1.711127 \tValidation Loss: 2.347302\n",
      "Epoch: 15911 \tTraining Loss: 1.698014 \tValidation Loss: 2.347884\n",
      "Epoch: 15912 \tTraining Loss: 1.726120 \tValidation Loss: 2.347454\n",
      "Epoch: 15913 \tTraining Loss: 1.743597 \tValidation Loss: 2.347310\n",
      "Epoch: 15914 \tTraining Loss: 1.712540 \tValidation Loss: 2.346822\n",
      "Epoch: 15915 \tTraining Loss: 1.690298 \tValidation Loss: 2.346730\n",
      "Epoch: 15916 \tTraining Loss: 1.725411 \tValidation Loss: 2.346451\n",
      "Epoch: 15917 \tTraining Loss: 1.697149 \tValidation Loss: 2.346679\n",
      "Epoch: 15918 \tTraining Loss: 1.685814 \tValidation Loss: 2.347011\n",
      "Epoch: 15919 \tTraining Loss: 1.703880 \tValidation Loss: 2.347118\n",
      "Epoch: 15920 \tTraining Loss: 1.718872 \tValidation Loss: 2.346910\n",
      "Epoch: 15921 \tTraining Loss: 1.714771 \tValidation Loss: 2.347347\n",
      "Epoch: 15922 \tTraining Loss: 1.743382 \tValidation Loss: 2.346682\n",
      "Epoch: 15923 \tTraining Loss: 1.726211 \tValidation Loss: 2.346997\n",
      "Epoch: 15924 \tTraining Loss: 1.687224 \tValidation Loss: 2.347378\n",
      "Epoch: 15925 \tTraining Loss: 1.677266 \tValidation Loss: 2.347617\n",
      "Epoch: 15926 \tTraining Loss: 1.733272 \tValidation Loss: 2.347199\n",
      "Epoch: 15927 \tTraining Loss: 1.724081 \tValidation Loss: 2.346891\n",
      "Epoch: 15928 \tTraining Loss: 1.664381 \tValidation Loss: 2.347328\n",
      "Epoch: 15929 \tTraining Loss: 1.702031 \tValidation Loss: 2.346789\n",
      "Epoch: 15930 \tTraining Loss: 1.666312 \tValidation Loss: 2.347191\n",
      "Epoch: 15931 \tTraining Loss: 1.718164 \tValidation Loss: 2.347064\n",
      "Epoch: 15932 \tTraining Loss: 1.737383 \tValidation Loss: 2.347304\n",
      "Epoch: 15933 \tTraining Loss: 1.758729 \tValidation Loss: 2.347506\n",
      "Epoch: 15934 \tTraining Loss: 1.733306 \tValidation Loss: 2.347207\n",
      "Epoch: 15935 \tTraining Loss: 1.717795 \tValidation Loss: 2.346884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15936 \tTraining Loss: 1.711913 \tValidation Loss: 2.346949\n",
      "Epoch: 15937 \tTraining Loss: 1.693058 \tValidation Loss: 2.347016\n",
      "Epoch: 15938 \tTraining Loss: 1.696944 \tValidation Loss: 2.347246\n",
      "Epoch: 15939 \tTraining Loss: 1.694635 \tValidation Loss: 2.347369\n",
      "Epoch: 15940 \tTraining Loss: 1.683880 \tValidation Loss: 2.347775\n",
      "Epoch: 15941 \tTraining Loss: 1.693875 \tValidation Loss: 2.347611\n",
      "Epoch: 15942 \tTraining Loss: 1.678270 \tValidation Loss: 2.347720\n",
      "Epoch: 15943 \tTraining Loss: 1.678105 \tValidation Loss: 2.347817\n",
      "Epoch: 15944 \tTraining Loss: 1.691926 \tValidation Loss: 2.347627\n",
      "Epoch: 15945 \tTraining Loss: 1.685642 \tValidation Loss: 2.347666\n",
      "Epoch: 15946 \tTraining Loss: 1.729987 \tValidation Loss: 2.347715\n",
      "Epoch: 15947 \tTraining Loss: 1.717842 \tValidation Loss: 2.347154\n",
      "Epoch: 15948 \tTraining Loss: 1.698146 \tValidation Loss: 2.347469\n",
      "Epoch: 15949 \tTraining Loss: 1.738218 \tValidation Loss: 2.347579\n",
      "Epoch: 15950 \tTraining Loss: 1.678554 \tValidation Loss: 2.347858\n",
      "Epoch: 15951 \tTraining Loss: 1.674492 \tValidation Loss: 2.347804\n",
      "Epoch: 15952 \tTraining Loss: 1.677781 \tValidation Loss: 2.347799\n",
      "Epoch: 15953 \tTraining Loss: 1.705755 \tValidation Loss: 2.348170\n",
      "Epoch: 15954 \tTraining Loss: 1.694072 \tValidation Loss: 2.347969\n",
      "Epoch: 15955 \tTraining Loss: 1.698826 \tValidation Loss: 2.347534\n",
      "Epoch: 15956 \tTraining Loss: 1.729045 \tValidation Loss: 2.347890\n",
      "Epoch: 15957 \tTraining Loss: 1.699110 \tValidation Loss: 2.348145\n",
      "Epoch: 15958 \tTraining Loss: 1.716630 \tValidation Loss: 2.348196\n",
      "Epoch: 15959 \tTraining Loss: 1.698878 \tValidation Loss: 2.347573\n",
      "Epoch: 15960 \tTraining Loss: 1.721110 \tValidation Loss: 2.347396\n",
      "Epoch: 15961 \tTraining Loss: 1.691630 \tValidation Loss: 2.348112\n",
      "Epoch: 15962 \tTraining Loss: 1.689364 \tValidation Loss: 2.347779\n",
      "Epoch: 15963 \tTraining Loss: 1.675058 \tValidation Loss: 2.347957\n",
      "Epoch: 15964 \tTraining Loss: 1.675529 \tValidation Loss: 2.348823\n",
      "Epoch: 15965 \tTraining Loss: 1.698443 \tValidation Loss: 2.348884\n",
      "Epoch: 15966 \tTraining Loss: 1.683893 \tValidation Loss: 2.348513\n",
      "Epoch: 15967 \tTraining Loss: 1.690815 \tValidation Loss: 2.348411\n",
      "Epoch: 15968 \tTraining Loss: 1.696779 \tValidation Loss: 2.348118\n",
      "Epoch: 15969 \tTraining Loss: 1.715558 \tValidation Loss: 2.348077\n",
      "Epoch: 15970 \tTraining Loss: 1.680822 \tValidation Loss: 2.347814\n",
      "Epoch: 15971 \tTraining Loss: 1.693566 \tValidation Loss: 2.347937\n",
      "Epoch: 15972 \tTraining Loss: 1.726278 \tValidation Loss: 2.347554\n",
      "Epoch: 15973 \tTraining Loss: 1.680460 \tValidation Loss: 2.347937\n",
      "Epoch: 15974 \tTraining Loss: 1.705509 \tValidation Loss: 2.347996\n",
      "Epoch: 15975 \tTraining Loss: 1.725981 \tValidation Loss: 2.347240\n",
      "Epoch: 15976 \tTraining Loss: 1.696742 \tValidation Loss: 2.347177\n",
      "Epoch: 15977 \tTraining Loss: 1.742142 \tValidation Loss: 2.347394\n",
      "Epoch: 15978 \tTraining Loss: 1.700609 \tValidation Loss: 2.347757\n",
      "Epoch: 15979 \tTraining Loss: 1.748550 \tValidation Loss: 2.347807\n",
      "Epoch: 15980 \tTraining Loss: 1.691259 \tValidation Loss: 2.348103\n",
      "Epoch: 15981 \tTraining Loss: 1.707188 \tValidation Loss: 2.347612\n",
      "Epoch: 15982 \tTraining Loss: 1.700776 \tValidation Loss: 2.348109\n",
      "Epoch: 15983 \tTraining Loss: 1.751075 \tValidation Loss: 2.347720\n",
      "Epoch: 15984 \tTraining Loss: 1.706180 \tValidation Loss: 2.347590\n",
      "Epoch: 15985 \tTraining Loss: 1.709422 \tValidation Loss: 2.347902\n",
      "Epoch: 15986 \tTraining Loss: 1.678109 \tValidation Loss: 2.348203\n",
      "Epoch: 15987 \tTraining Loss: 1.724916 \tValidation Loss: 2.347879\n",
      "Epoch: 15988 \tTraining Loss: 1.707550 \tValidation Loss: 2.348256\n",
      "Epoch: 15989 \tTraining Loss: 1.671834 \tValidation Loss: 2.348392\n",
      "Epoch: 15990 \tTraining Loss: 1.720325 \tValidation Loss: 2.347960\n",
      "Epoch: 15991 \tTraining Loss: 1.692844 \tValidation Loss: 2.348006\n",
      "Epoch: 15992 \tTraining Loss: 1.715358 \tValidation Loss: 2.347796\n",
      "Epoch: 15993 \tTraining Loss: 1.683756 \tValidation Loss: 2.348142\n",
      "Epoch: 15994 \tTraining Loss: 1.687288 \tValidation Loss: 2.348018\n",
      "Epoch: 15995 \tTraining Loss: 1.705944 \tValidation Loss: 2.348470\n",
      "Epoch: 15996 \tTraining Loss: 1.691364 \tValidation Loss: 2.347948\n",
      "Epoch: 15997 \tTraining Loss: 1.679911 \tValidation Loss: 2.348124\n",
      "Epoch: 15998 \tTraining Loss: 1.668557 \tValidation Loss: 2.347849\n",
      "Epoch: 15999 \tTraining Loss: 1.688645 \tValidation Loss: 2.347968\n",
      "Epoch: 16000 \tTraining Loss: 1.661462 \tValidation Loss: 2.347689\n",
      "Epoch: 16001 \tTraining Loss: 1.672300 \tValidation Loss: 2.347819\n",
      "Epoch: 16002 \tTraining Loss: 1.671235 \tValidation Loss: 2.348745\n",
      "Epoch: 16003 \tTraining Loss: 1.643730 \tValidation Loss: 2.348350\n",
      "Epoch: 16004 \tTraining Loss: 1.731464 \tValidation Loss: 2.348432\n",
      "Epoch: 16005 \tTraining Loss: 1.677094 \tValidation Loss: 2.348831\n",
      "Epoch: 16006 \tTraining Loss: 1.683511 \tValidation Loss: 2.348698\n",
      "Epoch: 16007 \tTraining Loss: 1.713836 \tValidation Loss: 2.348238\n",
      "Epoch: 16008 \tTraining Loss: 1.693694 \tValidation Loss: 2.348301\n",
      "Epoch: 16009 \tTraining Loss: 1.722739 \tValidation Loss: 2.348118\n",
      "Epoch: 16010 \tTraining Loss: 1.684049 \tValidation Loss: 2.348210\n",
      "Epoch: 16011 \tTraining Loss: 1.681777 \tValidation Loss: 2.348185\n",
      "Epoch: 16012 \tTraining Loss: 1.689820 \tValidation Loss: 2.347973\n",
      "Epoch: 16013 \tTraining Loss: 1.689399 \tValidation Loss: 2.348170\n",
      "Epoch: 16014 \tTraining Loss: 1.689897 \tValidation Loss: 2.348433\n",
      "Epoch: 16015 \tTraining Loss: 1.735778 \tValidation Loss: 2.348337\n",
      "Epoch: 16016 \tTraining Loss: 1.701342 \tValidation Loss: 2.348062\n",
      "Epoch: 16017 \tTraining Loss: 1.704678 \tValidation Loss: 2.348948\n",
      "Epoch: 16018 \tTraining Loss: 1.649960 \tValidation Loss: 2.348843\n",
      "Epoch: 16019 \tTraining Loss: 1.672300 \tValidation Loss: 2.348926\n",
      "Epoch: 16020 \tTraining Loss: 1.723381 \tValidation Loss: 2.348521\n",
      "Epoch: 16021 \tTraining Loss: 1.677608 \tValidation Loss: 2.348690\n",
      "Epoch: 16022 \tTraining Loss: 1.694802 \tValidation Loss: 2.348531\n",
      "Epoch: 16023 \tTraining Loss: 1.709967 \tValidation Loss: 2.348498\n",
      "Epoch: 16024 \tTraining Loss: 1.675452 \tValidation Loss: 2.348533\n",
      "Epoch: 16025 \tTraining Loss: 1.690513 \tValidation Loss: 2.348393\n",
      "Epoch: 16026 \tTraining Loss: 1.678449 \tValidation Loss: 2.348349\n",
      "Epoch: 16027 \tTraining Loss: 1.684179 \tValidation Loss: 2.348581\n",
      "Epoch: 16028 \tTraining Loss: 1.673122 \tValidation Loss: 2.348873\n",
      "Epoch: 16029 \tTraining Loss: 1.674955 \tValidation Loss: 2.348965\n",
      "Epoch: 16030 \tTraining Loss: 1.704232 \tValidation Loss: 2.349155\n",
      "Epoch: 16031 \tTraining Loss: 1.665186 \tValidation Loss: 2.349512\n",
      "Epoch: 16032 \tTraining Loss: 1.718271 \tValidation Loss: 2.348834\n",
      "Epoch: 16033 \tTraining Loss: 1.694342 \tValidation Loss: 2.348860\n",
      "Epoch: 16034 \tTraining Loss: 1.715233 \tValidation Loss: 2.348819\n",
      "Epoch: 16035 \tTraining Loss: 1.715359 \tValidation Loss: 2.349110\n",
      "Epoch: 16036 \tTraining Loss: 1.691168 \tValidation Loss: 2.348879\n",
      "Epoch: 16037 \tTraining Loss: 1.696571 \tValidation Loss: 2.349109\n",
      "Epoch: 16038 \tTraining Loss: 1.700669 \tValidation Loss: 2.348652\n",
      "Epoch: 16039 \tTraining Loss: 1.715683 \tValidation Loss: 2.348438\n",
      "Epoch: 16040 \tTraining Loss: 1.693993 \tValidation Loss: 2.348356\n",
      "Epoch: 16041 \tTraining Loss: 1.691625 \tValidation Loss: 2.348629\n",
      "Epoch: 16042 \tTraining Loss: 1.714052 \tValidation Loss: 2.348321\n",
      "Epoch: 16043 \tTraining Loss: 1.701803 \tValidation Loss: 2.348022\n",
      "Epoch: 16044 \tTraining Loss: 1.709128 \tValidation Loss: 2.348120\n",
      "Epoch: 16045 \tTraining Loss: 1.714403 \tValidation Loss: 2.348389\n",
      "Epoch: 16046 \tTraining Loss: 1.693122 \tValidation Loss: 2.348459\n",
      "Epoch: 16047 \tTraining Loss: 1.716269 \tValidation Loss: 2.348317\n",
      "Epoch: 16048 \tTraining Loss: 1.725204 \tValidation Loss: 2.347599\n",
      "Epoch: 16049 \tTraining Loss: 1.718833 \tValidation Loss: 2.347615\n",
      "Epoch: 16050 \tTraining Loss: 1.650259 \tValidation Loss: 2.348017\n",
      "Epoch: 16051 \tTraining Loss: 1.701312 \tValidation Loss: 2.348186\n",
      "Epoch: 16052 \tTraining Loss: 1.706188 \tValidation Loss: 2.347997\n",
      "Epoch: 16053 \tTraining Loss: 1.672159 \tValidation Loss: 2.347982\n",
      "Epoch: 16054 \tTraining Loss: 1.684148 \tValidation Loss: 2.348496\n",
      "Epoch: 16055 \tTraining Loss: 1.689468 \tValidation Loss: 2.348827\n",
      "Epoch: 16056 \tTraining Loss: 1.694771 \tValidation Loss: 2.348569\n",
      "Epoch: 16057 \tTraining Loss: 1.702280 \tValidation Loss: 2.349063\n",
      "Epoch: 16058 \tTraining Loss: 1.711670 \tValidation Loss: 2.348833\n",
      "Epoch: 16059 \tTraining Loss: 1.699880 \tValidation Loss: 2.348752\n",
      "Epoch: 16060 \tTraining Loss: 1.711702 \tValidation Loss: 2.348187\n",
      "Epoch: 16061 \tTraining Loss: 1.681049 \tValidation Loss: 2.348256\n",
      "Epoch: 16062 \tTraining Loss: 1.696396 \tValidation Loss: 2.348229\n",
      "Epoch: 16063 \tTraining Loss: 1.673663 \tValidation Loss: 2.348386\n",
      "Epoch: 16064 \tTraining Loss: 1.700876 \tValidation Loss: 2.348401\n",
      "Epoch: 16065 \tTraining Loss: 1.713008 \tValidation Loss: 2.348618\n",
      "Epoch: 16066 \tTraining Loss: 1.661458 \tValidation Loss: 2.348807\n",
      "Epoch: 16067 \tTraining Loss: 1.722281 \tValidation Loss: 2.348162\n",
      "Epoch: 16068 \tTraining Loss: 1.657232 \tValidation Loss: 2.348973\n",
      "Epoch: 16069 \tTraining Loss: 1.723883 \tValidation Loss: 2.349096\n",
      "Epoch: 16070 \tTraining Loss: 1.690654 \tValidation Loss: 2.348558\n",
      "Epoch: 16071 \tTraining Loss: 1.688895 \tValidation Loss: 2.349030\n",
      "Epoch: 16072 \tTraining Loss: 1.662497 \tValidation Loss: 2.349083\n",
      "Epoch: 16073 \tTraining Loss: 1.685942 \tValidation Loss: 2.348993\n",
      "Epoch: 16074 \tTraining Loss: 1.654552 \tValidation Loss: 2.349068\n",
      "Epoch: 16075 \tTraining Loss: 1.666266 \tValidation Loss: 2.349392\n",
      "Epoch: 16076 \tTraining Loss: 1.715302 \tValidation Loss: 2.349612\n",
      "Epoch: 16077 \tTraining Loss: 1.723118 \tValidation Loss: 2.349061\n",
      "Epoch: 16078 \tTraining Loss: 1.662959 \tValidation Loss: 2.349264\n",
      "Epoch: 16079 \tTraining Loss: 1.690508 \tValidation Loss: 2.349459\n",
      "Epoch: 16080 \tTraining Loss: 1.685355 \tValidation Loss: 2.349580\n",
      "Epoch: 16081 \tTraining Loss: 1.665178 \tValidation Loss: 2.349690\n",
      "Epoch: 16082 \tTraining Loss: 1.678478 \tValidation Loss: 2.349356\n",
      "Epoch: 16083 \tTraining Loss: 1.661685 \tValidation Loss: 2.349559\n",
      "Epoch: 16084 \tTraining Loss: 1.706846 \tValidation Loss: 2.349494\n",
      "Epoch: 16085 \tTraining Loss: 1.693040 \tValidation Loss: 2.349190\n",
      "Epoch: 16086 \tTraining Loss: 1.702292 \tValidation Loss: 2.348807\n",
      "Epoch: 16087 \tTraining Loss: 1.719786 \tValidation Loss: 2.349094\n",
      "Epoch: 16088 \tTraining Loss: 1.686233 \tValidation Loss: 2.348973\n",
      "Epoch: 16089 \tTraining Loss: 1.697950 \tValidation Loss: 2.348982\n",
      "Epoch: 16090 \tTraining Loss: 1.707635 \tValidation Loss: 2.349382\n",
      "Epoch: 16091 \tTraining Loss: 1.700750 \tValidation Loss: 2.349025\n",
      "Epoch: 16092 \tTraining Loss: 1.699604 \tValidation Loss: 2.348986\n",
      "Epoch: 16093 \tTraining Loss: 1.717347 \tValidation Loss: 2.348544\n",
      "Epoch: 16094 \tTraining Loss: 1.701421 \tValidation Loss: 2.348380\n",
      "Epoch: 16095 \tTraining Loss: 1.733640 \tValidation Loss: 2.348643\n",
      "Epoch: 16096 \tTraining Loss: 1.693725 \tValidation Loss: 2.348874\n",
      "Epoch: 16097 \tTraining Loss: 1.701598 \tValidation Loss: 2.348523\n",
      "Epoch: 16098 \tTraining Loss: 1.695671 \tValidation Loss: 2.349080\n",
      "Epoch: 16099 \tTraining Loss: 1.674335 \tValidation Loss: 2.348998\n",
      "Epoch: 16100 \tTraining Loss: 1.680789 \tValidation Loss: 2.348936\n",
      "Epoch: 16101 \tTraining Loss: 1.677828 \tValidation Loss: 2.349285\n",
      "Epoch: 16102 \tTraining Loss: 1.704608 \tValidation Loss: 2.349298\n",
      "Epoch: 16103 \tTraining Loss: 1.683517 \tValidation Loss: 2.349437\n",
      "Epoch: 16104 \tTraining Loss: 1.716977 \tValidation Loss: 2.349058\n",
      "Epoch: 16105 \tTraining Loss: 1.720841 \tValidation Loss: 2.348782\n",
      "Epoch: 16106 \tTraining Loss: 1.683263 \tValidation Loss: 2.349542\n",
      "Epoch: 16107 \tTraining Loss: 1.703437 \tValidation Loss: 2.349285\n",
      "Epoch: 16108 \tTraining Loss: 1.689502 \tValidation Loss: 2.348611\n",
      "Epoch: 16109 \tTraining Loss: 1.691024 \tValidation Loss: 2.348472\n",
      "Epoch: 16110 \tTraining Loss: 1.703160 \tValidation Loss: 2.348336\n",
      "Epoch: 16111 \tTraining Loss: 1.699521 \tValidation Loss: 2.349062\n",
      "Epoch: 16112 \tTraining Loss: 1.680450 \tValidation Loss: 2.348980\n",
      "Epoch: 16113 \tTraining Loss: 1.660888 \tValidation Loss: 2.349163\n",
      "Epoch: 16114 \tTraining Loss: 1.671537 \tValidation Loss: 2.348999\n",
      "Epoch: 16115 \tTraining Loss: 1.737969 \tValidation Loss: 2.348967\n",
      "Epoch: 16116 \tTraining Loss: 1.671357 \tValidation Loss: 2.349301\n",
      "Epoch: 16117 \tTraining Loss: 1.692721 \tValidation Loss: 2.349183\n",
      "Epoch: 16118 \tTraining Loss: 1.713288 \tValidation Loss: 2.349007\n",
      "Epoch: 16119 \tTraining Loss: 1.663324 \tValidation Loss: 2.349146\n",
      "Epoch: 16120 \tTraining Loss: 1.708562 \tValidation Loss: 2.348866\n",
      "Epoch: 16121 \tTraining Loss: 1.682372 \tValidation Loss: 2.349353\n",
      "Epoch: 16122 \tTraining Loss: 1.676302 \tValidation Loss: 2.349030\n",
      "Epoch: 16123 \tTraining Loss: 1.693830 \tValidation Loss: 2.349457\n",
      "Epoch: 16124 \tTraining Loss: 1.729454 \tValidation Loss: 2.349202\n",
      "Epoch: 16125 \tTraining Loss: 1.716009 \tValidation Loss: 2.348811\n",
      "Epoch: 16126 \tTraining Loss: 1.680142 \tValidation Loss: 2.348986\n",
      "Epoch: 16127 \tTraining Loss: 1.688272 \tValidation Loss: 2.349266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16128 \tTraining Loss: 1.709107 \tValidation Loss: 2.348978\n",
      "Epoch: 16129 \tTraining Loss: 1.730815 \tValidation Loss: 2.348552\n",
      "Epoch: 16130 \tTraining Loss: 1.672720 \tValidation Loss: 2.348539\n",
      "Epoch: 16131 \tTraining Loss: 1.738884 \tValidation Loss: 2.348523\n",
      "Epoch: 16132 \tTraining Loss: 1.703949 \tValidation Loss: 2.348809\n",
      "Epoch: 16133 \tTraining Loss: 1.684316 \tValidation Loss: 2.348189\n",
      "Epoch: 16134 \tTraining Loss: 1.725653 \tValidation Loss: 2.348597\n",
      "Epoch: 16135 \tTraining Loss: 1.683969 \tValidation Loss: 2.348806\n",
      "Epoch: 16136 \tTraining Loss: 1.686346 \tValidation Loss: 2.348475\n",
      "Epoch: 16137 \tTraining Loss: 1.689594 \tValidation Loss: 2.348985\n",
      "Epoch: 16138 \tTraining Loss: 1.692182 \tValidation Loss: 2.349215\n",
      "Epoch: 16139 \tTraining Loss: 1.717692 \tValidation Loss: 2.349394\n",
      "Epoch: 16140 \tTraining Loss: 1.711132 \tValidation Loss: 2.349158\n",
      "Epoch: 16141 \tTraining Loss: 1.704172 \tValidation Loss: 2.349305\n",
      "Epoch: 16142 \tTraining Loss: 1.680422 \tValidation Loss: 2.349447\n",
      "Epoch: 16143 \tTraining Loss: 1.699782 \tValidation Loss: 2.349115\n",
      "Epoch: 16144 \tTraining Loss: 1.730502 \tValidation Loss: 2.348514\n",
      "Epoch: 16145 \tTraining Loss: 1.694211 \tValidation Loss: 2.348839\n",
      "Epoch: 16146 \tTraining Loss: 1.684655 \tValidation Loss: 2.349741\n",
      "Epoch: 16147 \tTraining Loss: 1.687063 \tValidation Loss: 2.349653\n",
      "Epoch: 16148 \tTraining Loss: 1.677225 \tValidation Loss: 2.349754\n",
      "Epoch: 16149 \tTraining Loss: 1.732057 \tValidation Loss: 2.349545\n",
      "Epoch: 16150 \tTraining Loss: 1.665012 \tValidation Loss: 2.349801\n",
      "Epoch: 16151 \tTraining Loss: 1.690438 \tValidation Loss: 2.349604\n",
      "Epoch: 16152 \tTraining Loss: 1.686395 \tValidation Loss: 2.349729\n",
      "Epoch: 16153 \tTraining Loss: 1.704652 \tValidation Loss: 2.349621\n",
      "Epoch: 16154 \tTraining Loss: 1.657218 \tValidation Loss: 2.349679\n",
      "Epoch: 16155 \tTraining Loss: 1.700680 \tValidation Loss: 2.349823\n",
      "Epoch: 16156 \tTraining Loss: 1.737905 \tValidation Loss: 2.349067\n",
      "Epoch: 16157 \tTraining Loss: 1.718794 \tValidation Loss: 2.349454\n",
      "Epoch: 16158 \tTraining Loss: 1.692875 \tValidation Loss: 2.348835\n",
      "Epoch: 16159 \tTraining Loss: 1.728183 \tValidation Loss: 2.348104\n",
      "Epoch: 16160 \tTraining Loss: 1.670160 \tValidation Loss: 2.349081\n",
      "Epoch: 16161 \tTraining Loss: 1.696280 \tValidation Loss: 2.348850\n",
      "Epoch: 16162 \tTraining Loss: 1.710250 \tValidation Loss: 2.348590\n",
      "Epoch: 16163 \tTraining Loss: 1.664651 \tValidation Loss: 2.348986\n",
      "Epoch: 16164 \tTraining Loss: 1.726230 \tValidation Loss: 2.348649\n",
      "Epoch: 16165 \tTraining Loss: 1.674319 \tValidation Loss: 2.348990\n",
      "Epoch: 16166 \tTraining Loss: 1.727791 \tValidation Loss: 2.349355\n",
      "Epoch: 16167 \tTraining Loss: 1.692315 \tValidation Loss: 2.349584\n",
      "Epoch: 16168 \tTraining Loss: 1.722980 \tValidation Loss: 2.349034\n",
      "Epoch: 16169 \tTraining Loss: 1.676764 \tValidation Loss: 2.349504\n",
      "Epoch: 16170 \tTraining Loss: 1.712477 \tValidation Loss: 2.349739\n",
      "Epoch: 16171 \tTraining Loss: 1.684416 \tValidation Loss: 2.349626\n",
      "Epoch: 16172 \tTraining Loss: 1.712189 \tValidation Loss: 2.349601\n",
      "Epoch: 16173 \tTraining Loss: 1.688124 \tValidation Loss: 2.349760\n",
      "Epoch: 16174 \tTraining Loss: 1.686031 \tValidation Loss: 2.349794\n",
      "Epoch: 16175 \tTraining Loss: 1.685136 \tValidation Loss: 2.350147\n",
      "Epoch: 16176 \tTraining Loss: 1.710920 \tValidation Loss: 2.350281\n",
      "Epoch: 16177 \tTraining Loss: 1.678681 \tValidation Loss: 2.350120\n",
      "Epoch: 16178 \tTraining Loss: 1.687147 \tValidation Loss: 2.350485\n",
      "Epoch: 16179 \tTraining Loss: 1.695169 \tValidation Loss: 2.349532\n",
      "Epoch: 16180 \tTraining Loss: 1.705813 \tValidation Loss: 2.349710\n",
      "Epoch: 16181 \tTraining Loss: 1.742266 \tValidation Loss: 2.349805\n",
      "Epoch: 16182 \tTraining Loss: 1.688354 \tValidation Loss: 2.349518\n",
      "Epoch: 16183 \tTraining Loss: 1.696981 \tValidation Loss: 2.349060\n",
      "Epoch: 16184 \tTraining Loss: 1.706190 \tValidation Loss: 2.349135\n",
      "Epoch: 16185 \tTraining Loss: 1.667403 \tValidation Loss: 2.349776\n",
      "Epoch: 16186 \tTraining Loss: 1.681452 \tValidation Loss: 2.349559\n",
      "Epoch: 16187 \tTraining Loss: 1.647953 \tValidation Loss: 2.349842\n",
      "Epoch: 16188 \tTraining Loss: 1.671921 \tValidation Loss: 2.350318\n",
      "Epoch: 16189 \tTraining Loss: 1.682513 \tValidation Loss: 2.350505\n",
      "Epoch: 16190 \tTraining Loss: 1.718181 \tValidation Loss: 2.349727\n",
      "Epoch: 16191 \tTraining Loss: 1.708614 \tValidation Loss: 2.350114\n",
      "Epoch: 16192 \tTraining Loss: 1.671424 \tValidation Loss: 2.350034\n",
      "Epoch: 16193 \tTraining Loss: 1.690705 \tValidation Loss: 2.350121\n",
      "Epoch: 16194 \tTraining Loss: 1.701693 \tValidation Loss: 2.349545\n",
      "Epoch: 16195 \tTraining Loss: 1.730985 \tValidation Loss: 2.349482\n",
      "Epoch: 16196 \tTraining Loss: 1.667113 \tValidation Loss: 2.349926\n",
      "Epoch: 16197 \tTraining Loss: 1.728577 \tValidation Loss: 2.350014\n",
      "Epoch: 16198 \tTraining Loss: 1.692386 \tValidation Loss: 2.349933\n",
      "Epoch: 16199 \tTraining Loss: 1.625820 \tValidation Loss: 2.350636\n",
      "Epoch: 16200 \tTraining Loss: 1.691814 \tValidation Loss: 2.350072\n",
      "Epoch: 16201 \tTraining Loss: 1.738362 \tValidation Loss: 2.349418\n",
      "Epoch: 16202 \tTraining Loss: 1.677610 \tValidation Loss: 2.350371\n",
      "Epoch: 16203 \tTraining Loss: 1.667730 \tValidation Loss: 2.349693\n",
      "Epoch: 16204 \tTraining Loss: 1.692512 \tValidation Loss: 2.349411\n",
      "Epoch: 16205 \tTraining Loss: 1.694781 \tValidation Loss: 2.349683\n",
      "Epoch: 16206 \tTraining Loss: 1.678174 \tValidation Loss: 2.349517\n",
      "Epoch: 16207 \tTraining Loss: 1.707808 \tValidation Loss: 2.349858\n",
      "Epoch: 16208 \tTraining Loss: 1.643668 \tValidation Loss: 2.350330\n",
      "Epoch: 16209 \tTraining Loss: 1.687791 \tValidation Loss: 2.350171\n",
      "Epoch: 16210 \tTraining Loss: 1.714844 \tValidation Loss: 2.349795\n",
      "Epoch: 16211 \tTraining Loss: 1.688726 \tValidation Loss: 2.350437\n",
      "Epoch: 16212 \tTraining Loss: 1.699360 \tValidation Loss: 2.349821\n",
      "Epoch: 16213 \tTraining Loss: 1.699469 \tValidation Loss: 2.349495\n",
      "Epoch: 16214 \tTraining Loss: 1.699972 \tValidation Loss: 2.350055\n",
      "Epoch: 16215 \tTraining Loss: 1.667043 \tValidation Loss: 2.350558\n",
      "Epoch: 16216 \tTraining Loss: 1.705412 \tValidation Loss: 2.350063\n",
      "Epoch: 16217 \tTraining Loss: 1.711632 \tValidation Loss: 2.350272\n",
      "Epoch: 16218 \tTraining Loss: 1.661562 \tValidation Loss: 2.350452\n",
      "Epoch: 16219 \tTraining Loss: 1.717324 \tValidation Loss: 2.350628\n",
      "Epoch: 16220 \tTraining Loss: 1.686449 \tValidation Loss: 2.350500\n",
      "Epoch: 16221 \tTraining Loss: 1.712876 \tValidation Loss: 2.350236\n",
      "Epoch: 16222 \tTraining Loss: 1.686030 \tValidation Loss: 2.350032\n",
      "Epoch: 16223 \tTraining Loss: 1.708335 \tValidation Loss: 2.349919\n",
      "Epoch: 16224 \tTraining Loss: 1.696482 \tValidation Loss: 2.349794\n",
      "Epoch: 16225 \tTraining Loss: 1.720863 \tValidation Loss: 2.349773\n",
      "Epoch: 16226 \tTraining Loss: 1.715775 \tValidation Loss: 2.349999\n",
      "Epoch: 16227 \tTraining Loss: 1.670284 \tValidation Loss: 2.350246\n",
      "Epoch: 16228 \tTraining Loss: 1.687329 \tValidation Loss: 2.350526\n",
      "Epoch: 16229 \tTraining Loss: 1.679172 \tValidation Loss: 2.350162\n",
      "Epoch: 16230 \tTraining Loss: 1.677364 \tValidation Loss: 2.350127\n",
      "Epoch: 16231 \tTraining Loss: 1.696685 \tValidation Loss: 2.350163\n",
      "Epoch: 16232 \tTraining Loss: 1.692343 \tValidation Loss: 2.350357\n",
      "Epoch: 16233 \tTraining Loss: 1.695587 \tValidation Loss: 2.350413\n",
      "Epoch: 16234 \tTraining Loss: 1.656082 \tValidation Loss: 2.350474\n",
      "Epoch: 16235 \tTraining Loss: 1.674052 \tValidation Loss: 2.350225\n",
      "Epoch: 16236 \tTraining Loss: 1.701558 \tValidation Loss: 2.350407\n",
      "Epoch: 16237 \tTraining Loss: 1.704487 \tValidation Loss: 2.350335\n",
      "Epoch: 16238 \tTraining Loss: 1.704402 \tValidation Loss: 2.350452\n",
      "Epoch: 16239 \tTraining Loss: 1.651890 \tValidation Loss: 2.350427\n",
      "Epoch: 16240 \tTraining Loss: 1.695932 \tValidation Loss: 2.350141\n",
      "Epoch: 16241 \tTraining Loss: 1.684067 \tValidation Loss: 2.350274\n",
      "Epoch: 16242 \tTraining Loss: 1.679553 \tValidation Loss: 2.350234\n",
      "Epoch: 16243 \tTraining Loss: 1.666042 \tValidation Loss: 2.350680\n",
      "Epoch: 16244 \tTraining Loss: 1.652417 \tValidation Loss: 2.351058\n",
      "Epoch: 16245 \tTraining Loss: 1.649396 \tValidation Loss: 2.351015\n",
      "Epoch: 16246 \tTraining Loss: 1.683258 \tValidation Loss: 2.350740\n",
      "Epoch: 16247 \tTraining Loss: 1.700712 \tValidation Loss: 2.350847\n",
      "Epoch: 16248 \tTraining Loss: 1.734710 \tValidation Loss: 2.350779\n",
      "Epoch: 16249 \tTraining Loss: 1.686907 \tValidation Loss: 2.350439\n",
      "Epoch: 16250 \tTraining Loss: 1.691006 \tValidation Loss: 2.350321\n",
      "Epoch: 16251 \tTraining Loss: 1.662367 \tValidation Loss: 2.351225\n",
      "Epoch: 16252 \tTraining Loss: 1.716775 \tValidation Loss: 2.350699\n",
      "Epoch: 16253 \tTraining Loss: 1.703759 \tValidation Loss: 2.350777\n",
      "Epoch: 16254 \tTraining Loss: 1.681847 \tValidation Loss: 2.350658\n",
      "Epoch: 16255 \tTraining Loss: 1.715506 \tValidation Loss: 2.350532\n",
      "Epoch: 16256 \tTraining Loss: 1.673211 \tValidation Loss: 2.350531\n",
      "Epoch: 16257 \tTraining Loss: 1.699933 \tValidation Loss: 2.350617\n",
      "Epoch: 16258 \tTraining Loss: 1.687114 \tValidation Loss: 2.350686\n",
      "Epoch: 16259 \tTraining Loss: 1.704816 \tValidation Loss: 2.350830\n",
      "Epoch: 16260 \tTraining Loss: 1.689249 \tValidation Loss: 2.350884\n",
      "Epoch: 16261 \tTraining Loss: 1.722507 \tValidation Loss: 2.350641\n",
      "Epoch: 16262 \tTraining Loss: 1.671537 \tValidation Loss: 2.350524\n",
      "Epoch: 16263 \tTraining Loss: 1.674376 \tValidation Loss: 2.350807\n",
      "Epoch: 16264 \tTraining Loss: 1.669417 \tValidation Loss: 2.350594\n",
      "Epoch: 16265 \tTraining Loss: 1.714278 \tValidation Loss: 2.350381\n",
      "Epoch: 16266 \tTraining Loss: 1.672546 \tValidation Loss: 2.350423\n",
      "Epoch: 16267 \tTraining Loss: 1.654286 \tValidation Loss: 2.350843\n",
      "Epoch: 16268 \tTraining Loss: 1.696152 \tValidation Loss: 2.350591\n",
      "Epoch: 16269 \tTraining Loss: 1.696010 \tValidation Loss: 2.351048\n",
      "Epoch: 16270 \tTraining Loss: 1.714086 \tValidation Loss: 2.350871\n",
      "Epoch: 16271 \tTraining Loss: 1.713811 \tValidation Loss: 2.350759\n",
      "Epoch: 16272 \tTraining Loss: 1.679616 \tValidation Loss: 2.351015\n",
      "Epoch: 16273 \tTraining Loss: 1.708892 \tValidation Loss: 2.350996\n",
      "Epoch: 16274 \tTraining Loss: 1.684206 \tValidation Loss: 2.350892\n",
      "Epoch: 16275 \tTraining Loss: 1.711944 \tValidation Loss: 2.350585\n",
      "Epoch: 16276 \tTraining Loss: 1.696294 \tValidation Loss: 2.350118\n",
      "Epoch: 16277 \tTraining Loss: 1.670068 \tValidation Loss: 2.351001\n",
      "Epoch: 16278 \tTraining Loss: 1.683807 \tValidation Loss: 2.350713\n",
      "Epoch: 16279 \tTraining Loss: 1.698441 \tValidation Loss: 2.350756\n",
      "Epoch: 16280 \tTraining Loss: 1.680392 \tValidation Loss: 2.350470\n",
      "Epoch: 16281 \tTraining Loss: 1.737264 \tValidation Loss: 2.349911\n",
      "Epoch: 16282 \tTraining Loss: 1.681993 \tValidation Loss: 2.350314\n",
      "Epoch: 16283 \tTraining Loss: 1.668968 \tValidation Loss: 2.351187\n",
      "Epoch: 16284 \tTraining Loss: 1.681640 \tValidation Loss: 2.350848\n",
      "Epoch: 16285 \tTraining Loss: 1.704609 \tValidation Loss: 2.350840\n",
      "Epoch: 16286 \tTraining Loss: 1.684490 \tValidation Loss: 2.351053\n",
      "Epoch: 16287 \tTraining Loss: 1.662704 \tValidation Loss: 2.350800\n",
      "Epoch: 16288 \tTraining Loss: 1.682499 \tValidation Loss: 2.350889\n",
      "Epoch: 16289 \tTraining Loss: 1.720850 \tValidation Loss: 2.350864\n",
      "Epoch: 16290 \tTraining Loss: 1.674387 \tValidation Loss: 2.350950\n",
      "Epoch: 16291 \tTraining Loss: 1.661909 \tValidation Loss: 2.351515\n",
      "Epoch: 16292 \tTraining Loss: 1.710196 \tValidation Loss: 2.351675\n",
      "Epoch: 16293 \tTraining Loss: 1.700300 \tValidation Loss: 2.351461\n",
      "Epoch: 16294 \tTraining Loss: 1.719657 \tValidation Loss: 2.351318\n",
      "Epoch: 16295 \tTraining Loss: 1.746527 \tValidation Loss: 2.350914\n",
      "Epoch: 16296 \tTraining Loss: 1.714475 \tValidation Loss: 2.350961\n",
      "Epoch: 16297 \tTraining Loss: 1.692977 \tValidation Loss: 2.350936\n",
      "Epoch: 16298 \tTraining Loss: 1.677767 \tValidation Loss: 2.350932\n",
      "Epoch: 16299 \tTraining Loss: 1.700243 \tValidation Loss: 2.350491\n",
      "Epoch: 16300 \tTraining Loss: 1.675130 \tValidation Loss: 2.350736\n",
      "Epoch: 16301 \tTraining Loss: 1.643308 \tValidation Loss: 2.350903\n",
      "Epoch: 16302 \tTraining Loss: 1.678594 \tValidation Loss: 2.350976\n",
      "Epoch: 16303 \tTraining Loss: 1.702207 \tValidation Loss: 2.350931\n",
      "Epoch: 16304 \tTraining Loss: 1.659439 \tValidation Loss: 2.351590\n",
      "Epoch: 16305 \tTraining Loss: 1.704978 \tValidation Loss: 2.351256\n",
      "Epoch: 16306 \tTraining Loss: 1.650289 \tValidation Loss: 2.351232\n",
      "Epoch: 16307 \tTraining Loss: 1.713572 \tValidation Loss: 2.351125\n",
      "Epoch: 16308 \tTraining Loss: 1.688648 \tValidation Loss: 2.350936\n",
      "Epoch: 16309 \tTraining Loss: 1.712385 \tValidation Loss: 2.350368\n",
      "Epoch: 16310 \tTraining Loss: 1.690576 \tValidation Loss: 2.350668\n",
      "Epoch: 16311 \tTraining Loss: 1.665233 \tValidation Loss: 2.350862\n",
      "Epoch: 16312 \tTraining Loss: 1.675268 \tValidation Loss: 2.350918\n",
      "Epoch: 16313 \tTraining Loss: 1.677414 \tValidation Loss: 2.350976\n",
      "Epoch: 16314 \tTraining Loss: 1.673848 \tValidation Loss: 2.351557\n",
      "Epoch: 16315 \tTraining Loss: 1.665687 \tValidation Loss: 2.351287\n",
      "Epoch: 16316 \tTraining Loss: 1.674321 \tValidation Loss: 2.351019\n",
      "Epoch: 16317 \tTraining Loss: 1.694514 \tValidation Loss: 2.351553\n",
      "Epoch: 16318 \tTraining Loss: 1.678283 \tValidation Loss: 2.351381\n",
      "Epoch: 16319 \tTraining Loss: 1.673079 \tValidation Loss: 2.351553\n",
      "Epoch: 16320 \tTraining Loss: 1.676473 \tValidation Loss: 2.351116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16321 \tTraining Loss: 1.723689 \tValidation Loss: 2.351103\n",
      "Epoch: 16322 \tTraining Loss: 1.663313 \tValidation Loss: 2.351285\n",
      "Epoch: 16323 \tTraining Loss: 1.697314 \tValidation Loss: 2.351430\n",
      "Epoch: 16324 \tTraining Loss: 1.700158 \tValidation Loss: 2.351275\n",
      "Epoch: 16325 \tTraining Loss: 1.690448 \tValidation Loss: 2.351068\n",
      "Epoch: 16326 \tTraining Loss: 1.678213 \tValidation Loss: 2.351131\n",
      "Epoch: 16327 \tTraining Loss: 1.643474 \tValidation Loss: 2.351526\n",
      "Epoch: 16328 \tTraining Loss: 1.689566 \tValidation Loss: 2.351588\n",
      "Epoch: 16329 \tTraining Loss: 1.684637 \tValidation Loss: 2.351784\n",
      "Epoch: 16330 \tTraining Loss: 1.639741 \tValidation Loss: 2.351975\n",
      "Epoch: 16331 \tTraining Loss: 1.698992 \tValidation Loss: 2.351630\n",
      "Epoch: 16332 \tTraining Loss: 1.690985 \tValidation Loss: 2.351536\n",
      "Epoch: 16333 \tTraining Loss: 1.683116 \tValidation Loss: 2.351813\n",
      "Epoch: 16334 \tTraining Loss: 1.668222 \tValidation Loss: 2.351854\n",
      "Epoch: 16335 \tTraining Loss: 1.691331 \tValidation Loss: 2.350881\n",
      "Epoch: 16336 \tTraining Loss: 1.667337 \tValidation Loss: 2.351433\n",
      "Epoch: 16337 \tTraining Loss: 1.680128 \tValidation Loss: 2.351868\n",
      "Epoch: 16338 \tTraining Loss: 1.665081 \tValidation Loss: 2.351775\n",
      "Epoch: 16339 \tTraining Loss: 1.713379 \tValidation Loss: 2.351555\n",
      "Epoch: 16340 \tTraining Loss: 1.645288 \tValidation Loss: 2.351815\n",
      "Epoch: 16341 \tTraining Loss: 1.716597 \tValidation Loss: 2.351529\n",
      "Epoch: 16342 \tTraining Loss: 1.694037 \tValidation Loss: 2.351191\n",
      "Epoch: 16343 \tTraining Loss: 1.694779 \tValidation Loss: 2.351092\n",
      "Epoch: 16344 \tTraining Loss: 1.694039 \tValidation Loss: 2.350964\n",
      "Epoch: 16345 \tTraining Loss: 1.689390 \tValidation Loss: 2.351674\n",
      "Epoch: 16346 \tTraining Loss: 1.717233 \tValidation Loss: 2.351708\n",
      "Epoch: 16347 \tTraining Loss: 1.656637 \tValidation Loss: 2.351805\n",
      "Epoch: 16348 \tTraining Loss: 1.706292 \tValidation Loss: 2.351688\n",
      "Epoch: 16349 \tTraining Loss: 1.677566 \tValidation Loss: 2.351565\n",
      "Epoch: 16350 \tTraining Loss: 1.686360 \tValidation Loss: 2.351275\n",
      "Epoch: 16351 \tTraining Loss: 1.667737 \tValidation Loss: 2.351576\n",
      "Epoch: 16352 \tTraining Loss: 1.682165 \tValidation Loss: 2.352009\n",
      "Epoch: 16353 \tTraining Loss: 1.707152 \tValidation Loss: 2.351506\n",
      "Epoch: 16354 \tTraining Loss: 1.665730 \tValidation Loss: 2.351729\n",
      "Epoch: 16355 \tTraining Loss: 1.688975 \tValidation Loss: 2.352009\n",
      "Epoch: 16356 \tTraining Loss: 1.673683 \tValidation Loss: 2.351981\n",
      "Epoch: 16357 \tTraining Loss: 1.686128 \tValidation Loss: 2.351888\n",
      "Epoch: 16358 \tTraining Loss: 1.643833 \tValidation Loss: 2.352367\n",
      "Epoch: 16359 \tTraining Loss: 1.671798 \tValidation Loss: 2.352042\n",
      "Epoch: 16360 \tTraining Loss: 1.687715 \tValidation Loss: 2.351556\n",
      "Epoch: 16361 \tTraining Loss: 1.661697 \tValidation Loss: 2.351758\n",
      "Epoch: 16362 \tTraining Loss: 1.691527 \tValidation Loss: 2.351558\n",
      "Epoch: 16363 \tTraining Loss: 1.696516 \tValidation Loss: 2.351484\n",
      "Epoch: 16364 \tTraining Loss: 1.683412 \tValidation Loss: 2.351575\n",
      "Epoch: 16365 \tTraining Loss: 1.709324 \tValidation Loss: 2.351762\n",
      "Epoch: 16366 \tTraining Loss: 1.716277 \tValidation Loss: 2.351641\n",
      "Epoch: 16367 \tTraining Loss: 1.664768 \tValidation Loss: 2.351526\n",
      "Epoch: 16368 \tTraining Loss: 1.703812 \tValidation Loss: 2.351501\n",
      "Epoch: 16369 \tTraining Loss: 1.678896 \tValidation Loss: 2.351660\n",
      "Epoch: 16370 \tTraining Loss: 1.727967 \tValidation Loss: 2.351566\n",
      "Epoch: 16371 \tTraining Loss: 1.723276 \tValidation Loss: 2.351503\n",
      "Epoch: 16372 \tTraining Loss: 1.658498 \tValidation Loss: 2.351653\n",
      "Epoch: 16373 \tTraining Loss: 1.670785 \tValidation Loss: 2.351639\n",
      "Epoch: 16374 \tTraining Loss: 1.699415 \tValidation Loss: 2.351845\n",
      "Epoch: 16375 \tTraining Loss: 1.690187 \tValidation Loss: 2.351566\n",
      "Epoch: 16376 \tTraining Loss: 1.665736 \tValidation Loss: 2.351193\n",
      "Epoch: 16377 \tTraining Loss: 1.681443 \tValidation Loss: 2.351458\n",
      "Epoch: 16378 \tTraining Loss: 1.679620 \tValidation Loss: 2.351678\n",
      "Epoch: 16379 \tTraining Loss: 1.689944 \tValidation Loss: 2.351886\n",
      "Epoch: 16380 \tTraining Loss: 1.699039 \tValidation Loss: 2.351695\n",
      "Epoch: 16381 \tTraining Loss: 1.677295 \tValidation Loss: 2.351226\n",
      "Epoch: 16382 \tTraining Loss: 1.661574 \tValidation Loss: 2.351590\n",
      "Epoch: 16383 \tTraining Loss: 1.707732 \tValidation Loss: 2.351493\n",
      "Epoch: 16384 \tTraining Loss: 1.688980 \tValidation Loss: 2.351429\n",
      "Epoch: 16385 \tTraining Loss: 1.666584 \tValidation Loss: 2.351854\n",
      "Epoch: 16386 \tTraining Loss: 1.713980 \tValidation Loss: 2.351671\n",
      "Epoch: 16387 \tTraining Loss: 1.688184 \tValidation Loss: 2.351608\n",
      "Epoch: 16388 \tTraining Loss: 1.679475 \tValidation Loss: 2.351660\n",
      "Epoch: 16389 \tTraining Loss: 1.671068 \tValidation Loss: 2.351810\n",
      "Epoch: 16390 \tTraining Loss: 1.641538 \tValidation Loss: 2.351341\n",
      "Epoch: 16391 \tTraining Loss: 1.672551 \tValidation Loss: 2.351231\n",
      "Epoch: 16392 \tTraining Loss: 1.680513 \tValidation Loss: 2.351578\n",
      "Epoch: 16393 \tTraining Loss: 1.683325 \tValidation Loss: 2.351620\n",
      "Epoch: 16394 \tTraining Loss: 1.692460 \tValidation Loss: 2.351660\n",
      "Epoch: 16395 \tTraining Loss: 1.684623 \tValidation Loss: 2.351949\n",
      "Epoch: 16396 \tTraining Loss: 1.702227 \tValidation Loss: 2.351512\n",
      "Epoch: 16397 \tTraining Loss: 1.715934 \tValidation Loss: 2.351900\n",
      "Epoch: 16398 \tTraining Loss: 1.656823 \tValidation Loss: 2.352296\n",
      "Epoch: 16399 \tTraining Loss: 1.677462 \tValidation Loss: 2.352103\n",
      "Epoch: 16400 \tTraining Loss: 1.696178 \tValidation Loss: 2.351905\n",
      "Epoch: 16401 \tTraining Loss: 1.687981 \tValidation Loss: 2.352441\n",
      "Epoch: 16402 \tTraining Loss: 1.692704 \tValidation Loss: 2.352170\n",
      "Epoch: 16403 \tTraining Loss: 1.693459 \tValidation Loss: 2.352301\n",
      "Epoch: 16404 \tTraining Loss: 1.703416 \tValidation Loss: 2.351804\n",
      "Epoch: 16405 \tTraining Loss: 1.691514 \tValidation Loss: 2.351992\n",
      "Epoch: 16406 \tTraining Loss: 1.702411 \tValidation Loss: 2.352208\n",
      "Epoch: 16407 \tTraining Loss: 1.653890 \tValidation Loss: 2.352503\n",
      "Epoch: 16408 \tTraining Loss: 1.696022 \tValidation Loss: 2.352149\n",
      "Epoch: 16409 \tTraining Loss: 1.691847 \tValidation Loss: 2.351622\n",
      "Epoch: 16410 \tTraining Loss: 1.685967 \tValidation Loss: 2.352290\n",
      "Epoch: 16411 \tTraining Loss: 1.714190 \tValidation Loss: 2.351637\n",
      "Epoch: 16412 \tTraining Loss: 1.682510 \tValidation Loss: 2.351577\n",
      "Epoch: 16413 \tTraining Loss: 1.688288 \tValidation Loss: 2.352322\n",
      "Epoch: 16414 \tTraining Loss: 1.668856 \tValidation Loss: 2.352370\n",
      "Epoch: 16415 \tTraining Loss: 1.686288 \tValidation Loss: 2.351954\n",
      "Epoch: 16416 \tTraining Loss: 1.713650 \tValidation Loss: 2.352013\n",
      "Epoch: 16417 \tTraining Loss: 1.665539 \tValidation Loss: 2.352221\n",
      "Epoch: 16418 \tTraining Loss: 1.650138 \tValidation Loss: 2.352386\n",
      "Epoch: 16419 \tTraining Loss: 1.722212 \tValidation Loss: 2.352152\n",
      "Epoch: 16420 \tTraining Loss: 1.707687 \tValidation Loss: 2.351824\n",
      "Epoch: 16421 \tTraining Loss: 1.662078 \tValidation Loss: 2.352353\n",
      "Epoch: 16422 \tTraining Loss: 1.653096 \tValidation Loss: 2.352268\n",
      "Epoch: 16423 \tTraining Loss: 1.674487 \tValidation Loss: 2.352086\n",
      "Epoch: 16424 \tTraining Loss: 1.676073 \tValidation Loss: 2.351992\n",
      "Epoch: 16425 \tTraining Loss: 1.667095 \tValidation Loss: 2.352642\n",
      "Epoch: 16426 \tTraining Loss: 1.670835 \tValidation Loss: 2.352832\n",
      "Epoch: 16427 \tTraining Loss: 1.660737 \tValidation Loss: 2.352429\n",
      "Epoch: 16428 \tTraining Loss: 1.659772 \tValidation Loss: 2.352518\n",
      "Epoch: 16429 \tTraining Loss: 1.715893 \tValidation Loss: 2.352276\n",
      "Epoch: 16430 \tTraining Loss: 1.686532 \tValidation Loss: 2.351812\n",
      "Epoch: 16431 \tTraining Loss: 1.700876 \tValidation Loss: 2.352467\n",
      "Epoch: 16432 \tTraining Loss: 1.695570 \tValidation Loss: 2.352601\n",
      "Epoch: 16433 \tTraining Loss: 1.628596 \tValidation Loss: 2.352576\n",
      "Epoch: 16434 \tTraining Loss: 1.673862 \tValidation Loss: 2.353315\n",
      "Epoch: 16435 \tTraining Loss: 1.667970 \tValidation Loss: 2.352599\n",
      "Epoch: 16436 \tTraining Loss: 1.706539 \tValidation Loss: 2.352697\n",
      "Epoch: 16437 \tTraining Loss: 1.717699 \tValidation Loss: 2.352619\n",
      "Epoch: 16438 \tTraining Loss: 1.688202 \tValidation Loss: 2.352432\n",
      "Epoch: 16439 \tTraining Loss: 1.690494 \tValidation Loss: 2.352217\n",
      "Epoch: 16440 \tTraining Loss: 1.654898 \tValidation Loss: 2.352236\n",
      "Epoch: 16441 \tTraining Loss: 1.659954 \tValidation Loss: 2.352312\n",
      "Epoch: 16442 \tTraining Loss: 1.690306 \tValidation Loss: 2.351920\n",
      "Epoch: 16443 \tTraining Loss: 1.734630 \tValidation Loss: 2.352109\n",
      "Epoch: 16444 \tTraining Loss: 1.658034 \tValidation Loss: 2.352556\n",
      "Epoch: 16445 \tTraining Loss: 1.667272 \tValidation Loss: 2.352234\n",
      "Epoch: 16446 \tTraining Loss: 1.639697 \tValidation Loss: 2.352473\n",
      "Epoch: 16447 \tTraining Loss: 1.694351 \tValidation Loss: 2.352391\n",
      "Epoch: 16448 \tTraining Loss: 1.697711 \tValidation Loss: 2.352746\n",
      "Epoch: 16449 \tTraining Loss: 1.650207 \tValidation Loss: 2.353400\n",
      "Epoch: 16450 \tTraining Loss: 1.663866 \tValidation Loss: 2.352927\n",
      "Epoch: 16451 \tTraining Loss: 1.671677 \tValidation Loss: 2.352673\n",
      "Epoch: 16452 \tTraining Loss: 1.707148 \tValidation Loss: 2.352404\n",
      "Epoch: 16453 \tTraining Loss: 1.726552 \tValidation Loss: 2.352384\n",
      "Epoch: 16454 \tTraining Loss: 1.669599 \tValidation Loss: 2.351830\n",
      "Epoch: 16455 \tTraining Loss: 1.669195 \tValidation Loss: 2.352335\n",
      "Epoch: 16456 \tTraining Loss: 1.686337 \tValidation Loss: 2.352773\n",
      "Epoch: 16457 \tTraining Loss: 1.709146 \tValidation Loss: 2.352190\n",
      "Epoch: 16458 \tTraining Loss: 1.695425 \tValidation Loss: 2.352308\n",
      "Epoch: 16459 \tTraining Loss: 1.675599 \tValidation Loss: 2.352316\n",
      "Epoch: 16460 \tTraining Loss: 1.675411 \tValidation Loss: 2.352475\n",
      "Epoch: 16461 \tTraining Loss: 1.705513 \tValidation Loss: 2.352598\n",
      "Epoch: 16462 \tTraining Loss: 1.706430 \tValidation Loss: 2.352600\n",
      "Epoch: 16463 \tTraining Loss: 1.665685 \tValidation Loss: 2.352629\n",
      "Epoch: 16464 \tTraining Loss: 1.724535 \tValidation Loss: 2.352172\n",
      "Epoch: 16465 \tTraining Loss: 1.673607 \tValidation Loss: 2.352895\n",
      "Epoch: 16466 \tTraining Loss: 1.669935 \tValidation Loss: 2.352396\n",
      "Epoch: 16467 \tTraining Loss: 1.661268 \tValidation Loss: 2.352601\n",
      "Epoch: 16468 \tTraining Loss: 1.665700 \tValidation Loss: 2.352482\n",
      "Epoch: 16469 \tTraining Loss: 1.689812 \tValidation Loss: 2.352279\n",
      "Epoch: 16470 \tTraining Loss: 1.648741 \tValidation Loss: 2.352762\n",
      "Epoch: 16471 \tTraining Loss: 1.650560 \tValidation Loss: 2.353261\n",
      "Epoch: 16472 \tTraining Loss: 1.677775 \tValidation Loss: 2.352740\n",
      "Epoch: 16473 \tTraining Loss: 1.652234 \tValidation Loss: 2.353355\n",
      "Epoch: 16474 \tTraining Loss: 1.681794 \tValidation Loss: 2.352562\n",
      "Epoch: 16475 \tTraining Loss: 1.678652 \tValidation Loss: 2.353022\n",
      "Epoch: 16476 \tTraining Loss: 1.676081 \tValidation Loss: 2.353515\n",
      "Epoch: 16477 \tTraining Loss: 1.660522 \tValidation Loss: 2.353561\n",
      "Epoch: 16478 \tTraining Loss: 1.680882 \tValidation Loss: 2.353667\n",
      "Epoch: 16479 \tTraining Loss: 1.670317 \tValidation Loss: 2.353629\n",
      "Epoch: 16480 \tTraining Loss: 1.694189 \tValidation Loss: 2.353243\n",
      "Epoch: 16481 \tTraining Loss: 1.679993 \tValidation Loss: 2.353041\n",
      "Epoch: 16482 \tTraining Loss: 1.667612 \tValidation Loss: 2.353363\n",
      "Epoch: 16483 \tTraining Loss: 1.635345 \tValidation Loss: 2.353649\n",
      "Epoch: 16484 \tTraining Loss: 1.670109 \tValidation Loss: 2.353517\n",
      "Epoch: 16485 \tTraining Loss: 1.669532 \tValidation Loss: 2.352957\n",
      "Epoch: 16486 \tTraining Loss: 1.686895 \tValidation Loss: 2.353216\n",
      "Epoch: 16487 \tTraining Loss: 1.674474 \tValidation Loss: 2.352745\n",
      "Epoch: 16488 \tTraining Loss: 1.737323 \tValidation Loss: 2.352763\n",
      "Epoch: 16489 \tTraining Loss: 1.674405 \tValidation Loss: 2.352790\n",
      "Epoch: 16490 \tTraining Loss: 1.683447 \tValidation Loss: 2.352650\n",
      "Epoch: 16491 \tTraining Loss: 1.673355 \tValidation Loss: 2.353306\n",
      "Epoch: 16492 \tTraining Loss: 1.660423 \tValidation Loss: 2.353140\n",
      "Epoch: 16493 \tTraining Loss: 1.688730 \tValidation Loss: 2.352272\n",
      "Epoch: 16494 \tTraining Loss: 1.703927 \tValidation Loss: 2.352870\n",
      "Epoch: 16495 \tTraining Loss: 1.665060 \tValidation Loss: 2.353245\n",
      "Epoch: 16496 \tTraining Loss: 1.648221 \tValidation Loss: 2.353055\n",
      "Epoch: 16497 \tTraining Loss: 1.659354 \tValidation Loss: 2.352768\n",
      "Epoch: 16498 \tTraining Loss: 1.674012 \tValidation Loss: 2.353053\n",
      "Epoch: 16499 \tTraining Loss: 1.717370 \tValidation Loss: 2.352896\n",
      "Epoch: 16500 \tTraining Loss: 1.687991 \tValidation Loss: 2.352923\n",
      "Epoch: 16501 \tTraining Loss: 1.716796 \tValidation Loss: 2.352693\n",
      "Epoch: 16502 \tTraining Loss: 1.678282 \tValidation Loss: 2.352853\n",
      "Epoch: 16503 \tTraining Loss: 1.691578 \tValidation Loss: 2.352939\n",
      "Epoch: 16504 \tTraining Loss: 1.664491 \tValidation Loss: 2.353132\n",
      "Epoch: 16505 \tTraining Loss: 1.688528 \tValidation Loss: 2.352497\n",
      "Epoch: 16506 \tTraining Loss: 1.686707 \tValidation Loss: 2.352764\n",
      "Epoch: 16507 \tTraining Loss: 1.664509 \tValidation Loss: 2.352912\n",
      "Epoch: 16508 \tTraining Loss: 1.691029 \tValidation Loss: 2.353092\n",
      "Epoch: 16509 \tTraining Loss: 1.698451 \tValidation Loss: 2.352952\n",
      "Epoch: 16510 \tTraining Loss: 1.741158 \tValidation Loss: 2.352576\n",
      "Epoch: 16511 \tTraining Loss: 1.693416 \tValidation Loss: 2.352901\n",
      "Epoch: 16512 \tTraining Loss: 1.683558 \tValidation Loss: 2.353167\n",
      "Epoch: 16513 \tTraining Loss: 1.687523 \tValidation Loss: 2.352964\n",
      "Epoch: 16514 \tTraining Loss: 1.683099 \tValidation Loss: 2.352941\n",
      "Epoch: 16515 \tTraining Loss: 1.709829 \tValidation Loss: 2.352634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16516 \tTraining Loss: 1.677828 \tValidation Loss: 2.352765\n",
      "Epoch: 16517 \tTraining Loss: 1.683632 \tValidation Loss: 2.353390\n",
      "Epoch: 16518 \tTraining Loss: 1.666311 \tValidation Loss: 2.353401\n",
      "Epoch: 16519 \tTraining Loss: 1.675659 \tValidation Loss: 2.353735\n",
      "Epoch: 16520 \tTraining Loss: 1.685350 \tValidation Loss: 2.352940\n",
      "Epoch: 16521 \tTraining Loss: 1.660895 \tValidation Loss: 2.353436\n",
      "Epoch: 16522 \tTraining Loss: 1.679213 \tValidation Loss: 2.353205\n",
      "Epoch: 16523 \tTraining Loss: 1.681514 \tValidation Loss: 2.353229\n",
      "Epoch: 16524 \tTraining Loss: 1.696602 \tValidation Loss: 2.352969\n",
      "Epoch: 16525 \tTraining Loss: 1.671525 \tValidation Loss: 2.352593\n",
      "Epoch: 16526 \tTraining Loss: 1.658769 \tValidation Loss: 2.353062\n",
      "Epoch: 16527 \tTraining Loss: 1.681785 \tValidation Loss: 2.353246\n",
      "Epoch: 16528 \tTraining Loss: 1.651853 \tValidation Loss: 2.352625\n",
      "Epoch: 16529 \tTraining Loss: 1.653619 \tValidation Loss: 2.353462\n",
      "Epoch: 16530 \tTraining Loss: 1.675667 \tValidation Loss: 2.353337\n",
      "Epoch: 16531 \tTraining Loss: 1.684648 \tValidation Loss: 2.353770\n",
      "Epoch: 16532 \tTraining Loss: 1.667254 \tValidation Loss: 2.353705\n",
      "Epoch: 16533 \tTraining Loss: 1.700482 \tValidation Loss: 2.353667\n",
      "Epoch: 16534 \tTraining Loss: 1.666095 \tValidation Loss: 2.353856\n",
      "Epoch: 16535 \tTraining Loss: 1.703167 \tValidation Loss: 2.353249\n",
      "Epoch: 16536 \tTraining Loss: 1.662675 \tValidation Loss: 2.353713\n",
      "Epoch: 16537 \tTraining Loss: 1.682921 \tValidation Loss: 2.353863\n",
      "Epoch: 16538 \tTraining Loss: 1.665992 \tValidation Loss: 2.354369\n",
      "Epoch: 16539 \tTraining Loss: 1.646691 \tValidation Loss: 2.353982\n",
      "Epoch: 16540 \tTraining Loss: 1.653059 \tValidation Loss: 2.353724\n",
      "Epoch: 16541 \tTraining Loss: 1.687321 \tValidation Loss: 2.353277\n",
      "Epoch: 16542 \tTraining Loss: 1.677382 \tValidation Loss: 2.353431\n",
      "Epoch: 16543 \tTraining Loss: 1.697464 \tValidation Loss: 2.353199\n",
      "Epoch: 16544 \tTraining Loss: 1.720438 \tValidation Loss: 2.352765\n",
      "Epoch: 16545 \tTraining Loss: 1.668911 \tValidation Loss: 2.353176\n",
      "Epoch: 16546 \tTraining Loss: 1.692910 \tValidation Loss: 2.353337\n",
      "Epoch: 16547 \tTraining Loss: 1.718230 \tValidation Loss: 2.353250\n",
      "Epoch: 16548 \tTraining Loss: 1.654180 \tValidation Loss: 2.353636\n",
      "Epoch: 16549 \tTraining Loss: 1.696032 \tValidation Loss: 2.353192\n",
      "Epoch: 16550 \tTraining Loss: 1.671608 \tValidation Loss: 2.353041\n",
      "Epoch: 16551 \tTraining Loss: 1.644794 \tValidation Loss: 2.353631\n",
      "Epoch: 16552 \tTraining Loss: 1.690847 \tValidation Loss: 2.353291\n",
      "Epoch: 16553 \tTraining Loss: 1.695697 \tValidation Loss: 2.353201\n",
      "Epoch: 16554 \tTraining Loss: 1.711875 \tValidation Loss: 2.353262\n",
      "Epoch: 16555 \tTraining Loss: 1.637880 \tValidation Loss: 2.353009\n",
      "Epoch: 16556 \tTraining Loss: 1.666731 \tValidation Loss: 2.353513\n",
      "Epoch: 16557 \tTraining Loss: 1.692264 \tValidation Loss: 2.352941\n",
      "Epoch: 16558 \tTraining Loss: 1.649472 \tValidation Loss: 2.353219\n",
      "Epoch: 16559 \tTraining Loss: 1.694464 \tValidation Loss: 2.353491\n",
      "Epoch: 16560 \tTraining Loss: 1.650499 \tValidation Loss: 2.353810\n",
      "Epoch: 16561 \tTraining Loss: 1.684182 \tValidation Loss: 2.352938\n",
      "Epoch: 16562 \tTraining Loss: 1.690235 \tValidation Loss: 2.353357\n",
      "Epoch: 16563 \tTraining Loss: 1.699466 \tValidation Loss: 2.353030\n",
      "Epoch: 16564 \tTraining Loss: 1.723428 \tValidation Loss: 2.352986\n",
      "Epoch: 16565 \tTraining Loss: 1.682243 \tValidation Loss: 2.353044\n",
      "Epoch: 16566 \tTraining Loss: 1.637744 \tValidation Loss: 2.354320\n",
      "Epoch: 16567 \tTraining Loss: 1.694010 \tValidation Loss: 2.353731\n",
      "Epoch: 16568 \tTraining Loss: 1.648225 \tValidation Loss: 2.353676\n",
      "Epoch: 16569 \tTraining Loss: 1.694792 \tValidation Loss: 2.353912\n",
      "Epoch: 16570 \tTraining Loss: 1.641693 \tValidation Loss: 2.353898\n",
      "Epoch: 16571 \tTraining Loss: 1.711943 \tValidation Loss: 2.353388\n",
      "Epoch: 16572 \tTraining Loss: 1.674009 \tValidation Loss: 2.353380\n",
      "Epoch: 16573 \tTraining Loss: 1.707702 \tValidation Loss: 2.353769\n",
      "Epoch: 16574 \tTraining Loss: 1.664390 \tValidation Loss: 2.353662\n",
      "Epoch: 16575 \tTraining Loss: 1.684599 \tValidation Loss: 2.353733\n",
      "Epoch: 16576 \tTraining Loss: 1.676116 \tValidation Loss: 2.353991\n",
      "Epoch: 16577 \tTraining Loss: 1.676844 \tValidation Loss: 2.353752\n",
      "Epoch: 16578 \tTraining Loss: 1.671402 \tValidation Loss: 2.353680\n",
      "Epoch: 16579 \tTraining Loss: 1.673759 \tValidation Loss: 2.353879\n",
      "Epoch: 16580 \tTraining Loss: 1.717436 \tValidation Loss: 2.353577\n",
      "Epoch: 16581 \tTraining Loss: 1.657729 \tValidation Loss: 2.353604\n",
      "Epoch: 16582 \tTraining Loss: 1.674975 \tValidation Loss: 2.353987\n",
      "Epoch: 16583 \tTraining Loss: 1.696417 \tValidation Loss: 2.353842\n",
      "Epoch: 16584 \tTraining Loss: 1.653410 \tValidation Loss: 2.353722\n",
      "Epoch: 16585 \tTraining Loss: 1.668223 \tValidation Loss: 2.353570\n",
      "Epoch: 16586 \tTraining Loss: 1.665145 \tValidation Loss: 2.353268\n",
      "Epoch: 16587 \tTraining Loss: 1.707855 \tValidation Loss: 2.353013\n",
      "Epoch: 16588 \tTraining Loss: 1.673232 \tValidation Loss: 2.353175\n",
      "Epoch: 16589 \tTraining Loss: 1.652959 \tValidation Loss: 2.353300\n",
      "Epoch: 16590 \tTraining Loss: 1.643705 \tValidation Loss: 2.352893\n",
      "Epoch: 16591 \tTraining Loss: 1.694637 \tValidation Loss: 2.353156\n",
      "Epoch: 16592 \tTraining Loss: 1.716796 \tValidation Loss: 2.353626\n",
      "Epoch: 16593 \tTraining Loss: 1.710225 \tValidation Loss: 2.353517\n",
      "Epoch: 16594 \tTraining Loss: 1.662522 \tValidation Loss: 2.353366\n",
      "Epoch: 16595 \tTraining Loss: 1.674697 \tValidation Loss: 2.353060\n",
      "Epoch: 16596 \tTraining Loss: 1.675756 \tValidation Loss: 2.353460\n",
      "Epoch: 16597 \tTraining Loss: 1.726375 \tValidation Loss: 2.353228\n",
      "Epoch: 16598 \tTraining Loss: 1.690461 \tValidation Loss: 2.353342\n",
      "Epoch: 16599 \tTraining Loss: 1.682925 \tValidation Loss: 2.353046\n",
      "Epoch: 16600 \tTraining Loss: 1.703970 \tValidation Loss: 2.352952\n",
      "Epoch: 16601 \tTraining Loss: 1.667728 \tValidation Loss: 2.353124\n",
      "Epoch: 16602 \tTraining Loss: 1.703642 \tValidation Loss: 2.353323\n",
      "Epoch: 16603 \tTraining Loss: 1.699423 \tValidation Loss: 2.353355\n",
      "Epoch: 16604 \tTraining Loss: 1.671816 \tValidation Loss: 2.353245\n",
      "Epoch: 16605 \tTraining Loss: 1.690745 \tValidation Loss: 2.353492\n",
      "Epoch: 16606 \tTraining Loss: 1.677648 \tValidation Loss: 2.353329\n",
      "Epoch: 16607 \tTraining Loss: 1.649786 \tValidation Loss: 2.353325\n",
      "Epoch: 16608 \tTraining Loss: 1.684906 \tValidation Loss: 2.354089\n",
      "Epoch: 16609 \tTraining Loss: 1.689954 \tValidation Loss: 2.354005\n",
      "Epoch: 16610 \tTraining Loss: 1.682276 \tValidation Loss: 2.353697\n",
      "Epoch: 16611 \tTraining Loss: 1.676559 \tValidation Loss: 2.354519\n",
      "Epoch: 16612 \tTraining Loss: 1.674122 \tValidation Loss: 2.353710\n",
      "Epoch: 16613 \tTraining Loss: 1.663460 \tValidation Loss: 2.354160\n",
      "Epoch: 16614 \tTraining Loss: 1.700489 \tValidation Loss: 2.354075\n",
      "Epoch: 16615 \tTraining Loss: 1.690148 \tValidation Loss: 2.353487\n",
      "Epoch: 16616 \tTraining Loss: 1.712194 \tValidation Loss: 2.353918\n",
      "Epoch: 16617 \tTraining Loss: 1.659489 \tValidation Loss: 2.353848\n",
      "Epoch: 16618 \tTraining Loss: 1.722407 \tValidation Loss: 2.353879\n",
      "Epoch: 16619 \tTraining Loss: 1.682688 \tValidation Loss: 2.353673\n",
      "Epoch: 16620 \tTraining Loss: 1.674923 \tValidation Loss: 2.353928\n",
      "Epoch: 16621 \tTraining Loss: 1.684468 \tValidation Loss: 2.353163\n",
      "Epoch: 16622 \tTraining Loss: 1.630632 \tValidation Loss: 2.353865\n",
      "Epoch: 16623 \tTraining Loss: 1.661967 \tValidation Loss: 2.354278\n",
      "Epoch: 16624 \tTraining Loss: 1.652424 \tValidation Loss: 2.354210\n",
      "Epoch: 16625 \tTraining Loss: 1.694643 \tValidation Loss: 2.353852\n",
      "Epoch: 16626 \tTraining Loss: 1.686539 \tValidation Loss: 2.353741\n",
      "Epoch: 16627 \tTraining Loss: 1.703219 \tValidation Loss: 2.353437\n",
      "Epoch: 16628 \tTraining Loss: 1.685412 \tValidation Loss: 2.353800\n",
      "Epoch: 16629 \tTraining Loss: 1.690868 \tValidation Loss: 2.353539\n",
      "Epoch: 16630 \tTraining Loss: 1.672361 \tValidation Loss: 2.353718\n",
      "Epoch: 16631 \tTraining Loss: 1.673256 \tValidation Loss: 2.353842\n",
      "Epoch: 16632 \tTraining Loss: 1.665312 \tValidation Loss: 2.353821\n",
      "Epoch: 16633 \tTraining Loss: 1.679377 \tValidation Loss: 2.353634\n",
      "Epoch: 16634 \tTraining Loss: 1.642476 \tValidation Loss: 2.353717\n",
      "Epoch: 16635 \tTraining Loss: 1.693048 \tValidation Loss: 2.353937\n",
      "Epoch: 16636 \tTraining Loss: 1.639734 \tValidation Loss: 2.354055\n",
      "Epoch: 16637 \tTraining Loss: 1.680684 \tValidation Loss: 2.353860\n",
      "Epoch: 16638 \tTraining Loss: 1.672132 \tValidation Loss: 2.353870\n",
      "Epoch: 16639 \tTraining Loss: 1.694155 \tValidation Loss: 2.354563\n",
      "Epoch: 16640 \tTraining Loss: 1.691390 \tValidation Loss: 2.354144\n",
      "Epoch: 16641 \tTraining Loss: 1.657254 \tValidation Loss: 2.354076\n",
      "Epoch: 16642 \tTraining Loss: 1.663890 \tValidation Loss: 2.354359\n",
      "Epoch: 16643 \tTraining Loss: 1.692452 \tValidation Loss: 2.353988\n",
      "Epoch: 16644 \tTraining Loss: 1.698445 \tValidation Loss: 2.354177\n",
      "Epoch: 16645 \tTraining Loss: 1.681691 \tValidation Loss: 2.354276\n",
      "Epoch: 16646 \tTraining Loss: 1.656570 \tValidation Loss: 2.353797\n",
      "Epoch: 16647 \tTraining Loss: 1.684148 \tValidation Loss: 2.354273\n",
      "Epoch: 16648 \tTraining Loss: 1.658209 \tValidation Loss: 2.354550\n",
      "Epoch: 16649 \tTraining Loss: 1.683288 \tValidation Loss: 2.354281\n",
      "Epoch: 16650 \tTraining Loss: 1.661176 \tValidation Loss: 2.354207\n",
      "Epoch: 16651 \tTraining Loss: 1.666608 \tValidation Loss: 2.354078\n",
      "Epoch: 16652 \tTraining Loss: 1.642596 \tValidation Loss: 2.354406\n",
      "Epoch: 16653 \tTraining Loss: 1.672912 \tValidation Loss: 2.354279\n",
      "Epoch: 16654 \tTraining Loss: 1.710644 \tValidation Loss: 2.354145\n",
      "Epoch: 16655 \tTraining Loss: 1.676265 \tValidation Loss: 2.354409\n",
      "Epoch: 16656 \tTraining Loss: 1.714363 \tValidation Loss: 2.354425\n",
      "Epoch: 16657 \tTraining Loss: 1.681117 \tValidation Loss: 2.354131\n",
      "Epoch: 16658 \tTraining Loss: 1.685861 \tValidation Loss: 2.353979\n",
      "Epoch: 16659 \tTraining Loss: 1.674838 \tValidation Loss: 2.354077\n",
      "Epoch: 16660 \tTraining Loss: 1.681229 \tValidation Loss: 2.354148\n",
      "Epoch: 16661 \tTraining Loss: 1.643350 \tValidation Loss: 2.354599\n",
      "Epoch: 16662 \tTraining Loss: 1.629511 \tValidation Loss: 2.354430\n",
      "Epoch: 16663 \tTraining Loss: 1.672066 \tValidation Loss: 2.354326\n",
      "Epoch: 16664 \tTraining Loss: 1.701930 \tValidation Loss: 2.354425\n",
      "Epoch: 16665 \tTraining Loss: 1.658488 \tValidation Loss: 2.354469\n",
      "Epoch: 16666 \tTraining Loss: 1.687798 \tValidation Loss: 2.354209\n",
      "Epoch: 16667 \tTraining Loss: 1.653792 \tValidation Loss: 2.354222\n",
      "Epoch: 16668 \tTraining Loss: 1.661214 \tValidation Loss: 2.354390\n",
      "Epoch: 16669 \tTraining Loss: 1.672052 \tValidation Loss: 2.355010\n",
      "Epoch: 16670 \tTraining Loss: 1.647474 \tValidation Loss: 2.354934\n",
      "Epoch: 16671 \tTraining Loss: 1.669560 \tValidation Loss: 2.355042\n",
      "Epoch: 16672 \tTraining Loss: 1.678347 \tValidation Loss: 2.354114\n",
      "Epoch: 16673 \tTraining Loss: 1.669922 \tValidation Loss: 2.354091\n",
      "Epoch: 16674 \tTraining Loss: 1.654765 \tValidation Loss: 2.354600\n",
      "Epoch: 16675 \tTraining Loss: 1.648907 \tValidation Loss: 2.354660\n",
      "Epoch: 16676 \tTraining Loss: 1.644241 \tValidation Loss: 2.354836\n",
      "Epoch: 16677 \tTraining Loss: 1.690737 \tValidation Loss: 2.354539\n",
      "Epoch: 16678 \tTraining Loss: 1.679261 \tValidation Loss: 2.354657\n",
      "Epoch: 16679 \tTraining Loss: 1.686567 \tValidation Loss: 2.354309\n",
      "Epoch: 16680 \tTraining Loss: 1.685242 \tValidation Loss: 2.354453\n",
      "Epoch: 16681 \tTraining Loss: 1.685661 \tValidation Loss: 2.354935\n",
      "Epoch: 16682 \tTraining Loss: 1.687041 \tValidation Loss: 2.354302\n",
      "Epoch: 16683 \tTraining Loss: 1.651410 \tValidation Loss: 2.354453\n",
      "Epoch: 16684 \tTraining Loss: 1.650126 \tValidation Loss: 2.354455\n",
      "Epoch: 16685 \tTraining Loss: 1.656789 \tValidation Loss: 2.354481\n",
      "Epoch: 16686 \tTraining Loss: 1.685554 \tValidation Loss: 2.354315\n",
      "Epoch: 16687 \tTraining Loss: 1.708788 \tValidation Loss: 2.354795\n",
      "Epoch: 16688 \tTraining Loss: 1.692704 \tValidation Loss: 2.354575\n",
      "Epoch: 16689 \tTraining Loss: 1.674500 \tValidation Loss: 2.355267\n",
      "Epoch: 16690 \tTraining Loss: 1.682529 \tValidation Loss: 2.355348\n",
      "Epoch: 16691 \tTraining Loss: 1.704584 \tValidation Loss: 2.354940\n",
      "Epoch: 16692 \tTraining Loss: 1.648206 \tValidation Loss: 2.355284\n",
      "Epoch: 16693 \tTraining Loss: 1.692020 \tValidation Loss: 2.354912\n",
      "Epoch: 16694 \tTraining Loss: 1.624443 \tValidation Loss: 2.355151\n",
      "Epoch: 16695 \tTraining Loss: 1.690429 \tValidation Loss: 2.355370\n",
      "Epoch: 16696 \tTraining Loss: 1.667621 \tValidation Loss: 2.354918\n",
      "Epoch: 16697 \tTraining Loss: 1.677586 \tValidation Loss: 2.355372\n",
      "Epoch: 16698 \tTraining Loss: 1.657446 \tValidation Loss: 2.354927\n",
      "Epoch: 16699 \tTraining Loss: 1.671282 \tValidation Loss: 2.354834\n",
      "Epoch: 16700 \tTraining Loss: 1.680753 \tValidation Loss: 2.354419\n",
      "Epoch: 16701 \tTraining Loss: 1.676726 \tValidation Loss: 2.354643\n",
      "Epoch: 16702 \tTraining Loss: 1.659442 \tValidation Loss: 2.354846\n",
      "Epoch: 16703 \tTraining Loss: 1.642631 \tValidation Loss: 2.355231\n",
      "Epoch: 16704 \tTraining Loss: 1.646604 \tValidation Loss: 2.355232\n",
      "Epoch: 16705 \tTraining Loss: 1.665072 \tValidation Loss: 2.354927\n",
      "Epoch: 16706 \tTraining Loss: 1.660910 \tValidation Loss: 2.354749\n",
      "Epoch: 16707 \tTraining Loss: 1.676766 \tValidation Loss: 2.354793\n",
      "Epoch: 16708 \tTraining Loss: 1.701214 \tValidation Loss: 2.354305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16709 \tTraining Loss: 1.669479 \tValidation Loss: 2.354365\n",
      "Epoch: 16710 \tTraining Loss: 1.687577 \tValidation Loss: 2.354586\n",
      "Epoch: 16711 \tTraining Loss: 1.672436 \tValidation Loss: 2.354622\n",
      "Epoch: 16712 \tTraining Loss: 1.685713 \tValidation Loss: 2.355143\n",
      "Epoch: 16713 \tTraining Loss: 1.633446 \tValidation Loss: 2.355110\n",
      "Epoch: 16714 \tTraining Loss: 1.644111 \tValidation Loss: 2.355539\n",
      "Epoch: 16715 \tTraining Loss: 1.718551 \tValidation Loss: 2.355305\n",
      "Epoch: 16716 \tTraining Loss: 1.689365 \tValidation Loss: 2.354802\n",
      "Epoch: 16717 \tTraining Loss: 1.673683 \tValidation Loss: 2.354810\n",
      "Epoch: 16718 \tTraining Loss: 1.664169 \tValidation Loss: 2.355405\n",
      "Epoch: 16719 \tTraining Loss: 1.674758 \tValidation Loss: 2.355547\n",
      "Epoch: 16720 \tTraining Loss: 1.674243 \tValidation Loss: 2.355218\n",
      "Epoch: 16721 \tTraining Loss: 1.690778 \tValidation Loss: 2.354919\n",
      "Epoch: 16722 \tTraining Loss: 1.713652 \tValidation Loss: 2.354636\n",
      "Epoch: 16723 \tTraining Loss: 1.653504 \tValidation Loss: 2.354749\n",
      "Epoch: 16724 \tTraining Loss: 1.648434 \tValidation Loss: 2.355290\n",
      "Epoch: 16725 \tTraining Loss: 1.703153 \tValidation Loss: 2.355621\n",
      "Epoch: 16726 \tTraining Loss: 1.677040 \tValidation Loss: 2.355036\n",
      "Epoch: 16727 \tTraining Loss: 1.677242 \tValidation Loss: 2.355042\n",
      "Epoch: 16728 \tTraining Loss: 1.626278 \tValidation Loss: 2.355256\n",
      "Epoch: 16729 \tTraining Loss: 1.655389 \tValidation Loss: 2.355070\n",
      "Epoch: 16730 \tTraining Loss: 1.687173 \tValidation Loss: 2.355361\n",
      "Epoch: 16731 \tTraining Loss: 1.680692 \tValidation Loss: 2.355238\n",
      "Epoch: 16732 \tTraining Loss: 1.686206 \tValidation Loss: 2.355082\n",
      "Epoch: 16733 \tTraining Loss: 1.679149 \tValidation Loss: 2.355128\n",
      "Epoch: 16734 \tTraining Loss: 1.663657 \tValidation Loss: 2.355569\n",
      "Epoch: 16735 \tTraining Loss: 1.677011 \tValidation Loss: 2.355480\n",
      "Epoch: 16736 \tTraining Loss: 1.665808 \tValidation Loss: 2.355258\n",
      "Epoch: 16737 \tTraining Loss: 1.642904 \tValidation Loss: 2.355280\n",
      "Epoch: 16738 \tTraining Loss: 1.665278 \tValidation Loss: 2.354874\n",
      "Epoch: 16739 \tTraining Loss: 1.688167 \tValidation Loss: 2.355196\n",
      "Epoch: 16740 \tTraining Loss: 1.668630 \tValidation Loss: 2.355563\n",
      "Epoch: 16741 \tTraining Loss: 1.659901 \tValidation Loss: 2.355815\n",
      "Epoch: 16742 \tTraining Loss: 1.657326 \tValidation Loss: 2.355290\n",
      "Epoch: 16743 \tTraining Loss: 1.658863 \tValidation Loss: 2.356051\n",
      "Epoch: 16744 \tTraining Loss: 1.642117 \tValidation Loss: 2.355829\n",
      "Epoch: 16745 \tTraining Loss: 1.700793 \tValidation Loss: 2.355395\n",
      "Epoch: 16746 \tTraining Loss: 1.685979 \tValidation Loss: 2.355314\n",
      "Epoch: 16747 \tTraining Loss: 1.657983 \tValidation Loss: 2.355015\n",
      "Epoch: 16748 \tTraining Loss: 1.681389 \tValidation Loss: 2.355403\n",
      "Epoch: 16749 \tTraining Loss: 1.667008 \tValidation Loss: 2.356114\n",
      "Epoch: 16750 \tTraining Loss: 1.668776 \tValidation Loss: 2.355590\n",
      "Epoch: 16751 \tTraining Loss: 1.632366 \tValidation Loss: 2.355598\n",
      "Epoch: 16752 \tTraining Loss: 1.638475 \tValidation Loss: 2.355687\n",
      "Epoch: 16753 \tTraining Loss: 1.649528 \tValidation Loss: 2.356320\n",
      "Epoch: 16754 \tTraining Loss: 1.669931 \tValidation Loss: 2.355976\n",
      "Epoch: 16755 \tTraining Loss: 1.690744 \tValidation Loss: 2.355944\n",
      "Epoch: 16756 \tTraining Loss: 1.640241 \tValidation Loss: 2.356166\n",
      "Epoch: 16757 \tTraining Loss: 1.685232 \tValidation Loss: 2.356095\n",
      "Epoch: 16758 \tTraining Loss: 1.684181 \tValidation Loss: 2.355801\n",
      "Epoch: 16759 \tTraining Loss: 1.690636 \tValidation Loss: 2.355885\n",
      "Epoch: 16760 \tTraining Loss: 1.665035 \tValidation Loss: 2.355546\n",
      "Epoch: 16761 \tTraining Loss: 1.669362 \tValidation Loss: 2.355498\n",
      "Epoch: 16762 \tTraining Loss: 1.669679 \tValidation Loss: 2.355508\n",
      "Epoch: 16763 \tTraining Loss: 1.690131 \tValidation Loss: 2.355354\n",
      "Epoch: 16764 \tTraining Loss: 1.678179 \tValidation Loss: 2.355754\n",
      "Epoch: 16765 \tTraining Loss: 1.639005 \tValidation Loss: 2.356311\n",
      "Epoch: 16766 \tTraining Loss: 1.669954 \tValidation Loss: 2.356162\n",
      "Epoch: 16767 \tTraining Loss: 1.693930 \tValidation Loss: 2.355892\n",
      "Epoch: 16768 \tTraining Loss: 1.650971 \tValidation Loss: 2.355671\n",
      "Epoch: 16769 \tTraining Loss: 1.655070 \tValidation Loss: 2.355544\n",
      "Epoch: 16770 \tTraining Loss: 1.678295 \tValidation Loss: 2.355762\n",
      "Epoch: 16771 \tTraining Loss: 1.698452 \tValidation Loss: 2.356354\n",
      "Epoch: 16772 \tTraining Loss: 1.680361 \tValidation Loss: 2.355769\n",
      "Epoch: 16773 \tTraining Loss: 1.692753 \tValidation Loss: 2.356066\n",
      "Epoch: 16774 \tTraining Loss: 1.661489 \tValidation Loss: 2.356016\n",
      "Epoch: 16775 \tTraining Loss: 1.677172 \tValidation Loss: 2.356350\n",
      "Epoch: 16776 \tTraining Loss: 1.686979 \tValidation Loss: 2.356159\n",
      "Epoch: 16777 \tTraining Loss: 1.667064 \tValidation Loss: 2.355937\n",
      "Epoch: 16778 \tTraining Loss: 1.659153 \tValidation Loss: 2.356184\n",
      "Epoch: 16779 \tTraining Loss: 1.666803 \tValidation Loss: 2.355947\n",
      "Epoch: 16780 \tTraining Loss: 1.652704 \tValidation Loss: 2.356246\n",
      "Epoch: 16781 \tTraining Loss: 1.670988 \tValidation Loss: 2.355392\n",
      "Epoch: 16782 \tTraining Loss: 1.689029 \tValidation Loss: 2.355727\n",
      "Epoch: 16783 \tTraining Loss: 1.656881 \tValidation Loss: 2.355651\n",
      "Epoch: 16784 \tTraining Loss: 1.670760 \tValidation Loss: 2.355302\n",
      "Epoch: 16785 \tTraining Loss: 1.653571 \tValidation Loss: 2.355454\n",
      "Epoch: 16786 \tTraining Loss: 1.683811 \tValidation Loss: 2.354923\n",
      "Epoch: 16787 \tTraining Loss: 1.685217 \tValidation Loss: 2.355386\n",
      "Epoch: 16788 \tTraining Loss: 1.708008 \tValidation Loss: 2.355203\n",
      "Epoch: 16789 \tTraining Loss: 1.626495 \tValidation Loss: 2.355358\n",
      "Epoch: 16790 \tTraining Loss: 1.671091 \tValidation Loss: 2.355230\n",
      "Epoch: 16791 \tTraining Loss: 1.655372 \tValidation Loss: 2.355738\n",
      "Epoch: 16792 \tTraining Loss: 1.675536 \tValidation Loss: 2.355599\n",
      "Epoch: 16793 \tTraining Loss: 1.653207 \tValidation Loss: 2.356069\n",
      "Epoch: 16794 \tTraining Loss: 1.675143 \tValidation Loss: 2.355921\n",
      "Epoch: 16795 \tTraining Loss: 1.685712 \tValidation Loss: 2.355656\n",
      "Epoch: 16796 \tTraining Loss: 1.694688 \tValidation Loss: 2.355400\n",
      "Epoch: 16797 \tTraining Loss: 1.665027 \tValidation Loss: 2.355371\n",
      "Epoch: 16798 \tTraining Loss: 1.674069 \tValidation Loss: 2.355479\n",
      "Epoch: 16799 \tTraining Loss: 1.669410 \tValidation Loss: 2.355492\n",
      "Epoch: 16800 \tTraining Loss: 1.671685 \tValidation Loss: 2.356377\n",
      "Epoch: 16801 \tTraining Loss: 1.675534 \tValidation Loss: 2.356156\n",
      "Epoch: 16802 \tTraining Loss: 1.683725 \tValidation Loss: 2.356044\n",
      "Epoch: 16803 \tTraining Loss: 1.680288 \tValidation Loss: 2.356019\n",
      "Epoch: 16804 \tTraining Loss: 1.674489 \tValidation Loss: 2.356060\n",
      "Epoch: 16805 \tTraining Loss: 1.660217 \tValidation Loss: 2.356056\n",
      "Epoch: 16806 \tTraining Loss: 1.664485 \tValidation Loss: 2.356537\n",
      "Epoch: 16807 \tTraining Loss: 1.665243 \tValidation Loss: 2.356049\n",
      "Epoch: 16808 \tTraining Loss: 1.627839 \tValidation Loss: 2.357184\n",
      "Epoch: 16809 \tTraining Loss: 1.685929 \tValidation Loss: 2.356583\n",
      "Epoch: 16810 \tTraining Loss: 1.680480 \tValidation Loss: 2.356662\n",
      "Epoch: 16811 \tTraining Loss: 1.664512 \tValidation Loss: 2.356476\n",
      "Epoch: 16812 \tTraining Loss: 1.634123 \tValidation Loss: 2.356800\n",
      "Epoch: 16813 \tTraining Loss: 1.641330 \tValidation Loss: 2.357068\n",
      "Epoch: 16814 \tTraining Loss: 1.675147 \tValidation Loss: 2.357156\n",
      "Epoch: 16815 \tTraining Loss: 1.618286 \tValidation Loss: 2.356678\n",
      "Epoch: 16816 \tTraining Loss: 1.661037 \tValidation Loss: 2.356851\n",
      "Epoch: 16817 \tTraining Loss: 1.716863 \tValidation Loss: 2.356053\n",
      "Epoch: 16818 \tTraining Loss: 1.681431 \tValidation Loss: 2.356645\n",
      "Epoch: 16819 \tTraining Loss: 1.649981 \tValidation Loss: 2.356612\n",
      "Epoch: 16820 \tTraining Loss: 1.690412 \tValidation Loss: 2.356050\n",
      "Epoch: 16821 \tTraining Loss: 1.705042 \tValidation Loss: 2.356265\n",
      "Epoch: 16822 \tTraining Loss: 1.680561 \tValidation Loss: 2.356294\n",
      "Epoch: 16823 \tTraining Loss: 1.702374 \tValidation Loss: 2.356688\n",
      "Epoch: 16824 \tTraining Loss: 1.668918 \tValidation Loss: 2.356747\n",
      "Epoch: 16825 \tTraining Loss: 1.686426 \tValidation Loss: 2.356656\n",
      "Epoch: 16826 \tTraining Loss: 1.637081 \tValidation Loss: 2.356955\n",
      "Epoch: 16827 \tTraining Loss: 1.662930 \tValidation Loss: 2.356592\n",
      "Epoch: 16828 \tTraining Loss: 1.661998 \tValidation Loss: 2.356741\n",
      "Epoch: 16829 \tTraining Loss: 1.695460 \tValidation Loss: 2.356404\n",
      "Epoch: 16830 \tTraining Loss: 1.681528 \tValidation Loss: 2.356790\n",
      "Epoch: 16831 \tTraining Loss: 1.699337 \tValidation Loss: 2.356222\n",
      "Epoch: 16832 \tTraining Loss: 1.633490 \tValidation Loss: 2.356747\n",
      "Epoch: 16833 \tTraining Loss: 1.681457 \tValidation Loss: 2.356656\n",
      "Epoch: 16834 \tTraining Loss: 1.641215 \tValidation Loss: 2.356426\n",
      "Epoch: 16835 \tTraining Loss: 1.707851 \tValidation Loss: 2.356246\n",
      "Epoch: 16836 \tTraining Loss: 1.646409 \tValidation Loss: 2.356368\n",
      "Epoch: 16837 \tTraining Loss: 1.676125 \tValidation Loss: 2.356486\n",
      "Epoch: 16838 \tTraining Loss: 1.697048 \tValidation Loss: 2.356317\n",
      "Epoch: 16839 \tTraining Loss: 1.671656 \tValidation Loss: 2.356668\n",
      "Epoch: 16840 \tTraining Loss: 1.660981 \tValidation Loss: 2.356213\n",
      "Epoch: 16841 \tTraining Loss: 1.662356 \tValidation Loss: 2.356199\n",
      "Epoch: 16842 \tTraining Loss: 1.659078 \tValidation Loss: 2.356677\n",
      "Epoch: 16843 \tTraining Loss: 1.685481 \tValidation Loss: 2.356845\n",
      "Epoch: 16844 \tTraining Loss: 1.660738 \tValidation Loss: 2.356845\n",
      "Epoch: 16845 \tTraining Loss: 1.696994 \tValidation Loss: 2.356265\n",
      "Epoch: 16846 \tTraining Loss: 1.693876 \tValidation Loss: 2.356327\n",
      "Epoch: 16847 \tTraining Loss: 1.677875 \tValidation Loss: 2.357017\n",
      "Epoch: 16848 \tTraining Loss: 1.723323 \tValidation Loss: 2.356460\n",
      "Epoch: 16849 \tTraining Loss: 1.653195 \tValidation Loss: 2.356905\n",
      "Epoch: 16850 \tTraining Loss: 1.671727 \tValidation Loss: 2.356654\n",
      "Epoch: 16851 \tTraining Loss: 1.652087 \tValidation Loss: 2.356833\n",
      "Epoch: 16852 \tTraining Loss: 1.686008 \tValidation Loss: 2.356970\n",
      "Epoch: 16853 \tTraining Loss: 1.678457 \tValidation Loss: 2.356873\n",
      "Epoch: 16854 \tTraining Loss: 1.629728 \tValidation Loss: 2.357112\n",
      "Epoch: 16855 \tTraining Loss: 1.668589 \tValidation Loss: 2.357091\n",
      "Epoch: 16856 \tTraining Loss: 1.652039 \tValidation Loss: 2.357178\n",
      "Epoch: 16857 \tTraining Loss: 1.675102 \tValidation Loss: 2.356824\n",
      "Epoch: 16858 \tTraining Loss: 1.664759 \tValidation Loss: 2.356647\n",
      "Epoch: 16859 \tTraining Loss: 1.674214 \tValidation Loss: 2.356693\n",
      "Epoch: 16860 \tTraining Loss: 1.645707 \tValidation Loss: 2.357477\n",
      "Epoch: 16861 \tTraining Loss: 1.665130 \tValidation Loss: 2.356918\n",
      "Epoch: 16862 \tTraining Loss: 1.680667 \tValidation Loss: 2.356764\n",
      "Epoch: 16863 \tTraining Loss: 1.650238 \tValidation Loss: 2.356717\n",
      "Epoch: 16864 \tTraining Loss: 1.642469 \tValidation Loss: 2.356481\n",
      "Epoch: 16865 \tTraining Loss: 1.690810 \tValidation Loss: 2.356439\n",
      "Epoch: 16866 \tTraining Loss: 1.715767 \tValidation Loss: 2.356245\n",
      "Epoch: 16867 \tTraining Loss: 1.659811 \tValidation Loss: 2.356639\n",
      "Epoch: 16868 \tTraining Loss: 1.667506 \tValidation Loss: 2.356829\n",
      "Epoch: 16869 \tTraining Loss: 1.649249 \tValidation Loss: 2.356710\n",
      "Epoch: 16870 \tTraining Loss: 1.646225 \tValidation Loss: 2.356984\n",
      "Epoch: 16871 \tTraining Loss: 1.661513 \tValidation Loss: 2.357188\n",
      "Epoch: 16872 \tTraining Loss: 1.660031 \tValidation Loss: 2.357032\n",
      "Epoch: 16873 \tTraining Loss: 1.719831 \tValidation Loss: 2.357059\n",
      "Epoch: 16874 \tTraining Loss: 1.702518 \tValidation Loss: 2.357097\n",
      "Epoch: 16875 \tTraining Loss: 1.651235 \tValidation Loss: 2.357609\n",
      "Epoch: 16876 \tTraining Loss: 1.661307 \tValidation Loss: 2.357738\n",
      "Epoch: 16877 \tTraining Loss: 1.651090 \tValidation Loss: 2.357036\n",
      "Epoch: 16878 \tTraining Loss: 1.649284 \tValidation Loss: 2.356665\n",
      "Epoch: 16879 \tTraining Loss: 1.666130 \tValidation Loss: 2.356447\n",
      "Epoch: 16880 \tTraining Loss: 1.689724 \tValidation Loss: 2.356361\n",
      "Epoch: 16881 \tTraining Loss: 1.709321 \tValidation Loss: 2.357027\n",
      "Epoch: 16882 \tTraining Loss: 1.668091 \tValidation Loss: 2.357192\n",
      "Epoch: 16883 \tTraining Loss: 1.687138 \tValidation Loss: 2.357002\n",
      "Epoch: 16884 \tTraining Loss: 1.631428 \tValidation Loss: 2.357352\n",
      "Epoch: 16885 \tTraining Loss: 1.694582 \tValidation Loss: 2.356760\n",
      "Epoch: 16886 \tTraining Loss: 1.669914 \tValidation Loss: 2.356815\n",
      "Epoch: 16887 \tTraining Loss: 1.692112 \tValidation Loss: 2.357415\n",
      "Epoch: 16888 \tTraining Loss: 1.725045 \tValidation Loss: 2.357591\n",
      "Epoch: 16889 \tTraining Loss: 1.638733 \tValidation Loss: 2.357662\n",
      "Epoch: 16890 \tTraining Loss: 1.692635 \tValidation Loss: 2.357703\n",
      "Epoch: 16891 \tTraining Loss: 1.677803 \tValidation Loss: 2.357691\n",
      "Epoch: 16892 \tTraining Loss: 1.688615 \tValidation Loss: 2.357359\n",
      "Epoch: 16893 \tTraining Loss: 1.655094 \tValidation Loss: 2.357210\n",
      "Epoch: 16894 \tTraining Loss: 1.678891 \tValidation Loss: 2.357332\n",
      "Epoch: 16895 \tTraining Loss: 1.651446 \tValidation Loss: 2.357219\n",
      "Epoch: 16896 \tTraining Loss: 1.700267 \tValidation Loss: 2.356987\n",
      "Epoch: 16897 \tTraining Loss: 1.692975 \tValidation Loss: 2.357029\n",
      "Epoch: 16898 \tTraining Loss: 1.676167 \tValidation Loss: 2.356605\n",
      "Epoch: 16899 \tTraining Loss: 1.695320 \tValidation Loss: 2.356886\n",
      "Epoch: 16900 \tTraining Loss: 1.661289 \tValidation Loss: 2.357428\n",
      "Epoch: 16901 \tTraining Loss: 1.663513 \tValidation Loss: 2.357600\n",
      "Epoch: 16902 \tTraining Loss: 1.685655 \tValidation Loss: 2.357065\n",
      "Epoch: 16903 \tTraining Loss: 1.662295 \tValidation Loss: 2.357151\n",
      "Epoch: 16904 \tTraining Loss: 1.698773 \tValidation Loss: 2.357279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16905 \tTraining Loss: 1.671057 \tValidation Loss: 2.357751\n",
      "Epoch: 16906 \tTraining Loss: 1.663972 \tValidation Loss: 2.357296\n",
      "Epoch: 16907 \tTraining Loss: 1.683466 \tValidation Loss: 2.357095\n",
      "Epoch: 16908 \tTraining Loss: 1.661205 \tValidation Loss: 2.357483\n",
      "Epoch: 16909 \tTraining Loss: 1.653477 \tValidation Loss: 2.357450\n",
      "Epoch: 16910 \tTraining Loss: 1.681466 \tValidation Loss: 2.357299\n",
      "Epoch: 16911 \tTraining Loss: 1.675198 \tValidation Loss: 2.357687\n",
      "Epoch: 16912 \tTraining Loss: 1.723533 \tValidation Loss: 2.357239\n",
      "Epoch: 16913 \tTraining Loss: 1.634455 \tValidation Loss: 2.357321\n",
      "Epoch: 16914 \tTraining Loss: 1.687577 \tValidation Loss: 2.357100\n",
      "Epoch: 16915 \tTraining Loss: 1.689045 \tValidation Loss: 2.357609\n",
      "Epoch: 16916 \tTraining Loss: 1.630502 \tValidation Loss: 2.357252\n",
      "Epoch: 16917 \tTraining Loss: 1.657296 \tValidation Loss: 2.357722\n",
      "Epoch: 16918 \tTraining Loss: 1.672086 \tValidation Loss: 2.357472\n",
      "Epoch: 16919 \tTraining Loss: 1.670981 \tValidation Loss: 2.358165\n",
      "Epoch: 16920 \tTraining Loss: 1.711996 \tValidation Loss: 2.357892\n",
      "Epoch: 16921 \tTraining Loss: 1.676297 \tValidation Loss: 2.357919\n",
      "Epoch: 16922 \tTraining Loss: 1.672368 \tValidation Loss: 2.357388\n",
      "Epoch: 16923 \tTraining Loss: 1.701020 \tValidation Loss: 2.357221\n",
      "Epoch: 16924 \tTraining Loss: 1.650970 \tValidation Loss: 2.357358\n",
      "Epoch: 16925 \tTraining Loss: 1.668411 \tValidation Loss: 2.357214\n",
      "Epoch: 16926 \tTraining Loss: 1.680007 \tValidation Loss: 2.357035\n",
      "Epoch: 16927 \tTraining Loss: 1.660699 \tValidation Loss: 2.357116\n",
      "Epoch: 16928 \tTraining Loss: 1.644093 \tValidation Loss: 2.357270\n",
      "Epoch: 16929 \tTraining Loss: 1.668688 \tValidation Loss: 2.357483\n",
      "Epoch: 16930 \tTraining Loss: 1.642380 \tValidation Loss: 2.358200\n",
      "Epoch: 16931 \tTraining Loss: 1.648561 \tValidation Loss: 2.357942\n",
      "Epoch: 16932 \tTraining Loss: 1.666114 \tValidation Loss: 2.357721\n",
      "Epoch: 16933 \tTraining Loss: 1.665587 \tValidation Loss: 2.358150\n",
      "Epoch: 16934 \tTraining Loss: 1.644938 \tValidation Loss: 2.357718\n",
      "Epoch: 16935 \tTraining Loss: 1.671854 \tValidation Loss: 2.357911\n",
      "Epoch: 16936 \tTraining Loss: 1.659898 \tValidation Loss: 2.358463\n",
      "Epoch: 16937 \tTraining Loss: 1.668163 \tValidation Loss: 2.358079\n",
      "Epoch: 16938 \tTraining Loss: 1.648448 \tValidation Loss: 2.357912\n",
      "Epoch: 16939 \tTraining Loss: 1.687086 \tValidation Loss: 2.357676\n",
      "Epoch: 16940 \tTraining Loss: 1.663112 \tValidation Loss: 2.358158\n",
      "Epoch: 16941 \tTraining Loss: 1.661124 \tValidation Loss: 2.357897\n",
      "Epoch: 16942 \tTraining Loss: 1.635336 \tValidation Loss: 2.358094\n",
      "Epoch: 16943 \tTraining Loss: 1.632920 \tValidation Loss: 2.357746\n",
      "Epoch: 16944 \tTraining Loss: 1.658098 \tValidation Loss: 2.358139\n",
      "Epoch: 16945 \tTraining Loss: 1.677504 \tValidation Loss: 2.357888\n",
      "Epoch: 16946 \tTraining Loss: 1.663771 \tValidation Loss: 2.358341\n",
      "Epoch: 16947 \tTraining Loss: 1.650286 \tValidation Loss: 2.357942\n",
      "Epoch: 16948 \tTraining Loss: 1.690060 \tValidation Loss: 2.357870\n",
      "Epoch: 16949 \tTraining Loss: 1.651269 \tValidation Loss: 2.358133\n",
      "Epoch: 16950 \tTraining Loss: 1.704368 \tValidation Loss: 2.358155\n",
      "Epoch: 16951 \tTraining Loss: 1.698115 \tValidation Loss: 2.357135\n",
      "Epoch: 16952 \tTraining Loss: 1.637990 \tValidation Loss: 2.357990\n",
      "Epoch: 16953 \tTraining Loss: 1.659731 \tValidation Loss: 2.357914\n",
      "Epoch: 16954 \tTraining Loss: 1.665714 \tValidation Loss: 2.357851\n",
      "Epoch: 16955 \tTraining Loss: 1.691470 \tValidation Loss: 2.357591\n",
      "Epoch: 16956 \tTraining Loss: 1.663837 \tValidation Loss: 2.357999\n",
      "Epoch: 16957 \tTraining Loss: 1.616723 \tValidation Loss: 2.357833\n",
      "Epoch: 16958 \tTraining Loss: 1.648346 \tValidation Loss: 2.358300\n",
      "Epoch: 16959 \tTraining Loss: 1.681340 \tValidation Loss: 2.357921\n",
      "Epoch: 16960 \tTraining Loss: 1.661930 \tValidation Loss: 2.358259\n",
      "Epoch: 16961 \tTraining Loss: 1.673982 \tValidation Loss: 2.358326\n",
      "Epoch: 16962 \tTraining Loss: 1.669295 \tValidation Loss: 2.357884\n",
      "Epoch: 16963 \tTraining Loss: 1.669466 \tValidation Loss: 2.357999\n",
      "Epoch: 16964 \tTraining Loss: 1.705266 \tValidation Loss: 2.357729\n",
      "Epoch: 16965 \tTraining Loss: 1.633615 \tValidation Loss: 2.357625\n",
      "Epoch: 16966 \tTraining Loss: 1.644286 \tValidation Loss: 2.358021\n",
      "Epoch: 16967 \tTraining Loss: 1.658171 \tValidation Loss: 2.357961\n",
      "Epoch: 16968 \tTraining Loss: 1.674303 \tValidation Loss: 2.357747\n",
      "Epoch: 16969 \tTraining Loss: 1.681696 \tValidation Loss: 2.357773\n",
      "Epoch: 16970 \tTraining Loss: 1.647187 \tValidation Loss: 2.357824\n",
      "Epoch: 16971 \tTraining Loss: 1.674980 \tValidation Loss: 2.357699\n",
      "Epoch: 16972 \tTraining Loss: 1.665696 \tValidation Loss: 2.358349\n",
      "Epoch: 16973 \tTraining Loss: 1.680261 \tValidation Loss: 2.358552\n",
      "Epoch: 16974 \tTraining Loss: 1.645748 \tValidation Loss: 2.358617\n",
      "Epoch: 16975 \tTraining Loss: 1.704574 \tValidation Loss: 2.357963\n",
      "Epoch: 16976 \tTraining Loss: 1.690227 \tValidation Loss: 2.358367\n",
      "Epoch: 16977 \tTraining Loss: 1.688213 \tValidation Loss: 2.358319\n",
      "Epoch: 16978 \tTraining Loss: 1.685947 \tValidation Loss: 2.357824\n",
      "Epoch: 16979 \tTraining Loss: 1.673764 \tValidation Loss: 2.357920\n",
      "Epoch: 16980 \tTraining Loss: 1.661800 \tValidation Loss: 2.357830\n",
      "Epoch: 16981 \tTraining Loss: 1.691972 \tValidation Loss: 2.358195\n",
      "Epoch: 16982 \tTraining Loss: 1.661206 \tValidation Loss: 2.358842\n",
      "Epoch: 16983 \tTraining Loss: 1.636335 \tValidation Loss: 2.358661\n",
      "Epoch: 16984 \tTraining Loss: 1.646587 \tValidation Loss: 2.357951\n",
      "Epoch: 16985 \tTraining Loss: 1.593042 \tValidation Loss: 2.358878\n",
      "Epoch: 16986 \tTraining Loss: 1.652624 \tValidation Loss: 2.358773\n",
      "Epoch: 16987 \tTraining Loss: 1.701707 \tValidation Loss: 2.358293\n",
      "Epoch: 16988 \tTraining Loss: 1.668921 \tValidation Loss: 2.358338\n",
      "Epoch: 16989 \tTraining Loss: 1.655299 \tValidation Loss: 2.358589\n",
      "Epoch: 16990 \tTraining Loss: 1.674381 \tValidation Loss: 2.358189\n",
      "Epoch: 16991 \tTraining Loss: 1.672942 \tValidation Loss: 2.358079\n",
      "Epoch: 16992 \tTraining Loss: 1.684385 \tValidation Loss: 2.358513\n",
      "Epoch: 16993 \tTraining Loss: 1.723101 \tValidation Loss: 2.357749\n",
      "Epoch: 16994 \tTraining Loss: 1.616431 \tValidation Loss: 2.358625\n",
      "Epoch: 16995 \tTraining Loss: 1.682997 \tValidation Loss: 2.358236\n",
      "Epoch: 16996 \tTraining Loss: 1.673463 \tValidation Loss: 2.358497\n",
      "Epoch: 16997 \tTraining Loss: 1.684007 \tValidation Loss: 2.357843\n",
      "Epoch: 16998 \tTraining Loss: 1.675003 \tValidation Loss: 2.358186\n",
      "Epoch: 16999 \tTraining Loss: 1.677856 \tValidation Loss: 2.358382\n",
      "Epoch: 17000 \tTraining Loss: 1.657393 \tValidation Loss: 2.357622\n",
      "Epoch: 17001 \tTraining Loss: 1.682664 \tValidation Loss: 2.357674\n",
      "Epoch: 17002 \tTraining Loss: 1.646452 \tValidation Loss: 2.358658\n",
      "Epoch: 17003 \tTraining Loss: 1.684764 \tValidation Loss: 2.358335\n",
      "Epoch: 17004 \tTraining Loss: 1.684204 \tValidation Loss: 2.358268\n",
      "Epoch: 17005 \tTraining Loss: 1.697725 \tValidation Loss: 2.357887\n",
      "Epoch: 17006 \tTraining Loss: 1.684704 \tValidation Loss: 2.358513\n",
      "Epoch: 17007 \tTraining Loss: 1.634295 \tValidation Loss: 2.358297\n",
      "Epoch: 17008 \tTraining Loss: 1.688978 \tValidation Loss: 2.358538\n",
      "Epoch: 17009 \tTraining Loss: 1.658041 \tValidation Loss: 2.358377\n",
      "Epoch: 17010 \tTraining Loss: 1.700552 \tValidation Loss: 2.357804\n",
      "Epoch: 17011 \tTraining Loss: 1.657635 \tValidation Loss: 2.357804\n",
      "Epoch: 17012 \tTraining Loss: 1.649708 \tValidation Loss: 2.358406\n",
      "Epoch: 17013 \tTraining Loss: 1.637790 \tValidation Loss: 2.358238\n",
      "Epoch: 17014 \tTraining Loss: 1.660953 \tValidation Loss: 2.358148\n",
      "Epoch: 17015 \tTraining Loss: 1.629698 \tValidation Loss: 2.358285\n",
      "Epoch: 17016 \tTraining Loss: 1.682094 \tValidation Loss: 2.358158\n",
      "Epoch: 17017 \tTraining Loss: 1.637410 \tValidation Loss: 2.358013\n",
      "Epoch: 17018 \tTraining Loss: 1.662750 \tValidation Loss: 2.358809\n",
      "Epoch: 17019 \tTraining Loss: 1.688831 \tValidation Loss: 2.358713\n",
      "Epoch: 17020 \tTraining Loss: 1.642790 \tValidation Loss: 2.358803\n",
      "Epoch: 17021 \tTraining Loss: 1.663743 \tValidation Loss: 2.359163\n",
      "Epoch: 17022 \tTraining Loss: 1.639304 \tValidation Loss: 2.359270\n",
      "Epoch: 17023 \tTraining Loss: 1.704196 \tValidation Loss: 2.358507\n",
      "Epoch: 17024 \tTraining Loss: 1.657612 \tValidation Loss: 2.358752\n",
      "Epoch: 17025 \tTraining Loss: 1.645465 \tValidation Loss: 2.358670\n",
      "Epoch: 17026 \tTraining Loss: 1.684185 \tValidation Loss: 2.358632\n",
      "Epoch: 17027 \tTraining Loss: 1.661602 \tValidation Loss: 2.358383\n",
      "Epoch: 17028 \tTraining Loss: 1.704727 \tValidation Loss: 2.358729\n",
      "Epoch: 17029 \tTraining Loss: 1.674880 \tValidation Loss: 2.358417\n",
      "Epoch: 17030 \tTraining Loss: 1.673794 \tValidation Loss: 2.358509\n",
      "Epoch: 17031 \tTraining Loss: 1.665945 \tValidation Loss: 2.358736\n",
      "Epoch: 17032 \tTraining Loss: 1.670294 \tValidation Loss: 2.358999\n",
      "Epoch: 17033 \tTraining Loss: 1.638216 \tValidation Loss: 2.359051\n",
      "Epoch: 17034 \tTraining Loss: 1.677777 \tValidation Loss: 2.358869\n",
      "Epoch: 17035 \tTraining Loss: 1.654023 \tValidation Loss: 2.358710\n",
      "Epoch: 17036 \tTraining Loss: 1.675749 \tValidation Loss: 2.359497\n",
      "Epoch: 17037 \tTraining Loss: 1.634184 \tValidation Loss: 2.359288\n",
      "Epoch: 17038 \tTraining Loss: 1.645252 \tValidation Loss: 2.359115\n",
      "Epoch: 17039 \tTraining Loss: 1.642078 \tValidation Loss: 2.358490\n",
      "Epoch: 17040 \tTraining Loss: 1.635540 \tValidation Loss: 2.358525\n",
      "Epoch: 17041 \tTraining Loss: 1.650294 \tValidation Loss: 2.358828\n",
      "Epoch: 17042 \tTraining Loss: 1.690130 \tValidation Loss: 2.358869\n",
      "Epoch: 17043 \tTraining Loss: 1.666801 \tValidation Loss: 2.358287\n",
      "Epoch: 17044 \tTraining Loss: 1.665989 \tValidation Loss: 2.358109\n",
      "Epoch: 17045 \tTraining Loss: 1.665521 \tValidation Loss: 2.358552\n",
      "Epoch: 17046 \tTraining Loss: 1.703231 \tValidation Loss: 2.358089\n",
      "Epoch: 17047 \tTraining Loss: 1.670465 \tValidation Loss: 2.358562\n",
      "Epoch: 17048 \tTraining Loss: 1.689518 \tValidation Loss: 2.358181\n",
      "Epoch: 17049 \tTraining Loss: 1.678328 \tValidation Loss: 2.358109\n",
      "Epoch: 17050 \tTraining Loss: 1.703398 \tValidation Loss: 2.357851\n",
      "Epoch: 17051 \tTraining Loss: 1.648373 \tValidation Loss: 2.358373\n",
      "Epoch: 17052 \tTraining Loss: 1.692091 \tValidation Loss: 2.357749\n",
      "Epoch: 17053 \tTraining Loss: 1.688174 \tValidation Loss: 2.358309\n",
      "Epoch: 17054 \tTraining Loss: 1.680677 \tValidation Loss: 2.358265\n",
      "Epoch: 17055 \tTraining Loss: 1.667024 \tValidation Loss: 2.358375\n",
      "Epoch: 17056 \tTraining Loss: 1.641416 \tValidation Loss: 2.358314\n",
      "Epoch: 17057 \tTraining Loss: 1.663196 \tValidation Loss: 2.358662\n",
      "Epoch: 17058 \tTraining Loss: 1.663488 \tValidation Loss: 2.358511\n",
      "Epoch: 17059 \tTraining Loss: 1.680929 \tValidation Loss: 2.357962\n",
      "Epoch: 17060 \tTraining Loss: 1.675053 \tValidation Loss: 2.358454\n",
      "Epoch: 17061 \tTraining Loss: 1.611834 \tValidation Loss: 2.358852\n",
      "Epoch: 17062 \tTraining Loss: 1.655432 \tValidation Loss: 2.358073\n",
      "Epoch: 17063 \tTraining Loss: 1.666578 \tValidation Loss: 2.358511\n",
      "Epoch: 17064 \tTraining Loss: 1.690274 \tValidation Loss: 2.358040\n",
      "Epoch: 17065 \tTraining Loss: 1.669782 \tValidation Loss: 2.358304\n",
      "Epoch: 17066 \tTraining Loss: 1.664022 \tValidation Loss: 2.357969\n",
      "Epoch: 17067 \tTraining Loss: 1.662409 \tValidation Loss: 2.358591\n",
      "Epoch: 17068 \tTraining Loss: 1.665505 \tValidation Loss: 2.359053\n",
      "Epoch: 17069 \tTraining Loss: 1.671678 \tValidation Loss: 2.359239\n",
      "Epoch: 17070 \tTraining Loss: 1.626810 \tValidation Loss: 2.359263\n",
      "Epoch: 17071 \tTraining Loss: 1.642690 \tValidation Loss: 2.359253\n",
      "Epoch: 17072 \tTraining Loss: 1.678574 \tValidation Loss: 2.358497\n",
      "Epoch: 17073 \tTraining Loss: 1.672768 \tValidation Loss: 2.358362\n",
      "Epoch: 17074 \tTraining Loss: 1.666259 \tValidation Loss: 2.358801\n",
      "Epoch: 17075 \tTraining Loss: 1.688725 \tValidation Loss: 2.358679\n",
      "Epoch: 17076 \tTraining Loss: 1.661458 \tValidation Loss: 2.358873\n",
      "Epoch: 17077 \tTraining Loss: 1.669000 \tValidation Loss: 2.358481\n",
      "Epoch: 17078 \tTraining Loss: 1.699940 \tValidation Loss: 2.358544\n",
      "Epoch: 17079 \tTraining Loss: 1.647807 \tValidation Loss: 2.358142\n",
      "Epoch: 17080 \tTraining Loss: 1.650319 \tValidation Loss: 2.358840\n",
      "Epoch: 17081 \tTraining Loss: 1.631066 \tValidation Loss: 2.358850\n",
      "Epoch: 17082 \tTraining Loss: 1.633965 \tValidation Loss: 2.358886\n",
      "Epoch: 17083 \tTraining Loss: 1.657169 \tValidation Loss: 2.359685\n",
      "Epoch: 17084 \tTraining Loss: 1.647947 \tValidation Loss: 2.359519\n",
      "Epoch: 17085 \tTraining Loss: 1.675143 \tValidation Loss: 2.359034\n",
      "Epoch: 17086 \tTraining Loss: 1.656940 \tValidation Loss: 2.358969\n",
      "Epoch: 17087 \tTraining Loss: 1.646499 \tValidation Loss: 2.358625\n",
      "Epoch: 17088 \tTraining Loss: 1.637354 \tValidation Loss: 2.359325\n",
      "Epoch: 17089 \tTraining Loss: 1.646311 \tValidation Loss: 2.359399\n",
      "Epoch: 17090 \tTraining Loss: 1.670514 \tValidation Loss: 2.358761\n",
      "Epoch: 17091 \tTraining Loss: 1.668987 \tValidation Loss: 2.358223\n",
      "Epoch: 17092 \tTraining Loss: 1.647449 \tValidation Loss: 2.358552\n",
      "Epoch: 17093 \tTraining Loss: 1.696599 \tValidation Loss: 2.358439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17094 \tTraining Loss: 1.646100 \tValidation Loss: 2.358651\n",
      "Epoch: 17095 \tTraining Loss: 1.679893 \tValidation Loss: 2.358168\n",
      "Epoch: 17096 \tTraining Loss: 1.640602 \tValidation Loss: 2.358341\n",
      "Epoch: 17097 \tTraining Loss: 1.628482 \tValidation Loss: 2.358713\n",
      "Epoch: 17098 \tTraining Loss: 1.655959 \tValidation Loss: 2.358998\n",
      "Epoch: 17099 \tTraining Loss: 1.596985 \tValidation Loss: 2.358966\n",
      "Epoch: 17100 \tTraining Loss: 1.676389 \tValidation Loss: 2.358485\n",
      "Epoch: 17101 \tTraining Loss: 1.689798 \tValidation Loss: 2.358860\n",
      "Epoch: 17102 \tTraining Loss: 1.677256 \tValidation Loss: 2.358674\n",
      "Epoch: 17103 \tTraining Loss: 1.696961 \tValidation Loss: 2.359001\n",
      "Epoch: 17104 \tTraining Loss: 1.630666 \tValidation Loss: 2.359448\n",
      "Epoch: 17105 \tTraining Loss: 1.684657 \tValidation Loss: 2.358962\n",
      "Epoch: 17106 \tTraining Loss: 1.650453 \tValidation Loss: 2.358709\n",
      "Epoch: 17107 \tTraining Loss: 1.651987 \tValidation Loss: 2.358612\n",
      "Epoch: 17108 \tTraining Loss: 1.658350 \tValidation Loss: 2.358902\n",
      "Epoch: 17109 \tTraining Loss: 1.653075 \tValidation Loss: 2.359145\n",
      "Epoch: 17110 \tTraining Loss: 1.618756 \tValidation Loss: 2.358908\n",
      "Epoch: 17111 \tTraining Loss: 1.666299 \tValidation Loss: 2.358942\n",
      "Epoch: 17112 \tTraining Loss: 1.661563 \tValidation Loss: 2.359011\n",
      "Epoch: 17113 \tTraining Loss: 1.673499 \tValidation Loss: 2.358996\n",
      "Epoch: 17114 \tTraining Loss: 1.634984 \tValidation Loss: 2.359411\n",
      "Epoch: 17115 \tTraining Loss: 1.667741 \tValidation Loss: 2.359534\n",
      "Epoch: 17116 \tTraining Loss: 1.698922 \tValidation Loss: 2.358327\n",
      "Epoch: 17117 \tTraining Loss: 1.648734 \tValidation Loss: 2.359119\n",
      "Epoch: 17118 \tTraining Loss: 1.685045 \tValidation Loss: 2.359440\n",
      "Epoch: 17119 \tTraining Loss: 1.655675 \tValidation Loss: 2.359434\n",
      "Epoch: 17120 \tTraining Loss: 1.687132 \tValidation Loss: 2.359530\n",
      "Epoch: 17121 \tTraining Loss: 1.651694 \tValidation Loss: 2.359485\n",
      "Epoch: 17122 \tTraining Loss: 1.663299 \tValidation Loss: 2.359336\n",
      "Epoch: 17123 \tTraining Loss: 1.635078 \tValidation Loss: 2.359094\n",
      "Epoch: 17124 \tTraining Loss: 1.687425 \tValidation Loss: 2.359144\n",
      "Epoch: 17125 \tTraining Loss: 1.685088 \tValidation Loss: 2.358956\n",
      "Epoch: 17126 \tTraining Loss: 1.646652 \tValidation Loss: 2.359283\n",
      "Epoch: 17127 \tTraining Loss: 1.659493 \tValidation Loss: 2.359246\n",
      "Epoch: 17128 \tTraining Loss: 1.626328 \tValidation Loss: 2.359553\n",
      "Epoch: 17129 \tTraining Loss: 1.684603 \tValidation Loss: 2.359272\n",
      "Epoch: 17130 \tTraining Loss: 1.630868 \tValidation Loss: 2.359380\n",
      "Epoch: 17131 \tTraining Loss: 1.661759 \tValidation Loss: 2.359250\n",
      "Epoch: 17132 \tTraining Loss: 1.636794 \tValidation Loss: 2.359677\n",
      "Epoch: 17133 \tTraining Loss: 1.683727 \tValidation Loss: 2.359059\n",
      "Epoch: 17134 \tTraining Loss: 1.649943 \tValidation Loss: 2.359280\n",
      "Epoch: 17135 \tTraining Loss: 1.633620 \tValidation Loss: 2.359764\n",
      "Epoch: 17136 \tTraining Loss: 1.693049 \tValidation Loss: 2.359784\n",
      "Epoch: 17137 \tTraining Loss: 1.655260 \tValidation Loss: 2.359237\n",
      "Epoch: 17138 \tTraining Loss: 1.684019 \tValidation Loss: 2.359525\n",
      "Epoch: 17139 \tTraining Loss: 1.616117 \tValidation Loss: 2.360166\n",
      "Epoch: 17140 \tTraining Loss: 1.652199 \tValidation Loss: 2.360307\n",
      "Epoch: 17141 \tTraining Loss: 1.627576 \tValidation Loss: 2.359931\n",
      "Epoch: 17142 \tTraining Loss: 1.629884 \tValidation Loss: 2.360039\n",
      "Epoch: 17143 \tTraining Loss: 1.617019 \tValidation Loss: 2.360010\n",
      "Epoch: 17144 \tTraining Loss: 1.649675 \tValidation Loss: 2.360006\n",
      "Epoch: 17145 \tTraining Loss: 1.653338 \tValidation Loss: 2.360620\n",
      "Epoch: 17146 \tTraining Loss: 1.727055 \tValidation Loss: 2.359964\n",
      "Epoch: 17147 \tTraining Loss: 1.646569 \tValidation Loss: 2.359570\n",
      "Epoch: 17148 \tTraining Loss: 1.634022 \tValidation Loss: 2.360280\n",
      "Epoch: 17149 \tTraining Loss: 1.672697 \tValidation Loss: 2.360174\n",
      "Epoch: 17150 \tTraining Loss: 1.670464 \tValidation Loss: 2.360122\n",
      "Epoch: 17151 \tTraining Loss: 1.671767 \tValidation Loss: 2.359957\n",
      "Epoch: 17152 \tTraining Loss: 1.651781 \tValidation Loss: 2.360108\n",
      "Epoch: 17153 \tTraining Loss: 1.667529 \tValidation Loss: 2.359924\n",
      "Epoch: 17154 \tTraining Loss: 1.649363 \tValidation Loss: 2.359780\n",
      "Epoch: 17155 \tTraining Loss: 1.642010 \tValidation Loss: 2.360235\n",
      "Epoch: 17156 \tTraining Loss: 1.635633 \tValidation Loss: 2.359866\n",
      "Epoch: 17157 \tTraining Loss: 1.641614 \tValidation Loss: 2.359654\n",
      "Epoch: 17158 \tTraining Loss: 1.636617 \tValidation Loss: 2.359424\n",
      "Epoch: 17159 \tTraining Loss: 1.645666 \tValidation Loss: 2.359408\n",
      "Epoch: 17160 \tTraining Loss: 1.675140 \tValidation Loss: 2.360018\n",
      "Epoch: 17161 \tTraining Loss: 1.628405 \tValidation Loss: 2.360737\n",
      "Epoch: 17162 \tTraining Loss: 1.640091 \tValidation Loss: 2.360534\n",
      "Epoch: 17163 \tTraining Loss: 1.668819 \tValidation Loss: 2.359976\n",
      "Epoch: 17164 \tTraining Loss: 1.669797 \tValidation Loss: 2.359862\n",
      "Epoch: 17165 \tTraining Loss: 1.647760 \tValidation Loss: 2.360765\n",
      "Epoch: 17166 \tTraining Loss: 1.648721 \tValidation Loss: 2.359861\n",
      "Epoch: 17167 \tTraining Loss: 1.696872 \tValidation Loss: 2.360069\n",
      "Epoch: 17168 \tTraining Loss: 1.634408 \tValidation Loss: 2.360190\n",
      "Epoch: 17169 \tTraining Loss: 1.657573 \tValidation Loss: 2.360438\n",
      "Epoch: 17170 \tTraining Loss: 1.648871 \tValidation Loss: 2.360321\n",
      "Epoch: 17171 \tTraining Loss: 1.639048 \tValidation Loss: 2.360037\n",
      "Epoch: 17172 \tTraining Loss: 1.670300 \tValidation Loss: 2.359029\n",
      "Epoch: 17173 \tTraining Loss: 1.625157 \tValidation Loss: 2.360069\n",
      "Epoch: 17174 \tTraining Loss: 1.662863 \tValidation Loss: 2.359759\n",
      "Epoch: 17175 \tTraining Loss: 1.679093 \tValidation Loss: 2.359782\n",
      "Epoch: 17176 \tTraining Loss: 1.613872 \tValidation Loss: 2.360229\n",
      "Epoch: 17177 \tTraining Loss: 1.648743 \tValidation Loss: 2.360820\n",
      "Epoch: 17178 \tTraining Loss: 1.658724 \tValidation Loss: 2.360358\n",
      "Epoch: 17179 \tTraining Loss: 1.657675 \tValidation Loss: 2.360471\n",
      "Epoch: 17180 \tTraining Loss: 1.654228 \tValidation Loss: 2.359995\n",
      "Epoch: 17181 \tTraining Loss: 1.671372 \tValidation Loss: 2.360050\n",
      "Epoch: 17182 \tTraining Loss: 1.615568 \tValidation Loss: 2.360452\n",
      "Epoch: 17183 \tTraining Loss: 1.711010 \tValidation Loss: 2.360138\n",
      "Epoch: 17184 \tTraining Loss: 1.679576 \tValidation Loss: 2.360197\n",
      "Epoch: 17185 \tTraining Loss: 1.654502 \tValidation Loss: 2.360099\n",
      "Epoch: 17186 \tTraining Loss: 1.656088 \tValidation Loss: 2.360248\n",
      "Epoch: 17187 \tTraining Loss: 1.641059 \tValidation Loss: 2.360212\n",
      "Epoch: 17188 \tTraining Loss: 1.635699 \tValidation Loss: 2.360230\n",
      "Epoch: 17189 \tTraining Loss: 1.619851 \tValidation Loss: 2.359934\n",
      "Epoch: 17190 \tTraining Loss: 1.649278 \tValidation Loss: 2.360337\n",
      "Epoch: 17191 \tTraining Loss: 1.657900 \tValidation Loss: 2.360436\n",
      "Epoch: 17192 \tTraining Loss: 1.647669 \tValidation Loss: 2.360833\n",
      "Epoch: 17193 \tTraining Loss: 1.631611 \tValidation Loss: 2.360309\n",
      "Epoch: 17194 \tTraining Loss: 1.661466 \tValidation Loss: 2.360463\n",
      "Epoch: 17195 \tTraining Loss: 1.613559 \tValidation Loss: 2.360543\n",
      "Epoch: 17196 \tTraining Loss: 1.645577 \tValidation Loss: 2.360730\n",
      "Epoch: 17197 \tTraining Loss: 1.694386 \tValidation Loss: 2.360228\n",
      "Epoch: 17198 \tTraining Loss: 1.629567 \tValidation Loss: 2.361027\n",
      "Epoch: 17199 \tTraining Loss: 1.673133 \tValidation Loss: 2.360408\n",
      "Epoch: 17200 \tTraining Loss: 1.635957 \tValidation Loss: 2.360674\n",
      "Epoch: 17201 \tTraining Loss: 1.639306 \tValidation Loss: 2.361099\n",
      "Epoch: 17202 \tTraining Loss: 1.623587 \tValidation Loss: 2.360801\n",
      "Epoch: 17203 \tTraining Loss: 1.652819 \tValidation Loss: 2.360904\n",
      "Epoch: 17204 \tTraining Loss: 1.661853 \tValidation Loss: 2.360669\n",
      "Epoch: 17205 \tTraining Loss: 1.699493 \tValidation Loss: 2.360458\n",
      "Epoch: 17206 \tTraining Loss: 1.631142 \tValidation Loss: 2.360085\n",
      "Epoch: 17207 \tTraining Loss: 1.649762 \tValidation Loss: 2.359973\n",
      "Epoch: 17208 \tTraining Loss: 1.661299 \tValidation Loss: 2.360217\n",
      "Epoch: 17209 \tTraining Loss: 1.661649 \tValidation Loss: 2.360541\n",
      "Epoch: 17210 \tTraining Loss: 1.655980 \tValidation Loss: 2.360694\n",
      "Epoch: 17211 \tTraining Loss: 1.611483 \tValidation Loss: 2.360563\n",
      "Epoch: 17212 \tTraining Loss: 1.632318 \tValidation Loss: 2.360382\n",
      "Epoch: 17213 \tTraining Loss: 1.684254 \tValidation Loss: 2.360461\n",
      "Epoch: 17214 \tTraining Loss: 1.658195 \tValidation Loss: 2.360554\n",
      "Epoch: 17215 \tTraining Loss: 1.620556 \tValidation Loss: 2.361120\n",
      "Epoch: 17216 \tTraining Loss: 1.618700 \tValidation Loss: 2.361206\n",
      "Epoch: 17217 \tTraining Loss: 1.615552 \tValidation Loss: 2.361175\n",
      "Epoch: 17218 \tTraining Loss: 1.679861 \tValidation Loss: 2.361221\n",
      "Epoch: 17219 \tTraining Loss: 1.658765 \tValidation Loss: 2.361117\n",
      "Epoch: 17220 \tTraining Loss: 1.661158 \tValidation Loss: 2.360964\n",
      "Epoch: 17221 \tTraining Loss: 1.656927 \tValidation Loss: 2.361038\n",
      "Epoch: 17222 \tTraining Loss: 1.676338 \tValidation Loss: 2.360937\n",
      "Epoch: 17223 \tTraining Loss: 1.664354 \tValidation Loss: 2.360635\n",
      "Epoch: 17224 \tTraining Loss: 1.649930 \tValidation Loss: 2.360708\n",
      "Epoch: 17225 \tTraining Loss: 1.659125 \tValidation Loss: 2.359989\n",
      "Epoch: 17226 \tTraining Loss: 1.617260 \tValidation Loss: 2.360612\n",
      "Epoch: 17227 \tTraining Loss: 1.652803 \tValidation Loss: 2.360006\n",
      "Epoch: 17228 \tTraining Loss: 1.674210 \tValidation Loss: 2.360090\n",
      "Epoch: 17229 \tTraining Loss: 1.637356 \tValidation Loss: 2.360221\n",
      "Epoch: 17230 \tTraining Loss: 1.677227 \tValidation Loss: 2.360277\n",
      "Epoch: 17231 \tTraining Loss: 1.653724 \tValidation Loss: 2.360790\n",
      "Epoch: 17232 \tTraining Loss: 1.664920 \tValidation Loss: 2.360448\n",
      "Epoch: 17233 \tTraining Loss: 1.668735 \tValidation Loss: 2.360443\n",
      "Epoch: 17234 \tTraining Loss: 1.661711 \tValidation Loss: 2.360173\n",
      "Epoch: 17235 \tTraining Loss: 1.659026 \tValidation Loss: 2.360261\n",
      "Epoch: 17236 \tTraining Loss: 1.661456 \tValidation Loss: 2.360716\n",
      "Epoch: 17237 \tTraining Loss: 1.662416 \tValidation Loss: 2.360605\n",
      "Epoch: 17238 \tTraining Loss: 1.652177 \tValidation Loss: 2.360532\n",
      "Epoch: 17239 \tTraining Loss: 1.693947 \tValidation Loss: 2.360061\n",
      "Epoch: 17240 \tTraining Loss: 1.642244 \tValidation Loss: 2.359749\n",
      "Epoch: 17241 \tTraining Loss: 1.667455 \tValidation Loss: 2.359915\n",
      "Epoch: 17242 \tTraining Loss: 1.640456 \tValidation Loss: 2.360377\n",
      "Epoch: 17243 \tTraining Loss: 1.677952 \tValidation Loss: 2.360038\n",
      "Epoch: 17244 \tTraining Loss: 1.625870 \tValidation Loss: 2.360718\n",
      "Epoch: 17245 \tTraining Loss: 1.678011 \tValidation Loss: 2.361132\n",
      "Epoch: 17246 \tTraining Loss: 1.686150 \tValidation Loss: 2.360330\n",
      "Epoch: 17247 \tTraining Loss: 1.673169 \tValidation Loss: 2.360222\n",
      "Epoch: 17248 \tTraining Loss: 1.676980 \tValidation Loss: 2.360494\n",
      "Epoch: 17249 \tTraining Loss: 1.656411 \tValidation Loss: 2.360308\n",
      "Epoch: 17250 \tTraining Loss: 1.619485 \tValidation Loss: 2.361031\n",
      "Epoch: 17251 \tTraining Loss: 1.700279 \tValidation Loss: 2.360461\n",
      "Epoch: 17252 \tTraining Loss: 1.655718 \tValidation Loss: 2.360834\n",
      "Epoch: 17253 \tTraining Loss: 1.649049 \tValidation Loss: 2.361334\n",
      "Epoch: 17254 \tTraining Loss: 1.648544 \tValidation Loss: 2.360779\n",
      "Epoch: 17255 \tTraining Loss: 1.691073 \tValidation Loss: 2.360530\n",
      "Epoch: 17256 \tTraining Loss: 1.674003 \tValidation Loss: 2.360628\n",
      "Epoch: 17257 \tTraining Loss: 1.642883 \tValidation Loss: 2.360639\n",
      "Epoch: 17258 \tTraining Loss: 1.664980 \tValidation Loss: 2.360800\n",
      "Epoch: 17259 \tTraining Loss: 1.634609 \tValidation Loss: 2.361091\n",
      "Epoch: 17260 \tTraining Loss: 1.689352 \tValidation Loss: 2.361050\n",
      "Epoch: 17261 \tTraining Loss: 1.640292 \tValidation Loss: 2.361646\n",
      "Epoch: 17262 \tTraining Loss: 1.621801 \tValidation Loss: 2.361503\n",
      "Epoch: 17263 \tTraining Loss: 1.656366 \tValidation Loss: 2.361293\n",
      "Epoch: 17264 \tTraining Loss: 1.689859 \tValidation Loss: 2.360877\n",
      "Epoch: 17265 \tTraining Loss: 1.683441 \tValidation Loss: 2.361293\n",
      "Epoch: 17266 \tTraining Loss: 1.672590 \tValidation Loss: 2.360927\n",
      "Epoch: 17267 \tTraining Loss: 1.651915 \tValidation Loss: 2.361507\n",
      "Epoch: 17268 \tTraining Loss: 1.641802 \tValidation Loss: 2.361169\n",
      "Epoch: 17269 \tTraining Loss: 1.692254 \tValidation Loss: 2.360945\n",
      "Epoch: 17270 \tTraining Loss: 1.676538 \tValidation Loss: 2.361125\n",
      "Epoch: 17271 \tTraining Loss: 1.608914 \tValidation Loss: 2.361454\n",
      "Epoch: 17272 \tTraining Loss: 1.642847 \tValidation Loss: 2.361613\n",
      "Epoch: 17273 \tTraining Loss: 1.663850 \tValidation Loss: 2.361088\n",
      "Epoch: 17274 \tTraining Loss: 1.663912 \tValidation Loss: 2.361439\n",
      "Epoch: 17275 \tTraining Loss: 1.650855 \tValidation Loss: 2.361153\n",
      "Epoch: 17276 \tTraining Loss: 1.658052 \tValidation Loss: 2.360919\n",
      "Epoch: 17277 \tTraining Loss: 1.632343 \tValidation Loss: 2.361004\n",
      "Epoch: 17278 \tTraining Loss: 1.642662 \tValidation Loss: 2.360869\n",
      "Epoch: 17279 \tTraining Loss: 1.646942 \tValidation Loss: 2.361294\n",
      "Epoch: 17280 \tTraining Loss: 1.646052 \tValidation Loss: 2.360892\n",
      "Epoch: 17281 \tTraining Loss: 1.691061 \tValidation Loss: 2.360959\n",
      "Epoch: 17282 \tTraining Loss: 1.644513 \tValidation Loss: 2.361215\n",
      "Epoch: 17283 \tTraining Loss: 1.637083 \tValidation Loss: 2.361658\n",
      "Epoch: 17284 \tTraining Loss: 1.662847 \tValidation Loss: 2.361304\n",
      "Epoch: 17285 \tTraining Loss: 1.710061 \tValidation Loss: 2.360932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17286 \tTraining Loss: 1.627728 \tValidation Loss: 2.361212\n",
      "Epoch: 17287 \tTraining Loss: 1.648299 \tValidation Loss: 2.361121\n",
      "Epoch: 17288 \tTraining Loss: 1.643472 \tValidation Loss: 2.360950\n",
      "Epoch: 17289 \tTraining Loss: 1.646309 \tValidation Loss: 2.361317\n",
      "Epoch: 17290 \tTraining Loss: 1.642271 \tValidation Loss: 2.361825\n",
      "Epoch: 17291 \tTraining Loss: 1.662258 \tValidation Loss: 2.361320\n",
      "Epoch: 17292 \tTraining Loss: 1.612771 \tValidation Loss: 2.361463\n",
      "Epoch: 17293 \tTraining Loss: 1.650043 \tValidation Loss: 2.361295\n",
      "Epoch: 17294 \tTraining Loss: 1.646288 \tValidation Loss: 2.361600\n",
      "Epoch: 17295 \tTraining Loss: 1.673449 \tValidation Loss: 2.361693\n",
      "Epoch: 17296 \tTraining Loss: 1.692898 \tValidation Loss: 2.361979\n",
      "Epoch: 17297 \tTraining Loss: 1.663821 \tValidation Loss: 2.361560\n",
      "Epoch: 17298 \tTraining Loss: 1.655836 \tValidation Loss: 2.361355\n",
      "Epoch: 17299 \tTraining Loss: 1.670735 \tValidation Loss: 2.361101\n",
      "Epoch: 17300 \tTraining Loss: 1.648105 \tValidation Loss: 2.361770\n",
      "Epoch: 17301 \tTraining Loss: 1.684276 \tValidation Loss: 2.361767\n",
      "Epoch: 17302 \tTraining Loss: 1.669390 \tValidation Loss: 2.361448\n",
      "Epoch: 17303 \tTraining Loss: 1.699268 \tValidation Loss: 2.361847\n",
      "Epoch: 17304 \tTraining Loss: 1.655170 \tValidation Loss: 2.362241\n",
      "Epoch: 17305 \tTraining Loss: 1.666894 \tValidation Loss: 2.361961\n",
      "Epoch: 17306 \tTraining Loss: 1.623798 \tValidation Loss: 2.361841\n",
      "Epoch: 17307 \tTraining Loss: 1.673355 \tValidation Loss: 2.361536\n",
      "Epoch: 17308 \tTraining Loss: 1.690394 \tValidation Loss: 2.361343\n",
      "Epoch: 17309 \tTraining Loss: 1.683983 \tValidation Loss: 2.361212\n",
      "Epoch: 17310 \tTraining Loss: 1.640025 \tValidation Loss: 2.360835\n",
      "Epoch: 17311 \tTraining Loss: 1.702663 \tValidation Loss: 2.360878\n",
      "Epoch: 17312 \tTraining Loss: 1.619892 \tValidation Loss: 2.361042\n",
      "Epoch: 17313 \tTraining Loss: 1.686208 \tValidation Loss: 2.361276\n",
      "Epoch: 17314 \tTraining Loss: 1.651949 \tValidation Loss: 2.361124\n",
      "Epoch: 17315 \tTraining Loss: 1.697276 \tValidation Loss: 2.360991\n",
      "Epoch: 17316 \tTraining Loss: 1.643911 \tValidation Loss: 2.361348\n",
      "Epoch: 17317 \tTraining Loss: 1.654973 \tValidation Loss: 2.360505\n",
      "Epoch: 17318 \tTraining Loss: 1.669238 \tValidation Loss: 2.360940\n",
      "Epoch: 17319 \tTraining Loss: 1.628021 \tValidation Loss: 2.361127\n",
      "Epoch: 17320 \tTraining Loss: 1.691800 \tValidation Loss: 2.362014\n",
      "Epoch: 17321 \tTraining Loss: 1.687157 \tValidation Loss: 2.361606\n",
      "Epoch: 17322 \tTraining Loss: 1.644575 \tValidation Loss: 2.362236\n",
      "Epoch: 17323 \tTraining Loss: 1.633278 \tValidation Loss: 2.361949\n",
      "Epoch: 17324 \tTraining Loss: 1.616305 \tValidation Loss: 2.361480\n",
      "Epoch: 17325 \tTraining Loss: 1.709115 \tValidation Loss: 2.361017\n",
      "Epoch: 17326 \tTraining Loss: 1.653077 \tValidation Loss: 2.361080\n",
      "Epoch: 17327 \tTraining Loss: 1.661620 \tValidation Loss: 2.361832\n",
      "Epoch: 17328 \tTraining Loss: 1.658989 \tValidation Loss: 2.361664\n",
      "Epoch: 17329 \tTraining Loss: 1.669760 \tValidation Loss: 2.361690\n",
      "Epoch: 17330 \tTraining Loss: 1.653349 \tValidation Loss: 2.362228\n",
      "Epoch: 17331 \tTraining Loss: 1.639133 \tValidation Loss: 2.362738\n",
      "Epoch: 17332 \tTraining Loss: 1.616033 \tValidation Loss: 2.362757\n",
      "Epoch: 17333 \tTraining Loss: 1.608323 \tValidation Loss: 2.361920\n",
      "Epoch: 17334 \tTraining Loss: 1.598617 \tValidation Loss: 2.362365\n",
      "Epoch: 17335 \tTraining Loss: 1.686131 \tValidation Loss: 2.361792\n",
      "Epoch: 17336 \tTraining Loss: 1.630138 \tValidation Loss: 2.361765\n",
      "Epoch: 17337 \tTraining Loss: 1.670125 \tValidation Loss: 2.361433\n",
      "Epoch: 17338 \tTraining Loss: 1.658414 \tValidation Loss: 2.361880\n",
      "Epoch: 17339 \tTraining Loss: 1.607913 \tValidation Loss: 2.362043\n",
      "Epoch: 17340 \tTraining Loss: 1.655767 \tValidation Loss: 2.361793\n",
      "Epoch: 17341 \tTraining Loss: 1.603560 \tValidation Loss: 2.361853\n",
      "Epoch: 17342 \tTraining Loss: 1.637125 \tValidation Loss: 2.361819\n",
      "Epoch: 17343 \tTraining Loss: 1.645067 \tValidation Loss: 2.361871\n",
      "Epoch: 17344 \tTraining Loss: 1.655084 \tValidation Loss: 2.362550\n",
      "Epoch: 17345 \tTraining Loss: 1.647173 \tValidation Loss: 2.362282\n",
      "Epoch: 17346 \tTraining Loss: 1.625516 \tValidation Loss: 2.362161\n",
      "Epoch: 17347 \tTraining Loss: 1.665055 \tValidation Loss: 2.361909\n",
      "Epoch: 17348 \tTraining Loss: 1.628683 \tValidation Loss: 2.362222\n",
      "Epoch: 17349 \tTraining Loss: 1.650583 \tValidation Loss: 2.362481\n",
      "Epoch: 17350 \tTraining Loss: 1.663269 \tValidation Loss: 2.362185\n",
      "Epoch: 17351 \tTraining Loss: 1.615250 \tValidation Loss: 2.362210\n",
      "Epoch: 17352 \tTraining Loss: 1.683469 \tValidation Loss: 2.361753\n",
      "Epoch: 17353 \tTraining Loss: 1.626614 \tValidation Loss: 2.361965\n",
      "Epoch: 17354 \tTraining Loss: 1.641993 \tValidation Loss: 2.362071\n",
      "Epoch: 17355 \tTraining Loss: 1.650576 \tValidation Loss: 2.362370\n",
      "Epoch: 17356 \tTraining Loss: 1.620518 \tValidation Loss: 2.362890\n",
      "Epoch: 17357 \tTraining Loss: 1.655057 \tValidation Loss: 2.362284\n",
      "Epoch: 17358 \tTraining Loss: 1.641026 \tValidation Loss: 2.362300\n",
      "Epoch: 17359 \tTraining Loss: 1.655942 \tValidation Loss: 2.362482\n",
      "Epoch: 17360 \tTraining Loss: 1.699850 \tValidation Loss: 2.361840\n",
      "Epoch: 17361 \tTraining Loss: 1.653376 \tValidation Loss: 2.362114\n",
      "Epoch: 17362 \tTraining Loss: 1.689365 \tValidation Loss: 2.362029\n",
      "Epoch: 17363 \tTraining Loss: 1.641381 \tValidation Loss: 2.362025\n",
      "Epoch: 17364 \tTraining Loss: 1.661989 \tValidation Loss: 2.361920\n",
      "Epoch: 17365 \tTraining Loss: 1.644447 \tValidation Loss: 2.361836\n",
      "Epoch: 17366 \tTraining Loss: 1.638775 \tValidation Loss: 2.362512\n",
      "Epoch: 17367 \tTraining Loss: 1.594416 \tValidation Loss: 2.363025\n",
      "Epoch: 17368 \tTraining Loss: 1.615360 \tValidation Loss: 2.363153\n",
      "Epoch: 17369 \tTraining Loss: 1.654171 \tValidation Loss: 2.362814\n",
      "Epoch: 17370 \tTraining Loss: 1.642294 \tValidation Loss: 2.362797\n",
      "Epoch: 17371 \tTraining Loss: 1.629885 \tValidation Loss: 2.362527\n",
      "Epoch: 17372 \tTraining Loss: 1.676043 \tValidation Loss: 2.362111\n",
      "Epoch: 17373 \tTraining Loss: 1.652747 \tValidation Loss: 2.362042\n",
      "Epoch: 17374 \tTraining Loss: 1.658567 \tValidation Loss: 2.362003\n",
      "Epoch: 17375 \tTraining Loss: 1.649222 \tValidation Loss: 2.362395\n",
      "Epoch: 17376 \tTraining Loss: 1.677968 \tValidation Loss: 2.362019\n",
      "Epoch: 17377 \tTraining Loss: 1.635680 \tValidation Loss: 2.362658\n",
      "Epoch: 17378 \tTraining Loss: 1.639416 \tValidation Loss: 2.362668\n",
      "Epoch: 17379 \tTraining Loss: 1.682507 \tValidation Loss: 2.362420\n",
      "Epoch: 17380 \tTraining Loss: 1.627265 \tValidation Loss: 2.362905\n",
      "Epoch: 17381 \tTraining Loss: 1.658726 \tValidation Loss: 2.362935\n",
      "Epoch: 17382 \tTraining Loss: 1.621882 \tValidation Loss: 2.363177\n",
      "Epoch: 17383 \tTraining Loss: 1.607377 \tValidation Loss: 2.363522\n",
      "Epoch: 17384 \tTraining Loss: 1.672011 \tValidation Loss: 2.363595\n",
      "Epoch: 17385 \tTraining Loss: 1.658568 \tValidation Loss: 2.363504\n",
      "Epoch: 17386 \tTraining Loss: 1.665634 \tValidation Loss: 2.362646\n",
      "Epoch: 17387 \tTraining Loss: 1.629965 \tValidation Loss: 2.362579\n",
      "Epoch: 17388 \tTraining Loss: 1.624698 \tValidation Loss: 2.362895\n",
      "Epoch: 17389 \tTraining Loss: 1.673489 \tValidation Loss: 2.362753\n",
      "Epoch: 17390 \tTraining Loss: 1.653545 \tValidation Loss: 2.362567\n",
      "Epoch: 17391 \tTraining Loss: 1.639097 \tValidation Loss: 2.362490\n",
      "Epoch: 17392 \tTraining Loss: 1.641944 \tValidation Loss: 2.363374\n",
      "Epoch: 17393 \tTraining Loss: 1.666317 \tValidation Loss: 2.363186\n",
      "Epoch: 17394 \tTraining Loss: 1.635918 \tValidation Loss: 2.363214\n",
      "Epoch: 17395 \tTraining Loss: 1.624695 \tValidation Loss: 2.363091\n",
      "Epoch: 17396 \tTraining Loss: 1.623720 \tValidation Loss: 2.363087\n",
      "Epoch: 17397 \tTraining Loss: 1.644521 \tValidation Loss: 2.363606\n",
      "Epoch: 17398 \tTraining Loss: 1.627978 \tValidation Loss: 2.363670\n",
      "Epoch: 17399 \tTraining Loss: 1.657346 \tValidation Loss: 2.363372\n",
      "Epoch: 17400 \tTraining Loss: 1.651112 \tValidation Loss: 2.362798\n",
      "Epoch: 17401 \tTraining Loss: 1.669736 \tValidation Loss: 2.362747\n",
      "Epoch: 17402 \tTraining Loss: 1.626739 \tValidation Loss: 2.362928\n",
      "Epoch: 17403 \tTraining Loss: 1.641713 \tValidation Loss: 2.362842\n",
      "Epoch: 17404 \tTraining Loss: 1.639843 \tValidation Loss: 2.362575\n",
      "Epoch: 17405 \tTraining Loss: 1.622579 \tValidation Loss: 2.363159\n",
      "Epoch: 17406 \tTraining Loss: 1.690417 \tValidation Loss: 2.363143\n",
      "Epoch: 17407 \tTraining Loss: 1.641451 \tValidation Loss: 2.363235\n",
      "Epoch: 17408 \tTraining Loss: 1.661469 \tValidation Loss: 2.363128\n",
      "Epoch: 17409 \tTraining Loss: 1.651312 \tValidation Loss: 2.363660\n",
      "Epoch: 17410 \tTraining Loss: 1.648738 \tValidation Loss: 2.363399\n",
      "Epoch: 17411 \tTraining Loss: 1.647821 \tValidation Loss: 2.362785\n",
      "Epoch: 17412 \tTraining Loss: 1.629922 \tValidation Loss: 2.363117\n",
      "Epoch: 17413 \tTraining Loss: 1.614091 \tValidation Loss: 2.363363\n",
      "Epoch: 17414 \tTraining Loss: 1.634599 \tValidation Loss: 2.363240\n",
      "Epoch: 17415 \tTraining Loss: 1.671230 \tValidation Loss: 2.363291\n",
      "Epoch: 17416 \tTraining Loss: 1.608265 \tValidation Loss: 2.364175\n",
      "Epoch: 17417 \tTraining Loss: 1.655253 \tValidation Loss: 2.363985\n",
      "Epoch: 17418 \tTraining Loss: 1.622536 \tValidation Loss: 2.363852\n",
      "Epoch: 17419 \tTraining Loss: 1.684272 \tValidation Loss: 2.363073\n",
      "Epoch: 17420 \tTraining Loss: 1.682276 \tValidation Loss: 2.362682\n",
      "Epoch: 17421 \tTraining Loss: 1.627056 \tValidation Loss: 2.362747\n",
      "Epoch: 17422 \tTraining Loss: 1.617171 \tValidation Loss: 2.363219\n",
      "Epoch: 17423 \tTraining Loss: 1.654131 \tValidation Loss: 2.363458\n",
      "Epoch: 17424 \tTraining Loss: 1.652021 \tValidation Loss: 2.363185\n",
      "Epoch: 17425 \tTraining Loss: 1.643740 \tValidation Loss: 2.363163\n",
      "Epoch: 17426 \tTraining Loss: 1.681074 \tValidation Loss: 2.363158\n",
      "Epoch: 17427 \tTraining Loss: 1.628567 \tValidation Loss: 2.363296\n",
      "Epoch: 17428 \tTraining Loss: 1.673984 \tValidation Loss: 2.363122\n",
      "Epoch: 17429 \tTraining Loss: 1.675481 \tValidation Loss: 2.363358\n",
      "Epoch: 17430 \tTraining Loss: 1.661433 \tValidation Loss: 2.363661\n",
      "Epoch: 17431 \tTraining Loss: 1.673947 \tValidation Loss: 2.363412\n",
      "Epoch: 17432 \tTraining Loss: 1.643671 \tValidation Loss: 2.363625\n",
      "Epoch: 17433 \tTraining Loss: 1.589386 \tValidation Loss: 2.363896\n",
      "Epoch: 17434 \tTraining Loss: 1.663278 \tValidation Loss: 2.363574\n",
      "Epoch: 17435 \tTraining Loss: 1.620956 \tValidation Loss: 2.363546\n",
      "Epoch: 17436 \tTraining Loss: 1.663097 \tValidation Loss: 2.362955\n",
      "Epoch: 17437 \tTraining Loss: 1.672008 \tValidation Loss: 2.363482\n",
      "Epoch: 17438 \tTraining Loss: 1.659958 \tValidation Loss: 2.362960\n",
      "Epoch: 17439 \tTraining Loss: 1.656715 \tValidation Loss: 2.363125\n",
      "Epoch: 17440 \tTraining Loss: 1.648722 \tValidation Loss: 2.363441\n",
      "Epoch: 17441 \tTraining Loss: 1.651682 \tValidation Loss: 2.363284\n",
      "Epoch: 17442 \tTraining Loss: 1.624267 \tValidation Loss: 2.363806\n",
      "Epoch: 17443 \tTraining Loss: 1.621151 \tValidation Loss: 2.364205\n",
      "Epoch: 17444 \tTraining Loss: 1.640752 \tValidation Loss: 2.363969\n",
      "Epoch: 17445 \tTraining Loss: 1.674433 \tValidation Loss: 2.364101\n",
      "Epoch: 17446 \tTraining Loss: 1.661936 \tValidation Loss: 2.363929\n",
      "Epoch: 17447 \tTraining Loss: 1.650685 \tValidation Loss: 2.363914\n",
      "Epoch: 17448 \tTraining Loss: 1.667414 \tValidation Loss: 2.363932\n",
      "Epoch: 17449 \tTraining Loss: 1.616642 \tValidation Loss: 2.363616\n",
      "Epoch: 17450 \tTraining Loss: 1.634524 \tValidation Loss: 2.363829\n",
      "Epoch: 17451 \tTraining Loss: 1.638020 \tValidation Loss: 2.363028\n",
      "Epoch: 17452 \tTraining Loss: 1.639261 \tValidation Loss: 2.363286\n",
      "Epoch: 17453 \tTraining Loss: 1.639929 \tValidation Loss: 2.363459\n",
      "Epoch: 17454 \tTraining Loss: 1.661352 \tValidation Loss: 2.363392\n",
      "Epoch: 17455 \tTraining Loss: 1.654123 \tValidation Loss: 2.363228\n",
      "Epoch: 17456 \tTraining Loss: 1.655722 \tValidation Loss: 2.363158\n",
      "Epoch: 17457 \tTraining Loss: 1.641007 \tValidation Loss: 2.363105\n",
      "Epoch: 17458 \tTraining Loss: 1.598958 \tValidation Loss: 2.363616\n",
      "Epoch: 17459 \tTraining Loss: 1.617619 \tValidation Loss: 2.363605\n",
      "Epoch: 17460 \tTraining Loss: 1.650819 \tValidation Loss: 2.363349\n",
      "Epoch: 17461 \tTraining Loss: 1.616403 \tValidation Loss: 2.363486\n",
      "Epoch: 17462 \tTraining Loss: 1.675091 \tValidation Loss: 2.363238\n",
      "Epoch: 17463 \tTraining Loss: 1.642820 \tValidation Loss: 2.363793\n",
      "Epoch: 17464 \tTraining Loss: 1.622530 \tValidation Loss: 2.364086\n",
      "Epoch: 17465 \tTraining Loss: 1.618300 \tValidation Loss: 2.363156\n",
      "Epoch: 17466 \tTraining Loss: 1.642417 \tValidation Loss: 2.363039\n",
      "Epoch: 17467 \tTraining Loss: 1.626893 \tValidation Loss: 2.363996\n",
      "Epoch: 17468 \tTraining Loss: 1.647795 \tValidation Loss: 2.364183\n",
      "Epoch: 17469 \tTraining Loss: 1.637578 \tValidation Loss: 2.364156\n",
      "Epoch: 17470 \tTraining Loss: 1.655744 \tValidation Loss: 2.363796\n",
      "Epoch: 17471 \tTraining Loss: 1.625281 \tValidation Loss: 2.363302\n",
      "Epoch: 17472 \tTraining Loss: 1.648974 \tValidation Loss: 2.363657\n",
      "Epoch: 17473 \tTraining Loss: 1.597219 \tValidation Loss: 2.363964\n",
      "Epoch: 17474 \tTraining Loss: 1.663926 \tValidation Loss: 2.364752\n",
      "Epoch: 17475 \tTraining Loss: 1.669088 \tValidation Loss: 2.364582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17476 \tTraining Loss: 1.657318 \tValidation Loss: 2.364756\n",
      "Epoch: 17477 \tTraining Loss: 1.661498 \tValidation Loss: 2.363858\n",
      "Epoch: 17478 \tTraining Loss: 1.646195 \tValidation Loss: 2.363890\n",
      "Epoch: 17479 \tTraining Loss: 1.635768 \tValidation Loss: 2.363805\n",
      "Epoch: 17480 \tTraining Loss: 1.650514 \tValidation Loss: 2.364240\n",
      "Epoch: 17481 \tTraining Loss: 1.625834 \tValidation Loss: 2.364204\n",
      "Epoch: 17482 \tTraining Loss: 1.625451 \tValidation Loss: 2.364342\n",
      "Epoch: 17483 \tTraining Loss: 1.637167 \tValidation Loss: 2.364249\n",
      "Epoch: 17484 \tTraining Loss: 1.626386 \tValidation Loss: 2.364342\n",
      "Epoch: 17485 \tTraining Loss: 1.664575 \tValidation Loss: 2.363816\n",
      "Epoch: 17486 \tTraining Loss: 1.681096 \tValidation Loss: 2.363241\n",
      "Epoch: 17487 \tTraining Loss: 1.680541 \tValidation Loss: 2.363488\n",
      "Epoch: 17488 \tTraining Loss: 1.657474 \tValidation Loss: 2.363407\n",
      "Epoch: 17489 \tTraining Loss: 1.648002 \tValidation Loss: 2.364070\n",
      "Epoch: 17490 \tTraining Loss: 1.607354 \tValidation Loss: 2.364106\n",
      "Epoch: 17491 \tTraining Loss: 1.653829 \tValidation Loss: 2.364056\n",
      "Epoch: 17492 \tTraining Loss: 1.637156 \tValidation Loss: 2.364169\n",
      "Epoch: 17493 \tTraining Loss: 1.638844 \tValidation Loss: 2.363758\n",
      "Epoch: 17494 \tTraining Loss: 1.653722 \tValidation Loss: 2.362974\n",
      "Epoch: 17495 \tTraining Loss: 1.630901 \tValidation Loss: 2.363869\n",
      "Epoch: 17496 \tTraining Loss: 1.651257 \tValidation Loss: 2.363485\n",
      "Epoch: 17497 \tTraining Loss: 1.696662 \tValidation Loss: 2.363207\n",
      "Epoch: 17498 \tTraining Loss: 1.616407 \tValidation Loss: 2.363256\n",
      "Epoch: 17499 \tTraining Loss: 1.613869 \tValidation Loss: 2.363668\n",
      "Epoch: 17500 \tTraining Loss: 1.631010 \tValidation Loss: 2.364122\n",
      "Epoch: 17501 \tTraining Loss: 1.665619 \tValidation Loss: 2.364528\n",
      "Epoch: 17502 \tTraining Loss: 1.668090 \tValidation Loss: 2.364273\n",
      "Epoch: 17503 \tTraining Loss: 1.652951 \tValidation Loss: 2.363716\n",
      "Epoch: 17504 \tTraining Loss: 1.647603 \tValidation Loss: 2.364059\n",
      "Epoch: 17505 \tTraining Loss: 1.634400 \tValidation Loss: 2.363894\n",
      "Epoch: 17506 \tTraining Loss: 1.610368 \tValidation Loss: 2.364217\n",
      "Epoch: 17507 \tTraining Loss: 1.606215 \tValidation Loss: 2.363915\n",
      "Epoch: 17508 \tTraining Loss: 1.668290 \tValidation Loss: 2.364105\n",
      "Epoch: 17509 \tTraining Loss: 1.642620 \tValidation Loss: 2.363892\n",
      "Epoch: 17510 \tTraining Loss: 1.660448 \tValidation Loss: 2.363963\n",
      "Epoch: 17511 \tTraining Loss: 1.657724 \tValidation Loss: 2.364034\n",
      "Epoch: 17512 \tTraining Loss: 1.614854 \tValidation Loss: 2.364610\n",
      "Epoch: 17513 \tTraining Loss: 1.616564 \tValidation Loss: 2.364368\n",
      "Epoch: 17514 \tTraining Loss: 1.624110 \tValidation Loss: 2.364290\n",
      "Epoch: 17515 \tTraining Loss: 1.668870 \tValidation Loss: 2.364075\n",
      "Epoch: 17516 \tTraining Loss: 1.613571 \tValidation Loss: 2.364172\n",
      "Epoch: 17517 \tTraining Loss: 1.662384 \tValidation Loss: 2.364134\n",
      "Epoch: 17518 \tTraining Loss: 1.662510 \tValidation Loss: 2.364042\n",
      "Epoch: 17519 \tTraining Loss: 1.643417 \tValidation Loss: 2.363880\n",
      "Epoch: 17520 \tTraining Loss: 1.640176 \tValidation Loss: 2.364174\n",
      "Epoch: 17521 \tTraining Loss: 1.646326 \tValidation Loss: 2.363916\n",
      "Epoch: 17522 \tTraining Loss: 1.640948 \tValidation Loss: 2.363833\n",
      "Epoch: 17523 \tTraining Loss: 1.685522 \tValidation Loss: 2.363862\n",
      "Epoch: 17524 \tTraining Loss: 1.637259 \tValidation Loss: 2.363972\n",
      "Epoch: 17525 \tTraining Loss: 1.682310 \tValidation Loss: 2.363820\n",
      "Epoch: 17526 \tTraining Loss: 1.616181 \tValidation Loss: 2.364204\n",
      "Epoch: 17527 \tTraining Loss: 1.629487 \tValidation Loss: 2.364712\n",
      "Epoch: 17528 \tTraining Loss: 1.658326 \tValidation Loss: 2.364346\n",
      "Epoch: 17529 \tTraining Loss: 1.699817 \tValidation Loss: 2.363744\n",
      "Epoch: 17530 \tTraining Loss: 1.664916 \tValidation Loss: 2.364031\n",
      "Epoch: 17531 \tTraining Loss: 1.627206 \tValidation Loss: 2.364486\n",
      "Epoch: 17532 \tTraining Loss: 1.590673 \tValidation Loss: 2.364991\n",
      "Epoch: 17533 \tTraining Loss: 1.643699 \tValidation Loss: 2.364162\n",
      "Epoch: 17534 \tTraining Loss: 1.631434 \tValidation Loss: 2.364900\n",
      "Epoch: 17535 \tTraining Loss: 1.662775 \tValidation Loss: 2.364789\n",
      "Epoch: 17536 \tTraining Loss: 1.653004 \tValidation Loss: 2.364722\n",
      "Epoch: 17537 \tTraining Loss: 1.614889 \tValidation Loss: 2.364995\n",
      "Epoch: 17538 \tTraining Loss: 1.659990 \tValidation Loss: 2.364733\n",
      "Epoch: 17539 \tTraining Loss: 1.651859 \tValidation Loss: 2.364830\n",
      "Epoch: 17540 \tTraining Loss: 1.641931 \tValidation Loss: 2.364902\n",
      "Epoch: 17541 \tTraining Loss: 1.650854 \tValidation Loss: 2.364846\n",
      "Epoch: 17542 \tTraining Loss: 1.645251 \tValidation Loss: 2.364619\n",
      "Epoch: 17543 \tTraining Loss: 1.654647 \tValidation Loss: 2.364739\n",
      "Epoch: 17544 \tTraining Loss: 1.637097 \tValidation Loss: 2.364733\n",
      "Epoch: 17545 \tTraining Loss: 1.650457 \tValidation Loss: 2.364932\n",
      "Epoch: 17546 \tTraining Loss: 1.602769 \tValidation Loss: 2.364760\n",
      "Epoch: 17547 \tTraining Loss: 1.654698 \tValidation Loss: 2.364571\n",
      "Epoch: 17548 \tTraining Loss: 1.664084 \tValidation Loss: 2.364712\n",
      "Epoch: 17549 \tTraining Loss: 1.631259 \tValidation Loss: 2.364774\n",
      "Epoch: 17550 \tTraining Loss: 1.689627 \tValidation Loss: 2.363861\n",
      "Epoch: 17551 \tTraining Loss: 1.626544 \tValidation Loss: 2.364200\n",
      "Epoch: 17552 \tTraining Loss: 1.630196 \tValidation Loss: 2.364246\n",
      "Epoch: 17553 \tTraining Loss: 1.640940 \tValidation Loss: 2.364571\n",
      "Epoch: 17554 \tTraining Loss: 1.637148 \tValidation Loss: 2.364379\n",
      "Epoch: 17555 \tTraining Loss: 1.655220 \tValidation Loss: 2.364131\n",
      "Epoch: 17556 \tTraining Loss: 1.639099 \tValidation Loss: 2.364415\n",
      "Epoch: 17557 \tTraining Loss: 1.613819 \tValidation Loss: 2.364633\n",
      "Epoch: 17558 \tTraining Loss: 1.663340 \tValidation Loss: 2.364789\n",
      "Epoch: 17559 \tTraining Loss: 1.631240 \tValidation Loss: 2.365311\n",
      "Epoch: 17560 \tTraining Loss: 1.622087 \tValidation Loss: 2.365242\n",
      "Epoch: 17561 \tTraining Loss: 1.628508 \tValidation Loss: 2.365546\n",
      "Epoch: 17562 \tTraining Loss: 1.655377 \tValidation Loss: 2.365186\n",
      "Epoch: 17563 \tTraining Loss: 1.652309 \tValidation Loss: 2.365119\n",
      "Epoch: 17564 \tTraining Loss: 1.626428 \tValidation Loss: 2.364914\n",
      "Epoch: 17565 \tTraining Loss: 1.631577 \tValidation Loss: 2.365409\n",
      "Epoch: 17566 \tTraining Loss: 1.670251 \tValidation Loss: 2.365407\n",
      "Epoch: 17567 \tTraining Loss: 1.658399 \tValidation Loss: 2.365142\n",
      "Epoch: 17568 \tTraining Loss: 1.628974 \tValidation Loss: 2.365090\n",
      "Epoch: 17569 \tTraining Loss: 1.598917 \tValidation Loss: 2.365451\n",
      "Epoch: 17570 \tTraining Loss: 1.645324 \tValidation Loss: 2.365213\n",
      "Epoch: 17571 \tTraining Loss: 1.663383 \tValidation Loss: 2.364794\n",
      "Epoch: 17572 \tTraining Loss: 1.642262 \tValidation Loss: 2.365504\n",
      "Epoch: 17573 \tTraining Loss: 1.620532 \tValidation Loss: 2.364857\n",
      "Epoch: 17574 \tTraining Loss: 1.677074 \tValidation Loss: 2.363956\n",
      "Epoch: 17575 \tTraining Loss: 1.661134 \tValidation Loss: 2.364395\n",
      "Epoch: 17576 \tTraining Loss: 1.688358 \tValidation Loss: 2.364361\n",
      "Epoch: 17577 \tTraining Loss: 1.603436 \tValidation Loss: 2.364986\n",
      "Epoch: 17578 \tTraining Loss: 1.613640 \tValidation Loss: 2.365360\n",
      "Epoch: 17579 \tTraining Loss: 1.617165 \tValidation Loss: 2.365412\n",
      "Epoch: 17580 \tTraining Loss: 1.677542 \tValidation Loss: 2.364621\n",
      "Epoch: 17581 \tTraining Loss: 1.657143 \tValidation Loss: 2.364758\n",
      "Epoch: 17582 \tTraining Loss: 1.645929 \tValidation Loss: 2.365369\n",
      "Epoch: 17583 \tTraining Loss: 1.660200 \tValidation Loss: 2.365305\n",
      "Epoch: 17584 \tTraining Loss: 1.667328 \tValidation Loss: 2.365502\n",
      "Epoch: 17585 \tTraining Loss: 1.628374 \tValidation Loss: 2.364933\n",
      "Epoch: 17586 \tTraining Loss: 1.662255 \tValidation Loss: 2.364852\n",
      "Epoch: 17587 \tTraining Loss: 1.634990 \tValidation Loss: 2.364604\n",
      "Epoch: 17588 \tTraining Loss: 1.642695 \tValidation Loss: 2.364383\n",
      "Epoch: 17589 \tTraining Loss: 1.629231 \tValidation Loss: 2.364790\n",
      "Epoch: 17590 \tTraining Loss: 1.624594 \tValidation Loss: 2.365188\n",
      "Epoch: 17591 \tTraining Loss: 1.648356 \tValidation Loss: 2.364894\n",
      "Epoch: 17592 \tTraining Loss: 1.606510 \tValidation Loss: 2.365183\n",
      "Epoch: 17593 \tTraining Loss: 1.679059 \tValidation Loss: 2.364975\n",
      "Epoch: 17594 \tTraining Loss: 1.620051 \tValidation Loss: 2.365193\n",
      "Epoch: 17595 \tTraining Loss: 1.640073 \tValidation Loss: 2.365289\n",
      "Epoch: 17596 \tTraining Loss: 1.627277 \tValidation Loss: 2.365561\n",
      "Epoch: 17597 \tTraining Loss: 1.617728 \tValidation Loss: 2.365839\n",
      "Epoch: 17598 \tTraining Loss: 1.621969 \tValidation Loss: 2.365241\n",
      "Epoch: 17599 \tTraining Loss: 1.621555 \tValidation Loss: 2.365627\n",
      "Epoch: 17600 \tTraining Loss: 1.638039 \tValidation Loss: 2.365401\n",
      "Epoch: 17601 \tTraining Loss: 1.670632 \tValidation Loss: 2.365514\n",
      "Epoch: 17602 \tTraining Loss: 1.645640 \tValidation Loss: 2.365214\n",
      "Epoch: 17603 \tTraining Loss: 1.646041 \tValidation Loss: 2.365264\n",
      "Epoch: 17604 \tTraining Loss: 1.650660 \tValidation Loss: 2.365421\n",
      "Epoch: 17605 \tTraining Loss: 1.626695 \tValidation Loss: 2.365436\n",
      "Epoch: 17606 \tTraining Loss: 1.633932 \tValidation Loss: 2.364580\n",
      "Epoch: 17607 \tTraining Loss: 1.634227 \tValidation Loss: 2.365182\n",
      "Epoch: 17608 \tTraining Loss: 1.605054 \tValidation Loss: 2.365767\n",
      "Epoch: 17609 \tTraining Loss: 1.656163 \tValidation Loss: 2.366169\n",
      "Epoch: 17610 \tTraining Loss: 1.616706 \tValidation Loss: 2.366120\n",
      "Epoch: 17611 \tTraining Loss: 1.638750 \tValidation Loss: 2.365870\n",
      "Epoch: 17612 \tTraining Loss: 1.613109 \tValidation Loss: 2.365737\n",
      "Epoch: 17613 \tTraining Loss: 1.671658 \tValidation Loss: 2.365962\n",
      "Epoch: 17614 \tTraining Loss: 1.667139 \tValidation Loss: 2.365646\n",
      "Epoch: 17615 \tTraining Loss: 1.683995 \tValidation Loss: 2.365476\n",
      "Epoch: 17616 \tTraining Loss: 1.679521 \tValidation Loss: 2.365444\n",
      "Epoch: 17617 \tTraining Loss: 1.641324 \tValidation Loss: 2.365144\n",
      "Epoch: 17618 \tTraining Loss: 1.654583 \tValidation Loss: 2.365447\n",
      "Epoch: 17619 \tTraining Loss: 1.632509 \tValidation Loss: 2.365793\n",
      "Epoch: 17620 \tTraining Loss: 1.624176 \tValidation Loss: 2.365431\n",
      "Epoch: 17621 \tTraining Loss: 1.654560 \tValidation Loss: 2.365232\n",
      "Epoch: 17622 \tTraining Loss: 1.660917 \tValidation Loss: 2.364926\n",
      "Epoch: 17623 \tTraining Loss: 1.665102 \tValidation Loss: 2.365506\n",
      "Epoch: 17624 \tTraining Loss: 1.674472 \tValidation Loss: 2.365750\n",
      "Epoch: 17625 \tTraining Loss: 1.588876 \tValidation Loss: 2.365573\n",
      "Epoch: 17626 \tTraining Loss: 1.612511 \tValidation Loss: 2.364944\n",
      "Epoch: 17627 \tTraining Loss: 1.654949 \tValidation Loss: 2.364958\n",
      "Epoch: 17628 \tTraining Loss: 1.713431 \tValidation Loss: 2.365579\n",
      "Epoch: 17629 \tTraining Loss: 1.693977 \tValidation Loss: 2.365751\n",
      "Epoch: 17630 \tTraining Loss: 1.649006 \tValidation Loss: 2.365840\n",
      "Epoch: 17631 \tTraining Loss: 1.636150 \tValidation Loss: 2.365255\n",
      "Epoch: 17632 \tTraining Loss: 1.629314 \tValidation Loss: 2.365687\n",
      "Epoch: 17633 \tTraining Loss: 1.624803 \tValidation Loss: 2.365789\n",
      "Epoch: 17634 \tTraining Loss: 1.599129 \tValidation Loss: 2.365840\n",
      "Epoch: 17635 \tTraining Loss: 1.661683 \tValidation Loss: 2.365321\n",
      "Epoch: 17636 \tTraining Loss: 1.650141 \tValidation Loss: 2.365819\n",
      "Epoch: 17637 \tTraining Loss: 1.640193 \tValidation Loss: 2.365876\n",
      "Epoch: 17638 \tTraining Loss: 1.600277 \tValidation Loss: 2.365411\n",
      "Epoch: 17639 \tTraining Loss: 1.645594 \tValidation Loss: 2.365840\n",
      "Epoch: 17640 \tTraining Loss: 1.637189 \tValidation Loss: 2.365863\n",
      "Epoch: 17641 \tTraining Loss: 1.649981 \tValidation Loss: 2.366276\n",
      "Epoch: 17642 \tTraining Loss: 1.662095 \tValidation Loss: 2.365662\n",
      "Epoch: 17643 \tTraining Loss: 1.638383 \tValidation Loss: 2.365864\n",
      "Epoch: 17644 \tTraining Loss: 1.606788 \tValidation Loss: 2.366359\n",
      "Epoch: 17645 \tTraining Loss: 1.607932 \tValidation Loss: 2.366048\n",
      "Epoch: 17646 \tTraining Loss: 1.617049 \tValidation Loss: 2.366118\n",
      "Epoch: 17647 \tTraining Loss: 1.609864 \tValidation Loss: 2.366086\n",
      "Epoch: 17648 \tTraining Loss: 1.627539 \tValidation Loss: 2.366199\n",
      "Epoch: 17649 \tTraining Loss: 1.643511 \tValidation Loss: 2.366018\n",
      "Epoch: 17650 \tTraining Loss: 1.655380 \tValidation Loss: 2.366325\n",
      "Epoch: 17651 \tTraining Loss: 1.655792 \tValidation Loss: 2.366109\n",
      "Epoch: 17652 \tTraining Loss: 1.630286 \tValidation Loss: 2.366172\n",
      "Epoch: 17653 \tTraining Loss: 1.667934 \tValidation Loss: 2.365954\n",
      "Epoch: 17654 \tTraining Loss: 1.664898 \tValidation Loss: 2.366206\n",
      "Epoch: 17655 \tTraining Loss: 1.663639 \tValidation Loss: 2.366216\n",
      "Epoch: 17656 \tTraining Loss: 1.637864 \tValidation Loss: 2.365880\n",
      "Epoch: 17657 \tTraining Loss: 1.612632 \tValidation Loss: 2.366213\n",
      "Epoch: 17658 \tTraining Loss: 1.644561 \tValidation Loss: 2.365993\n",
      "Epoch: 17659 \tTraining Loss: 1.629359 \tValidation Loss: 2.365937\n",
      "Epoch: 17660 \tTraining Loss: 1.645765 \tValidation Loss: 2.366318\n",
      "Epoch: 17661 \tTraining Loss: 1.639507 \tValidation Loss: 2.366458\n",
      "Epoch: 17662 \tTraining Loss: 1.646709 \tValidation Loss: 2.366368\n",
      "Epoch: 17663 \tTraining Loss: 1.630241 \tValidation Loss: 2.366893\n",
      "Epoch: 17664 \tTraining Loss: 1.664076 \tValidation Loss: 2.365630\n",
      "Epoch: 17665 \tTraining Loss: 1.613053 \tValidation Loss: 2.366223\n",
      "Epoch: 17666 \tTraining Loss: 1.635347 \tValidation Loss: 2.366333\n",
      "Epoch: 17667 \tTraining Loss: 1.689155 \tValidation Loss: 2.366305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17668 \tTraining Loss: 1.638288 \tValidation Loss: 2.365953\n",
      "Epoch: 17669 \tTraining Loss: 1.603764 \tValidation Loss: 2.366595\n",
      "Epoch: 17670 \tTraining Loss: 1.654796 \tValidation Loss: 2.366814\n",
      "Epoch: 17671 \tTraining Loss: 1.642796 \tValidation Loss: 2.367004\n",
      "Epoch: 17672 \tTraining Loss: 1.643160 \tValidation Loss: 2.366395\n",
      "Epoch: 17673 \tTraining Loss: 1.681493 \tValidation Loss: 2.365854\n",
      "Epoch: 17674 \tTraining Loss: 1.645759 \tValidation Loss: 2.366151\n",
      "Epoch: 17675 \tTraining Loss: 1.623069 \tValidation Loss: 2.366934\n",
      "Epoch: 17676 \tTraining Loss: 1.635386 \tValidation Loss: 2.366861\n",
      "Epoch: 17677 \tTraining Loss: 1.653177 \tValidation Loss: 2.366067\n",
      "Epoch: 17678 \tTraining Loss: 1.667777 \tValidation Loss: 2.366361\n",
      "Epoch: 17679 \tTraining Loss: 1.649494 \tValidation Loss: 2.366120\n",
      "Epoch: 17680 \tTraining Loss: 1.661634 \tValidation Loss: 2.366652\n",
      "Epoch: 17681 \tTraining Loss: 1.633401 \tValidation Loss: 2.366045\n",
      "Epoch: 17682 \tTraining Loss: 1.623401 \tValidation Loss: 2.366176\n",
      "Epoch: 17683 \tTraining Loss: 1.627930 \tValidation Loss: 2.366050\n",
      "Epoch: 17684 \tTraining Loss: 1.617967 \tValidation Loss: 2.366222\n",
      "Epoch: 17685 \tTraining Loss: 1.661485 \tValidation Loss: 2.366726\n",
      "Epoch: 17686 \tTraining Loss: 1.669346 \tValidation Loss: 2.366431\n",
      "Epoch: 17687 \tTraining Loss: 1.624304 \tValidation Loss: 2.367009\n",
      "Epoch: 17688 \tTraining Loss: 1.646544 \tValidation Loss: 2.366655\n",
      "Epoch: 17689 \tTraining Loss: 1.635218 \tValidation Loss: 2.366690\n",
      "Epoch: 17690 \tTraining Loss: 1.623532 \tValidation Loss: 2.366252\n",
      "Epoch: 17691 \tTraining Loss: 1.667804 \tValidation Loss: 2.366184\n",
      "Epoch: 17692 \tTraining Loss: 1.663361 \tValidation Loss: 2.366418\n",
      "Epoch: 17693 \tTraining Loss: 1.632848 \tValidation Loss: 2.366113\n",
      "Epoch: 17694 \tTraining Loss: 1.622477 \tValidation Loss: 2.366483\n",
      "Epoch: 17695 \tTraining Loss: 1.642043 \tValidation Loss: 2.366520\n",
      "Epoch: 17696 \tTraining Loss: 1.603749 \tValidation Loss: 2.366886\n",
      "Epoch: 17697 \tTraining Loss: 1.633456 \tValidation Loss: 2.367304\n",
      "Epoch: 17698 \tTraining Loss: 1.603199 \tValidation Loss: 2.367506\n",
      "Epoch: 17699 \tTraining Loss: 1.654326 \tValidation Loss: 2.367210\n",
      "Epoch: 17700 \tTraining Loss: 1.680969 \tValidation Loss: 2.366948\n",
      "Epoch: 17701 \tTraining Loss: 1.637700 \tValidation Loss: 2.366931\n",
      "Epoch: 17702 \tTraining Loss: 1.666552 \tValidation Loss: 2.366555\n",
      "Epoch: 17703 \tTraining Loss: 1.606919 \tValidation Loss: 2.366927\n",
      "Epoch: 17704 \tTraining Loss: 1.637224 \tValidation Loss: 2.366848\n",
      "Epoch: 17705 \tTraining Loss: 1.636377 \tValidation Loss: 2.366850\n",
      "Epoch: 17706 \tTraining Loss: 1.621838 \tValidation Loss: 2.367070\n",
      "Epoch: 17707 \tTraining Loss: 1.601338 \tValidation Loss: 2.367260\n",
      "Epoch: 17708 \tTraining Loss: 1.621570 \tValidation Loss: 2.367569\n",
      "Epoch: 17709 \tTraining Loss: 1.636206 \tValidation Loss: 2.367629\n",
      "Epoch: 17710 \tTraining Loss: 1.615871 \tValidation Loss: 2.367766\n",
      "Epoch: 17711 \tTraining Loss: 1.601843 \tValidation Loss: 2.367454\n",
      "Epoch: 17712 \tTraining Loss: 1.683689 \tValidation Loss: 2.366804\n",
      "Epoch: 17713 \tTraining Loss: 1.667139 \tValidation Loss: 2.366704\n",
      "Epoch: 17714 \tTraining Loss: 1.642870 \tValidation Loss: 2.366377\n",
      "Epoch: 17715 \tTraining Loss: 1.673255 \tValidation Loss: 2.366407\n",
      "Epoch: 17716 \tTraining Loss: 1.629686 \tValidation Loss: 2.366815\n",
      "Epoch: 17717 \tTraining Loss: 1.600671 \tValidation Loss: 2.368074\n",
      "Epoch: 17718 \tTraining Loss: 1.642440 \tValidation Loss: 2.366884\n",
      "Epoch: 17719 \tTraining Loss: 1.585509 \tValidation Loss: 2.367488\n",
      "Epoch: 17720 \tTraining Loss: 1.689440 \tValidation Loss: 2.367620\n",
      "Epoch: 17721 \tTraining Loss: 1.627228 \tValidation Loss: 2.367169\n",
      "Epoch: 17722 \tTraining Loss: 1.628848 \tValidation Loss: 2.367138\n",
      "Epoch: 17723 \tTraining Loss: 1.647195 \tValidation Loss: 2.366850\n",
      "Epoch: 17724 \tTraining Loss: 1.634780 \tValidation Loss: 2.366982\n",
      "Epoch: 17725 \tTraining Loss: 1.621258 \tValidation Loss: 2.367733\n",
      "Epoch: 17726 \tTraining Loss: 1.648753 \tValidation Loss: 2.367611\n",
      "Epoch: 17727 \tTraining Loss: 1.649154 \tValidation Loss: 2.367198\n",
      "Epoch: 17728 \tTraining Loss: 1.638323 \tValidation Loss: 2.367191\n",
      "Epoch: 17729 \tTraining Loss: 1.623807 \tValidation Loss: 2.367274\n",
      "Epoch: 17730 \tTraining Loss: 1.639231 \tValidation Loss: 2.367390\n",
      "Epoch: 17731 \tTraining Loss: 1.630148 \tValidation Loss: 2.367934\n",
      "Epoch: 17732 \tTraining Loss: 1.656513 \tValidation Loss: 2.367393\n",
      "Epoch: 17733 \tTraining Loss: 1.655181 \tValidation Loss: 2.367194\n",
      "Epoch: 17734 \tTraining Loss: 1.650848 \tValidation Loss: 2.367302\n",
      "Epoch: 17735 \tTraining Loss: 1.653540 \tValidation Loss: 2.366813\n",
      "Epoch: 17736 \tTraining Loss: 1.644191 \tValidation Loss: 2.366897\n",
      "Epoch: 17737 \tTraining Loss: 1.638608 \tValidation Loss: 2.367228\n",
      "Epoch: 17738 \tTraining Loss: 1.599706 \tValidation Loss: 2.367579\n",
      "Epoch: 17739 \tTraining Loss: 1.657939 \tValidation Loss: 2.367465\n",
      "Epoch: 17740 \tTraining Loss: 1.626214 \tValidation Loss: 2.367364\n",
      "Epoch: 17741 \tTraining Loss: 1.635666 \tValidation Loss: 2.367589\n",
      "Epoch: 17742 \tTraining Loss: 1.655560 \tValidation Loss: 2.367349\n",
      "Epoch: 17743 \tTraining Loss: 1.631176 \tValidation Loss: 2.367743\n",
      "Epoch: 17744 \tTraining Loss: 1.643183 \tValidation Loss: 2.367312\n",
      "Epoch: 17745 \tTraining Loss: 1.598622 \tValidation Loss: 2.367582\n",
      "Epoch: 17746 \tTraining Loss: 1.668642 \tValidation Loss: 2.367175\n",
      "Epoch: 17747 \tTraining Loss: 1.648286 \tValidation Loss: 2.366757\n",
      "Epoch: 17748 \tTraining Loss: 1.673145 \tValidation Loss: 2.366189\n",
      "Epoch: 17749 \tTraining Loss: 1.611384 \tValidation Loss: 2.366378\n",
      "Epoch: 17750 \tTraining Loss: 1.652574 \tValidation Loss: 2.366671\n",
      "Epoch: 17751 \tTraining Loss: 1.617877 \tValidation Loss: 2.367479\n",
      "Epoch: 17752 \tTraining Loss: 1.599277 \tValidation Loss: 2.367779\n",
      "Epoch: 17753 \tTraining Loss: 1.628926 \tValidation Loss: 2.367677\n",
      "Epoch: 17754 \tTraining Loss: 1.648261 \tValidation Loss: 2.367342\n",
      "Epoch: 17755 \tTraining Loss: 1.634553 \tValidation Loss: 2.367584\n",
      "Epoch: 17756 \tTraining Loss: 1.621545 \tValidation Loss: 2.367468\n",
      "Epoch: 17757 \tTraining Loss: 1.671082 \tValidation Loss: 2.367306\n",
      "Epoch: 17758 \tTraining Loss: 1.595345 \tValidation Loss: 2.366926\n",
      "Epoch: 17759 \tTraining Loss: 1.614215 \tValidation Loss: 2.367375\n",
      "Epoch: 17760 \tTraining Loss: 1.608730 \tValidation Loss: 2.368321\n",
      "Epoch: 17761 \tTraining Loss: 1.660518 \tValidation Loss: 2.368559\n",
      "Epoch: 17762 \tTraining Loss: 1.642533 \tValidation Loss: 2.367981\n",
      "Epoch: 17763 \tTraining Loss: 1.609575 \tValidation Loss: 2.367650\n",
      "Epoch: 17764 \tTraining Loss: 1.635411 \tValidation Loss: 2.367622\n",
      "Epoch: 17765 \tTraining Loss: 1.644718 \tValidation Loss: 2.367477\n",
      "Epoch: 17766 \tTraining Loss: 1.620651 \tValidation Loss: 2.368054\n",
      "Epoch: 17767 \tTraining Loss: 1.623994 \tValidation Loss: 2.368454\n",
      "Epoch: 17768 \tTraining Loss: 1.646485 \tValidation Loss: 2.368738\n",
      "Epoch: 17769 \tTraining Loss: 1.634382 \tValidation Loss: 2.368695\n",
      "Epoch: 17770 \tTraining Loss: 1.627351 \tValidation Loss: 2.367966\n",
      "Epoch: 17771 \tTraining Loss: 1.635080 \tValidation Loss: 2.367853\n",
      "Epoch: 17772 \tTraining Loss: 1.634859 \tValidation Loss: 2.368253\n",
      "Epoch: 17773 \tTraining Loss: 1.574045 \tValidation Loss: 2.368449\n",
      "Epoch: 17774 \tTraining Loss: 1.653509 \tValidation Loss: 2.368082\n",
      "Epoch: 17775 \tTraining Loss: 1.657845 \tValidation Loss: 2.368020\n",
      "Epoch: 17776 \tTraining Loss: 1.630481 \tValidation Loss: 2.367618\n",
      "Epoch: 17777 \tTraining Loss: 1.620955 \tValidation Loss: 2.368127\n",
      "Epoch: 17778 \tTraining Loss: 1.619355 \tValidation Loss: 2.367693\n",
      "Epoch: 17779 \tTraining Loss: 1.640588 \tValidation Loss: 2.368068\n",
      "Epoch: 17780 \tTraining Loss: 1.616910 \tValidation Loss: 2.368844\n",
      "Epoch: 17781 \tTraining Loss: 1.634292 \tValidation Loss: 2.368221\n",
      "Epoch: 17782 \tTraining Loss: 1.648066 \tValidation Loss: 2.367987\n",
      "Epoch: 17783 \tTraining Loss: 1.631333 \tValidation Loss: 2.367214\n",
      "Epoch: 17784 \tTraining Loss: 1.636377 \tValidation Loss: 2.368299\n",
      "Epoch: 17785 \tTraining Loss: 1.617252 \tValidation Loss: 2.368231\n",
      "Epoch: 17786 \tTraining Loss: 1.619858 \tValidation Loss: 2.367990\n",
      "Epoch: 17787 \tTraining Loss: 1.625497 \tValidation Loss: 2.367924\n",
      "Epoch: 17788 \tTraining Loss: 1.596420 \tValidation Loss: 2.368312\n",
      "Epoch: 17789 \tTraining Loss: 1.605957 \tValidation Loss: 2.368522\n",
      "Epoch: 17790 \tTraining Loss: 1.637296 \tValidation Loss: 2.368283\n",
      "Epoch: 17791 \tTraining Loss: 1.606131 \tValidation Loss: 2.368145\n",
      "Epoch: 17792 \tTraining Loss: 1.624943 \tValidation Loss: 2.368382\n",
      "Epoch: 17793 \tTraining Loss: 1.655344 \tValidation Loss: 2.368528\n",
      "Epoch: 17794 \tTraining Loss: 1.608601 \tValidation Loss: 2.368626\n",
      "Epoch: 17795 \tTraining Loss: 1.697417 \tValidation Loss: 2.367939\n",
      "Epoch: 17796 \tTraining Loss: 1.600519 \tValidation Loss: 2.368006\n",
      "Epoch: 17797 \tTraining Loss: 1.586254 \tValidation Loss: 2.368861\n",
      "Epoch: 17798 \tTraining Loss: 1.619675 \tValidation Loss: 2.368810\n",
      "Epoch: 17799 \tTraining Loss: 1.636054 \tValidation Loss: 2.368691\n",
      "Epoch: 17800 \tTraining Loss: 1.627794 \tValidation Loss: 2.368879\n",
      "Epoch: 17801 \tTraining Loss: 1.647013 \tValidation Loss: 2.368371\n",
      "Epoch: 17802 \tTraining Loss: 1.633468 \tValidation Loss: 2.368352\n",
      "Epoch: 17803 \tTraining Loss: 1.650820 \tValidation Loss: 2.367890\n",
      "Epoch: 17804 \tTraining Loss: 1.608550 \tValidation Loss: 2.367238\n",
      "Epoch: 17805 \tTraining Loss: 1.637471 \tValidation Loss: 2.368403\n",
      "Epoch: 17806 \tTraining Loss: 1.634498 \tValidation Loss: 2.368431\n",
      "Epoch: 17807 \tTraining Loss: 1.659472 \tValidation Loss: 2.369165\n",
      "Epoch: 17808 \tTraining Loss: 1.617220 \tValidation Loss: 2.368973\n",
      "Epoch: 17809 \tTraining Loss: 1.631343 \tValidation Loss: 2.368268\n",
      "Epoch: 17810 \tTraining Loss: 1.642587 \tValidation Loss: 2.368379\n",
      "Epoch: 17811 \tTraining Loss: 1.655239 \tValidation Loss: 2.368097\n",
      "Epoch: 17812 \tTraining Loss: 1.613209 \tValidation Loss: 2.368639\n",
      "Epoch: 17813 \tTraining Loss: 1.655954 \tValidation Loss: 2.368449\n",
      "Epoch: 17814 \tTraining Loss: 1.670390 \tValidation Loss: 2.368423\n",
      "Epoch: 17815 \tTraining Loss: 1.691821 \tValidation Loss: 2.367538\n",
      "Epoch: 17816 \tTraining Loss: 1.620901 \tValidation Loss: 2.368488\n",
      "Epoch: 17817 \tTraining Loss: 1.620373 \tValidation Loss: 2.368742\n",
      "Epoch: 17818 \tTraining Loss: 1.675280 \tValidation Loss: 2.368810\n",
      "Epoch: 17819 \tTraining Loss: 1.617089 \tValidation Loss: 2.368528\n",
      "Epoch: 17820 \tTraining Loss: 1.634834 \tValidation Loss: 2.368786\n",
      "Epoch: 17821 \tTraining Loss: 1.699106 \tValidation Loss: 2.368080\n",
      "Epoch: 17822 \tTraining Loss: 1.629604 \tValidation Loss: 2.368573\n",
      "Epoch: 17823 \tTraining Loss: 1.649208 \tValidation Loss: 2.367982\n",
      "Epoch: 17824 \tTraining Loss: 1.633583 \tValidation Loss: 2.368386\n",
      "Epoch: 17825 \tTraining Loss: 1.633271 \tValidation Loss: 2.368294\n",
      "Epoch: 17826 \tTraining Loss: 1.599322 \tValidation Loss: 2.368613\n",
      "Epoch: 17827 \tTraining Loss: 1.626383 \tValidation Loss: 2.368814\n",
      "Epoch: 17828 \tTraining Loss: 1.605145 \tValidation Loss: 2.369470\n",
      "Epoch: 17829 \tTraining Loss: 1.621535 \tValidation Loss: 2.368556\n",
      "Epoch: 17830 \tTraining Loss: 1.616325 \tValidation Loss: 2.368424\n",
      "Epoch: 17831 \tTraining Loss: 1.645496 \tValidation Loss: 2.368375\n",
      "Epoch: 17832 \tTraining Loss: 1.600350 \tValidation Loss: 2.369190\n",
      "Epoch: 17833 \tTraining Loss: 1.622621 \tValidation Loss: 2.369069\n",
      "Epoch: 17834 \tTraining Loss: 1.674857 \tValidation Loss: 2.368225\n",
      "Epoch: 17835 \tTraining Loss: 1.644113 \tValidation Loss: 2.369155\n",
      "Epoch: 17836 \tTraining Loss: 1.619912 \tValidation Loss: 2.369227\n",
      "Epoch: 17837 \tTraining Loss: 1.660761 \tValidation Loss: 2.369189\n",
      "Epoch: 17838 \tTraining Loss: 1.639004 \tValidation Loss: 2.368622\n",
      "Epoch: 17839 \tTraining Loss: 1.641498 \tValidation Loss: 2.368277\n",
      "Epoch: 17840 \tTraining Loss: 1.620826 \tValidation Loss: 2.368879\n",
      "Epoch: 17841 \tTraining Loss: 1.597369 \tValidation Loss: 2.369654\n",
      "Epoch: 17842 \tTraining Loss: 1.671815 \tValidation Loss: 2.369650\n",
      "Epoch: 17843 \tTraining Loss: 1.620523 \tValidation Loss: 2.369541\n",
      "Epoch: 17844 \tTraining Loss: 1.643587 \tValidation Loss: 2.368943\n",
      "Epoch: 17845 \tTraining Loss: 1.623114 \tValidation Loss: 2.368621\n",
      "Epoch: 17846 \tTraining Loss: 1.640821 \tValidation Loss: 2.368679\n",
      "Epoch: 17847 \tTraining Loss: 1.611636 \tValidation Loss: 2.369016\n",
      "Epoch: 17848 \tTraining Loss: 1.635033 \tValidation Loss: 2.368784\n",
      "Epoch: 17849 \tTraining Loss: 1.643356 \tValidation Loss: 2.368952\n",
      "Epoch: 17850 \tTraining Loss: 1.661111 \tValidation Loss: 2.369202\n",
      "Epoch: 17851 \tTraining Loss: 1.579247 \tValidation Loss: 2.369792\n",
      "Epoch: 17852 \tTraining Loss: 1.645459 \tValidation Loss: 2.369376\n",
      "Epoch: 17853 \tTraining Loss: 1.637390 \tValidation Loss: 2.369442\n",
      "Epoch: 17854 \tTraining Loss: 1.639874 \tValidation Loss: 2.368799\n",
      "Epoch: 17855 \tTraining Loss: 1.684185 \tValidation Loss: 2.368274\n",
      "Epoch: 17856 \tTraining Loss: 1.651475 \tValidation Loss: 2.368572\n",
      "Epoch: 17857 \tTraining Loss: 1.635090 \tValidation Loss: 2.369394\n",
      "Epoch: 17858 \tTraining Loss: 1.604903 \tValidation Loss: 2.369059\n",
      "Epoch: 17859 \tTraining Loss: 1.628159 \tValidation Loss: 2.369171\n",
      "Epoch: 17860 \tTraining Loss: 1.642208 \tValidation Loss: 2.369207\n",
      "Epoch: 17861 \tTraining Loss: 1.626404 \tValidation Loss: 2.368988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17862 \tTraining Loss: 1.620135 \tValidation Loss: 2.369294\n",
      "Epoch: 17863 \tTraining Loss: 1.647535 \tValidation Loss: 2.369057\n",
      "Epoch: 17864 \tTraining Loss: 1.633153 \tValidation Loss: 2.369155\n",
      "Epoch: 17865 \tTraining Loss: 1.614164 \tValidation Loss: 2.369218\n",
      "Epoch: 17866 \tTraining Loss: 1.629366 \tValidation Loss: 2.369359\n",
      "Epoch: 17867 \tTraining Loss: 1.622859 \tValidation Loss: 2.369530\n",
      "Epoch: 17868 \tTraining Loss: 1.624777 \tValidation Loss: 2.369548\n",
      "Epoch: 17869 \tTraining Loss: 1.608705 \tValidation Loss: 2.369314\n",
      "Epoch: 17870 \tTraining Loss: 1.647512 \tValidation Loss: 2.368803\n",
      "Epoch: 17871 \tTraining Loss: 1.644371 \tValidation Loss: 2.369200\n",
      "Epoch: 17872 \tTraining Loss: 1.658170 \tValidation Loss: 2.369380\n",
      "Epoch: 17873 \tTraining Loss: 1.624738 \tValidation Loss: 2.369083\n",
      "Epoch: 17874 \tTraining Loss: 1.659812 \tValidation Loss: 2.369864\n",
      "Epoch: 17875 \tTraining Loss: 1.639757 \tValidation Loss: 2.369450\n",
      "Epoch: 17876 \tTraining Loss: 1.669389 \tValidation Loss: 2.369535\n",
      "Epoch: 17877 \tTraining Loss: 1.611919 \tValidation Loss: 2.368988\n",
      "Epoch: 17878 \tTraining Loss: 1.598088 \tValidation Loss: 2.369153\n",
      "Epoch: 17879 \tTraining Loss: 1.665119 \tValidation Loss: 2.368676\n",
      "Epoch: 17880 \tTraining Loss: 1.622428 \tValidation Loss: 2.369189\n",
      "Epoch: 17881 \tTraining Loss: 1.614877 \tValidation Loss: 2.369904\n",
      "Epoch: 17882 \tTraining Loss: 1.599092 \tValidation Loss: 2.370172\n",
      "Epoch: 17883 \tTraining Loss: 1.623127 \tValidation Loss: 2.369610\n",
      "Epoch: 17884 \tTraining Loss: 1.586700 \tValidation Loss: 2.369407\n",
      "Epoch: 17885 \tTraining Loss: 1.623268 \tValidation Loss: 2.369499\n",
      "Epoch: 17886 \tTraining Loss: 1.635492 \tValidation Loss: 2.369205\n",
      "Epoch: 17887 \tTraining Loss: 1.597506 \tValidation Loss: 2.369877\n",
      "Epoch: 17888 \tTraining Loss: 1.674112 \tValidation Loss: 2.369416\n",
      "Epoch: 17889 \tTraining Loss: 1.646505 \tValidation Loss: 2.369442\n",
      "Epoch: 17890 \tTraining Loss: 1.615031 \tValidation Loss: 2.369573\n",
      "Epoch: 17891 \tTraining Loss: 1.638725 \tValidation Loss: 2.369401\n",
      "Epoch: 17892 \tTraining Loss: 1.634105 \tValidation Loss: 2.369774\n",
      "Epoch: 17893 \tTraining Loss: 1.640895 \tValidation Loss: 2.369114\n",
      "Epoch: 17894 \tTraining Loss: 1.612235 \tValidation Loss: 2.369758\n",
      "Epoch: 17895 \tTraining Loss: 1.629099 \tValidation Loss: 2.369775\n",
      "Epoch: 17896 \tTraining Loss: 1.647888 \tValidation Loss: 2.369186\n",
      "Epoch: 17897 \tTraining Loss: 1.605804 \tValidation Loss: 2.369462\n",
      "Epoch: 17898 \tTraining Loss: 1.657033 \tValidation Loss: 2.369510\n",
      "Epoch: 17899 \tTraining Loss: 1.628254 \tValidation Loss: 2.369230\n",
      "Epoch: 17900 \tTraining Loss: 1.637007 \tValidation Loss: 2.369025\n",
      "Epoch: 17901 \tTraining Loss: 1.614911 \tValidation Loss: 2.370018\n",
      "Epoch: 17902 \tTraining Loss: 1.627355 \tValidation Loss: 2.370522\n",
      "Epoch: 17903 \tTraining Loss: 1.644298 \tValidation Loss: 2.370160\n",
      "Epoch: 17904 \tTraining Loss: 1.635631 \tValidation Loss: 2.370079\n",
      "Epoch: 17905 \tTraining Loss: 1.628176 \tValidation Loss: 2.369637\n",
      "Epoch: 17906 \tTraining Loss: 1.629840 \tValidation Loss: 2.369900\n",
      "Epoch: 17907 \tTraining Loss: 1.611689 \tValidation Loss: 2.370156\n",
      "Epoch: 17908 \tTraining Loss: 1.652880 \tValidation Loss: 2.369954\n",
      "Epoch: 17909 \tTraining Loss: 1.660913 \tValidation Loss: 2.370137\n",
      "Epoch: 17910 \tTraining Loss: 1.661144 \tValidation Loss: 2.370052\n",
      "Epoch: 17911 \tTraining Loss: 1.657676 \tValidation Loss: 2.369615\n",
      "Epoch: 17912 \tTraining Loss: 1.646173 \tValidation Loss: 2.369578\n",
      "Epoch: 17913 \tTraining Loss: 1.649028 \tValidation Loss: 2.369740\n",
      "Epoch: 17914 \tTraining Loss: 1.615982 \tValidation Loss: 2.369985\n",
      "Epoch: 17915 \tTraining Loss: 1.627627 \tValidation Loss: 2.370442\n",
      "Epoch: 17916 \tTraining Loss: 1.620643 \tValidation Loss: 2.369660\n",
      "Epoch: 17917 \tTraining Loss: 1.618995 \tValidation Loss: 2.369524\n",
      "Epoch: 17918 \tTraining Loss: 1.625708 \tValidation Loss: 2.369776\n",
      "Epoch: 17919 \tTraining Loss: 1.622532 \tValidation Loss: 2.369795\n",
      "Epoch: 17920 \tTraining Loss: 1.650782 \tValidation Loss: 2.369617\n",
      "Epoch: 17921 \tTraining Loss: 1.649421 \tValidation Loss: 2.370116\n",
      "Epoch: 17922 \tTraining Loss: 1.657008 \tValidation Loss: 2.370171\n",
      "Epoch: 17923 \tTraining Loss: 1.642533 \tValidation Loss: 2.370152\n",
      "Epoch: 17924 \tTraining Loss: 1.644136 \tValidation Loss: 2.369395\n",
      "Epoch: 17925 \tTraining Loss: 1.614628 \tValidation Loss: 2.369938\n",
      "Epoch: 17926 \tTraining Loss: 1.643249 \tValidation Loss: 2.369424\n",
      "Epoch: 17927 \tTraining Loss: 1.622656 \tValidation Loss: 2.369844\n",
      "Epoch: 17928 \tTraining Loss: 1.642506 \tValidation Loss: 2.370046\n",
      "Epoch: 17929 \tTraining Loss: 1.671334 \tValidation Loss: 2.370131\n",
      "Epoch: 17930 \tTraining Loss: 1.653585 \tValidation Loss: 2.369401\n",
      "Epoch: 17931 \tTraining Loss: 1.631871 \tValidation Loss: 2.369871\n",
      "Epoch: 17932 \tTraining Loss: 1.644468 \tValidation Loss: 2.369977\n",
      "Epoch: 17933 \tTraining Loss: 1.634109 \tValidation Loss: 2.370247\n",
      "Epoch: 17934 \tTraining Loss: 1.628852 \tValidation Loss: 2.369923\n",
      "Epoch: 17935 \tTraining Loss: 1.616377 \tValidation Loss: 2.370032\n",
      "Epoch: 17936 \tTraining Loss: 1.624190 \tValidation Loss: 2.370188\n",
      "Epoch: 17937 \tTraining Loss: 1.635324 \tValidation Loss: 2.370173\n",
      "Epoch: 17938 \tTraining Loss: 1.662705 \tValidation Loss: 2.369684\n",
      "Epoch: 17939 \tTraining Loss: 1.627379 \tValidation Loss: 2.369834\n",
      "Epoch: 17940 \tTraining Loss: 1.663582 \tValidation Loss: 2.369864\n",
      "Epoch: 17941 \tTraining Loss: 1.603721 \tValidation Loss: 2.370213\n",
      "Epoch: 17942 \tTraining Loss: 1.599291 \tValidation Loss: 2.369546\n",
      "Epoch: 17943 \tTraining Loss: 1.663862 \tValidation Loss: 2.369266\n",
      "Epoch: 17944 \tTraining Loss: 1.599045 \tValidation Loss: 2.369737\n",
      "Epoch: 17945 \tTraining Loss: 1.593012 \tValidation Loss: 2.370259\n",
      "Epoch: 17946 \tTraining Loss: 1.607156 \tValidation Loss: 2.370105\n",
      "Epoch: 17947 \tTraining Loss: 1.624969 \tValidation Loss: 2.370668\n",
      "Epoch: 17948 \tTraining Loss: 1.633674 \tValidation Loss: 2.370358\n",
      "Epoch: 17949 \tTraining Loss: 1.621962 \tValidation Loss: 2.370323\n",
      "Epoch: 17950 \tTraining Loss: 1.637843 \tValidation Loss: 2.370633\n",
      "Epoch: 17951 \tTraining Loss: 1.611300 \tValidation Loss: 2.370570\n",
      "Epoch: 17952 \tTraining Loss: 1.617382 \tValidation Loss: 2.370713\n",
      "Epoch: 17953 \tTraining Loss: 1.624352 \tValidation Loss: 2.370913\n",
      "Epoch: 17954 \tTraining Loss: 1.607808 \tValidation Loss: 2.370441\n",
      "Epoch: 17955 \tTraining Loss: 1.644960 \tValidation Loss: 2.370062\n",
      "Epoch: 17956 \tTraining Loss: 1.627827 \tValidation Loss: 2.370510\n",
      "Epoch: 17957 \tTraining Loss: 1.642710 \tValidation Loss: 2.370365\n",
      "Epoch: 17958 \tTraining Loss: 1.618713 \tValidation Loss: 2.370647\n",
      "Epoch: 17959 \tTraining Loss: 1.664783 \tValidation Loss: 2.370122\n",
      "Epoch: 17960 \tTraining Loss: 1.646240 \tValidation Loss: 2.370195\n",
      "Epoch: 17961 \tTraining Loss: 1.585558 \tValidation Loss: 2.370472\n",
      "Epoch: 17962 \tTraining Loss: 1.613069 \tValidation Loss: 2.370299\n",
      "Epoch: 17963 \tTraining Loss: 1.618874 \tValidation Loss: 2.370123\n",
      "Epoch: 17964 \tTraining Loss: 1.627608 \tValidation Loss: 2.370298\n",
      "Epoch: 17965 \tTraining Loss: 1.623443 \tValidation Loss: 2.369527\n",
      "Epoch: 17966 \tTraining Loss: 1.593679 \tValidation Loss: 2.370324\n",
      "Epoch: 17967 \tTraining Loss: 1.614967 \tValidation Loss: 2.370432\n",
      "Epoch: 17968 \tTraining Loss: 1.649917 \tValidation Loss: 2.370481\n",
      "Epoch: 17969 \tTraining Loss: 1.667353 \tValidation Loss: 2.370589\n",
      "Epoch: 17970 \tTraining Loss: 1.607818 \tValidation Loss: 2.370716\n",
      "Epoch: 17971 \tTraining Loss: 1.614611 \tValidation Loss: 2.370102\n",
      "Epoch: 17972 \tTraining Loss: 1.648336 \tValidation Loss: 2.370283\n",
      "Epoch: 17973 \tTraining Loss: 1.625901 \tValidation Loss: 2.370026\n",
      "Epoch: 17974 \tTraining Loss: 1.661782 \tValidation Loss: 2.369596\n",
      "Epoch: 17975 \tTraining Loss: 1.631912 \tValidation Loss: 2.370193\n",
      "Epoch: 17976 \tTraining Loss: 1.678932 \tValidation Loss: 2.370828\n",
      "Epoch: 17977 \tTraining Loss: 1.612935 \tValidation Loss: 2.371092\n",
      "Epoch: 17978 \tTraining Loss: 1.631744 \tValidation Loss: 2.370682\n",
      "Epoch: 17979 \tTraining Loss: 1.620473 \tValidation Loss: 2.370456\n",
      "Epoch: 17980 \tTraining Loss: 1.608896 \tValidation Loss: 2.370698\n",
      "Epoch: 17981 \tTraining Loss: 1.655507 \tValidation Loss: 2.370132\n",
      "Epoch: 17982 \tTraining Loss: 1.628523 \tValidation Loss: 2.370217\n",
      "Epoch: 17983 \tTraining Loss: 1.592881 \tValidation Loss: 2.370623\n",
      "Epoch: 17984 \tTraining Loss: 1.644635 \tValidation Loss: 2.370400\n",
      "Epoch: 17985 \tTraining Loss: 1.638828 \tValidation Loss: 2.370423\n",
      "Epoch: 17986 \tTraining Loss: 1.651289 \tValidation Loss: 2.371125\n",
      "Epoch: 17987 \tTraining Loss: 1.604240 \tValidation Loss: 2.370999\n",
      "Epoch: 17988 \tTraining Loss: 1.621370 \tValidation Loss: 2.370961\n",
      "Epoch: 17989 \tTraining Loss: 1.618140 \tValidation Loss: 2.370559\n",
      "Epoch: 17990 \tTraining Loss: 1.627711 \tValidation Loss: 2.370551\n",
      "Epoch: 17991 \tTraining Loss: 1.618343 \tValidation Loss: 2.370803\n",
      "Epoch: 17992 \tTraining Loss: 1.594928 \tValidation Loss: 2.370676\n",
      "Epoch: 17993 \tTraining Loss: 1.635075 \tValidation Loss: 2.370442\n",
      "Epoch: 17994 \tTraining Loss: 1.640638 \tValidation Loss: 2.371000\n",
      "Epoch: 17995 \tTraining Loss: 1.632949 \tValidation Loss: 2.370722\n",
      "Epoch: 17996 \tTraining Loss: 1.627002 \tValidation Loss: 2.371104\n",
      "Epoch: 17997 \tTraining Loss: 1.621825 \tValidation Loss: 2.371098\n",
      "Epoch: 17998 \tTraining Loss: 1.650101 \tValidation Loss: 2.370742\n",
      "Epoch: 17999 \tTraining Loss: 1.630977 \tValidation Loss: 2.370293\n",
      "Epoch: 18000 \tTraining Loss: 1.616246 \tValidation Loss: 2.370540\n",
      "Epoch: 18001 \tTraining Loss: 1.635513 \tValidation Loss: 2.370494\n",
      "Epoch: 18002 \tTraining Loss: 1.603135 \tValidation Loss: 2.370943\n",
      "Epoch: 18003 \tTraining Loss: 1.615683 \tValidation Loss: 2.370495\n",
      "Epoch: 18004 \tTraining Loss: 1.643800 \tValidation Loss: 2.370452\n",
      "Epoch: 18005 \tTraining Loss: 1.606809 \tValidation Loss: 2.370870\n",
      "Epoch: 18006 \tTraining Loss: 1.616597 \tValidation Loss: 2.370280\n",
      "Epoch: 18007 \tTraining Loss: 1.654436 \tValidation Loss: 2.370630\n",
      "Epoch: 18008 \tTraining Loss: 1.616798 \tValidation Loss: 2.370686\n",
      "Epoch: 18009 \tTraining Loss: 1.631530 \tValidation Loss: 2.370578\n",
      "Epoch: 18010 \tTraining Loss: 1.622302 \tValidation Loss: 2.371237\n",
      "Epoch: 18011 \tTraining Loss: 1.654485 \tValidation Loss: 2.370257\n",
      "Epoch: 18012 \tTraining Loss: 1.643194 \tValidation Loss: 2.370496\n",
      "Epoch: 18013 \tTraining Loss: 1.586621 \tValidation Loss: 2.371076\n",
      "Epoch: 18014 \tTraining Loss: 1.613690 \tValidation Loss: 2.370849\n",
      "Epoch: 18015 \tTraining Loss: 1.666610 \tValidation Loss: 2.371184\n",
      "Epoch: 18016 \tTraining Loss: 1.614037 \tValidation Loss: 2.371563\n",
      "Epoch: 18017 \tTraining Loss: 1.637224 \tValidation Loss: 2.370947\n",
      "Epoch: 18018 \tTraining Loss: 1.620886 \tValidation Loss: 2.370672\n",
      "Epoch: 18019 \tTraining Loss: 1.622190 \tValidation Loss: 2.370625\n",
      "Epoch: 18020 \tTraining Loss: 1.673391 \tValidation Loss: 2.370568\n",
      "Epoch: 18021 \tTraining Loss: 1.639408 \tValidation Loss: 2.371161\n",
      "Epoch: 18022 \tTraining Loss: 1.648240 \tValidation Loss: 2.371531\n",
      "Epoch: 18023 \tTraining Loss: 1.646948 \tValidation Loss: 2.371029\n",
      "Epoch: 18024 \tTraining Loss: 1.657892 \tValidation Loss: 2.371365\n",
      "Epoch: 18025 \tTraining Loss: 1.590921 \tValidation Loss: 2.371477\n",
      "Epoch: 18026 \tTraining Loss: 1.641369 \tValidation Loss: 2.371007\n",
      "Epoch: 18027 \tTraining Loss: 1.684541 \tValidation Loss: 2.370644\n",
      "Epoch: 18028 \tTraining Loss: 1.653046 \tValidation Loss: 2.370903\n",
      "Epoch: 18029 \tTraining Loss: 1.642047 \tValidation Loss: 2.370854\n",
      "Epoch: 18030 \tTraining Loss: 1.613711 \tValidation Loss: 2.370763\n",
      "Epoch: 18031 \tTraining Loss: 1.599355 \tValidation Loss: 2.370501\n",
      "Epoch: 18032 \tTraining Loss: 1.617780 \tValidation Loss: 2.370841\n",
      "Epoch: 18033 \tTraining Loss: 1.602873 \tValidation Loss: 2.370910\n",
      "Epoch: 18034 \tTraining Loss: 1.630121 \tValidation Loss: 2.370578\n",
      "Epoch: 18035 \tTraining Loss: 1.575337 \tValidation Loss: 2.370791\n",
      "Epoch: 18036 \tTraining Loss: 1.613907 \tValidation Loss: 2.370455\n",
      "Epoch: 18037 \tTraining Loss: 1.606652 \tValidation Loss: 2.370630\n",
      "Epoch: 18038 \tTraining Loss: 1.640680 \tValidation Loss: 2.370725\n",
      "Epoch: 18039 \tTraining Loss: 1.628242 \tValidation Loss: 2.370871\n",
      "Epoch: 18040 \tTraining Loss: 1.610487 \tValidation Loss: 2.370965\n",
      "Epoch: 18041 \tTraining Loss: 1.649814 \tValidation Loss: 2.370406\n",
      "Epoch: 18042 \tTraining Loss: 1.612002 \tValidation Loss: 2.370927\n",
      "Epoch: 18043 \tTraining Loss: 1.650101 \tValidation Loss: 2.371100\n",
      "Epoch: 18044 \tTraining Loss: 1.621159 \tValidation Loss: 2.370913\n",
      "Epoch: 18045 \tTraining Loss: 1.575070 \tValidation Loss: 2.370801\n",
      "Epoch: 18046 \tTraining Loss: 1.622678 \tValidation Loss: 2.371135\n",
      "Epoch: 18047 \tTraining Loss: 1.643803 \tValidation Loss: 2.370886\n",
      "Epoch: 18048 \tTraining Loss: 1.627743 \tValidation Loss: 2.371478\n",
      "Epoch: 18049 \tTraining Loss: 1.685771 \tValidation Loss: 2.371086\n",
      "Epoch: 18050 \tTraining Loss: 1.626946 \tValidation Loss: 2.371298\n",
      "Epoch: 18051 \tTraining Loss: 1.644274 \tValidation Loss: 2.371374\n",
      "Epoch: 18052 \tTraining Loss: 1.619818 \tValidation Loss: 2.371598\n",
      "Epoch: 18053 \tTraining Loss: 1.645366 \tValidation Loss: 2.371538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18054 \tTraining Loss: 1.620320 \tValidation Loss: 2.371252\n",
      "Epoch: 18055 \tTraining Loss: 1.639470 \tValidation Loss: 2.370767\n",
      "Epoch: 18056 \tTraining Loss: 1.633055 \tValidation Loss: 2.371239\n",
      "Epoch: 18057 \tTraining Loss: 1.598257 \tValidation Loss: 2.372133\n",
      "Epoch: 18058 \tTraining Loss: 1.639720 \tValidation Loss: 2.371515\n",
      "Epoch: 18059 \tTraining Loss: 1.618423 \tValidation Loss: 2.372101\n",
      "Epoch: 18060 \tTraining Loss: 1.631714 \tValidation Loss: 2.371107\n",
      "Epoch: 18061 \tTraining Loss: 1.622960 \tValidation Loss: 2.371346\n",
      "Epoch: 18062 \tTraining Loss: 1.617023 \tValidation Loss: 2.371558\n",
      "Epoch: 18063 \tTraining Loss: 1.629033 \tValidation Loss: 2.371695\n",
      "Epoch: 18064 \tTraining Loss: 1.612380 \tValidation Loss: 2.371955\n",
      "Epoch: 18065 \tTraining Loss: 1.617039 \tValidation Loss: 2.371885\n",
      "Epoch: 18066 \tTraining Loss: 1.617408 \tValidation Loss: 2.371752\n",
      "Epoch: 18067 \tTraining Loss: 1.606494 \tValidation Loss: 2.372278\n",
      "Epoch: 18068 \tTraining Loss: 1.653270 \tValidation Loss: 2.371937\n",
      "Epoch: 18069 \tTraining Loss: 1.654607 \tValidation Loss: 2.371981\n",
      "Epoch: 18070 \tTraining Loss: 1.637994 \tValidation Loss: 2.371938\n",
      "Epoch: 18071 \tTraining Loss: 1.659230 \tValidation Loss: 2.371227\n",
      "Epoch: 18072 \tTraining Loss: 1.677527 \tValidation Loss: 2.371096\n",
      "Epoch: 18073 \tTraining Loss: 1.607938 \tValidation Loss: 2.371620\n",
      "Epoch: 18074 \tTraining Loss: 1.636130 \tValidation Loss: 2.371950\n",
      "Epoch: 18075 \tTraining Loss: 1.612491 \tValidation Loss: 2.371357\n",
      "Epoch: 18076 \tTraining Loss: 1.640937 \tValidation Loss: 2.371826\n",
      "Epoch: 18077 \tTraining Loss: 1.644893 \tValidation Loss: 2.371885\n",
      "Epoch: 18078 \tTraining Loss: 1.658248 \tValidation Loss: 2.372108\n",
      "Epoch: 18079 \tTraining Loss: 1.688443 \tValidation Loss: 2.371332\n",
      "Epoch: 18080 \tTraining Loss: 1.632424 \tValidation Loss: 2.371618\n",
      "Epoch: 18081 \tTraining Loss: 1.619011 \tValidation Loss: 2.371147\n",
      "Epoch: 18082 \tTraining Loss: 1.619881 \tValidation Loss: 2.371346\n",
      "Epoch: 18083 \tTraining Loss: 1.631003 \tValidation Loss: 2.371464\n",
      "Epoch: 18084 \tTraining Loss: 1.617441 \tValidation Loss: 2.371864\n",
      "Epoch: 18085 \tTraining Loss: 1.616462 \tValidation Loss: 2.371985\n",
      "Epoch: 18086 \tTraining Loss: 1.655274 \tValidation Loss: 2.371151\n",
      "Epoch: 18087 \tTraining Loss: 1.631435 \tValidation Loss: 2.371972\n",
      "Epoch: 18088 \tTraining Loss: 1.658057 \tValidation Loss: 2.372072\n",
      "Epoch: 18089 \tTraining Loss: 1.606515 \tValidation Loss: 2.372544\n",
      "Epoch: 18090 \tTraining Loss: 1.612478 \tValidation Loss: 2.372165\n",
      "Epoch: 18091 \tTraining Loss: 1.711392 \tValidation Loss: 2.371153\n",
      "Epoch: 18092 \tTraining Loss: 1.605070 \tValidation Loss: 2.371642\n",
      "Epoch: 18093 \tTraining Loss: 1.607986 \tValidation Loss: 2.372072\n",
      "Epoch: 18094 \tTraining Loss: 1.584292 \tValidation Loss: 2.371518\n",
      "Epoch: 18095 \tTraining Loss: 1.584013 \tValidation Loss: 2.371665\n",
      "Epoch: 18096 \tTraining Loss: 1.624251 \tValidation Loss: 2.371763\n",
      "Epoch: 18097 \tTraining Loss: 1.642112 \tValidation Loss: 2.371880\n",
      "Epoch: 18098 \tTraining Loss: 1.630472 \tValidation Loss: 2.371442\n",
      "Epoch: 18099 \tTraining Loss: 1.605958 \tValidation Loss: 2.372305\n",
      "Epoch: 18100 \tTraining Loss: 1.590496 \tValidation Loss: 2.372296\n",
      "Epoch: 18101 \tTraining Loss: 1.596766 \tValidation Loss: 2.372062\n",
      "Epoch: 18102 \tTraining Loss: 1.588850 \tValidation Loss: 2.372941\n",
      "Epoch: 18103 \tTraining Loss: 1.616915 \tValidation Loss: 2.372561\n",
      "Epoch: 18104 \tTraining Loss: 1.638118 \tValidation Loss: 2.372362\n",
      "Epoch: 18105 \tTraining Loss: 1.635172 \tValidation Loss: 2.372535\n",
      "Epoch: 18106 \tTraining Loss: 1.652726 \tValidation Loss: 2.372513\n",
      "Epoch: 18107 \tTraining Loss: 1.625898 \tValidation Loss: 2.372194\n",
      "Epoch: 18108 \tTraining Loss: 1.663098 \tValidation Loss: 2.371822\n",
      "Epoch: 18109 \tTraining Loss: 1.638880 \tValidation Loss: 2.372339\n",
      "Epoch: 18110 \tTraining Loss: 1.616256 \tValidation Loss: 2.371942\n",
      "Epoch: 18111 \tTraining Loss: 1.656858 \tValidation Loss: 2.371506\n",
      "Epoch: 18112 \tTraining Loss: 1.588588 \tValidation Loss: 2.372038\n",
      "Epoch: 18113 \tTraining Loss: 1.620894 \tValidation Loss: 2.372057\n",
      "Epoch: 18114 \tTraining Loss: 1.612352 \tValidation Loss: 2.371730\n",
      "Epoch: 18115 \tTraining Loss: 1.626501 \tValidation Loss: 2.371696\n",
      "Epoch: 18116 \tTraining Loss: 1.636794 \tValidation Loss: 2.371897\n",
      "Epoch: 18117 \tTraining Loss: 1.606482 \tValidation Loss: 2.372078\n",
      "Epoch: 18118 \tTraining Loss: 1.610935 \tValidation Loss: 2.372675\n",
      "Epoch: 18119 \tTraining Loss: 1.679877 \tValidation Loss: 2.372372\n",
      "Epoch: 18120 \tTraining Loss: 1.601617 \tValidation Loss: 2.372380\n",
      "Epoch: 18121 \tTraining Loss: 1.638909 \tValidation Loss: 2.373116\n",
      "Epoch: 18122 \tTraining Loss: 1.593122 \tValidation Loss: 2.372825\n",
      "Epoch: 18123 \tTraining Loss: 1.631253 \tValidation Loss: 2.372161\n",
      "Epoch: 18124 \tTraining Loss: 1.651790 \tValidation Loss: 2.372364\n",
      "Epoch: 18125 \tTraining Loss: 1.624163 \tValidation Loss: 2.372597\n",
      "Epoch: 18126 \tTraining Loss: 1.608776 \tValidation Loss: 2.372685\n",
      "Epoch: 18127 \tTraining Loss: 1.644735 \tValidation Loss: 2.372298\n",
      "Epoch: 18128 \tTraining Loss: 1.604124 \tValidation Loss: 2.372443\n",
      "Epoch: 18129 \tTraining Loss: 1.648446 \tValidation Loss: 2.372356\n",
      "Epoch: 18130 \tTraining Loss: 1.618119 \tValidation Loss: 2.372458\n",
      "Epoch: 18131 \tTraining Loss: 1.637830 \tValidation Loss: 2.372531\n",
      "Epoch: 18132 \tTraining Loss: 1.616189 \tValidation Loss: 2.372043\n",
      "Epoch: 18133 \tTraining Loss: 1.660409 \tValidation Loss: 2.372625\n",
      "Epoch: 18134 \tTraining Loss: 1.688936 \tValidation Loss: 2.372046\n",
      "Epoch: 18135 \tTraining Loss: 1.628267 \tValidation Loss: 2.372133\n",
      "Epoch: 18136 \tTraining Loss: 1.664585 \tValidation Loss: 2.371462\n",
      "Epoch: 18137 \tTraining Loss: 1.584091 \tValidation Loss: 2.372173\n",
      "Epoch: 18138 \tTraining Loss: 1.623270 \tValidation Loss: 2.372517\n",
      "Epoch: 18139 \tTraining Loss: 1.655329 \tValidation Loss: 2.372271\n",
      "Epoch: 18140 \tTraining Loss: 1.663401 \tValidation Loss: 2.372361\n",
      "Epoch: 18141 \tTraining Loss: 1.604114 \tValidation Loss: 2.372348\n",
      "Epoch: 18142 \tTraining Loss: 1.595715 \tValidation Loss: 2.372221\n",
      "Epoch: 18143 \tTraining Loss: 1.610761 \tValidation Loss: 2.372431\n",
      "Epoch: 18144 \tTraining Loss: 1.582461 \tValidation Loss: 2.373283\n",
      "Epoch: 18145 \tTraining Loss: 1.619856 \tValidation Loss: 2.372973\n",
      "Epoch: 18146 \tTraining Loss: 1.589713 \tValidation Loss: 2.372489\n",
      "Epoch: 18147 \tTraining Loss: 1.648486 \tValidation Loss: 2.372629\n",
      "Epoch: 18148 \tTraining Loss: 1.596091 \tValidation Loss: 2.372374\n",
      "Epoch: 18149 \tTraining Loss: 1.618741 \tValidation Loss: 2.372946\n",
      "Epoch: 18150 \tTraining Loss: 1.612411 \tValidation Loss: 2.372927\n",
      "Epoch: 18151 \tTraining Loss: 1.607622 \tValidation Loss: 2.372577\n",
      "Epoch: 18152 \tTraining Loss: 1.597801 \tValidation Loss: 2.373054\n",
      "Epoch: 18153 \tTraining Loss: 1.623884 \tValidation Loss: 2.372905\n",
      "Epoch: 18154 \tTraining Loss: 1.618230 \tValidation Loss: 2.373208\n",
      "Epoch: 18155 \tTraining Loss: 1.678109 \tValidation Loss: 2.372775\n",
      "Epoch: 18156 \tTraining Loss: 1.624153 \tValidation Loss: 2.372795\n",
      "Epoch: 18157 \tTraining Loss: 1.605693 \tValidation Loss: 2.372625\n",
      "Epoch: 18158 \tTraining Loss: 1.608204 \tValidation Loss: 2.372332\n",
      "Epoch: 18159 \tTraining Loss: 1.623645 \tValidation Loss: 2.372256\n",
      "Epoch: 18160 \tTraining Loss: 1.645551 \tValidation Loss: 2.372445\n",
      "Epoch: 18161 \tTraining Loss: 1.607370 \tValidation Loss: 2.372254\n",
      "Epoch: 18162 \tTraining Loss: 1.620080 \tValidation Loss: 2.372113\n",
      "Epoch: 18163 \tTraining Loss: 1.584506 \tValidation Loss: 2.372754\n",
      "Epoch: 18164 \tTraining Loss: 1.644405 \tValidation Loss: 2.372722\n",
      "Epoch: 18165 \tTraining Loss: 1.637162 \tValidation Loss: 2.372510\n",
      "Epoch: 18166 \tTraining Loss: 1.650824 \tValidation Loss: 2.372076\n",
      "Epoch: 18167 \tTraining Loss: 1.641270 \tValidation Loss: 2.372590\n",
      "Epoch: 18168 \tTraining Loss: 1.645442 \tValidation Loss: 2.372358\n",
      "Epoch: 18169 \tTraining Loss: 1.638569 \tValidation Loss: 2.372504\n",
      "Epoch: 18170 \tTraining Loss: 1.600528 \tValidation Loss: 2.372172\n",
      "Epoch: 18171 \tTraining Loss: 1.632618 \tValidation Loss: 2.372612\n",
      "Epoch: 18172 \tTraining Loss: 1.589211 \tValidation Loss: 2.372796\n",
      "Epoch: 18173 \tTraining Loss: 1.642863 \tValidation Loss: 2.373132\n",
      "Epoch: 18174 \tTraining Loss: 1.646991 \tValidation Loss: 2.372895\n",
      "Epoch: 18175 \tTraining Loss: 1.657986 \tValidation Loss: 2.372488\n",
      "Epoch: 18176 \tTraining Loss: 1.641371 \tValidation Loss: 2.373102\n",
      "Epoch: 18177 \tTraining Loss: 1.591496 \tValidation Loss: 2.372794\n",
      "Epoch: 18178 \tTraining Loss: 1.639238 \tValidation Loss: 2.373009\n",
      "Epoch: 18179 \tTraining Loss: 1.625841 \tValidation Loss: 2.372888\n",
      "Epoch: 18180 \tTraining Loss: 1.615388 \tValidation Loss: 2.373656\n",
      "Epoch: 18181 \tTraining Loss: 1.594300 \tValidation Loss: 2.373315\n",
      "Epoch: 18182 \tTraining Loss: 1.638073 \tValidation Loss: 2.372658\n",
      "Epoch: 18183 \tTraining Loss: 1.574229 \tValidation Loss: 2.373073\n",
      "Epoch: 18184 \tTraining Loss: 1.626375 \tValidation Loss: 2.373170\n",
      "Epoch: 18185 \tTraining Loss: 1.586440 \tValidation Loss: 2.373719\n",
      "Epoch: 18186 \tTraining Loss: 1.589074 \tValidation Loss: 2.372791\n",
      "Epoch: 18187 \tTraining Loss: 1.610169 \tValidation Loss: 2.372860\n",
      "Epoch: 18188 \tTraining Loss: 1.625315 \tValidation Loss: 2.372669\n",
      "Epoch: 18189 \tTraining Loss: 1.590652 \tValidation Loss: 2.373217\n",
      "Epoch: 18190 \tTraining Loss: 1.632113 \tValidation Loss: 2.372980\n",
      "Epoch: 18191 \tTraining Loss: 1.621074 \tValidation Loss: 2.372797\n",
      "Epoch: 18192 \tTraining Loss: 1.596086 \tValidation Loss: 2.373212\n",
      "Epoch: 18193 \tTraining Loss: 1.591703 \tValidation Loss: 2.373707\n",
      "Epoch: 18194 \tTraining Loss: 1.650545 \tValidation Loss: 2.373230\n",
      "Epoch: 18195 \tTraining Loss: 1.620778 \tValidation Loss: 2.373337\n",
      "Epoch: 18196 \tTraining Loss: 1.626907 \tValidation Loss: 2.372794\n",
      "Epoch: 18197 \tTraining Loss: 1.611263 \tValidation Loss: 2.373082\n",
      "Epoch: 18198 \tTraining Loss: 1.583187 \tValidation Loss: 2.373929\n",
      "Epoch: 18199 \tTraining Loss: 1.662451 \tValidation Loss: 2.373122\n",
      "Epoch: 18200 \tTraining Loss: 1.633876 \tValidation Loss: 2.373101\n",
      "Epoch: 18201 \tTraining Loss: 1.623961 \tValidation Loss: 2.372700\n",
      "Epoch: 18202 \tTraining Loss: 1.645843 \tValidation Loss: 2.373298\n",
      "Epoch: 18203 \tTraining Loss: 1.622182 \tValidation Loss: 2.372917\n",
      "Epoch: 18204 \tTraining Loss: 1.605043 \tValidation Loss: 2.373633\n",
      "Epoch: 18205 \tTraining Loss: 1.591468 \tValidation Loss: 2.373531\n",
      "Epoch: 18206 \tTraining Loss: 1.608830 \tValidation Loss: 2.373093\n",
      "Epoch: 18207 \tTraining Loss: 1.600104 \tValidation Loss: 2.373075\n",
      "Epoch: 18208 \tTraining Loss: 1.585895 \tValidation Loss: 2.373005\n",
      "Epoch: 18209 \tTraining Loss: 1.622371 \tValidation Loss: 2.373361\n",
      "Epoch: 18210 \tTraining Loss: 1.590672 \tValidation Loss: 2.373398\n",
      "Epoch: 18211 \tTraining Loss: 1.600288 \tValidation Loss: 2.374011\n",
      "Epoch: 18212 \tTraining Loss: 1.595042 \tValidation Loss: 2.374077\n",
      "Epoch: 18213 \tTraining Loss: 1.603225 \tValidation Loss: 2.373649\n",
      "Epoch: 18214 \tTraining Loss: 1.580189 \tValidation Loss: 2.373641\n",
      "Epoch: 18215 \tTraining Loss: 1.610330 \tValidation Loss: 2.374080\n",
      "Epoch: 18216 \tTraining Loss: 1.619737 \tValidation Loss: 2.373510\n",
      "Epoch: 18217 \tTraining Loss: 1.596428 \tValidation Loss: 2.372792\n",
      "Epoch: 18218 \tTraining Loss: 1.626587 \tValidation Loss: 2.373634\n",
      "Epoch: 18219 \tTraining Loss: 1.555347 \tValidation Loss: 2.374193\n",
      "Epoch: 18220 \tTraining Loss: 1.637943 \tValidation Loss: 2.373807\n",
      "Epoch: 18221 \tTraining Loss: 1.636054 \tValidation Loss: 2.373859\n",
      "Epoch: 18222 \tTraining Loss: 1.648195 \tValidation Loss: 2.373096\n",
      "Epoch: 18223 \tTraining Loss: 1.598106 \tValidation Loss: 2.373710\n",
      "Epoch: 18224 \tTraining Loss: 1.556124 \tValidation Loss: 2.374417\n",
      "Epoch: 18225 \tTraining Loss: 1.635277 \tValidation Loss: 2.374372\n",
      "Epoch: 18226 \tTraining Loss: 1.631174 \tValidation Loss: 2.372975\n",
      "Epoch: 18227 \tTraining Loss: 1.635543 \tValidation Loss: 2.373659\n",
      "Epoch: 18228 \tTraining Loss: 1.595754 \tValidation Loss: 2.373302\n",
      "Epoch: 18229 \tTraining Loss: 1.592721 \tValidation Loss: 2.373541\n",
      "Epoch: 18230 \tTraining Loss: 1.599802 \tValidation Loss: 2.373423\n",
      "Epoch: 18231 \tTraining Loss: 1.657519 \tValidation Loss: 2.373334\n",
      "Epoch: 18232 \tTraining Loss: 1.634738 \tValidation Loss: 2.373884\n",
      "Epoch: 18233 \tTraining Loss: 1.583462 \tValidation Loss: 2.374163\n",
      "Epoch: 18234 \tTraining Loss: 1.594076 \tValidation Loss: 2.373482\n",
      "Epoch: 18235 \tTraining Loss: 1.635598 \tValidation Loss: 2.373200\n",
      "Epoch: 18236 \tTraining Loss: 1.590074 \tValidation Loss: 2.373485\n",
      "Epoch: 18237 \tTraining Loss: 1.626639 \tValidation Loss: 2.374167\n",
      "Epoch: 18238 \tTraining Loss: 1.645829 \tValidation Loss: 2.373329\n",
      "Epoch: 18239 \tTraining Loss: 1.662139 \tValidation Loss: 2.373361\n",
      "Epoch: 18240 \tTraining Loss: 1.650407 \tValidation Loss: 2.373196\n",
      "Epoch: 18241 \tTraining Loss: 1.650468 \tValidation Loss: 2.373066\n",
      "Epoch: 18242 \tTraining Loss: 1.662412 \tValidation Loss: 2.373108\n",
      "Epoch: 18243 \tTraining Loss: 1.591991 \tValidation Loss: 2.373486\n",
      "Epoch: 18244 \tTraining Loss: 1.588730 \tValidation Loss: 2.373470\n",
      "Epoch: 18245 \tTraining Loss: 1.619205 \tValidation Loss: 2.373287\n",
      "Epoch: 18246 \tTraining Loss: 1.644636 \tValidation Loss: 2.373967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18247 \tTraining Loss: 1.642748 \tValidation Loss: 2.373117\n",
      "Epoch: 18248 \tTraining Loss: 1.606931 \tValidation Loss: 2.373697\n",
      "Epoch: 18249 \tTraining Loss: 1.593513 \tValidation Loss: 2.374490\n",
      "Epoch: 18250 \tTraining Loss: 1.643159 \tValidation Loss: 2.373252\n",
      "Epoch: 18251 \tTraining Loss: 1.586643 \tValidation Loss: 2.373809\n",
      "Epoch: 18252 \tTraining Loss: 1.639544 \tValidation Loss: 2.373672\n",
      "Epoch: 18253 \tTraining Loss: 1.629256 \tValidation Loss: 2.373663\n",
      "Epoch: 18254 \tTraining Loss: 1.608947 \tValidation Loss: 2.373326\n",
      "Epoch: 18255 \tTraining Loss: 1.614835 \tValidation Loss: 2.373621\n",
      "Epoch: 18256 \tTraining Loss: 1.621580 \tValidation Loss: 2.374002\n",
      "Epoch: 18257 \tTraining Loss: 1.634562 \tValidation Loss: 2.373693\n",
      "Epoch: 18258 \tTraining Loss: 1.642673 \tValidation Loss: 2.373865\n",
      "Epoch: 18259 \tTraining Loss: 1.618661 \tValidation Loss: 2.374073\n",
      "Epoch: 18260 \tTraining Loss: 1.596107 \tValidation Loss: 2.373863\n",
      "Epoch: 18261 \tTraining Loss: 1.641976 \tValidation Loss: 2.374382\n",
      "Epoch: 18262 \tTraining Loss: 1.615698 \tValidation Loss: 2.373865\n",
      "Epoch: 18263 \tTraining Loss: 1.636460 \tValidation Loss: 2.374060\n",
      "Epoch: 18264 \tTraining Loss: 1.588217 \tValidation Loss: 2.374679\n",
      "Epoch: 18265 \tTraining Loss: 1.674989 \tValidation Loss: 2.373680\n",
      "Epoch: 18266 \tTraining Loss: 1.573935 \tValidation Loss: 2.373876\n",
      "Epoch: 18267 \tTraining Loss: 1.618091 \tValidation Loss: 2.373831\n",
      "Epoch: 18268 \tTraining Loss: 1.646464 \tValidation Loss: 2.373885\n",
      "Epoch: 18269 \tTraining Loss: 1.635787 \tValidation Loss: 2.373788\n",
      "Epoch: 18270 \tTraining Loss: 1.619950 \tValidation Loss: 2.374250\n",
      "Epoch: 18271 \tTraining Loss: 1.605682 \tValidation Loss: 2.374267\n",
      "Epoch: 18272 \tTraining Loss: 1.628455 \tValidation Loss: 2.374252\n",
      "Epoch: 18273 \tTraining Loss: 1.642612 \tValidation Loss: 2.373826\n",
      "Epoch: 18274 \tTraining Loss: 1.626538 \tValidation Loss: 2.374265\n",
      "Epoch: 18275 \tTraining Loss: 1.631608 \tValidation Loss: 2.373973\n",
      "Epoch: 18276 \tTraining Loss: 1.594619 \tValidation Loss: 2.374192\n",
      "Epoch: 18277 \tTraining Loss: 1.608314 \tValidation Loss: 2.374227\n",
      "Epoch: 18278 \tTraining Loss: 1.592365 \tValidation Loss: 2.374379\n",
      "Epoch: 18279 \tTraining Loss: 1.646333 \tValidation Loss: 2.374055\n",
      "Epoch: 18280 \tTraining Loss: 1.590434 \tValidation Loss: 2.374398\n",
      "Epoch: 18281 \tTraining Loss: 1.627972 \tValidation Loss: 2.374318\n",
      "Epoch: 18282 \tTraining Loss: 1.585947 \tValidation Loss: 2.374298\n",
      "Epoch: 18283 \tTraining Loss: 1.613055 \tValidation Loss: 2.374099\n",
      "Epoch: 18284 \tTraining Loss: 1.596512 \tValidation Loss: 2.373959\n",
      "Epoch: 18285 \tTraining Loss: 1.625803 \tValidation Loss: 2.374037\n",
      "Epoch: 18286 \tTraining Loss: 1.608250 \tValidation Loss: 2.374296\n",
      "Epoch: 18287 \tTraining Loss: 1.629478 \tValidation Loss: 2.374199\n",
      "Epoch: 18288 \tTraining Loss: 1.640384 \tValidation Loss: 2.374561\n",
      "Epoch: 18289 \tTraining Loss: 1.652127 \tValidation Loss: 2.374103\n",
      "Epoch: 18290 \tTraining Loss: 1.658225 \tValidation Loss: 2.373662\n",
      "Epoch: 18291 \tTraining Loss: 1.621707 \tValidation Loss: 2.373259\n",
      "Epoch: 18292 \tTraining Loss: 1.618523 \tValidation Loss: 2.373537\n",
      "Epoch: 18293 \tTraining Loss: 1.617950 \tValidation Loss: 2.373820\n",
      "Epoch: 18294 \tTraining Loss: 1.644679 \tValidation Loss: 2.373933\n",
      "Epoch: 18295 \tTraining Loss: 1.622989 \tValidation Loss: 2.374362\n",
      "Epoch: 18296 \tTraining Loss: 1.652164 \tValidation Loss: 2.373575\n",
      "Epoch: 18297 \tTraining Loss: 1.597718 \tValidation Loss: 2.374106\n",
      "Epoch: 18298 \tTraining Loss: 1.604647 \tValidation Loss: 2.374109\n",
      "Epoch: 18299 \tTraining Loss: 1.649905 \tValidation Loss: 2.374293\n",
      "Epoch: 18300 \tTraining Loss: 1.589275 \tValidation Loss: 2.374308\n",
      "Epoch: 18301 \tTraining Loss: 1.627691 \tValidation Loss: 2.374709\n",
      "Epoch: 18302 \tTraining Loss: 1.599595 \tValidation Loss: 2.374640\n",
      "Epoch: 18303 \tTraining Loss: 1.627688 \tValidation Loss: 2.374310\n",
      "Epoch: 18304 \tTraining Loss: 1.627492 \tValidation Loss: 2.374103\n",
      "Epoch: 18305 \tTraining Loss: 1.597090 \tValidation Loss: 2.374540\n",
      "Epoch: 18306 \tTraining Loss: 1.671816 \tValidation Loss: 2.373655\n",
      "Epoch: 18307 \tTraining Loss: 1.606049 \tValidation Loss: 2.374125\n",
      "Epoch: 18308 \tTraining Loss: 1.607157 \tValidation Loss: 2.374619\n",
      "Epoch: 18309 \tTraining Loss: 1.652499 \tValidation Loss: 2.374243\n",
      "Epoch: 18310 \tTraining Loss: 1.616281 \tValidation Loss: 2.374377\n",
      "Epoch: 18311 \tTraining Loss: 1.605997 \tValidation Loss: 2.374747\n",
      "Epoch: 18312 \tTraining Loss: 1.606556 \tValidation Loss: 2.375119\n",
      "Epoch: 18313 \tTraining Loss: 1.653340 \tValidation Loss: 2.374459\n",
      "Epoch: 18314 \tTraining Loss: 1.580533 \tValidation Loss: 2.374813\n",
      "Epoch: 18315 \tTraining Loss: 1.612768 \tValidation Loss: 2.374381\n",
      "Epoch: 18316 \tTraining Loss: 1.658614 \tValidation Loss: 2.374338\n",
      "Epoch: 18317 \tTraining Loss: 1.599682 \tValidation Loss: 2.374385\n",
      "Epoch: 18318 \tTraining Loss: 1.612241 \tValidation Loss: 2.374804\n",
      "Epoch: 18319 \tTraining Loss: 1.638860 \tValidation Loss: 2.374303\n",
      "Epoch: 18320 \tTraining Loss: 1.634180 \tValidation Loss: 2.374383\n",
      "Epoch: 18321 \tTraining Loss: 1.626831 \tValidation Loss: 2.374326\n",
      "Epoch: 18322 \tTraining Loss: 1.591825 \tValidation Loss: 2.374902\n",
      "Epoch: 18323 \tTraining Loss: 1.623057 \tValidation Loss: 2.374460\n",
      "Epoch: 18324 \tTraining Loss: 1.576091 \tValidation Loss: 2.374997\n",
      "Epoch: 18325 \tTraining Loss: 1.608790 \tValidation Loss: 2.375351\n",
      "Epoch: 18326 \tTraining Loss: 1.647620 \tValidation Loss: 2.374102\n",
      "Epoch: 18327 \tTraining Loss: 1.621789 \tValidation Loss: 2.374488\n",
      "Epoch: 18328 \tTraining Loss: 1.643476 \tValidation Loss: 2.374004\n",
      "Epoch: 18329 \tTraining Loss: 1.616845 \tValidation Loss: 2.374853\n",
      "Epoch: 18330 \tTraining Loss: 1.610412 \tValidation Loss: 2.374332\n",
      "Epoch: 18331 \tTraining Loss: 1.631141 \tValidation Loss: 2.374400\n",
      "Epoch: 18332 \tTraining Loss: 1.620841 \tValidation Loss: 2.374511\n",
      "Epoch: 18333 \tTraining Loss: 1.627055 \tValidation Loss: 2.374174\n",
      "Epoch: 18334 \tTraining Loss: 1.586805 \tValidation Loss: 2.375008\n",
      "Epoch: 18335 \tTraining Loss: 1.586756 \tValidation Loss: 2.374959\n",
      "Epoch: 18336 \tTraining Loss: 1.576165 \tValidation Loss: 2.374889\n",
      "Epoch: 18337 \tTraining Loss: 1.634796 \tValidation Loss: 2.374827\n",
      "Epoch: 18338 \tTraining Loss: 1.632191 \tValidation Loss: 2.375113\n",
      "Epoch: 18339 \tTraining Loss: 1.618196 \tValidation Loss: 2.375136\n",
      "Epoch: 18340 \tTraining Loss: 1.610330 \tValidation Loss: 2.374761\n",
      "Epoch: 18341 \tTraining Loss: 1.611083 \tValidation Loss: 2.374964\n",
      "Epoch: 18342 \tTraining Loss: 1.629947 \tValidation Loss: 2.374476\n",
      "Epoch: 18343 \tTraining Loss: 1.607984 \tValidation Loss: 2.374532\n",
      "Epoch: 18344 \tTraining Loss: 1.618928 \tValidation Loss: 2.375147\n",
      "Epoch: 18345 \tTraining Loss: 1.597870 \tValidation Loss: 2.375795\n",
      "Epoch: 18346 \tTraining Loss: 1.632469 \tValidation Loss: 2.375257\n",
      "Epoch: 18347 \tTraining Loss: 1.640006 \tValidation Loss: 2.375074\n",
      "Epoch: 18348 \tTraining Loss: 1.621643 \tValidation Loss: 2.374781\n",
      "Epoch: 18349 \tTraining Loss: 1.620991 \tValidation Loss: 2.374614\n",
      "Epoch: 18350 \tTraining Loss: 1.615698 \tValidation Loss: 2.374933\n",
      "Epoch: 18351 \tTraining Loss: 1.622732 \tValidation Loss: 2.374906\n",
      "Epoch: 18352 \tTraining Loss: 1.607076 \tValidation Loss: 2.375507\n",
      "Epoch: 18353 \tTraining Loss: 1.621450 \tValidation Loss: 2.375084\n",
      "Epoch: 18354 \tTraining Loss: 1.594717 \tValidation Loss: 2.375567\n",
      "Epoch: 18355 \tTraining Loss: 1.607895 \tValidation Loss: 2.375376\n",
      "Epoch: 18356 \tTraining Loss: 1.658227 \tValidation Loss: 2.375891\n",
      "Epoch: 18357 \tTraining Loss: 1.635276 \tValidation Loss: 2.375279\n",
      "Epoch: 18358 \tTraining Loss: 1.599327 \tValidation Loss: 2.375578\n",
      "Epoch: 18359 \tTraining Loss: 1.618830 \tValidation Loss: 2.375386\n",
      "Epoch: 18360 \tTraining Loss: 1.580784 \tValidation Loss: 2.375840\n",
      "Epoch: 18361 \tTraining Loss: 1.582906 \tValidation Loss: 2.376205\n",
      "Epoch: 18362 \tTraining Loss: 1.582395 \tValidation Loss: 2.376068\n",
      "Epoch: 18363 \tTraining Loss: 1.590514 \tValidation Loss: 2.375550\n",
      "Epoch: 18364 \tTraining Loss: 1.605255 \tValidation Loss: 2.375926\n",
      "Epoch: 18365 \tTraining Loss: 1.633850 \tValidation Loss: 2.375866\n",
      "Epoch: 18366 \tTraining Loss: 1.674955 \tValidation Loss: 2.375256\n",
      "Epoch: 18367 \tTraining Loss: 1.562578 \tValidation Loss: 2.375515\n",
      "Epoch: 18368 \tTraining Loss: 1.645910 \tValidation Loss: 2.375573\n",
      "Epoch: 18369 \tTraining Loss: 1.604562 \tValidation Loss: 2.375613\n",
      "Epoch: 18370 \tTraining Loss: 1.636747 \tValidation Loss: 2.375201\n",
      "Epoch: 18371 \tTraining Loss: 1.655476 \tValidation Loss: 2.375726\n",
      "Epoch: 18372 \tTraining Loss: 1.635336 \tValidation Loss: 2.376251\n",
      "Epoch: 18373 \tTraining Loss: 1.621970 \tValidation Loss: 2.375230\n",
      "Epoch: 18374 \tTraining Loss: 1.610065 \tValidation Loss: 2.375590\n",
      "Epoch: 18375 \tTraining Loss: 1.626115 \tValidation Loss: 2.375954\n",
      "Epoch: 18376 \tTraining Loss: 1.614238 \tValidation Loss: 2.375565\n",
      "Epoch: 18377 \tTraining Loss: 1.610614 \tValidation Loss: 2.375356\n",
      "Epoch: 18378 \tTraining Loss: 1.573348 \tValidation Loss: 2.374881\n",
      "Epoch: 18379 \tTraining Loss: 1.592607 \tValidation Loss: 2.375711\n",
      "Epoch: 18380 \tTraining Loss: 1.645437 \tValidation Loss: 2.376204\n",
      "Epoch: 18381 \tTraining Loss: 1.630916 \tValidation Loss: 2.375858\n",
      "Epoch: 18382 \tTraining Loss: 1.615044 \tValidation Loss: 2.375418\n",
      "Epoch: 18383 \tTraining Loss: 1.605061 \tValidation Loss: 2.375848\n",
      "Epoch: 18384 \tTraining Loss: 1.599460 \tValidation Loss: 2.375776\n",
      "Epoch: 18385 \tTraining Loss: 1.610114 \tValidation Loss: 2.375942\n",
      "Epoch: 18386 \tTraining Loss: 1.631506 \tValidation Loss: 2.376075\n",
      "Epoch: 18387 \tTraining Loss: 1.623057 \tValidation Loss: 2.375600\n",
      "Epoch: 18388 \tTraining Loss: 1.615521 \tValidation Loss: 2.375725\n",
      "Epoch: 18389 \tTraining Loss: 1.595276 \tValidation Loss: 2.376145\n",
      "Epoch: 18390 \tTraining Loss: 1.616193 \tValidation Loss: 2.376346\n",
      "Epoch: 18391 \tTraining Loss: 1.646724 \tValidation Loss: 2.375718\n",
      "Epoch: 18392 \tTraining Loss: 1.596783 \tValidation Loss: 2.375508\n",
      "Epoch: 18393 \tTraining Loss: 1.613805 \tValidation Loss: 2.375405\n",
      "Epoch: 18394 \tTraining Loss: 1.581783 \tValidation Loss: 2.375592\n",
      "Epoch: 18395 \tTraining Loss: 1.610215 \tValidation Loss: 2.375662\n",
      "Epoch: 18396 \tTraining Loss: 1.581986 \tValidation Loss: 2.375393\n",
      "Epoch: 18397 \tTraining Loss: 1.616818 \tValidation Loss: 2.375770\n",
      "Epoch: 18398 \tTraining Loss: 1.607729 \tValidation Loss: 2.375796\n",
      "Epoch: 18399 \tTraining Loss: 1.608043 \tValidation Loss: 2.375448\n",
      "Epoch: 18400 \tTraining Loss: 1.572069 \tValidation Loss: 2.375849\n",
      "Epoch: 18401 \tTraining Loss: 1.619860 \tValidation Loss: 2.376249\n",
      "Epoch: 18402 \tTraining Loss: 1.578648 \tValidation Loss: 2.376046\n",
      "Epoch: 18403 \tTraining Loss: 1.603331 \tValidation Loss: 2.376457\n",
      "Epoch: 18404 \tTraining Loss: 1.609999 \tValidation Loss: 2.375826\n",
      "Epoch: 18405 \tTraining Loss: 1.617691 \tValidation Loss: 2.375946\n",
      "Epoch: 18406 \tTraining Loss: 1.631259 \tValidation Loss: 2.375837\n",
      "Epoch: 18407 \tTraining Loss: 1.633793 \tValidation Loss: 2.375531\n",
      "Epoch: 18408 \tTraining Loss: 1.576779 \tValidation Loss: 2.375571\n",
      "Epoch: 18409 \tTraining Loss: 1.634014 \tValidation Loss: 2.376311\n",
      "Epoch: 18410 \tTraining Loss: 1.611483 \tValidation Loss: 2.375724\n",
      "Epoch: 18411 \tTraining Loss: 1.633855 \tValidation Loss: 2.375965\n",
      "Epoch: 18412 \tTraining Loss: 1.581018 \tValidation Loss: 2.375823\n",
      "Epoch: 18413 \tTraining Loss: 1.589427 \tValidation Loss: 2.376347\n",
      "Epoch: 18414 \tTraining Loss: 1.596746 \tValidation Loss: 2.375880\n",
      "Epoch: 18415 \tTraining Loss: 1.614242 \tValidation Loss: 2.376007\n",
      "Epoch: 18416 \tTraining Loss: 1.631194 \tValidation Loss: 2.376029\n",
      "Epoch: 18417 \tTraining Loss: 1.605140 \tValidation Loss: 2.376102\n",
      "Epoch: 18418 \tTraining Loss: 1.574232 \tValidation Loss: 2.376407\n",
      "Epoch: 18419 \tTraining Loss: 1.600701 \tValidation Loss: 2.376396\n",
      "Epoch: 18420 \tTraining Loss: 1.605275 \tValidation Loss: 2.376017\n",
      "Epoch: 18421 \tTraining Loss: 1.595685 \tValidation Loss: 2.376072\n",
      "Epoch: 18422 \tTraining Loss: 1.616973 \tValidation Loss: 2.375954\n",
      "Epoch: 18423 \tTraining Loss: 1.621230 \tValidation Loss: 2.375750\n",
      "Epoch: 18424 \tTraining Loss: 1.622683 \tValidation Loss: 2.376344\n",
      "Epoch: 18425 \tTraining Loss: 1.609686 \tValidation Loss: 2.376380\n",
      "Epoch: 18426 \tTraining Loss: 1.618188 \tValidation Loss: 2.376401\n",
      "Epoch: 18427 \tTraining Loss: 1.620434 \tValidation Loss: 2.376348\n",
      "Epoch: 18428 \tTraining Loss: 1.582394 \tValidation Loss: 2.376369\n",
      "Epoch: 18429 \tTraining Loss: 1.602514 \tValidation Loss: 2.376524\n",
      "Epoch: 18430 \tTraining Loss: 1.603692 \tValidation Loss: 2.376182\n",
      "Epoch: 18431 \tTraining Loss: 1.667683 \tValidation Loss: 2.376228\n",
      "Epoch: 18432 \tTraining Loss: 1.607228 \tValidation Loss: 2.375538\n",
      "Epoch: 18433 \tTraining Loss: 1.595143 \tValidation Loss: 2.375983\n",
      "Epoch: 18434 \tTraining Loss: 1.628245 \tValidation Loss: 2.376327\n",
      "Epoch: 18435 \tTraining Loss: 1.644452 \tValidation Loss: 2.375985\n",
      "Epoch: 18436 \tTraining Loss: 1.636562 \tValidation Loss: 2.376403\n",
      "Epoch: 18437 \tTraining Loss: 1.573469 \tValidation Loss: 2.376575\n",
      "Epoch: 18438 \tTraining Loss: 1.578866 \tValidation Loss: 2.376978\n",
      "Epoch: 18439 \tTraining Loss: 1.632759 \tValidation Loss: 2.376558\n",
      "Epoch: 18440 \tTraining Loss: 1.613277 \tValidation Loss: 2.376408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18441 \tTraining Loss: 1.610855 \tValidation Loss: 2.376104\n",
      "Epoch: 18442 \tTraining Loss: 1.586341 \tValidation Loss: 2.376288\n",
      "Epoch: 18443 \tTraining Loss: 1.612752 \tValidation Loss: 2.377007\n",
      "Epoch: 18444 \tTraining Loss: 1.636247 \tValidation Loss: 2.376556\n",
      "Epoch: 18445 \tTraining Loss: 1.634156 \tValidation Loss: 2.376181\n",
      "Epoch: 18446 \tTraining Loss: 1.581275 \tValidation Loss: 2.376505\n",
      "Epoch: 18447 \tTraining Loss: 1.592156 \tValidation Loss: 2.376137\n",
      "Epoch: 18448 \tTraining Loss: 1.589161 \tValidation Loss: 2.376334\n",
      "Epoch: 18449 \tTraining Loss: 1.638503 \tValidation Loss: 2.376749\n",
      "Epoch: 18450 \tTraining Loss: 1.599544 \tValidation Loss: 2.376650\n",
      "Epoch: 18451 \tTraining Loss: 1.621725 \tValidation Loss: 2.376316\n",
      "Epoch: 18452 \tTraining Loss: 1.603339 \tValidation Loss: 2.376690\n",
      "Epoch: 18453 \tTraining Loss: 1.652862 \tValidation Loss: 2.376457\n",
      "Epoch: 18454 \tTraining Loss: 1.603983 \tValidation Loss: 2.376493\n",
      "Epoch: 18455 \tTraining Loss: 1.629349 \tValidation Loss: 2.376901\n",
      "Epoch: 18456 \tTraining Loss: 1.627999 \tValidation Loss: 2.376878\n",
      "Epoch: 18457 \tTraining Loss: 1.639876 \tValidation Loss: 2.377367\n",
      "Epoch: 18458 \tTraining Loss: 1.617050 \tValidation Loss: 2.376788\n",
      "Epoch: 18459 \tTraining Loss: 1.622097 \tValidation Loss: 2.376479\n",
      "Epoch: 18460 \tTraining Loss: 1.616409 \tValidation Loss: 2.376399\n",
      "Epoch: 18461 \tTraining Loss: 1.593394 \tValidation Loss: 2.376707\n",
      "Epoch: 18462 \tTraining Loss: 1.612715 \tValidation Loss: 2.376506\n",
      "Epoch: 18463 \tTraining Loss: 1.587452 \tValidation Loss: 2.376645\n",
      "Epoch: 18464 \tTraining Loss: 1.612599 \tValidation Loss: 2.377500\n",
      "Epoch: 18465 \tTraining Loss: 1.647502 \tValidation Loss: 2.377064\n",
      "Epoch: 18466 \tTraining Loss: 1.592641 \tValidation Loss: 2.377258\n",
      "Epoch: 18467 \tTraining Loss: 1.606288 \tValidation Loss: 2.377220\n",
      "Epoch: 18468 \tTraining Loss: 1.612379 \tValidation Loss: 2.376829\n",
      "Epoch: 18469 \tTraining Loss: 1.574762 \tValidation Loss: 2.377196\n",
      "Epoch: 18470 \tTraining Loss: 1.606460 \tValidation Loss: 2.377522\n",
      "Epoch: 18471 \tTraining Loss: 1.624338 \tValidation Loss: 2.377053\n",
      "Epoch: 18472 \tTraining Loss: 1.594219 \tValidation Loss: 2.377470\n",
      "Epoch: 18473 \tTraining Loss: 1.548627 \tValidation Loss: 2.377497\n",
      "Epoch: 18474 \tTraining Loss: 1.591742 \tValidation Loss: 2.377567\n",
      "Epoch: 18475 \tTraining Loss: 1.599096 \tValidation Loss: 2.377768\n",
      "Epoch: 18476 \tTraining Loss: 1.634928 \tValidation Loss: 2.377086\n",
      "Epoch: 18477 \tTraining Loss: 1.623983 \tValidation Loss: 2.376688\n",
      "Epoch: 18478 \tTraining Loss: 1.621375 \tValidation Loss: 2.376996\n",
      "Epoch: 18479 \tTraining Loss: 1.597285 \tValidation Loss: 2.377603\n",
      "Epoch: 18480 \tTraining Loss: 1.589926 \tValidation Loss: 2.377775\n",
      "Epoch: 18481 \tTraining Loss: 1.619358 \tValidation Loss: 2.377335\n",
      "Epoch: 18482 \tTraining Loss: 1.598257 \tValidation Loss: 2.378287\n",
      "Epoch: 18483 \tTraining Loss: 1.631394 \tValidation Loss: 2.377917\n",
      "Epoch: 18484 \tTraining Loss: 1.578492 \tValidation Loss: 2.378188\n",
      "Epoch: 18485 \tTraining Loss: 1.550711 \tValidation Loss: 2.378076\n",
      "Epoch: 18486 \tTraining Loss: 1.604210 \tValidation Loss: 2.377118\n",
      "Epoch: 18487 \tTraining Loss: 1.584710 \tValidation Loss: 2.377047\n",
      "Epoch: 18488 \tTraining Loss: 1.641435 \tValidation Loss: 2.377368\n",
      "Epoch: 18489 \tTraining Loss: 1.610413 \tValidation Loss: 2.377579\n",
      "Epoch: 18490 \tTraining Loss: 1.599440 \tValidation Loss: 2.376673\n",
      "Epoch: 18491 \tTraining Loss: 1.599786 \tValidation Loss: 2.376835\n",
      "Epoch: 18492 \tTraining Loss: 1.599518 \tValidation Loss: 2.376820\n",
      "Epoch: 18493 \tTraining Loss: 1.597970 \tValidation Loss: 2.377285\n",
      "Epoch: 18494 \tTraining Loss: 1.587984 \tValidation Loss: 2.377223\n",
      "Epoch: 18495 \tTraining Loss: 1.620203 \tValidation Loss: 2.377029\n",
      "Epoch: 18496 \tTraining Loss: 1.598669 \tValidation Loss: 2.376506\n",
      "Epoch: 18497 \tTraining Loss: 1.570849 \tValidation Loss: 2.377348\n",
      "Epoch: 18498 \tTraining Loss: 1.570955 \tValidation Loss: 2.377566\n",
      "Epoch: 18499 \tTraining Loss: 1.625587 \tValidation Loss: 2.377007\n",
      "Epoch: 18500 \tTraining Loss: 1.582593 \tValidation Loss: 2.377021\n",
      "Epoch: 18501 \tTraining Loss: 1.622434 \tValidation Loss: 2.378038\n",
      "Epoch: 18502 \tTraining Loss: 1.561846 \tValidation Loss: 2.377953\n",
      "Epoch: 18503 \tTraining Loss: 1.601930 \tValidation Loss: 2.377836\n",
      "Epoch: 18504 \tTraining Loss: 1.607091 \tValidation Loss: 2.377732\n",
      "Epoch: 18505 \tTraining Loss: 1.618472 \tValidation Loss: 2.377281\n",
      "Epoch: 18506 \tTraining Loss: 1.633317 \tValidation Loss: 2.377042\n",
      "Epoch: 18507 \tTraining Loss: 1.587551 \tValidation Loss: 2.378284\n",
      "Epoch: 18508 \tTraining Loss: 1.610398 \tValidation Loss: 2.377759\n",
      "Epoch: 18509 \tTraining Loss: 1.594091 \tValidation Loss: 2.377694\n",
      "Epoch: 18510 \tTraining Loss: 1.627866 \tValidation Loss: 2.377027\n",
      "Epoch: 18511 \tTraining Loss: 1.615356 \tValidation Loss: 2.376781\n",
      "Epoch: 18512 \tTraining Loss: 1.608323 \tValidation Loss: 2.377297\n",
      "Epoch: 18513 \tTraining Loss: 1.607180 \tValidation Loss: 2.377835\n",
      "Epoch: 18514 \tTraining Loss: 1.640644 \tValidation Loss: 2.377730\n",
      "Epoch: 18515 \tTraining Loss: 1.600544 \tValidation Loss: 2.377804\n",
      "Epoch: 18516 \tTraining Loss: 1.644226 \tValidation Loss: 2.377708\n",
      "Epoch: 18517 \tTraining Loss: 1.634832 \tValidation Loss: 2.377403\n",
      "Epoch: 18518 \tTraining Loss: 1.632243 \tValidation Loss: 2.377489\n",
      "Epoch: 18519 \tTraining Loss: 1.602385 \tValidation Loss: 2.377346\n",
      "Epoch: 18520 \tTraining Loss: 1.620605 \tValidation Loss: 2.377393\n",
      "Epoch: 18521 \tTraining Loss: 1.588789 \tValidation Loss: 2.377189\n",
      "Epoch: 18522 \tTraining Loss: 1.602444 \tValidation Loss: 2.377431\n",
      "Epoch: 18523 \tTraining Loss: 1.648224 \tValidation Loss: 2.377637\n",
      "Epoch: 18524 \tTraining Loss: 1.582452 \tValidation Loss: 2.377871\n",
      "Epoch: 18525 \tTraining Loss: 1.608416 \tValidation Loss: 2.377500\n",
      "Epoch: 18526 \tTraining Loss: 1.569821 \tValidation Loss: 2.378414\n",
      "Epoch: 18527 \tTraining Loss: 1.626771 \tValidation Loss: 2.377729\n",
      "Epoch: 18528 \tTraining Loss: 1.643374 \tValidation Loss: 2.376920\n",
      "Epoch: 18529 \tTraining Loss: 1.619708 \tValidation Loss: 2.377531\n",
      "Epoch: 18530 \tTraining Loss: 1.613587 \tValidation Loss: 2.377602\n",
      "Epoch: 18531 \tTraining Loss: 1.603880 \tValidation Loss: 2.377788\n",
      "Epoch: 18532 \tTraining Loss: 1.608139 \tValidation Loss: 2.376924\n",
      "Epoch: 18533 \tTraining Loss: 1.595270 \tValidation Loss: 2.377682\n",
      "Epoch: 18534 \tTraining Loss: 1.610208 \tValidation Loss: 2.377968\n",
      "Epoch: 18535 \tTraining Loss: 1.583277 \tValidation Loss: 2.377858\n",
      "Epoch: 18536 \tTraining Loss: 1.597509 \tValidation Loss: 2.377523\n",
      "Epoch: 18537 \tTraining Loss: 1.622218 \tValidation Loss: 2.377774\n",
      "Epoch: 18538 \tTraining Loss: 1.634549 \tValidation Loss: 2.378098\n",
      "Epoch: 18539 \tTraining Loss: 1.588449 \tValidation Loss: 2.377457\n",
      "Epoch: 18540 \tTraining Loss: 1.610853 \tValidation Loss: 2.377884\n",
      "Epoch: 18541 \tTraining Loss: 1.591398 \tValidation Loss: 2.378569\n",
      "Epoch: 18542 \tTraining Loss: 1.584926 \tValidation Loss: 2.378699\n",
      "Epoch: 18543 \tTraining Loss: 1.585660 \tValidation Loss: 2.378272\n",
      "Epoch: 18544 \tTraining Loss: 1.579594 \tValidation Loss: 2.378143\n",
      "Epoch: 18545 \tTraining Loss: 1.634670 \tValidation Loss: 2.378298\n",
      "Epoch: 18546 \tTraining Loss: 1.597955 \tValidation Loss: 2.378142\n",
      "Epoch: 18547 \tTraining Loss: 1.635506 \tValidation Loss: 2.378216\n",
      "Epoch: 18548 \tTraining Loss: 1.572203 \tValidation Loss: 2.378293\n",
      "Epoch: 18549 \tTraining Loss: 1.606884 \tValidation Loss: 2.378872\n",
      "Epoch: 18550 \tTraining Loss: 1.620145 \tValidation Loss: 2.378276\n",
      "Epoch: 18551 \tTraining Loss: 1.639370 \tValidation Loss: 2.378156\n",
      "Epoch: 18552 \tTraining Loss: 1.603931 \tValidation Loss: 2.378725\n",
      "Epoch: 18553 \tTraining Loss: 1.625146 \tValidation Loss: 2.378543\n",
      "Epoch: 18554 \tTraining Loss: 1.632215 \tValidation Loss: 2.378806\n",
      "Epoch: 18555 \tTraining Loss: 1.617136 \tValidation Loss: 2.378180\n",
      "Epoch: 18556 \tTraining Loss: 1.603018 \tValidation Loss: 2.378985\n",
      "Epoch: 18557 \tTraining Loss: 1.615426 \tValidation Loss: 2.378588\n",
      "Epoch: 18558 \tTraining Loss: 1.639153 \tValidation Loss: 2.377988\n",
      "Epoch: 18559 \tTraining Loss: 1.599799 \tValidation Loss: 2.378174\n",
      "Epoch: 18560 \tTraining Loss: 1.600859 \tValidation Loss: 2.378747\n",
      "Epoch: 18561 \tTraining Loss: 1.613531 \tValidation Loss: 2.377597\n",
      "Epoch: 18562 \tTraining Loss: 1.616747 \tValidation Loss: 2.378264\n",
      "Epoch: 18563 \tTraining Loss: 1.603933 \tValidation Loss: 2.378375\n",
      "Epoch: 18564 \tTraining Loss: 1.599390 \tValidation Loss: 2.377786\n",
      "Epoch: 18565 \tTraining Loss: 1.599380 \tValidation Loss: 2.378196\n",
      "Epoch: 18566 \tTraining Loss: 1.580626 \tValidation Loss: 2.378543\n",
      "Epoch: 18567 \tTraining Loss: 1.612263 \tValidation Loss: 2.378352\n",
      "Epoch: 18568 \tTraining Loss: 1.583936 \tValidation Loss: 2.378486\n",
      "Epoch: 18569 \tTraining Loss: 1.609789 \tValidation Loss: 2.378740\n",
      "Epoch: 18570 \tTraining Loss: 1.618402 \tValidation Loss: 2.378710\n",
      "Epoch: 18571 \tTraining Loss: 1.651528 \tValidation Loss: 2.378601\n",
      "Epoch: 18572 \tTraining Loss: 1.617678 \tValidation Loss: 2.378058\n",
      "Epoch: 18573 \tTraining Loss: 1.598316 \tValidation Loss: 2.378565\n",
      "Epoch: 18574 \tTraining Loss: 1.590640 \tValidation Loss: 2.378334\n",
      "Epoch: 18575 \tTraining Loss: 1.646773 \tValidation Loss: 2.377779\n",
      "Epoch: 18576 \tTraining Loss: 1.602979 \tValidation Loss: 2.378523\n",
      "Epoch: 18577 \tTraining Loss: 1.600831 \tValidation Loss: 2.378418\n",
      "Epoch: 18578 \tTraining Loss: 1.643930 \tValidation Loss: 2.378384\n",
      "Epoch: 18579 \tTraining Loss: 1.634195 \tValidation Loss: 2.377936\n",
      "Epoch: 18580 \tTraining Loss: 1.583401 \tValidation Loss: 2.378531\n",
      "Epoch: 18581 \tTraining Loss: 1.629731 \tValidation Loss: 2.378524\n",
      "Epoch: 18582 \tTraining Loss: 1.610101 \tValidation Loss: 2.379071\n",
      "Epoch: 18583 \tTraining Loss: 1.603363 \tValidation Loss: 2.379347\n",
      "Epoch: 18584 \tTraining Loss: 1.559084 \tValidation Loss: 2.378647\n",
      "Epoch: 18585 \tTraining Loss: 1.620263 \tValidation Loss: 2.378404\n",
      "Epoch: 18586 \tTraining Loss: 1.570654 \tValidation Loss: 2.378726\n",
      "Epoch: 18587 \tTraining Loss: 1.587505 \tValidation Loss: 2.379411\n",
      "Epoch: 18588 \tTraining Loss: 1.604438 \tValidation Loss: 2.379040\n",
      "Epoch: 18589 \tTraining Loss: 1.614161 \tValidation Loss: 2.379369\n",
      "Epoch: 18590 \tTraining Loss: 1.618801 \tValidation Loss: 2.379039\n",
      "Epoch: 18591 \tTraining Loss: 1.578759 \tValidation Loss: 2.379400\n",
      "Epoch: 18592 \tTraining Loss: 1.578289 \tValidation Loss: 2.379764\n",
      "Epoch: 18593 \tTraining Loss: 1.636076 \tValidation Loss: 2.378692\n",
      "Epoch: 18594 \tTraining Loss: 1.581422 \tValidation Loss: 2.379390\n",
      "Epoch: 18595 \tTraining Loss: 1.595694 \tValidation Loss: 2.379486\n",
      "Epoch: 18596 \tTraining Loss: 1.642736 \tValidation Loss: 2.378792\n",
      "Epoch: 18597 \tTraining Loss: 1.616749 \tValidation Loss: 2.378512\n",
      "Epoch: 18598 \tTraining Loss: 1.594857 \tValidation Loss: 2.379286\n",
      "Epoch: 18599 \tTraining Loss: 1.657247 \tValidation Loss: 2.378755\n",
      "Epoch: 18600 \tTraining Loss: 1.621927 \tValidation Loss: 2.378751\n",
      "Epoch: 18601 \tTraining Loss: 1.634079 \tValidation Loss: 2.378688\n",
      "Epoch: 18602 \tTraining Loss: 1.579844 \tValidation Loss: 2.379100\n",
      "Epoch: 18603 \tTraining Loss: 1.572357 \tValidation Loss: 2.379361\n",
      "Epoch: 18604 \tTraining Loss: 1.624422 \tValidation Loss: 2.379175\n",
      "Epoch: 18605 \tTraining Loss: 1.610626 \tValidation Loss: 2.378379\n",
      "Epoch: 18606 \tTraining Loss: 1.621437 \tValidation Loss: 2.378662\n",
      "Epoch: 18607 \tTraining Loss: 1.628841 \tValidation Loss: 2.379079\n",
      "Epoch: 18608 \tTraining Loss: 1.639480 \tValidation Loss: 2.379002\n",
      "Epoch: 18609 \tTraining Loss: 1.622004 \tValidation Loss: 2.379622\n",
      "Epoch: 18610 \tTraining Loss: 1.631380 \tValidation Loss: 2.379069\n",
      "Epoch: 18611 \tTraining Loss: 1.622977 \tValidation Loss: 2.378906\n",
      "Epoch: 18612 \tTraining Loss: 1.625470 \tValidation Loss: 2.379129\n",
      "Epoch: 18613 \tTraining Loss: 1.614756 \tValidation Loss: 2.379003\n",
      "Epoch: 18614 \tTraining Loss: 1.594784 \tValidation Loss: 2.379190\n",
      "Epoch: 18615 \tTraining Loss: 1.619426 \tValidation Loss: 2.378495\n",
      "Epoch: 18616 \tTraining Loss: 1.592488 \tValidation Loss: 2.379265\n",
      "Epoch: 18617 \tTraining Loss: 1.617438 \tValidation Loss: 2.379130\n",
      "Epoch: 18618 \tTraining Loss: 1.630612 \tValidation Loss: 2.378669\n",
      "Epoch: 18619 \tTraining Loss: 1.607594 \tValidation Loss: 2.379137\n",
      "Epoch: 18620 \tTraining Loss: 1.623526 \tValidation Loss: 2.379296\n",
      "Epoch: 18621 \tTraining Loss: 1.632795 \tValidation Loss: 2.379473\n",
      "Epoch: 18622 \tTraining Loss: 1.595468 \tValidation Loss: 2.379474\n",
      "Epoch: 18623 \tTraining Loss: 1.612743 \tValidation Loss: 2.379896\n",
      "Epoch: 18624 \tTraining Loss: 1.580992 \tValidation Loss: 2.379244\n",
      "Epoch: 18625 \tTraining Loss: 1.587978 \tValidation Loss: 2.379356\n",
      "Epoch: 18626 \tTraining Loss: 1.588634 \tValidation Loss: 2.379408\n",
      "Epoch: 18627 \tTraining Loss: 1.574529 \tValidation Loss: 2.379679\n",
      "Epoch: 18628 \tTraining Loss: 1.616977 \tValidation Loss: 2.379583\n",
      "Epoch: 18629 \tTraining Loss: 1.651004 \tValidation Loss: 2.379584\n",
      "Epoch: 18630 \tTraining Loss: 1.635714 \tValidation Loss: 2.379223\n",
      "Epoch: 18631 \tTraining Loss: 1.570384 \tValidation Loss: 2.379213\n",
      "Epoch: 18632 \tTraining Loss: 1.633395 \tValidation Loss: 2.379035\n",
      "Epoch: 18633 \tTraining Loss: 1.621987 \tValidation Loss: 2.378884\n",
      "Epoch: 18634 \tTraining Loss: 1.612217 \tValidation Loss: 2.378395\n",
      "Epoch: 18635 \tTraining Loss: 1.636806 \tValidation Loss: 2.379012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18636 \tTraining Loss: 1.631159 \tValidation Loss: 2.378606\n",
      "Epoch: 18637 \tTraining Loss: 1.610792 \tValidation Loss: 2.379506\n",
      "Epoch: 18638 \tTraining Loss: 1.586082 \tValidation Loss: 2.378912\n",
      "Epoch: 18639 \tTraining Loss: 1.585998 \tValidation Loss: 2.379021\n",
      "Epoch: 18640 \tTraining Loss: 1.580407 \tValidation Loss: 2.379962\n",
      "Epoch: 18641 \tTraining Loss: 1.627809 \tValidation Loss: 2.380060\n",
      "Epoch: 18642 \tTraining Loss: 1.598904 \tValidation Loss: 2.379747\n",
      "Epoch: 18643 \tTraining Loss: 1.603440 \tValidation Loss: 2.379830\n",
      "Epoch: 18644 \tTraining Loss: 1.603696 \tValidation Loss: 2.379674\n",
      "Epoch: 18645 \tTraining Loss: 1.640945 \tValidation Loss: 2.379495\n",
      "Epoch: 18646 \tTraining Loss: 1.591504 \tValidation Loss: 2.379734\n",
      "Epoch: 18647 \tTraining Loss: 1.605398 \tValidation Loss: 2.380118\n",
      "Epoch: 18648 \tTraining Loss: 1.601915 \tValidation Loss: 2.379890\n",
      "Epoch: 18649 \tTraining Loss: 1.601919 \tValidation Loss: 2.380266\n",
      "Epoch: 18650 \tTraining Loss: 1.608564 \tValidation Loss: 2.379830\n",
      "Epoch: 18651 \tTraining Loss: 1.601163 \tValidation Loss: 2.380320\n",
      "Epoch: 18652 \tTraining Loss: 1.607431 \tValidation Loss: 2.379457\n",
      "Epoch: 18653 \tTraining Loss: 1.581102 \tValidation Loss: 2.379961\n",
      "Epoch: 18654 \tTraining Loss: 1.619925 \tValidation Loss: 2.379970\n",
      "Epoch: 18655 \tTraining Loss: 1.624799 \tValidation Loss: 2.379579\n",
      "Epoch: 18656 \tTraining Loss: 1.594015 \tValidation Loss: 2.380013\n",
      "Epoch: 18657 \tTraining Loss: 1.604247 \tValidation Loss: 2.379576\n",
      "Epoch: 18658 \tTraining Loss: 1.630983 \tValidation Loss: 2.379886\n",
      "Epoch: 18659 \tTraining Loss: 1.579155 \tValidation Loss: 2.379969\n",
      "Epoch: 18660 \tTraining Loss: 1.597716 \tValidation Loss: 2.379590\n",
      "Epoch: 18661 \tTraining Loss: 1.595727 \tValidation Loss: 2.379759\n",
      "Epoch: 18662 \tTraining Loss: 1.595513 \tValidation Loss: 2.379120\n",
      "Epoch: 18663 \tTraining Loss: 1.603803 \tValidation Loss: 2.379900\n",
      "Epoch: 18664 \tTraining Loss: 1.610582 \tValidation Loss: 2.379308\n",
      "Epoch: 18665 \tTraining Loss: 1.614611 \tValidation Loss: 2.379881\n",
      "Epoch: 18666 \tTraining Loss: 1.635426 \tValidation Loss: 2.379694\n",
      "Epoch: 18667 \tTraining Loss: 1.555828 \tValidation Loss: 2.380296\n",
      "Epoch: 18668 \tTraining Loss: 1.601246 \tValidation Loss: 2.380081\n",
      "Epoch: 18669 \tTraining Loss: 1.589590 \tValidation Loss: 2.380559\n",
      "Epoch: 18670 \tTraining Loss: 1.652236 \tValidation Loss: 2.380444\n",
      "Epoch: 18671 \tTraining Loss: 1.625473 \tValidation Loss: 2.379973\n",
      "Epoch: 18672 \tTraining Loss: 1.606077 \tValidation Loss: 2.380432\n",
      "Epoch: 18673 \tTraining Loss: 1.615836 \tValidation Loss: 2.380198\n",
      "Epoch: 18674 \tTraining Loss: 1.606743 \tValidation Loss: 2.380238\n",
      "Epoch: 18675 \tTraining Loss: 1.561873 \tValidation Loss: 2.380384\n",
      "Epoch: 18676 \tTraining Loss: 1.600585 \tValidation Loss: 2.380282\n",
      "Epoch: 18677 \tTraining Loss: 1.615684 \tValidation Loss: 2.380335\n",
      "Epoch: 18678 \tTraining Loss: 1.617709 \tValidation Loss: 2.380352\n",
      "Epoch: 18679 \tTraining Loss: 1.572675 \tValidation Loss: 2.381046\n",
      "Epoch: 18680 \tTraining Loss: 1.583229 \tValidation Loss: 2.380970\n",
      "Epoch: 18681 \tTraining Loss: 1.588796 \tValidation Loss: 2.380654\n",
      "Epoch: 18682 \tTraining Loss: 1.640090 \tValidation Loss: 2.380501\n",
      "Epoch: 18683 \tTraining Loss: 1.645559 \tValidation Loss: 2.380012\n",
      "Epoch: 18684 \tTraining Loss: 1.568713 \tValidation Loss: 2.380434\n",
      "Epoch: 18685 \tTraining Loss: 1.602400 \tValidation Loss: 2.380415\n",
      "Epoch: 18686 \tTraining Loss: 1.575887 \tValidation Loss: 2.380672\n",
      "Epoch: 18687 \tTraining Loss: 1.634465 \tValidation Loss: 2.380182\n",
      "Epoch: 18688 \tTraining Loss: 1.608787 \tValidation Loss: 2.380692\n",
      "Epoch: 18689 \tTraining Loss: 1.597445 \tValidation Loss: 2.380257\n",
      "Epoch: 18690 \tTraining Loss: 1.587942 \tValidation Loss: 2.380117\n",
      "Epoch: 18691 \tTraining Loss: 1.607571 \tValidation Loss: 2.380975\n",
      "Epoch: 18692 \tTraining Loss: 1.604948 \tValidation Loss: 2.380376\n",
      "Epoch: 18693 \tTraining Loss: 1.580904 \tValidation Loss: 2.380826\n",
      "Epoch: 18694 \tTraining Loss: 1.618800 \tValidation Loss: 2.380548\n",
      "Epoch: 18695 \tTraining Loss: 1.618782 \tValidation Loss: 2.380559\n",
      "Epoch: 18696 \tTraining Loss: 1.622765 \tValidation Loss: 2.380349\n",
      "Epoch: 18697 \tTraining Loss: 1.561765 \tValidation Loss: 2.380482\n",
      "Epoch: 18698 \tTraining Loss: 1.598617 \tValidation Loss: 2.380166\n",
      "Epoch: 18699 \tTraining Loss: 1.643983 \tValidation Loss: 2.379961\n",
      "Epoch: 18700 \tTraining Loss: 1.576304 \tValidation Loss: 2.380574\n",
      "Epoch: 18701 \tTraining Loss: 1.614498 \tValidation Loss: 2.380687\n",
      "Epoch: 18702 \tTraining Loss: 1.602710 \tValidation Loss: 2.380666\n",
      "Epoch: 18703 \tTraining Loss: 1.586784 \tValidation Loss: 2.380644\n",
      "Epoch: 18704 \tTraining Loss: 1.611123 \tValidation Loss: 2.380825\n",
      "Epoch: 18705 \tTraining Loss: 1.630241 \tValidation Loss: 2.380108\n",
      "Epoch: 18706 \tTraining Loss: 1.614028 \tValidation Loss: 2.380204\n",
      "Epoch: 18707 \tTraining Loss: 1.599465 \tValidation Loss: 2.380311\n",
      "Epoch: 18708 \tTraining Loss: 1.599851 \tValidation Loss: 2.380376\n",
      "Epoch: 18709 \tTraining Loss: 1.589880 \tValidation Loss: 2.380324\n",
      "Epoch: 18710 \tTraining Loss: 1.631221 \tValidation Loss: 2.380230\n",
      "Epoch: 18711 \tTraining Loss: 1.591097 \tValidation Loss: 2.380936\n",
      "Epoch: 18712 \tTraining Loss: 1.611523 \tValidation Loss: 2.380972\n",
      "Epoch: 18713 \tTraining Loss: 1.649129 \tValidation Loss: 2.379921\n",
      "Epoch: 18714 \tTraining Loss: 1.581640 \tValidation Loss: 2.380414\n",
      "Epoch: 18715 \tTraining Loss: 1.550959 \tValidation Loss: 2.380434\n",
      "Epoch: 18716 \tTraining Loss: 1.617621 \tValidation Loss: 2.380332\n",
      "Epoch: 18717 \tTraining Loss: 1.640236 \tValidation Loss: 2.380637\n",
      "Epoch: 18718 \tTraining Loss: 1.615779 \tValidation Loss: 2.380130\n",
      "Epoch: 18719 \tTraining Loss: 1.593330 \tValidation Loss: 2.380903\n",
      "Epoch: 18720 \tTraining Loss: 1.604253 \tValidation Loss: 2.380171\n",
      "Epoch: 18721 \tTraining Loss: 1.559971 \tValidation Loss: 2.380681\n",
      "Epoch: 18722 \tTraining Loss: 1.605595 \tValidation Loss: 2.381216\n",
      "Epoch: 18723 \tTraining Loss: 1.590152 \tValidation Loss: 2.380451\n",
      "Epoch: 18724 \tTraining Loss: 1.560934 \tValidation Loss: 2.380716\n",
      "Epoch: 18725 \tTraining Loss: 1.591792 \tValidation Loss: 2.380840\n",
      "Epoch: 18726 \tTraining Loss: 1.649532 \tValidation Loss: 2.381032\n",
      "Epoch: 18727 \tTraining Loss: 1.601277 \tValidation Loss: 2.380823\n",
      "Epoch: 18728 \tTraining Loss: 1.635760 \tValidation Loss: 2.381032\n",
      "Epoch: 18729 \tTraining Loss: 1.658884 \tValidation Loss: 2.380569\n",
      "Epoch: 18730 \tTraining Loss: 1.625379 \tValidation Loss: 2.380029\n",
      "Epoch: 18731 \tTraining Loss: 1.633030 \tValidation Loss: 2.380759\n",
      "Epoch: 18732 \tTraining Loss: 1.583446 \tValidation Loss: 2.380962\n",
      "Epoch: 18733 \tTraining Loss: 1.633922 \tValidation Loss: 2.380455\n",
      "Epoch: 18734 \tTraining Loss: 1.616936 \tValidation Loss: 2.380965\n",
      "Epoch: 18735 \tTraining Loss: 1.583416 \tValidation Loss: 2.381034\n",
      "Epoch: 18736 \tTraining Loss: 1.607925 \tValidation Loss: 2.380648\n",
      "Epoch: 18737 \tTraining Loss: 1.600494 \tValidation Loss: 2.380258\n",
      "Epoch: 18738 \tTraining Loss: 1.636275 \tValidation Loss: 2.379745\n",
      "Epoch: 18739 \tTraining Loss: 1.593506 \tValidation Loss: 2.380408\n",
      "Epoch: 18740 \tTraining Loss: 1.627831 \tValidation Loss: 2.381284\n",
      "Epoch: 18741 \tTraining Loss: 1.567382 \tValidation Loss: 2.381476\n",
      "Epoch: 18742 \tTraining Loss: 1.558171 \tValidation Loss: 2.381504\n",
      "Epoch: 18743 \tTraining Loss: 1.582069 \tValidation Loss: 2.382134\n",
      "Epoch: 18744 \tTraining Loss: 1.597264 \tValidation Loss: 2.381699\n",
      "Epoch: 18745 \tTraining Loss: 1.624302 \tValidation Loss: 2.381098\n",
      "Epoch: 18746 \tTraining Loss: 1.595040 \tValidation Loss: 2.381494\n",
      "Epoch: 18747 \tTraining Loss: 1.635995 \tValidation Loss: 2.380792\n",
      "Epoch: 18748 \tTraining Loss: 1.593250 \tValidation Loss: 2.381388\n",
      "Epoch: 18749 \tTraining Loss: 1.618026 \tValidation Loss: 2.380935\n",
      "Epoch: 18750 \tTraining Loss: 1.590039 \tValidation Loss: 2.381202\n",
      "Epoch: 18751 \tTraining Loss: 1.541345 \tValidation Loss: 2.381598\n",
      "Epoch: 18752 \tTraining Loss: 1.603046 \tValidation Loss: 2.381312\n",
      "Epoch: 18753 \tTraining Loss: 1.614246 \tValidation Loss: 2.380780\n",
      "Epoch: 18754 \tTraining Loss: 1.576809 \tValidation Loss: 2.381200\n",
      "Epoch: 18755 \tTraining Loss: 1.586464 \tValidation Loss: 2.381263\n",
      "Epoch: 18756 \tTraining Loss: 1.604154 \tValidation Loss: 2.381128\n",
      "Epoch: 18757 \tTraining Loss: 1.614669 \tValidation Loss: 2.380676\n",
      "Epoch: 18758 \tTraining Loss: 1.590010 \tValidation Loss: 2.381598\n",
      "Epoch: 18759 \tTraining Loss: 1.600733 \tValidation Loss: 2.381456\n",
      "Epoch: 18760 \tTraining Loss: 1.631987 \tValidation Loss: 2.380785\n",
      "Epoch: 18761 \tTraining Loss: 1.622350 \tValidation Loss: 2.380504\n",
      "Epoch: 18762 \tTraining Loss: 1.613082 \tValidation Loss: 2.380868\n",
      "Epoch: 18763 \tTraining Loss: 1.582193 \tValidation Loss: 2.381040\n",
      "Epoch: 18764 \tTraining Loss: 1.611693 \tValidation Loss: 2.381317\n",
      "Epoch: 18765 \tTraining Loss: 1.586398 \tValidation Loss: 2.381146\n",
      "Epoch: 18766 \tTraining Loss: 1.615892 \tValidation Loss: 2.381861\n",
      "Epoch: 18767 \tTraining Loss: 1.591397 \tValidation Loss: 2.381464\n",
      "Epoch: 18768 \tTraining Loss: 1.622718 \tValidation Loss: 2.380872\n",
      "Epoch: 18769 \tTraining Loss: 1.580863 \tValidation Loss: 2.381310\n",
      "Epoch: 18770 \tTraining Loss: 1.641725 \tValidation Loss: 2.381536\n",
      "Epoch: 18771 \tTraining Loss: 1.594826 \tValidation Loss: 2.381305\n",
      "Epoch: 18772 \tTraining Loss: 1.585554 \tValidation Loss: 2.381507\n",
      "Epoch: 18773 \tTraining Loss: 1.600454 \tValidation Loss: 2.382124\n",
      "Epoch: 18774 \tTraining Loss: 1.592108 \tValidation Loss: 2.382021\n",
      "Epoch: 18775 \tTraining Loss: 1.584743 \tValidation Loss: 2.381821\n",
      "Epoch: 18776 \tTraining Loss: 1.624058 \tValidation Loss: 2.381870\n",
      "Epoch: 18777 \tTraining Loss: 1.594901 \tValidation Loss: 2.382065\n",
      "Epoch: 18778 \tTraining Loss: 1.641591 \tValidation Loss: 2.381445\n",
      "Epoch: 18779 \tTraining Loss: 1.577330 \tValidation Loss: 2.382351\n",
      "Epoch: 18780 \tTraining Loss: 1.578915 \tValidation Loss: 2.381822\n",
      "Epoch: 18781 \tTraining Loss: 1.587916 \tValidation Loss: 2.381884\n",
      "Epoch: 18782 \tTraining Loss: 1.634552 \tValidation Loss: 2.380958\n",
      "Epoch: 18783 \tTraining Loss: 1.577985 \tValidation Loss: 2.381429\n",
      "Epoch: 18784 \tTraining Loss: 1.570451 \tValidation Loss: 2.381964\n",
      "Epoch: 18785 \tTraining Loss: 1.584343 \tValidation Loss: 2.381638\n",
      "Epoch: 18786 \tTraining Loss: 1.543402 \tValidation Loss: 2.381527\n",
      "Epoch: 18787 \tTraining Loss: 1.582854 \tValidation Loss: 2.382215\n",
      "Epoch: 18788 \tTraining Loss: 1.614139 \tValidation Loss: 2.382007\n",
      "Epoch: 18789 \tTraining Loss: 1.617282 \tValidation Loss: 2.382016\n",
      "Epoch: 18790 \tTraining Loss: 1.607171 \tValidation Loss: 2.381621\n",
      "Epoch: 18791 \tTraining Loss: 1.601209 \tValidation Loss: 2.381816\n",
      "Epoch: 18792 \tTraining Loss: 1.583907 \tValidation Loss: 2.381873\n",
      "Epoch: 18793 \tTraining Loss: 1.611731 \tValidation Loss: 2.381659\n",
      "Epoch: 18794 \tTraining Loss: 1.604728 \tValidation Loss: 2.381653\n",
      "Epoch: 18795 \tTraining Loss: 1.572033 \tValidation Loss: 2.382194\n",
      "Epoch: 18796 \tTraining Loss: 1.575365 \tValidation Loss: 2.381841\n",
      "Epoch: 18797 \tTraining Loss: 1.613236 \tValidation Loss: 2.381602\n",
      "Epoch: 18798 \tTraining Loss: 1.608760 \tValidation Loss: 2.381565\n",
      "Epoch: 18799 \tTraining Loss: 1.572724 \tValidation Loss: 2.382336\n",
      "Epoch: 18800 \tTraining Loss: 1.558465 \tValidation Loss: 2.382256\n",
      "Epoch: 18801 \tTraining Loss: 1.604332 \tValidation Loss: 2.381524\n",
      "Epoch: 18802 \tTraining Loss: 1.583266 \tValidation Loss: 2.381951\n",
      "Epoch: 18803 \tTraining Loss: 1.600662 \tValidation Loss: 2.382044\n",
      "Epoch: 18804 \tTraining Loss: 1.644243 \tValidation Loss: 2.382075\n",
      "Epoch: 18805 \tTraining Loss: 1.595598 \tValidation Loss: 2.381988\n",
      "Epoch: 18806 \tTraining Loss: 1.648761 \tValidation Loss: 2.382478\n",
      "Epoch: 18807 \tTraining Loss: 1.628045 \tValidation Loss: 2.382034\n",
      "Epoch: 18808 \tTraining Loss: 1.613504 \tValidation Loss: 2.381644\n",
      "Epoch: 18809 \tTraining Loss: 1.622837 \tValidation Loss: 2.381540\n",
      "Epoch: 18810 \tTraining Loss: 1.586697 \tValidation Loss: 2.382411\n",
      "Epoch: 18811 \tTraining Loss: 1.602490 \tValidation Loss: 2.382711\n",
      "Epoch: 18812 \tTraining Loss: 1.602551 \tValidation Loss: 2.382747\n",
      "Epoch: 18813 \tTraining Loss: 1.652790 \tValidation Loss: 2.382168\n",
      "Epoch: 18814 \tTraining Loss: 1.547092 \tValidation Loss: 2.383255\n",
      "Epoch: 18815 \tTraining Loss: 1.574290 \tValidation Loss: 2.383549\n",
      "Epoch: 18816 \tTraining Loss: 1.555545 \tValidation Loss: 2.383183\n",
      "Epoch: 18817 \tTraining Loss: 1.609892 \tValidation Loss: 2.383208\n",
      "Epoch: 18818 \tTraining Loss: 1.593127 \tValidation Loss: 2.383294\n",
      "Epoch: 18819 \tTraining Loss: 1.581087 \tValidation Loss: 2.382960\n",
      "Epoch: 18820 \tTraining Loss: 1.587610 \tValidation Loss: 2.383446\n",
      "Epoch: 18821 \tTraining Loss: 1.578470 \tValidation Loss: 2.383327\n",
      "Epoch: 18822 \tTraining Loss: 1.626176 \tValidation Loss: 2.382799\n",
      "Epoch: 18823 \tTraining Loss: 1.612354 \tValidation Loss: 2.382013\n",
      "Epoch: 18824 \tTraining Loss: 1.574497 \tValidation Loss: 2.382142\n",
      "Epoch: 18825 \tTraining Loss: 1.606852 \tValidation Loss: 2.382046\n",
      "Epoch: 18826 \tTraining Loss: 1.609102 \tValidation Loss: 2.382475\n",
      "Epoch: 18827 \tTraining Loss: 1.547079 \tValidation Loss: 2.382108\n",
      "Epoch: 18828 \tTraining Loss: 1.561078 \tValidation Loss: 2.382945\n",
      "Epoch: 18829 \tTraining Loss: 1.566166 \tValidation Loss: 2.382511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18830 \tTraining Loss: 1.563336 \tValidation Loss: 2.382782\n",
      "Epoch: 18831 \tTraining Loss: 1.607544 \tValidation Loss: 2.382127\n",
      "Epoch: 18832 \tTraining Loss: 1.605146 \tValidation Loss: 2.382957\n",
      "Epoch: 18833 \tTraining Loss: 1.623072 \tValidation Loss: 2.383011\n",
      "Epoch: 18834 \tTraining Loss: 1.578659 \tValidation Loss: 2.382354\n",
      "Epoch: 18835 \tTraining Loss: 1.638365 \tValidation Loss: 2.382026\n",
      "Epoch: 18836 \tTraining Loss: 1.561739 \tValidation Loss: 2.383507\n",
      "Epoch: 18837 \tTraining Loss: 1.600330 \tValidation Loss: 2.382947\n",
      "Epoch: 18838 \tTraining Loss: 1.590321 \tValidation Loss: 2.382927\n",
      "Epoch: 18839 \tTraining Loss: 1.575517 \tValidation Loss: 2.382591\n",
      "Epoch: 18840 \tTraining Loss: 1.595884 \tValidation Loss: 2.383005\n",
      "Epoch: 18841 \tTraining Loss: 1.611357 \tValidation Loss: 2.383490\n",
      "Epoch: 18842 \tTraining Loss: 1.651428 \tValidation Loss: 2.382755\n",
      "Epoch: 18843 \tTraining Loss: 1.637907 \tValidation Loss: 2.383248\n",
      "Epoch: 18844 \tTraining Loss: 1.577089 \tValidation Loss: 2.382571\n",
      "Epoch: 18845 \tTraining Loss: 1.628939 \tValidation Loss: 2.382462\n",
      "Epoch: 18846 \tTraining Loss: 1.567699 \tValidation Loss: 2.382769\n",
      "Epoch: 18847 \tTraining Loss: 1.597820 \tValidation Loss: 2.382415\n",
      "Epoch: 18848 \tTraining Loss: 1.620334 \tValidation Loss: 2.382063\n",
      "Epoch: 18849 \tTraining Loss: 1.564458 \tValidation Loss: 2.382941\n",
      "Epoch: 18850 \tTraining Loss: 1.577224 \tValidation Loss: 2.383228\n",
      "Epoch: 18851 \tTraining Loss: 1.568120 \tValidation Loss: 2.383036\n",
      "Epoch: 18852 \tTraining Loss: 1.616940 \tValidation Loss: 2.382331\n",
      "Epoch: 18853 \tTraining Loss: 1.615258 \tValidation Loss: 2.382861\n",
      "Epoch: 18854 \tTraining Loss: 1.588590 \tValidation Loss: 2.382850\n",
      "Epoch: 18855 \tTraining Loss: 1.636656 \tValidation Loss: 2.382935\n",
      "Epoch: 18856 \tTraining Loss: 1.582244 \tValidation Loss: 2.382866\n",
      "Epoch: 18857 \tTraining Loss: 1.602149 \tValidation Loss: 2.383039\n",
      "Epoch: 18858 \tTraining Loss: 1.588815 \tValidation Loss: 2.382287\n",
      "Epoch: 18859 \tTraining Loss: 1.608853 \tValidation Loss: 2.382197\n",
      "Epoch: 18860 \tTraining Loss: 1.586720 \tValidation Loss: 2.382755\n",
      "Epoch: 18861 \tTraining Loss: 1.564917 \tValidation Loss: 2.382600\n",
      "Epoch: 18862 \tTraining Loss: 1.616517 \tValidation Loss: 2.382089\n",
      "Epoch: 18863 \tTraining Loss: 1.617088 \tValidation Loss: 2.381458\n",
      "Epoch: 18864 \tTraining Loss: 1.595198 \tValidation Loss: 2.382162\n",
      "Epoch: 18865 \tTraining Loss: 1.566175 \tValidation Loss: 2.382218\n",
      "Epoch: 18866 \tTraining Loss: 1.617931 \tValidation Loss: 2.381979\n",
      "Epoch: 18867 \tTraining Loss: 1.620486 \tValidation Loss: 2.382381\n",
      "Epoch: 18868 \tTraining Loss: 1.639979 \tValidation Loss: 2.382078\n",
      "Epoch: 18869 \tTraining Loss: 1.613551 \tValidation Loss: 2.382072\n",
      "Epoch: 18870 \tTraining Loss: 1.567588 \tValidation Loss: 2.383181\n",
      "Epoch: 18871 \tTraining Loss: 1.632200 \tValidation Loss: 2.382239\n",
      "Epoch: 18872 \tTraining Loss: 1.606461 \tValidation Loss: 2.382337\n",
      "Epoch: 18873 \tTraining Loss: 1.593340 \tValidation Loss: 2.382747\n",
      "Epoch: 18874 \tTraining Loss: 1.601645 \tValidation Loss: 2.382965\n",
      "Epoch: 18875 \tTraining Loss: 1.628079 \tValidation Loss: 2.382848\n",
      "Epoch: 18876 \tTraining Loss: 1.571190 \tValidation Loss: 2.383447\n",
      "Epoch: 18877 \tTraining Loss: 1.581258 \tValidation Loss: 2.383304\n",
      "Epoch: 18878 \tTraining Loss: 1.549954 \tValidation Loss: 2.383132\n",
      "Epoch: 18879 \tTraining Loss: 1.589474 \tValidation Loss: 2.383291\n",
      "Epoch: 18880 \tTraining Loss: 1.546944 \tValidation Loss: 2.383332\n",
      "Epoch: 18881 \tTraining Loss: 1.592846 \tValidation Loss: 2.383931\n",
      "Epoch: 18882 \tTraining Loss: 1.585777 \tValidation Loss: 2.383685\n",
      "Epoch: 18883 \tTraining Loss: 1.594542 \tValidation Loss: 2.383741\n",
      "Epoch: 18884 \tTraining Loss: 1.577999 \tValidation Loss: 2.383626\n",
      "Epoch: 18885 \tTraining Loss: 1.567581 \tValidation Loss: 2.383319\n",
      "Epoch: 18886 \tTraining Loss: 1.583774 \tValidation Loss: 2.383699\n",
      "Epoch: 18887 \tTraining Loss: 1.612653 \tValidation Loss: 2.383490\n",
      "Epoch: 18888 \tTraining Loss: 1.587024 \tValidation Loss: 2.383253\n",
      "Epoch: 18889 \tTraining Loss: 1.597362 \tValidation Loss: 2.382562\n",
      "Epoch: 18890 \tTraining Loss: 1.578959 \tValidation Loss: 2.383182\n",
      "Epoch: 18891 \tTraining Loss: 1.624492 \tValidation Loss: 2.383775\n",
      "Epoch: 18892 \tTraining Loss: 1.590215 \tValidation Loss: 2.382844\n",
      "Epoch: 18893 \tTraining Loss: 1.595844 \tValidation Loss: 2.382620\n",
      "Epoch: 18894 \tTraining Loss: 1.609252 \tValidation Loss: 2.382862\n",
      "Epoch: 18895 \tTraining Loss: 1.596126 \tValidation Loss: 2.383494\n",
      "Epoch: 18896 \tTraining Loss: 1.614370 \tValidation Loss: 2.382742\n",
      "Epoch: 18897 \tTraining Loss: 1.607003 \tValidation Loss: 2.383265\n",
      "Epoch: 18898 \tTraining Loss: 1.585460 \tValidation Loss: 2.383400\n",
      "Epoch: 18899 \tTraining Loss: 1.635536 \tValidation Loss: 2.382545\n",
      "Epoch: 18900 \tTraining Loss: 1.580873 \tValidation Loss: 2.383305\n",
      "Epoch: 18901 \tTraining Loss: 1.627004 \tValidation Loss: 2.383812\n",
      "Epoch: 18902 \tTraining Loss: 1.601084 \tValidation Loss: 2.383831\n",
      "Epoch: 18903 \tTraining Loss: 1.611653 \tValidation Loss: 2.383227\n",
      "Epoch: 18904 \tTraining Loss: 1.615451 \tValidation Loss: 2.383160\n",
      "Epoch: 18905 \tTraining Loss: 1.600876 \tValidation Loss: 2.383701\n",
      "Epoch: 18906 \tTraining Loss: 1.606704 \tValidation Loss: 2.382480\n",
      "Epoch: 18907 \tTraining Loss: 1.557910 \tValidation Loss: 2.383047\n",
      "Epoch: 18908 \tTraining Loss: 1.621909 \tValidation Loss: 2.382751\n",
      "Epoch: 18909 \tTraining Loss: 1.608116 \tValidation Loss: 2.383258\n",
      "Epoch: 18910 \tTraining Loss: 1.600322 \tValidation Loss: 2.383154\n",
      "Epoch: 18911 \tTraining Loss: 1.586549 \tValidation Loss: 2.383288\n",
      "Epoch: 18912 \tTraining Loss: 1.582356 \tValidation Loss: 2.383991\n",
      "Epoch: 18913 \tTraining Loss: 1.571362 \tValidation Loss: 2.384217\n",
      "Epoch: 18914 \tTraining Loss: 1.627582 \tValidation Loss: 2.383047\n",
      "Epoch: 18915 \tTraining Loss: 1.594902 \tValidation Loss: 2.383032\n",
      "Epoch: 18916 \tTraining Loss: 1.588478 \tValidation Loss: 2.383656\n",
      "Epoch: 18917 \tTraining Loss: 1.579767 \tValidation Loss: 2.384088\n",
      "Epoch: 18918 \tTraining Loss: 1.589815 \tValidation Loss: 2.384351\n",
      "Epoch: 18919 \tTraining Loss: 1.604891 \tValidation Loss: 2.383983\n",
      "Epoch: 18920 \tTraining Loss: 1.559469 \tValidation Loss: 2.384395\n",
      "Epoch: 18921 \tTraining Loss: 1.614879 \tValidation Loss: 2.384288\n",
      "Epoch: 18922 \tTraining Loss: 1.577940 \tValidation Loss: 2.383878\n",
      "Epoch: 18923 \tTraining Loss: 1.566687 \tValidation Loss: 2.383098\n",
      "Epoch: 18924 \tTraining Loss: 1.623182 \tValidation Loss: 2.383048\n",
      "Epoch: 18925 \tTraining Loss: 1.581032 \tValidation Loss: 2.383893\n",
      "Epoch: 18926 \tTraining Loss: 1.612218 \tValidation Loss: 2.383702\n",
      "Epoch: 18927 \tTraining Loss: 1.605953 \tValidation Loss: 2.383942\n",
      "Epoch: 18928 \tTraining Loss: 1.547530 \tValidation Loss: 2.384846\n",
      "Epoch: 18929 \tTraining Loss: 1.599674 \tValidation Loss: 2.383833\n",
      "Epoch: 18930 \tTraining Loss: 1.601963 \tValidation Loss: 2.383507\n",
      "Epoch: 18931 \tTraining Loss: 1.578635 \tValidation Loss: 2.384264\n",
      "Epoch: 18932 \tTraining Loss: 1.555740 \tValidation Loss: 2.384000\n",
      "Epoch: 18933 \tTraining Loss: 1.596480 \tValidation Loss: 2.384434\n",
      "Epoch: 18934 \tTraining Loss: 1.660774 \tValidation Loss: 2.383250\n",
      "Epoch: 18935 \tTraining Loss: 1.566997 \tValidation Loss: 2.384070\n",
      "Epoch: 18936 \tTraining Loss: 1.570140 \tValidation Loss: 2.383756\n",
      "Epoch: 18937 \tTraining Loss: 1.611878 \tValidation Loss: 2.383655\n",
      "Epoch: 18938 \tTraining Loss: 1.576078 \tValidation Loss: 2.384052\n",
      "Epoch: 18939 \tTraining Loss: 1.593412 \tValidation Loss: 2.383413\n",
      "Epoch: 18940 \tTraining Loss: 1.588797 \tValidation Loss: 2.383508\n",
      "Epoch: 18941 \tTraining Loss: 1.569008 \tValidation Loss: 2.383700\n",
      "Epoch: 18942 \tTraining Loss: 1.579860 \tValidation Loss: 2.384021\n",
      "Epoch: 18943 \tTraining Loss: 1.566010 \tValidation Loss: 2.384185\n",
      "Epoch: 18944 \tTraining Loss: 1.608949 \tValidation Loss: 2.384084\n",
      "Epoch: 18945 \tTraining Loss: 1.600614 \tValidation Loss: 2.383803\n",
      "Epoch: 18946 \tTraining Loss: 1.584257 \tValidation Loss: 2.383697\n",
      "Epoch: 18947 \tTraining Loss: 1.629082 \tValidation Loss: 2.383570\n",
      "Epoch: 18948 \tTraining Loss: 1.596026 \tValidation Loss: 2.383786\n",
      "Epoch: 18949 \tTraining Loss: 1.603443 \tValidation Loss: 2.384281\n",
      "Epoch: 18950 \tTraining Loss: 1.658716 \tValidation Loss: 2.382976\n",
      "Epoch: 18951 \tTraining Loss: 1.600761 \tValidation Loss: 2.383134\n",
      "Epoch: 18952 \tTraining Loss: 1.592530 \tValidation Loss: 2.383208\n",
      "Epoch: 18953 \tTraining Loss: 1.590038 \tValidation Loss: 2.384085\n",
      "Epoch: 18954 \tTraining Loss: 1.616837 \tValidation Loss: 2.384357\n",
      "Epoch: 18955 \tTraining Loss: 1.593104 \tValidation Loss: 2.383359\n",
      "Epoch: 18956 \tTraining Loss: 1.621877 \tValidation Loss: 2.383495\n",
      "Epoch: 18957 \tTraining Loss: 1.591029 \tValidation Loss: 2.383602\n",
      "Epoch: 18958 \tTraining Loss: 1.598967 \tValidation Loss: 2.384180\n",
      "Epoch: 18959 \tTraining Loss: 1.624879 \tValidation Loss: 2.383662\n",
      "Epoch: 18960 \tTraining Loss: 1.623018 \tValidation Loss: 2.384384\n",
      "Epoch: 18961 \tTraining Loss: 1.564936 \tValidation Loss: 2.383968\n",
      "Epoch: 18962 \tTraining Loss: 1.611675 \tValidation Loss: 2.384073\n",
      "Epoch: 18963 \tTraining Loss: 1.615222 \tValidation Loss: 2.383725\n",
      "Epoch: 18964 \tTraining Loss: 1.561521 \tValidation Loss: 2.383824\n",
      "Epoch: 18965 \tTraining Loss: 1.635507 \tValidation Loss: 2.383273\n",
      "Epoch: 18966 \tTraining Loss: 1.673350 \tValidation Loss: 2.383046\n",
      "Epoch: 18967 \tTraining Loss: 1.608719 \tValidation Loss: 2.383227\n",
      "Epoch: 18968 \tTraining Loss: 1.560841 \tValidation Loss: 2.383756\n",
      "Epoch: 18969 \tTraining Loss: 1.597773 \tValidation Loss: 2.382880\n",
      "Epoch: 18970 \tTraining Loss: 1.623631 \tValidation Loss: 2.383550\n",
      "Epoch: 18971 \tTraining Loss: 1.593800 \tValidation Loss: 2.383693\n",
      "Epoch: 18972 \tTraining Loss: 1.565165 \tValidation Loss: 2.383841\n",
      "Epoch: 18973 \tTraining Loss: 1.573159 \tValidation Loss: 2.384314\n",
      "Epoch: 18974 \tTraining Loss: 1.571723 \tValidation Loss: 2.384857\n",
      "Epoch: 18975 \tTraining Loss: 1.604923 \tValidation Loss: 2.384508\n",
      "Epoch: 18976 \tTraining Loss: 1.627488 \tValidation Loss: 2.384575\n",
      "Epoch: 18977 \tTraining Loss: 1.574383 \tValidation Loss: 2.385103\n",
      "Epoch: 18978 \tTraining Loss: 1.634582 \tValidation Loss: 2.384467\n",
      "Epoch: 18979 \tTraining Loss: 1.586472 \tValidation Loss: 2.385240\n",
      "Epoch: 18980 \tTraining Loss: 1.549931 \tValidation Loss: 2.384986\n",
      "Epoch: 18981 \tTraining Loss: 1.625215 \tValidation Loss: 2.383852\n",
      "Epoch: 18982 \tTraining Loss: 1.603270 \tValidation Loss: 2.383733\n",
      "Epoch: 18983 \tTraining Loss: 1.605876 \tValidation Loss: 2.384875\n",
      "Epoch: 18984 \tTraining Loss: 1.591582 \tValidation Loss: 2.385205\n",
      "Epoch: 18985 \tTraining Loss: 1.559230 \tValidation Loss: 2.384761\n",
      "Epoch: 18986 \tTraining Loss: 1.577377 \tValidation Loss: 2.384713\n",
      "Epoch: 18987 \tTraining Loss: 1.569617 \tValidation Loss: 2.385019\n",
      "Epoch: 18988 \tTraining Loss: 1.591955 \tValidation Loss: 2.384478\n",
      "Epoch: 18989 \tTraining Loss: 1.562482 \tValidation Loss: 2.384372\n",
      "Epoch: 18990 \tTraining Loss: 1.594343 \tValidation Loss: 2.384985\n",
      "Epoch: 18991 \tTraining Loss: 1.604357 \tValidation Loss: 2.384878\n",
      "Epoch: 18992 \tTraining Loss: 1.616756 \tValidation Loss: 2.384383\n",
      "Epoch: 18993 \tTraining Loss: 1.593997 \tValidation Loss: 2.384962\n",
      "Epoch: 18994 \tTraining Loss: 1.637151 \tValidation Loss: 2.383945\n",
      "Epoch: 18995 \tTraining Loss: 1.616681 \tValidation Loss: 2.385119\n",
      "Epoch: 18996 \tTraining Loss: 1.617217 \tValidation Loss: 2.384080\n",
      "Epoch: 18997 \tTraining Loss: 1.595880 \tValidation Loss: 2.384223\n",
      "Epoch: 18998 \tTraining Loss: 1.594473 \tValidation Loss: 2.384680\n",
      "Epoch: 18999 \tTraining Loss: 1.605856 \tValidation Loss: 2.384639\n",
      "Epoch: 19000 \tTraining Loss: 1.583894 \tValidation Loss: 2.385038\n",
      "Epoch: 19001 \tTraining Loss: 1.617183 \tValidation Loss: 2.384250\n",
      "Epoch: 19002 \tTraining Loss: 1.569584 \tValidation Loss: 2.384005\n",
      "Epoch: 19003 \tTraining Loss: 1.578737 \tValidation Loss: 2.384257\n",
      "Epoch: 19004 \tTraining Loss: 1.610902 \tValidation Loss: 2.384378\n",
      "Epoch: 19005 \tTraining Loss: 1.616679 \tValidation Loss: 2.384688\n",
      "Epoch: 19006 \tTraining Loss: 1.580234 \tValidation Loss: 2.385176\n",
      "Epoch: 19007 \tTraining Loss: 1.578383 \tValidation Loss: 2.384997\n",
      "Epoch: 19008 \tTraining Loss: 1.620797 \tValidation Loss: 2.384876\n",
      "Epoch: 19009 \tTraining Loss: 1.570064 \tValidation Loss: 2.385230\n",
      "Epoch: 19010 \tTraining Loss: 1.604589 \tValidation Loss: 2.384654\n",
      "Epoch: 19011 \tTraining Loss: 1.574257 \tValidation Loss: 2.385370\n",
      "Epoch: 19012 \tTraining Loss: 1.568461 \tValidation Loss: 2.385828\n",
      "Epoch: 19013 \tTraining Loss: 1.585904 \tValidation Loss: 2.385421\n",
      "Epoch: 19014 \tTraining Loss: 1.544984 \tValidation Loss: 2.385163\n",
      "Epoch: 19015 \tTraining Loss: 1.631920 \tValidation Loss: 2.384690\n",
      "Epoch: 19016 \tTraining Loss: 1.598520 \tValidation Loss: 2.384468\n",
      "Epoch: 19017 \tTraining Loss: 1.624008 \tValidation Loss: 2.385035\n",
      "Epoch: 19018 \tTraining Loss: 1.610933 \tValidation Loss: 2.385438\n",
      "Epoch: 19019 \tTraining Loss: 1.594844 \tValidation Loss: 2.384838\n",
      "Epoch: 19020 \tTraining Loss: 1.572328 \tValidation Loss: 2.384907\n",
      "Epoch: 19021 \tTraining Loss: 1.576832 \tValidation Loss: 2.384934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19022 \tTraining Loss: 1.639780 \tValidation Loss: 2.384734\n",
      "Epoch: 19023 \tTraining Loss: 1.570652 \tValidation Loss: 2.385958\n",
      "Epoch: 19024 \tTraining Loss: 1.614084 \tValidation Loss: 2.385859\n",
      "Epoch: 19025 \tTraining Loss: 1.610665 \tValidation Loss: 2.384524\n",
      "Epoch: 19026 \tTraining Loss: 1.603192 \tValidation Loss: 2.385173\n",
      "Epoch: 19027 \tTraining Loss: 1.609853 \tValidation Loss: 2.384879\n",
      "Epoch: 19028 \tTraining Loss: 1.592983 \tValidation Loss: 2.384870\n",
      "Epoch: 19029 \tTraining Loss: 1.591853 \tValidation Loss: 2.385074\n",
      "Epoch: 19030 \tTraining Loss: 1.580044 \tValidation Loss: 2.385237\n",
      "Epoch: 19031 \tTraining Loss: 1.563225 \tValidation Loss: 2.385756\n",
      "Epoch: 19032 \tTraining Loss: 1.590154 \tValidation Loss: 2.385276\n",
      "Epoch: 19033 \tTraining Loss: 1.629733 \tValidation Loss: 2.384744\n",
      "Epoch: 19034 \tTraining Loss: 1.545913 \tValidation Loss: 2.385581\n",
      "Epoch: 19035 \tTraining Loss: 1.566913 \tValidation Loss: 2.386101\n",
      "Epoch: 19036 \tTraining Loss: 1.581716 \tValidation Loss: 2.385261\n",
      "Epoch: 19037 \tTraining Loss: 1.633681 \tValidation Loss: 2.385050\n",
      "Epoch: 19038 \tTraining Loss: 1.585723 \tValidation Loss: 2.384854\n",
      "Epoch: 19039 \tTraining Loss: 1.571842 \tValidation Loss: 2.384748\n",
      "Epoch: 19040 \tTraining Loss: 1.609114 \tValidation Loss: 2.384658\n",
      "Epoch: 19041 \tTraining Loss: 1.586571 \tValidation Loss: 2.385734\n",
      "Epoch: 19042 \tTraining Loss: 1.621303 \tValidation Loss: 2.385144\n",
      "Epoch: 19043 \tTraining Loss: 1.597496 \tValidation Loss: 2.385222\n",
      "Epoch: 19044 \tTraining Loss: 1.592416 \tValidation Loss: 2.385002\n",
      "Epoch: 19045 \tTraining Loss: 1.593031 \tValidation Loss: 2.385412\n",
      "Epoch: 19046 \tTraining Loss: 1.583442 \tValidation Loss: 2.385380\n",
      "Epoch: 19047 \tTraining Loss: 1.552351 \tValidation Loss: 2.385088\n",
      "Epoch: 19048 \tTraining Loss: 1.601078 \tValidation Loss: 2.385802\n",
      "Epoch: 19049 \tTraining Loss: 1.615439 \tValidation Loss: 2.385155\n",
      "Epoch: 19050 \tTraining Loss: 1.582546 \tValidation Loss: 2.385440\n",
      "Epoch: 19051 \tTraining Loss: 1.604937 \tValidation Loss: 2.385506\n",
      "Epoch: 19052 \tTraining Loss: 1.651782 \tValidation Loss: 2.384745\n",
      "Epoch: 19053 \tTraining Loss: 1.611422 \tValidation Loss: 2.384830\n",
      "Epoch: 19054 \tTraining Loss: 1.610895 \tValidation Loss: 2.384728\n",
      "Epoch: 19055 \tTraining Loss: 1.589330 \tValidation Loss: 2.384889\n",
      "Epoch: 19056 \tTraining Loss: 1.618836 \tValidation Loss: 2.385219\n",
      "Epoch: 19057 \tTraining Loss: 1.602186 \tValidation Loss: 2.384951\n",
      "Epoch: 19058 \tTraining Loss: 1.609287 \tValidation Loss: 2.385539\n",
      "Epoch: 19059 \tTraining Loss: 1.605829 \tValidation Loss: 2.385654\n",
      "Epoch: 19060 \tTraining Loss: 1.582230 \tValidation Loss: 2.385830\n",
      "Epoch: 19061 \tTraining Loss: 1.575682 \tValidation Loss: 2.385503\n",
      "Epoch: 19062 \tTraining Loss: 1.606820 \tValidation Loss: 2.385470\n",
      "Epoch: 19063 \tTraining Loss: 1.597742 \tValidation Loss: 2.385288\n",
      "Epoch: 19064 \tTraining Loss: 1.568799 \tValidation Loss: 2.385746\n",
      "Epoch: 19065 \tTraining Loss: 1.630039 \tValidation Loss: 2.385561\n",
      "Epoch: 19066 \tTraining Loss: 1.596454 \tValidation Loss: 2.385918\n",
      "Epoch: 19067 \tTraining Loss: 1.577346 \tValidation Loss: 2.385868\n",
      "Epoch: 19068 \tTraining Loss: 1.583173 \tValidation Loss: 2.385710\n",
      "Epoch: 19069 \tTraining Loss: 1.624289 \tValidation Loss: 2.385217\n",
      "Epoch: 19070 \tTraining Loss: 1.601241 \tValidation Loss: 2.384688\n",
      "Epoch: 19071 \tTraining Loss: 1.610865 \tValidation Loss: 2.384621\n",
      "Epoch: 19072 \tTraining Loss: 1.582293 \tValidation Loss: 2.384978\n",
      "Epoch: 19073 \tTraining Loss: 1.590143 \tValidation Loss: 2.385737\n",
      "Epoch: 19074 \tTraining Loss: 1.570070 \tValidation Loss: 2.385405\n",
      "Epoch: 19075 \tTraining Loss: 1.551657 \tValidation Loss: 2.385989\n",
      "Epoch: 19076 \tTraining Loss: 1.594907 \tValidation Loss: 2.386441\n",
      "Epoch: 19077 \tTraining Loss: 1.622325 \tValidation Loss: 2.385653\n",
      "Epoch: 19078 \tTraining Loss: 1.606014 \tValidation Loss: 2.385079\n",
      "Epoch: 19079 \tTraining Loss: 1.593228 \tValidation Loss: 2.385633\n",
      "Epoch: 19080 \tTraining Loss: 1.591145 \tValidation Loss: 2.385930\n",
      "Epoch: 19081 \tTraining Loss: 1.585320 \tValidation Loss: 2.386105\n",
      "Epoch: 19082 \tTraining Loss: 1.582395 \tValidation Loss: 2.386111\n",
      "Epoch: 19083 \tTraining Loss: 1.585183 \tValidation Loss: 2.386059\n",
      "Epoch: 19084 \tTraining Loss: 1.626733 \tValidation Loss: 2.385601\n",
      "Epoch: 19085 \tTraining Loss: 1.587965 \tValidation Loss: 2.385628\n",
      "Epoch: 19086 \tTraining Loss: 1.591437 \tValidation Loss: 2.385477\n",
      "Epoch: 19087 \tTraining Loss: 1.579382 \tValidation Loss: 2.386243\n",
      "Epoch: 19088 \tTraining Loss: 1.552517 \tValidation Loss: 2.386307\n",
      "Epoch: 19089 \tTraining Loss: 1.588073 \tValidation Loss: 2.386117\n",
      "Epoch: 19090 \tTraining Loss: 1.628810 \tValidation Loss: 2.385855\n",
      "Epoch: 19091 \tTraining Loss: 1.589869 \tValidation Loss: 2.386168\n",
      "Epoch: 19092 \tTraining Loss: 1.578886 \tValidation Loss: 2.386289\n",
      "Epoch: 19093 \tTraining Loss: 1.618322 \tValidation Loss: 2.385801\n",
      "Epoch: 19094 \tTraining Loss: 1.599712 \tValidation Loss: 2.386124\n",
      "Epoch: 19095 \tTraining Loss: 1.588715 \tValidation Loss: 2.385828\n",
      "Epoch: 19096 \tTraining Loss: 1.584236 \tValidation Loss: 2.385858\n",
      "Epoch: 19097 \tTraining Loss: 1.575710 \tValidation Loss: 2.386288\n",
      "Epoch: 19098 \tTraining Loss: 1.640886 \tValidation Loss: 2.385547\n",
      "Epoch: 19099 \tTraining Loss: 1.597503 \tValidation Loss: 2.386201\n",
      "Epoch: 19100 \tTraining Loss: 1.606216 \tValidation Loss: 2.385819\n",
      "Epoch: 19101 \tTraining Loss: 1.562344 \tValidation Loss: 2.385954\n",
      "Epoch: 19102 \tTraining Loss: 1.607821 \tValidation Loss: 2.385539\n",
      "Epoch: 19103 \tTraining Loss: 1.594607 \tValidation Loss: 2.385680\n",
      "Epoch: 19104 \tTraining Loss: 1.616586 \tValidation Loss: 2.385806\n",
      "Epoch: 19105 \tTraining Loss: 1.589198 \tValidation Loss: 2.385868\n",
      "Epoch: 19106 \tTraining Loss: 1.582802 \tValidation Loss: 2.385278\n",
      "Epoch: 19107 \tTraining Loss: 1.594131 \tValidation Loss: 2.385923\n",
      "Epoch: 19108 \tTraining Loss: 1.607372 \tValidation Loss: 2.385416\n",
      "Epoch: 19109 \tTraining Loss: 1.611238 \tValidation Loss: 2.385656\n",
      "Epoch: 19110 \tTraining Loss: 1.570628 \tValidation Loss: 2.385376\n",
      "Epoch: 19111 \tTraining Loss: 1.612949 \tValidation Loss: 2.385392\n",
      "Epoch: 19112 \tTraining Loss: 1.634680 \tValidation Loss: 2.385022\n",
      "Epoch: 19113 \tTraining Loss: 1.577102 \tValidation Loss: 2.385742\n",
      "Epoch: 19114 \tTraining Loss: 1.601030 \tValidation Loss: 2.385816\n",
      "Epoch: 19115 \tTraining Loss: 1.615315 \tValidation Loss: 2.386042\n",
      "Epoch: 19116 \tTraining Loss: 1.599881 \tValidation Loss: 2.385777\n",
      "Epoch: 19117 \tTraining Loss: 1.550888 \tValidation Loss: 2.386636\n",
      "Epoch: 19118 \tTraining Loss: 1.560868 \tValidation Loss: 2.385712\n",
      "Epoch: 19119 \tTraining Loss: 1.594707 \tValidation Loss: 2.385878\n",
      "Epoch: 19120 \tTraining Loss: 1.609571 \tValidation Loss: 2.385172\n",
      "Epoch: 19121 \tTraining Loss: 1.625726 \tValidation Loss: 2.386021\n",
      "Epoch: 19122 \tTraining Loss: 1.587406 \tValidation Loss: 2.385819\n",
      "Epoch: 19123 \tTraining Loss: 1.562078 \tValidation Loss: 2.386453\n",
      "Epoch: 19124 \tTraining Loss: 1.610529 \tValidation Loss: 2.385559\n",
      "Epoch: 19125 \tTraining Loss: 1.580568 \tValidation Loss: 2.385811\n",
      "Epoch: 19126 \tTraining Loss: 1.605986 \tValidation Loss: 2.386026\n",
      "Epoch: 19127 \tTraining Loss: 1.596792 \tValidation Loss: 2.385908\n",
      "Epoch: 19128 \tTraining Loss: 1.533011 \tValidation Loss: 2.386798\n",
      "Epoch: 19129 \tTraining Loss: 1.599929 \tValidation Loss: 2.386372\n",
      "Epoch: 19130 \tTraining Loss: 1.606758 \tValidation Loss: 2.386009\n",
      "Epoch: 19131 \tTraining Loss: 1.563557 \tValidation Loss: 2.386708\n",
      "Epoch: 19132 \tTraining Loss: 1.603400 \tValidation Loss: 2.386688\n",
      "Epoch: 19133 \tTraining Loss: 1.595622 \tValidation Loss: 2.386970\n",
      "Epoch: 19134 \tTraining Loss: 1.563101 \tValidation Loss: 2.387136\n",
      "Epoch: 19135 \tTraining Loss: 1.617931 \tValidation Loss: 2.387080\n",
      "Epoch: 19136 \tTraining Loss: 1.588288 \tValidation Loss: 2.386642\n",
      "Epoch: 19137 \tTraining Loss: 1.609982 \tValidation Loss: 2.386739\n",
      "Epoch: 19138 \tTraining Loss: 1.596154 \tValidation Loss: 2.386266\n",
      "Epoch: 19139 \tTraining Loss: 1.551151 \tValidation Loss: 2.387141\n",
      "Epoch: 19140 \tTraining Loss: 1.596528 \tValidation Loss: 2.386725\n",
      "Epoch: 19141 \tTraining Loss: 1.585786 \tValidation Loss: 2.386369\n",
      "Epoch: 19142 \tTraining Loss: 1.595348 \tValidation Loss: 2.386100\n",
      "Epoch: 19143 \tTraining Loss: 1.587841 \tValidation Loss: 2.387007\n",
      "Epoch: 19144 \tTraining Loss: 1.567845 \tValidation Loss: 2.386973\n",
      "Epoch: 19145 \tTraining Loss: 1.568270 \tValidation Loss: 2.386692\n",
      "Epoch: 19146 \tTraining Loss: 1.586924 \tValidation Loss: 2.386442\n",
      "Epoch: 19147 \tTraining Loss: 1.572283 \tValidation Loss: 2.386593\n",
      "Epoch: 19148 \tTraining Loss: 1.576197 \tValidation Loss: 2.386512\n",
      "Epoch: 19149 \tTraining Loss: 1.568136 \tValidation Loss: 2.386865\n",
      "Epoch: 19150 \tTraining Loss: 1.559748 \tValidation Loss: 2.387893\n",
      "Epoch: 19151 \tTraining Loss: 1.587651 \tValidation Loss: 2.386760\n",
      "Epoch: 19152 \tTraining Loss: 1.592197 \tValidation Loss: 2.387348\n",
      "Epoch: 19153 \tTraining Loss: 1.605827 \tValidation Loss: 2.387450\n",
      "Epoch: 19154 \tTraining Loss: 1.551355 \tValidation Loss: 2.386407\n",
      "Epoch: 19155 \tTraining Loss: 1.587036 \tValidation Loss: 2.386633\n",
      "Epoch: 19156 \tTraining Loss: 1.579267 \tValidation Loss: 2.386830\n",
      "Epoch: 19157 \tTraining Loss: 1.577714 \tValidation Loss: 2.386682\n",
      "Epoch: 19158 \tTraining Loss: 1.569963 \tValidation Loss: 2.387429\n",
      "Epoch: 19159 \tTraining Loss: 1.600902 \tValidation Loss: 2.387248\n",
      "Epoch: 19160 \tTraining Loss: 1.617451 \tValidation Loss: 2.386935\n",
      "Epoch: 19161 \tTraining Loss: 1.602941 \tValidation Loss: 2.386750\n",
      "Epoch: 19162 \tTraining Loss: 1.602295 \tValidation Loss: 2.386380\n",
      "Epoch: 19163 \tTraining Loss: 1.569874 \tValidation Loss: 2.386624\n",
      "Epoch: 19164 \tTraining Loss: 1.597455 \tValidation Loss: 2.386129\n",
      "Epoch: 19165 \tTraining Loss: 1.610694 \tValidation Loss: 2.386167\n",
      "Epoch: 19166 \tTraining Loss: 1.568153 \tValidation Loss: 2.387150\n",
      "Epoch: 19167 \tTraining Loss: 1.598411 \tValidation Loss: 2.386130\n",
      "Epoch: 19168 \tTraining Loss: 1.584406 \tValidation Loss: 2.386876\n",
      "Epoch: 19169 \tTraining Loss: 1.608959 \tValidation Loss: 2.386930\n",
      "Epoch: 19170 \tTraining Loss: 1.584443 \tValidation Loss: 2.387370\n",
      "Epoch: 19171 \tTraining Loss: 1.595614 \tValidation Loss: 2.387735\n",
      "Epoch: 19172 \tTraining Loss: 1.556690 \tValidation Loss: 2.387457\n",
      "Epoch: 19173 \tTraining Loss: 1.604797 \tValidation Loss: 2.387125\n",
      "Epoch: 19174 \tTraining Loss: 1.593196 \tValidation Loss: 2.386660\n",
      "Epoch: 19175 \tTraining Loss: 1.578134 \tValidation Loss: 2.387147\n",
      "Epoch: 19176 \tTraining Loss: 1.575733 \tValidation Loss: 2.387049\n",
      "Epoch: 19177 \tTraining Loss: 1.593895 \tValidation Loss: 2.387445\n",
      "Epoch: 19178 \tTraining Loss: 1.551029 \tValidation Loss: 2.387279\n",
      "Epoch: 19179 \tTraining Loss: 1.599094 \tValidation Loss: 2.386526\n",
      "Epoch: 19180 \tTraining Loss: 1.604354 \tValidation Loss: 2.386950\n",
      "Epoch: 19181 \tTraining Loss: 1.572845 \tValidation Loss: 2.386631\n",
      "Epoch: 19182 \tTraining Loss: 1.599152 \tValidation Loss: 2.386881\n",
      "Epoch: 19183 \tTraining Loss: 1.589076 \tValidation Loss: 2.386814\n",
      "Epoch: 19184 \tTraining Loss: 1.559674 \tValidation Loss: 2.387267\n",
      "Epoch: 19185 \tTraining Loss: 1.591013 \tValidation Loss: 2.386774\n",
      "Epoch: 19186 \tTraining Loss: 1.570523 \tValidation Loss: 2.387742\n",
      "Epoch: 19187 \tTraining Loss: 1.570320 \tValidation Loss: 2.387795\n",
      "Epoch: 19188 \tTraining Loss: 1.562660 \tValidation Loss: 2.387904\n",
      "Epoch: 19189 \tTraining Loss: 1.585853 \tValidation Loss: 2.387860\n",
      "Epoch: 19190 \tTraining Loss: 1.612112 \tValidation Loss: 2.387129\n",
      "Epoch: 19191 \tTraining Loss: 1.604868 \tValidation Loss: 2.387600\n",
      "Epoch: 19192 \tTraining Loss: 1.588768 \tValidation Loss: 2.387187\n",
      "Epoch: 19193 \tTraining Loss: 1.612186 \tValidation Loss: 2.387781\n",
      "Epoch: 19194 \tTraining Loss: 1.596712 \tValidation Loss: 2.388102\n",
      "Epoch: 19195 \tTraining Loss: 1.618197 \tValidation Loss: 2.387499\n",
      "Epoch: 19196 \tTraining Loss: 1.591706 \tValidation Loss: 2.387519\n",
      "Epoch: 19197 \tTraining Loss: 1.596182 \tValidation Loss: 2.386896\n",
      "Epoch: 19198 \tTraining Loss: 1.593574 \tValidation Loss: 2.386967\n",
      "Epoch: 19199 \tTraining Loss: 1.584092 \tValidation Loss: 2.387293\n",
      "Epoch: 19200 \tTraining Loss: 1.612671 \tValidation Loss: 2.387618\n",
      "Epoch: 19201 \tTraining Loss: 1.565792 \tValidation Loss: 2.388044\n",
      "Epoch: 19202 \tTraining Loss: 1.512720 \tValidation Loss: 2.388431\n",
      "Epoch: 19203 \tTraining Loss: 1.609147 \tValidation Loss: 2.387693\n",
      "Epoch: 19204 \tTraining Loss: 1.563099 \tValidation Loss: 2.388617\n",
      "Epoch: 19205 \tTraining Loss: 1.564700 \tValidation Loss: 2.387942\n",
      "Epoch: 19206 \tTraining Loss: 1.595457 \tValidation Loss: 2.388041\n",
      "Epoch: 19207 \tTraining Loss: 1.590540 \tValidation Loss: 2.388750\n",
      "Epoch: 19208 \tTraining Loss: 1.613328 \tValidation Loss: 2.388111\n",
      "Epoch: 19209 \tTraining Loss: 1.645930 \tValidation Loss: 2.387561\n",
      "Epoch: 19210 \tTraining Loss: 1.556607 \tValidation Loss: 2.387804\n",
      "Epoch: 19211 \tTraining Loss: 1.561770 \tValidation Loss: 2.387793\n",
      "Epoch: 19212 \tTraining Loss: 1.584936 \tValidation Loss: 2.387571\n",
      "Epoch: 19213 \tTraining Loss: 1.572949 \tValidation Loss: 2.387925\n",
      "Epoch: 19214 \tTraining Loss: 1.549979 \tValidation Loss: 2.388284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19215 \tTraining Loss: 1.617026 \tValidation Loss: 2.388217\n",
      "Epoch: 19216 \tTraining Loss: 1.574012 \tValidation Loss: 2.387967\n",
      "Epoch: 19217 \tTraining Loss: 1.576260 \tValidation Loss: 2.388452\n",
      "Epoch: 19218 \tTraining Loss: 1.556921 \tValidation Loss: 2.387913\n",
      "Epoch: 19219 \tTraining Loss: 1.591598 \tValidation Loss: 2.387864\n",
      "Epoch: 19220 \tTraining Loss: 1.590649 \tValidation Loss: 2.388018\n",
      "Epoch: 19221 \tTraining Loss: 1.604316 \tValidation Loss: 2.387561\n",
      "Epoch: 19222 \tTraining Loss: 1.594802 \tValidation Loss: 2.388279\n",
      "Epoch: 19223 \tTraining Loss: 1.596240 \tValidation Loss: 2.388125\n",
      "Epoch: 19224 \tTraining Loss: 1.614708 \tValidation Loss: 2.387761\n",
      "Epoch: 19225 \tTraining Loss: 1.614927 \tValidation Loss: 2.387289\n",
      "Epoch: 19226 \tTraining Loss: 1.601647 \tValidation Loss: 2.388256\n",
      "Epoch: 19227 \tTraining Loss: 1.656752 \tValidation Loss: 2.387543\n",
      "Epoch: 19228 \tTraining Loss: 1.600921 \tValidation Loss: 2.387293\n",
      "Epoch: 19229 \tTraining Loss: 1.585793 \tValidation Loss: 2.387787\n",
      "Epoch: 19230 \tTraining Loss: 1.615614 \tValidation Loss: 2.387241\n",
      "Epoch: 19231 \tTraining Loss: 1.628101 \tValidation Loss: 2.388016\n",
      "Epoch: 19232 \tTraining Loss: 1.596891 \tValidation Loss: 2.388008\n",
      "Epoch: 19233 \tTraining Loss: 1.585329 \tValidation Loss: 2.387921\n",
      "Epoch: 19234 \tTraining Loss: 1.592269 \tValidation Loss: 2.387799\n",
      "Epoch: 19235 \tTraining Loss: 1.607289 \tValidation Loss: 2.388111\n",
      "Epoch: 19236 \tTraining Loss: 1.575625 \tValidation Loss: 2.387935\n",
      "Epoch: 19237 \tTraining Loss: 1.596820 \tValidation Loss: 2.388003\n",
      "Epoch: 19238 \tTraining Loss: 1.588376 \tValidation Loss: 2.388289\n",
      "Epoch: 19239 \tTraining Loss: 1.576496 \tValidation Loss: 2.387805\n",
      "Epoch: 19240 \tTraining Loss: 1.597019 \tValidation Loss: 2.387407\n",
      "Epoch: 19241 \tTraining Loss: 1.621161 \tValidation Loss: 2.387461\n",
      "Epoch: 19242 \tTraining Loss: 1.534025 \tValidation Loss: 2.388192\n",
      "Epoch: 19243 \tTraining Loss: 1.553585 \tValidation Loss: 2.387923\n",
      "Epoch: 19244 \tTraining Loss: 1.569662 \tValidation Loss: 2.387701\n",
      "Epoch: 19245 \tTraining Loss: 1.621887 \tValidation Loss: 2.388191\n",
      "Epoch: 19246 \tTraining Loss: 1.622045 \tValidation Loss: 2.387155\n",
      "Epoch: 19247 \tTraining Loss: 1.579792 \tValidation Loss: 2.387474\n",
      "Epoch: 19248 \tTraining Loss: 1.624783 \tValidation Loss: 2.387970\n",
      "Epoch: 19249 \tTraining Loss: 1.598260 \tValidation Loss: 2.387311\n",
      "Epoch: 19250 \tTraining Loss: 1.582693 \tValidation Loss: 2.386611\n",
      "Epoch: 19251 \tTraining Loss: 1.575835 \tValidation Loss: 2.387455\n",
      "Epoch: 19252 \tTraining Loss: 1.585252 \tValidation Loss: 2.386883\n",
      "Epoch: 19253 \tTraining Loss: 1.607344 \tValidation Loss: 2.387161\n",
      "Epoch: 19254 \tTraining Loss: 1.589869 \tValidation Loss: 2.388253\n",
      "Epoch: 19255 \tTraining Loss: 1.587144 \tValidation Loss: 2.387316\n",
      "Epoch: 19256 \tTraining Loss: 1.601607 \tValidation Loss: 2.387515\n",
      "Epoch: 19257 \tTraining Loss: 1.599055 \tValidation Loss: 2.388328\n",
      "Epoch: 19258 \tTraining Loss: 1.569230 \tValidation Loss: 2.388464\n",
      "Epoch: 19259 \tTraining Loss: 1.582637 \tValidation Loss: 2.387913\n",
      "Epoch: 19260 \tTraining Loss: 1.581897 \tValidation Loss: 2.387527\n",
      "Epoch: 19261 \tTraining Loss: 1.561503 \tValidation Loss: 2.388188\n",
      "Epoch: 19262 \tTraining Loss: 1.610108 \tValidation Loss: 2.387953\n",
      "Epoch: 19263 \tTraining Loss: 1.597997 \tValidation Loss: 2.388199\n",
      "Epoch: 19264 \tTraining Loss: 1.561596 \tValidation Loss: 2.388204\n",
      "Epoch: 19265 \tTraining Loss: 1.563937 \tValidation Loss: 2.388631\n",
      "Epoch: 19266 \tTraining Loss: 1.574500 \tValidation Loss: 2.387668\n",
      "Epoch: 19267 \tTraining Loss: 1.607497 \tValidation Loss: 2.387859\n",
      "Epoch: 19268 \tTraining Loss: 1.549580 \tValidation Loss: 2.387772\n",
      "Epoch: 19269 \tTraining Loss: 1.619371 \tValidation Loss: 2.387511\n",
      "Epoch: 19270 \tTraining Loss: 1.564434 \tValidation Loss: 2.387864\n",
      "Epoch: 19271 \tTraining Loss: 1.608886 \tValidation Loss: 2.388070\n",
      "Epoch: 19272 \tTraining Loss: 1.593605 \tValidation Loss: 2.387807\n",
      "Epoch: 19273 \tTraining Loss: 1.575451 \tValidation Loss: 2.388143\n",
      "Epoch: 19274 \tTraining Loss: 1.550261 \tValidation Loss: 2.389046\n",
      "Epoch: 19275 \tTraining Loss: 1.569472 \tValidation Loss: 2.388574\n",
      "Epoch: 19276 \tTraining Loss: 1.621771 \tValidation Loss: 2.387956\n",
      "Epoch: 19277 \tTraining Loss: 1.581736 \tValidation Loss: 2.388925\n",
      "Epoch: 19278 \tTraining Loss: 1.594517 \tValidation Loss: 2.388369\n",
      "Epoch: 19279 \tTraining Loss: 1.581002 \tValidation Loss: 2.388539\n",
      "Epoch: 19280 \tTraining Loss: 1.571650 \tValidation Loss: 2.388760\n",
      "Epoch: 19281 \tTraining Loss: 1.591942 \tValidation Loss: 2.388706\n",
      "Epoch: 19282 \tTraining Loss: 1.544423 \tValidation Loss: 2.388811\n",
      "Epoch: 19283 \tTraining Loss: 1.567512 \tValidation Loss: 2.388280\n",
      "Epoch: 19284 \tTraining Loss: 1.619858 \tValidation Loss: 2.388129\n",
      "Epoch: 19285 \tTraining Loss: 1.568780 \tValidation Loss: 2.388818\n",
      "Epoch: 19286 \tTraining Loss: 1.609439 \tValidation Loss: 2.388069\n",
      "Epoch: 19287 \tTraining Loss: 1.583449 \tValidation Loss: 2.388684\n",
      "Epoch: 19288 \tTraining Loss: 1.597062 \tValidation Loss: 2.387843\n",
      "Epoch: 19289 \tTraining Loss: 1.589575 \tValidation Loss: 2.387679\n",
      "Epoch: 19290 \tTraining Loss: 1.626305 \tValidation Loss: 2.388178\n",
      "Epoch: 19291 \tTraining Loss: 1.616907 \tValidation Loss: 2.388204\n",
      "Epoch: 19292 \tTraining Loss: 1.560595 \tValidation Loss: 2.388816\n",
      "Epoch: 19293 \tTraining Loss: 1.592798 \tValidation Loss: 2.388098\n",
      "Epoch: 19294 \tTraining Loss: 1.565963 \tValidation Loss: 2.388340\n",
      "Epoch: 19295 \tTraining Loss: 1.563659 \tValidation Loss: 2.388188\n",
      "Epoch: 19296 \tTraining Loss: 1.554761 \tValidation Loss: 2.388154\n",
      "Epoch: 19297 \tTraining Loss: 1.619388 \tValidation Loss: 2.387623\n",
      "Epoch: 19298 \tTraining Loss: 1.538712 \tValidation Loss: 2.388829\n",
      "Epoch: 19299 \tTraining Loss: 1.641961 \tValidation Loss: 2.387760\n",
      "Epoch: 19300 \tTraining Loss: 1.584432 \tValidation Loss: 2.388188\n",
      "Epoch: 19301 \tTraining Loss: 1.583808 \tValidation Loss: 2.388849\n",
      "Epoch: 19302 \tTraining Loss: 1.583510 \tValidation Loss: 2.389005\n",
      "Epoch: 19303 \tTraining Loss: 1.572756 \tValidation Loss: 2.389347\n",
      "Epoch: 19304 \tTraining Loss: 1.577722 \tValidation Loss: 2.389242\n",
      "Epoch: 19305 \tTraining Loss: 1.575723 \tValidation Loss: 2.389174\n",
      "Epoch: 19306 \tTraining Loss: 1.555267 \tValidation Loss: 2.388409\n",
      "Epoch: 19307 \tTraining Loss: 1.589313 \tValidation Loss: 2.388501\n",
      "Epoch: 19308 \tTraining Loss: 1.596746 \tValidation Loss: 2.388339\n",
      "Epoch: 19309 \tTraining Loss: 1.583469 \tValidation Loss: 2.388558\n",
      "Epoch: 19310 \tTraining Loss: 1.582689 \tValidation Loss: 2.389285\n",
      "Epoch: 19311 \tTraining Loss: 1.550488 \tValidation Loss: 2.389324\n",
      "Epoch: 19312 \tTraining Loss: 1.601627 \tValidation Loss: 2.388466\n",
      "Epoch: 19313 \tTraining Loss: 1.566280 \tValidation Loss: 2.388328\n",
      "Epoch: 19314 \tTraining Loss: 1.602311 \tValidation Loss: 2.388886\n",
      "Epoch: 19315 \tTraining Loss: 1.561798 \tValidation Loss: 2.389036\n",
      "Epoch: 19316 \tTraining Loss: 1.608010 \tValidation Loss: 2.388661\n",
      "Epoch: 19317 \tTraining Loss: 1.644555 \tValidation Loss: 2.388965\n",
      "Epoch: 19318 \tTraining Loss: 1.601479 \tValidation Loss: 2.389343\n",
      "Epoch: 19319 \tTraining Loss: 1.624326 \tValidation Loss: 2.388885\n",
      "Epoch: 19320 \tTraining Loss: 1.578217 \tValidation Loss: 2.388386\n",
      "Epoch: 19321 \tTraining Loss: 1.573751 \tValidation Loss: 2.389340\n",
      "Epoch: 19322 \tTraining Loss: 1.572699 \tValidation Loss: 2.388741\n",
      "Epoch: 19323 \tTraining Loss: 1.582680 \tValidation Loss: 2.389147\n",
      "Epoch: 19324 \tTraining Loss: 1.573828 \tValidation Loss: 2.389107\n",
      "Epoch: 19325 \tTraining Loss: 1.604493 \tValidation Loss: 2.389311\n",
      "Epoch: 19326 \tTraining Loss: 1.623338 \tValidation Loss: 2.388683\n",
      "Epoch: 19327 \tTraining Loss: 1.577921 \tValidation Loss: 2.388966\n",
      "Epoch: 19328 \tTraining Loss: 1.607790 \tValidation Loss: 2.388392\n",
      "Epoch: 19329 \tTraining Loss: 1.573264 \tValidation Loss: 2.388428\n",
      "Epoch: 19330 \tTraining Loss: 1.579329 \tValidation Loss: 2.388680\n",
      "Epoch: 19331 \tTraining Loss: 1.560639 \tValidation Loss: 2.388755\n",
      "Epoch: 19332 \tTraining Loss: 1.563363 \tValidation Loss: 2.389364\n",
      "Epoch: 19333 \tTraining Loss: 1.563715 \tValidation Loss: 2.388886\n",
      "Epoch: 19334 \tTraining Loss: 1.583367 \tValidation Loss: 2.388769\n",
      "Epoch: 19335 \tTraining Loss: 1.609517 \tValidation Loss: 2.388265\n",
      "Epoch: 19336 \tTraining Loss: 1.575272 \tValidation Loss: 2.389153\n",
      "Epoch: 19337 \tTraining Loss: 1.595638 \tValidation Loss: 2.388859\n",
      "Epoch: 19338 \tTraining Loss: 1.562502 \tValidation Loss: 2.388856\n",
      "Epoch: 19339 \tTraining Loss: 1.620733 \tValidation Loss: 2.389115\n",
      "Epoch: 19340 \tTraining Loss: 1.503320 \tValidation Loss: 2.389590\n",
      "Epoch: 19341 \tTraining Loss: 1.568794 \tValidation Loss: 2.389307\n",
      "Epoch: 19342 \tTraining Loss: 1.592230 \tValidation Loss: 2.389149\n",
      "Epoch: 19343 \tTraining Loss: 1.596614 \tValidation Loss: 2.389070\n",
      "Epoch: 19344 \tTraining Loss: 1.580772 \tValidation Loss: 2.389106\n",
      "Epoch: 19345 \tTraining Loss: 1.604794 \tValidation Loss: 2.388469\n",
      "Epoch: 19346 \tTraining Loss: 1.559935 \tValidation Loss: 2.388933\n",
      "Epoch: 19347 \tTraining Loss: 1.558126 \tValidation Loss: 2.389271\n",
      "Epoch: 19348 \tTraining Loss: 1.550016 \tValidation Loss: 2.389942\n",
      "Epoch: 19349 \tTraining Loss: 1.598801 \tValidation Loss: 2.389267\n",
      "Epoch: 19350 \tTraining Loss: 1.552470 \tValidation Loss: 2.390221\n",
      "Epoch: 19351 \tTraining Loss: 1.587898 \tValidation Loss: 2.389604\n",
      "Epoch: 19352 \tTraining Loss: 1.584549 \tValidation Loss: 2.389804\n",
      "Epoch: 19353 \tTraining Loss: 1.586631 \tValidation Loss: 2.389381\n",
      "Epoch: 19354 \tTraining Loss: 1.614169 \tValidation Loss: 2.389123\n",
      "Epoch: 19355 \tTraining Loss: 1.592760 \tValidation Loss: 2.389395\n",
      "Epoch: 19356 \tTraining Loss: 1.597787 \tValidation Loss: 2.389472\n",
      "Epoch: 19357 \tTraining Loss: 1.600790 \tValidation Loss: 2.389497\n",
      "Epoch: 19358 \tTraining Loss: 1.554350 \tValidation Loss: 2.389414\n",
      "Epoch: 19359 \tTraining Loss: 1.599682 \tValidation Loss: 2.389520\n",
      "Epoch: 19360 \tTraining Loss: 1.622706 \tValidation Loss: 2.389515\n",
      "Epoch: 19361 \tTraining Loss: 1.563424 \tValidation Loss: 2.389414\n",
      "Epoch: 19362 \tTraining Loss: 1.582421 \tValidation Loss: 2.389553\n",
      "Epoch: 19363 \tTraining Loss: 1.593730 \tValidation Loss: 2.389807\n",
      "Epoch: 19364 \tTraining Loss: 1.542889 \tValidation Loss: 2.389737\n",
      "Epoch: 19365 \tTraining Loss: 1.575546 \tValidation Loss: 2.388783\n",
      "Epoch: 19366 \tTraining Loss: 1.553146 \tValidation Loss: 2.389614\n",
      "Epoch: 19367 \tTraining Loss: 1.580868 \tValidation Loss: 2.389822\n",
      "Epoch: 19368 \tTraining Loss: 1.610281 \tValidation Loss: 2.388973\n",
      "Epoch: 19369 \tTraining Loss: 1.589878 \tValidation Loss: 2.389288\n",
      "Epoch: 19370 \tTraining Loss: 1.571137 \tValidation Loss: 2.388922\n",
      "Epoch: 19371 \tTraining Loss: 1.600240 \tValidation Loss: 2.389084\n",
      "Epoch: 19372 \tTraining Loss: 1.567011 \tValidation Loss: 2.389125\n",
      "Epoch: 19373 \tTraining Loss: 1.564696 \tValidation Loss: 2.389243\n",
      "Epoch: 19374 \tTraining Loss: 1.590858 \tValidation Loss: 2.389078\n",
      "Epoch: 19375 \tTraining Loss: 1.587098 \tValidation Loss: 2.389498\n",
      "Epoch: 19376 \tTraining Loss: 1.594587 \tValidation Loss: 2.389462\n",
      "Epoch: 19377 \tTraining Loss: 1.584198 \tValidation Loss: 2.389902\n",
      "Epoch: 19378 \tTraining Loss: 1.542574 \tValidation Loss: 2.389943\n",
      "Epoch: 19379 \tTraining Loss: 1.599487 \tValidation Loss: 2.389484\n",
      "Epoch: 19380 \tTraining Loss: 1.598550 \tValidation Loss: 2.390239\n",
      "Epoch: 19381 \tTraining Loss: 1.585113 \tValidation Loss: 2.390598\n",
      "Epoch: 19382 \tTraining Loss: 1.599697 \tValidation Loss: 2.389926\n",
      "Epoch: 19383 \tTraining Loss: 1.599920 \tValidation Loss: 2.389933\n",
      "Epoch: 19384 \tTraining Loss: 1.531628 \tValidation Loss: 2.390188\n",
      "Epoch: 19385 \tTraining Loss: 1.564824 \tValidation Loss: 2.390729\n",
      "Epoch: 19386 \tTraining Loss: 1.558942 \tValidation Loss: 2.390400\n",
      "Epoch: 19387 \tTraining Loss: 1.565034 \tValidation Loss: 2.390611\n",
      "Epoch: 19388 \tTraining Loss: 1.607924 \tValidation Loss: 2.390044\n",
      "Epoch: 19389 \tTraining Loss: 1.570354 \tValidation Loss: 2.390237\n",
      "Epoch: 19390 \tTraining Loss: 1.579898 \tValidation Loss: 2.389688\n",
      "Epoch: 19391 \tTraining Loss: 1.601472 \tValidation Loss: 2.389228\n",
      "Epoch: 19392 \tTraining Loss: 1.582151 \tValidation Loss: 2.389926\n",
      "Epoch: 19393 \tTraining Loss: 1.579167 \tValidation Loss: 2.389700\n",
      "Epoch: 19394 \tTraining Loss: 1.589052 \tValidation Loss: 2.389976\n",
      "Epoch: 19395 \tTraining Loss: 1.594857 \tValidation Loss: 2.390408\n",
      "Epoch: 19396 \tTraining Loss: 1.587337 \tValidation Loss: 2.389625\n",
      "Epoch: 19397 \tTraining Loss: 1.556876 \tValidation Loss: 2.389660\n",
      "Epoch: 19398 \tTraining Loss: 1.613408 \tValidation Loss: 2.390393\n",
      "Epoch: 19399 \tTraining Loss: 1.532682 \tValidation Loss: 2.391374\n",
      "Epoch: 19400 \tTraining Loss: 1.605033 \tValidation Loss: 2.390675\n",
      "Epoch: 19401 \tTraining Loss: 1.600676 \tValidation Loss: 2.389655\n",
      "Epoch: 19402 \tTraining Loss: 1.578028 \tValidation Loss: 2.390346\n",
      "Epoch: 19403 \tTraining Loss: 1.623433 \tValidation Loss: 2.390577\n",
      "Epoch: 19404 \tTraining Loss: 1.569538 \tValidation Loss: 2.390254\n",
      "Epoch: 19405 \tTraining Loss: 1.591984 \tValidation Loss: 2.390570\n",
      "Epoch: 19406 \tTraining Loss: 1.532805 \tValidation Loss: 2.390920\n",
      "Epoch: 19407 \tTraining Loss: 1.588200 \tValidation Loss: 2.390848\n",
      "Epoch: 19408 \tTraining Loss: 1.578677 \tValidation Loss: 2.390868\n",
      "Epoch: 19409 \tTraining Loss: 1.580243 \tValidation Loss: 2.391418\n",
      "Epoch: 19410 \tTraining Loss: 1.576301 \tValidation Loss: 2.390820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19411 \tTraining Loss: 1.609555 \tValidation Loss: 2.391130\n",
      "Epoch: 19412 \tTraining Loss: 1.640350 \tValidation Loss: 2.390525\n",
      "Epoch: 19413 \tTraining Loss: 1.567284 \tValidation Loss: 2.390571\n",
      "Epoch: 19414 \tTraining Loss: 1.521439 \tValidation Loss: 2.391178\n",
      "Epoch: 19415 \tTraining Loss: 1.617481 \tValidation Loss: 2.390673\n",
      "Epoch: 19416 \tTraining Loss: 1.583845 \tValidation Loss: 2.391512\n",
      "Epoch: 19417 \tTraining Loss: 1.589967 \tValidation Loss: 2.390751\n",
      "Epoch: 19418 \tTraining Loss: 1.580651 \tValidation Loss: 2.390585\n",
      "Epoch: 19419 \tTraining Loss: 1.558627 \tValidation Loss: 2.390991\n",
      "Epoch: 19420 \tTraining Loss: 1.579358 \tValidation Loss: 2.391101\n",
      "Epoch: 19421 \tTraining Loss: 1.596442 \tValidation Loss: 2.390772\n",
      "Epoch: 19422 \tTraining Loss: 1.560983 \tValidation Loss: 2.391230\n",
      "Epoch: 19423 \tTraining Loss: 1.571492 \tValidation Loss: 2.391094\n",
      "Epoch: 19424 \tTraining Loss: 1.584354 \tValidation Loss: 2.391184\n",
      "Epoch: 19425 \tTraining Loss: 1.584589 \tValidation Loss: 2.390656\n",
      "Epoch: 19426 \tTraining Loss: 1.559399 \tValidation Loss: 2.391054\n",
      "Epoch: 19427 \tTraining Loss: 1.591827 \tValidation Loss: 2.391359\n",
      "Epoch: 19428 \tTraining Loss: 1.557332 \tValidation Loss: 2.391095\n",
      "Epoch: 19429 \tTraining Loss: 1.549756 \tValidation Loss: 2.390810\n",
      "Epoch: 19430 \tTraining Loss: 1.544049 \tValidation Loss: 2.391043\n",
      "Epoch: 19431 \tTraining Loss: 1.582940 \tValidation Loss: 2.390649\n",
      "Epoch: 19432 \tTraining Loss: 1.569198 \tValidation Loss: 2.390314\n",
      "Epoch: 19433 \tTraining Loss: 1.579275 \tValidation Loss: 2.391158\n",
      "Epoch: 19434 \tTraining Loss: 1.560891 \tValidation Loss: 2.391049\n",
      "Epoch: 19435 \tTraining Loss: 1.577245 \tValidation Loss: 2.390616\n",
      "Epoch: 19436 \tTraining Loss: 1.589136 \tValidation Loss: 2.390867\n",
      "Epoch: 19437 \tTraining Loss: 1.589141 \tValidation Loss: 2.391069\n",
      "Epoch: 19438 \tTraining Loss: 1.596161 \tValidation Loss: 2.390235\n",
      "Epoch: 19439 \tTraining Loss: 1.598092 \tValidation Loss: 2.390706\n",
      "Epoch: 19440 \tTraining Loss: 1.574583 \tValidation Loss: 2.391689\n",
      "Epoch: 19441 \tTraining Loss: 1.576819 \tValidation Loss: 2.391438\n",
      "Epoch: 19442 \tTraining Loss: 1.546739 \tValidation Loss: 2.391416\n",
      "Epoch: 19443 \tTraining Loss: 1.589632 \tValidation Loss: 2.391354\n",
      "Epoch: 19444 \tTraining Loss: 1.640529 \tValidation Loss: 2.390301\n",
      "Epoch: 19445 \tTraining Loss: 1.601884 \tValidation Loss: 2.391054\n",
      "Epoch: 19446 \tTraining Loss: 1.560713 \tValidation Loss: 2.390770\n",
      "Epoch: 19447 \tTraining Loss: 1.575949 \tValidation Loss: 2.390771\n",
      "Epoch: 19448 \tTraining Loss: 1.599300 \tValidation Loss: 2.390847\n",
      "Epoch: 19449 \tTraining Loss: 1.595392 \tValidation Loss: 2.391078\n",
      "Epoch: 19450 \tTraining Loss: 1.581910 \tValidation Loss: 2.390483\n",
      "Epoch: 19451 \tTraining Loss: 1.567350 \tValidation Loss: 2.391143\n",
      "Epoch: 19452 \tTraining Loss: 1.602253 \tValidation Loss: 2.391776\n",
      "Epoch: 19453 \tTraining Loss: 1.563530 \tValidation Loss: 2.391117\n",
      "Epoch: 19454 \tTraining Loss: 1.532538 \tValidation Loss: 2.391727\n",
      "Epoch: 19455 \tTraining Loss: 1.603040 \tValidation Loss: 2.391173\n",
      "Epoch: 19456 \tTraining Loss: 1.580969 \tValidation Loss: 2.391328\n",
      "Epoch: 19457 \tTraining Loss: 1.580626 \tValidation Loss: 2.391356\n",
      "Epoch: 19458 \tTraining Loss: 1.575130 \tValidation Loss: 2.391223\n",
      "Epoch: 19459 \tTraining Loss: 1.565593 \tValidation Loss: 2.391129\n",
      "Epoch: 19460 \tTraining Loss: 1.546954 \tValidation Loss: 2.391246\n",
      "Epoch: 19461 \tTraining Loss: 1.533555 \tValidation Loss: 2.392263\n",
      "Epoch: 19462 \tTraining Loss: 1.575376 \tValidation Loss: 2.391446\n",
      "Epoch: 19463 \tTraining Loss: 1.616134 \tValidation Loss: 2.391298\n",
      "Epoch: 19464 \tTraining Loss: 1.587539 \tValidation Loss: 2.391116\n",
      "Epoch: 19465 \tTraining Loss: 1.584900 \tValidation Loss: 2.391508\n",
      "Epoch: 19466 \tTraining Loss: 1.592631 \tValidation Loss: 2.391193\n",
      "Epoch: 19467 \tTraining Loss: 1.587700 \tValidation Loss: 2.391257\n",
      "Epoch: 19468 \tTraining Loss: 1.572466 \tValidation Loss: 2.391217\n",
      "Epoch: 19469 \tTraining Loss: 1.600728 \tValidation Loss: 2.391435\n",
      "Epoch: 19470 \tTraining Loss: 1.584540 \tValidation Loss: 2.390599\n",
      "Epoch: 19471 \tTraining Loss: 1.579733 \tValidation Loss: 2.391035\n",
      "Epoch: 19472 \tTraining Loss: 1.580180 \tValidation Loss: 2.391173\n",
      "Epoch: 19473 \tTraining Loss: 1.619042 \tValidation Loss: 2.390771\n",
      "Epoch: 19474 \tTraining Loss: 1.572969 \tValidation Loss: 2.391610\n",
      "Epoch: 19475 \tTraining Loss: 1.594323 \tValidation Loss: 2.391226\n",
      "Epoch: 19476 \tTraining Loss: 1.602172 \tValidation Loss: 2.391649\n",
      "Epoch: 19477 \tTraining Loss: 1.564359 \tValidation Loss: 2.391056\n",
      "Epoch: 19478 \tTraining Loss: 1.558792 \tValidation Loss: 2.391329\n",
      "Epoch: 19479 \tTraining Loss: 1.616624 \tValidation Loss: 2.391121\n",
      "Epoch: 19480 \tTraining Loss: 1.597996 \tValidation Loss: 2.391330\n",
      "Epoch: 19481 \tTraining Loss: 1.554049 \tValidation Loss: 2.391560\n",
      "Epoch: 19482 \tTraining Loss: 1.578408 \tValidation Loss: 2.391342\n",
      "Epoch: 19483 \tTraining Loss: 1.603720 \tValidation Loss: 2.391479\n",
      "Epoch: 19484 \tTraining Loss: 1.599402 \tValidation Loss: 2.391530\n",
      "Epoch: 19485 \tTraining Loss: 1.572047 \tValidation Loss: 2.391232\n",
      "Epoch: 19486 \tTraining Loss: 1.559259 \tValidation Loss: 2.391858\n",
      "Epoch: 19487 \tTraining Loss: 1.552100 \tValidation Loss: 2.392380\n",
      "Epoch: 19488 \tTraining Loss: 1.533949 \tValidation Loss: 2.391893\n",
      "Epoch: 19489 \tTraining Loss: 1.603118 \tValidation Loss: 2.392260\n",
      "Epoch: 19490 \tTraining Loss: 1.580324 \tValidation Loss: 2.391749\n",
      "Epoch: 19491 \tTraining Loss: 1.611019 \tValidation Loss: 2.391515\n",
      "Epoch: 19492 \tTraining Loss: 1.609926 \tValidation Loss: 2.391334\n",
      "Epoch: 19493 \tTraining Loss: 1.595329 \tValidation Loss: 2.391606\n",
      "Epoch: 19494 \tTraining Loss: 1.590132 \tValidation Loss: 2.391396\n",
      "Epoch: 19495 \tTraining Loss: 1.568109 \tValidation Loss: 2.391288\n",
      "Epoch: 19496 \tTraining Loss: 1.572878 \tValidation Loss: 2.391697\n",
      "Epoch: 19497 \tTraining Loss: 1.528872 \tValidation Loss: 2.391873\n",
      "Epoch: 19498 \tTraining Loss: 1.566204 \tValidation Loss: 2.391570\n",
      "Epoch: 19499 \tTraining Loss: 1.603563 \tValidation Loss: 2.391254\n",
      "Epoch: 19500 \tTraining Loss: 1.593174 \tValidation Loss: 2.391240\n",
      "Epoch: 19501 \tTraining Loss: 1.549419 \tValidation Loss: 2.391611\n",
      "Epoch: 19502 \tTraining Loss: 1.597009 \tValidation Loss: 2.392190\n",
      "Epoch: 19503 \tTraining Loss: 1.566653 \tValidation Loss: 2.391018\n",
      "Epoch: 19504 \tTraining Loss: 1.560968 \tValidation Loss: 2.391942\n",
      "Epoch: 19505 \tTraining Loss: 1.550076 \tValidation Loss: 2.392013\n",
      "Epoch: 19506 \tTraining Loss: 1.572837 \tValidation Loss: 2.393028\n",
      "Epoch: 19507 \tTraining Loss: 1.562230 \tValidation Loss: 2.391980\n",
      "Epoch: 19508 \tTraining Loss: 1.534179 \tValidation Loss: 2.392657\n",
      "Epoch: 19509 \tTraining Loss: 1.564813 \tValidation Loss: 2.391590\n",
      "Epoch: 19510 \tTraining Loss: 1.596730 \tValidation Loss: 2.391010\n",
      "Epoch: 19511 \tTraining Loss: 1.577765 \tValidation Loss: 2.391753\n",
      "Epoch: 19512 \tTraining Loss: 1.611966 \tValidation Loss: 2.391921\n",
      "Epoch: 19513 \tTraining Loss: 1.534702 \tValidation Loss: 2.391806\n",
      "Epoch: 19514 \tTraining Loss: 1.575354 \tValidation Loss: 2.391479\n",
      "Epoch: 19515 \tTraining Loss: 1.637812 \tValidation Loss: 2.390580\n",
      "Epoch: 19516 \tTraining Loss: 1.569950 \tValidation Loss: 2.391347\n",
      "Epoch: 19517 \tTraining Loss: 1.578221 \tValidation Loss: 2.391846\n",
      "Epoch: 19518 \tTraining Loss: 1.605953 \tValidation Loss: 2.390540\n",
      "Epoch: 19519 \tTraining Loss: 1.623103 \tValidation Loss: 2.390942\n",
      "Epoch: 19520 \tTraining Loss: 1.597098 \tValidation Loss: 2.391078\n",
      "Epoch: 19521 \tTraining Loss: 1.577613 \tValidation Loss: 2.390889\n",
      "Epoch: 19522 \tTraining Loss: 1.553735 \tValidation Loss: 2.392309\n",
      "Epoch: 19523 \tTraining Loss: 1.600282 \tValidation Loss: 2.391916\n",
      "Epoch: 19524 \tTraining Loss: 1.578146 \tValidation Loss: 2.391778\n",
      "Epoch: 19525 \tTraining Loss: 1.599665 \tValidation Loss: 2.392091\n",
      "Epoch: 19526 \tTraining Loss: 1.628743 \tValidation Loss: 2.392440\n",
      "Epoch: 19527 \tTraining Loss: 1.543327 \tValidation Loss: 2.391726\n",
      "Epoch: 19528 \tTraining Loss: 1.563939 \tValidation Loss: 2.392395\n",
      "Epoch: 19529 \tTraining Loss: 1.589477 \tValidation Loss: 2.391295\n",
      "Epoch: 19530 \tTraining Loss: 1.576225 \tValidation Loss: 2.392007\n",
      "Epoch: 19531 \tTraining Loss: 1.593261 \tValidation Loss: 2.391115\n",
      "Epoch: 19532 \tTraining Loss: 1.574817 \tValidation Loss: 2.391450\n",
      "Epoch: 19533 \tTraining Loss: 1.592642 \tValidation Loss: 2.391808\n",
      "Epoch: 19534 \tTraining Loss: 1.633127 \tValidation Loss: 2.390703\n",
      "Epoch: 19535 \tTraining Loss: 1.560519 \tValidation Loss: 2.391698\n",
      "Epoch: 19536 \tTraining Loss: 1.562644 \tValidation Loss: 2.391735\n",
      "Epoch: 19537 \tTraining Loss: 1.578828 \tValidation Loss: 2.391571\n",
      "Epoch: 19538 \tTraining Loss: 1.613344 \tValidation Loss: 2.391356\n",
      "Epoch: 19539 \tTraining Loss: 1.608608 \tValidation Loss: 2.391126\n",
      "Epoch: 19540 \tTraining Loss: 1.616615 \tValidation Loss: 2.391790\n",
      "Epoch: 19541 \tTraining Loss: 1.576394 \tValidation Loss: 2.391855\n",
      "Epoch: 19542 \tTraining Loss: 1.586994 \tValidation Loss: 2.391863\n",
      "Epoch: 19543 \tTraining Loss: 1.556378 \tValidation Loss: 2.392752\n",
      "Epoch: 19544 \tTraining Loss: 1.591903 \tValidation Loss: 2.392308\n",
      "Epoch: 19545 \tTraining Loss: 1.584887 \tValidation Loss: 2.391278\n",
      "Epoch: 19546 \tTraining Loss: 1.547104 \tValidation Loss: 2.392262\n",
      "Epoch: 19547 \tTraining Loss: 1.571854 \tValidation Loss: 2.392062\n",
      "Epoch: 19548 \tTraining Loss: 1.592235 \tValidation Loss: 2.392726\n",
      "Epoch: 19549 \tTraining Loss: 1.568959 \tValidation Loss: 2.392390\n",
      "Epoch: 19550 \tTraining Loss: 1.608826 \tValidation Loss: 2.392007\n",
      "Epoch: 19551 \tTraining Loss: 1.552408 \tValidation Loss: 2.392931\n",
      "Epoch: 19552 \tTraining Loss: 1.586316 \tValidation Loss: 2.392928\n",
      "Epoch: 19553 \tTraining Loss: 1.570258 \tValidation Loss: 2.392462\n",
      "Epoch: 19554 \tTraining Loss: 1.590452 \tValidation Loss: 2.392171\n",
      "Epoch: 19555 \tTraining Loss: 1.565810 \tValidation Loss: 2.392137\n",
      "Epoch: 19556 \tTraining Loss: 1.587643 \tValidation Loss: 2.391990\n",
      "Epoch: 19557 \tTraining Loss: 1.533233 \tValidation Loss: 2.392059\n",
      "Epoch: 19558 \tTraining Loss: 1.612504 \tValidation Loss: 2.392052\n",
      "Epoch: 19559 \tTraining Loss: 1.572002 \tValidation Loss: 2.392183\n",
      "Epoch: 19560 \tTraining Loss: 1.600781 \tValidation Loss: 2.392135\n",
      "Epoch: 19561 \tTraining Loss: 1.561939 \tValidation Loss: 2.392416\n",
      "Epoch: 19562 \tTraining Loss: 1.578827 \tValidation Loss: 2.392989\n",
      "Epoch: 19563 \tTraining Loss: 1.583258 \tValidation Loss: 2.392731\n",
      "Epoch: 19564 \tTraining Loss: 1.575442 \tValidation Loss: 2.392153\n",
      "Epoch: 19565 \tTraining Loss: 1.570166 \tValidation Loss: 2.392864\n",
      "Epoch: 19566 \tTraining Loss: 1.545772 \tValidation Loss: 2.392450\n",
      "Epoch: 19567 \tTraining Loss: 1.589433 \tValidation Loss: 2.392993\n",
      "Epoch: 19568 \tTraining Loss: 1.594872 \tValidation Loss: 2.392960\n",
      "Epoch: 19569 \tTraining Loss: 1.586684 \tValidation Loss: 2.392699\n",
      "Epoch: 19570 \tTraining Loss: 1.582290 \tValidation Loss: 2.392933\n",
      "Epoch: 19571 \tTraining Loss: 1.570296 \tValidation Loss: 2.392231\n",
      "Epoch: 19572 \tTraining Loss: 1.591459 \tValidation Loss: 2.392918\n",
      "Epoch: 19573 \tTraining Loss: 1.553857 \tValidation Loss: 2.392824\n",
      "Epoch: 19574 \tTraining Loss: 1.580560 \tValidation Loss: 2.392690\n",
      "Epoch: 19575 \tTraining Loss: 1.561146 \tValidation Loss: 2.392715\n",
      "Epoch: 19576 \tTraining Loss: 1.568253 \tValidation Loss: 2.392641\n",
      "Epoch: 19577 \tTraining Loss: 1.553655 \tValidation Loss: 2.392364\n",
      "Epoch: 19578 \tTraining Loss: 1.549265 \tValidation Loss: 2.392551\n",
      "Epoch: 19579 \tTraining Loss: 1.608053 \tValidation Loss: 2.392509\n",
      "Epoch: 19580 \tTraining Loss: 1.605370 \tValidation Loss: 2.392508\n",
      "Epoch: 19581 \tTraining Loss: 1.609422 \tValidation Loss: 2.392283\n",
      "Epoch: 19582 \tTraining Loss: 1.588696 \tValidation Loss: 2.392127\n",
      "Epoch: 19583 \tTraining Loss: 1.588935 \tValidation Loss: 2.392206\n",
      "Epoch: 19584 \tTraining Loss: 1.551725 \tValidation Loss: 2.392900\n",
      "Epoch: 19585 \tTraining Loss: 1.569633 \tValidation Loss: 2.392403\n",
      "Epoch: 19586 \tTraining Loss: 1.551895 \tValidation Loss: 2.392649\n",
      "Epoch: 19587 \tTraining Loss: 1.613732 \tValidation Loss: 2.391833\n",
      "Epoch: 19588 \tTraining Loss: 1.565166 \tValidation Loss: 2.392350\n",
      "Epoch: 19589 \tTraining Loss: 1.567788 \tValidation Loss: 2.392168\n",
      "Epoch: 19590 \tTraining Loss: 1.564514 \tValidation Loss: 2.392237\n",
      "Epoch: 19591 \tTraining Loss: 1.552536 \tValidation Loss: 2.392543\n",
      "Epoch: 19592 \tTraining Loss: 1.556925 \tValidation Loss: 2.392859\n",
      "Epoch: 19593 \tTraining Loss: 1.604570 \tValidation Loss: 2.392478\n",
      "Epoch: 19594 \tTraining Loss: 1.591569 \tValidation Loss: 2.392867\n",
      "Epoch: 19595 \tTraining Loss: 1.570888 \tValidation Loss: 2.393166\n",
      "Epoch: 19596 \tTraining Loss: 1.571229 \tValidation Loss: 2.393113\n",
      "Epoch: 19597 \tTraining Loss: 1.584887 \tValidation Loss: 2.392716\n",
      "Epoch: 19598 \tTraining Loss: 1.567890 \tValidation Loss: 2.393439\n",
      "Epoch: 19599 \tTraining Loss: 1.600041 \tValidation Loss: 2.392757\n",
      "Epoch: 19600 \tTraining Loss: 1.608714 \tValidation Loss: 2.392805\n",
      "Epoch: 19601 \tTraining Loss: 1.557472 \tValidation Loss: 2.392709\n",
      "Epoch: 19602 \tTraining Loss: 1.566631 \tValidation Loss: 2.392370\n",
      "Epoch: 19603 \tTraining Loss: 1.589913 \tValidation Loss: 2.392268\n",
      "Epoch: 19604 \tTraining Loss: 1.576064 \tValidation Loss: 2.392286\n",
      "Epoch: 19605 \tTraining Loss: 1.583000 \tValidation Loss: 2.392351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19606 \tTraining Loss: 1.605642 \tValidation Loss: 2.392528\n",
      "Epoch: 19607 \tTraining Loss: 1.598036 \tValidation Loss: 2.392723\n",
      "Epoch: 19608 \tTraining Loss: 1.584306 \tValidation Loss: 2.392799\n",
      "Epoch: 19609 \tTraining Loss: 1.572540 \tValidation Loss: 2.393189\n",
      "Epoch: 19610 \tTraining Loss: 1.574141 \tValidation Loss: 2.393597\n",
      "Epoch: 19611 \tTraining Loss: 1.547765 \tValidation Loss: 2.393104\n",
      "Epoch: 19612 \tTraining Loss: 1.538595 \tValidation Loss: 2.393095\n",
      "Epoch: 19613 \tTraining Loss: 1.569078 \tValidation Loss: 2.392774\n",
      "Epoch: 19614 \tTraining Loss: 1.594836 \tValidation Loss: 2.393048\n",
      "Epoch: 19615 \tTraining Loss: 1.601574 \tValidation Loss: 2.392686\n",
      "Epoch: 19616 \tTraining Loss: 1.600953 \tValidation Loss: 2.392610\n",
      "Epoch: 19617 \tTraining Loss: 1.575573 \tValidation Loss: 2.392604\n",
      "Epoch: 19618 \tTraining Loss: 1.538262 \tValidation Loss: 2.392740\n",
      "Epoch: 19619 \tTraining Loss: 1.564245 \tValidation Loss: 2.393710\n",
      "Epoch: 19620 \tTraining Loss: 1.571425 \tValidation Loss: 2.392755\n",
      "Epoch: 19621 \tTraining Loss: 1.564188 \tValidation Loss: 2.393054\n",
      "Epoch: 19622 \tTraining Loss: 1.583553 \tValidation Loss: 2.393484\n",
      "Epoch: 19623 \tTraining Loss: 1.567173 \tValidation Loss: 2.392911\n",
      "Epoch: 19624 \tTraining Loss: 1.560198 \tValidation Loss: 2.393111\n",
      "Epoch: 19625 \tTraining Loss: 1.574733 \tValidation Loss: 2.393429\n",
      "Epoch: 19626 \tTraining Loss: 1.583375 \tValidation Loss: 2.393094\n",
      "Epoch: 19627 \tTraining Loss: 1.574970 \tValidation Loss: 2.393967\n",
      "Epoch: 19628 \tTraining Loss: 1.559034 \tValidation Loss: 2.394062\n",
      "Epoch: 19629 \tTraining Loss: 1.555804 \tValidation Loss: 2.393480\n",
      "Epoch: 19630 \tTraining Loss: 1.592497 \tValidation Loss: 2.393380\n",
      "Epoch: 19631 \tTraining Loss: 1.541952 \tValidation Loss: 2.393496\n",
      "Epoch: 19632 \tTraining Loss: 1.594705 \tValidation Loss: 2.393704\n",
      "Epoch: 19633 \tTraining Loss: 1.582674 \tValidation Loss: 2.392828\n",
      "Epoch: 19634 \tTraining Loss: 1.597540 \tValidation Loss: 2.393574\n",
      "Epoch: 19635 \tTraining Loss: 1.583345 \tValidation Loss: 2.393770\n",
      "Epoch: 19636 \tTraining Loss: 1.574557 \tValidation Loss: 2.393753\n",
      "Epoch: 19637 \tTraining Loss: 1.585717 \tValidation Loss: 2.393602\n",
      "Epoch: 19638 \tTraining Loss: 1.529543 \tValidation Loss: 2.393420\n",
      "Epoch: 19639 \tTraining Loss: 1.565250 \tValidation Loss: 2.393798\n",
      "Epoch: 19640 \tTraining Loss: 1.573300 \tValidation Loss: 2.393386\n",
      "Epoch: 19641 \tTraining Loss: 1.604426 \tValidation Loss: 2.393180\n",
      "Epoch: 19642 \tTraining Loss: 1.552885 \tValidation Loss: 2.393159\n",
      "Epoch: 19643 \tTraining Loss: 1.532000 \tValidation Loss: 2.394080\n",
      "Epoch: 19644 \tTraining Loss: 1.594189 \tValidation Loss: 2.393156\n",
      "Epoch: 19645 \tTraining Loss: 1.569414 \tValidation Loss: 2.393500\n",
      "Epoch: 19646 \tTraining Loss: 1.568511 \tValidation Loss: 2.394004\n",
      "Epoch: 19647 \tTraining Loss: 1.533718 \tValidation Loss: 2.393437\n",
      "Epoch: 19648 \tTraining Loss: 1.608073 \tValidation Loss: 2.394221\n",
      "Epoch: 19649 \tTraining Loss: 1.573026 \tValidation Loss: 2.393503\n",
      "Epoch: 19650 \tTraining Loss: 1.531567 \tValidation Loss: 2.394063\n",
      "Epoch: 19651 \tTraining Loss: 1.618610 \tValidation Loss: 2.393283\n",
      "Epoch: 19652 \tTraining Loss: 1.568403 \tValidation Loss: 2.393698\n",
      "Epoch: 19653 \tTraining Loss: 1.565800 \tValidation Loss: 2.393477\n",
      "Epoch: 19654 \tTraining Loss: 1.541023 \tValidation Loss: 2.394212\n",
      "Epoch: 19655 \tTraining Loss: 1.543585 \tValidation Loss: 2.394235\n",
      "Epoch: 19656 \tTraining Loss: 1.575793 \tValidation Loss: 2.394217\n",
      "Epoch: 19657 \tTraining Loss: 1.546666 \tValidation Loss: 2.394386\n",
      "Epoch: 19658 \tTraining Loss: 1.539037 \tValidation Loss: 2.393693\n",
      "Epoch: 19659 \tTraining Loss: 1.565799 \tValidation Loss: 2.393323\n",
      "Epoch: 19660 \tTraining Loss: 1.606436 \tValidation Loss: 2.393887\n",
      "Epoch: 19661 \tTraining Loss: 1.560744 \tValidation Loss: 2.393589\n",
      "Epoch: 19662 \tTraining Loss: 1.583956 \tValidation Loss: 2.393905\n",
      "Epoch: 19663 \tTraining Loss: 1.601120 \tValidation Loss: 2.393629\n",
      "Epoch: 19664 \tTraining Loss: 1.554194 \tValidation Loss: 2.393902\n",
      "Epoch: 19665 \tTraining Loss: 1.581621 \tValidation Loss: 2.394024\n",
      "Epoch: 19666 \tTraining Loss: 1.598288 \tValidation Loss: 2.394062\n",
      "Epoch: 19667 \tTraining Loss: 1.598893 \tValidation Loss: 2.393500\n",
      "Epoch: 19668 \tTraining Loss: 1.576527 \tValidation Loss: 2.393002\n",
      "Epoch: 19669 \tTraining Loss: 1.597146 \tValidation Loss: 2.392764\n",
      "Epoch: 19670 \tTraining Loss: 1.587182 \tValidation Loss: 2.393257\n",
      "Epoch: 19671 \tTraining Loss: 1.557518 \tValidation Loss: 2.393386\n",
      "Epoch: 19672 \tTraining Loss: 1.567607 \tValidation Loss: 2.393245\n",
      "Epoch: 19673 \tTraining Loss: 1.572919 \tValidation Loss: 2.393625\n",
      "Epoch: 19674 \tTraining Loss: 1.546479 \tValidation Loss: 2.394703\n",
      "Epoch: 19675 \tTraining Loss: 1.577882 \tValidation Loss: 2.394100\n",
      "Epoch: 19676 \tTraining Loss: 1.587691 \tValidation Loss: 2.393580\n",
      "Epoch: 19677 \tTraining Loss: 1.587117 \tValidation Loss: 2.393720\n",
      "Epoch: 19678 \tTraining Loss: 1.549359 \tValidation Loss: 2.394221\n",
      "Epoch: 19679 \tTraining Loss: 1.597563 \tValidation Loss: 2.394407\n",
      "Epoch: 19680 \tTraining Loss: 1.551897 \tValidation Loss: 2.394597\n",
      "Epoch: 19681 \tTraining Loss: 1.558528 \tValidation Loss: 2.394533\n",
      "Epoch: 19682 \tTraining Loss: 1.538882 \tValidation Loss: 2.394796\n",
      "Epoch: 19683 \tTraining Loss: 1.572358 \tValidation Loss: 2.395017\n",
      "Epoch: 19684 \tTraining Loss: 1.561274 \tValidation Loss: 2.394719\n",
      "Epoch: 19685 \tTraining Loss: 1.572927 \tValidation Loss: 2.394754\n",
      "Epoch: 19686 \tTraining Loss: 1.586128 \tValidation Loss: 2.394154\n",
      "Epoch: 19687 \tTraining Loss: 1.592608 \tValidation Loss: 2.394157\n",
      "Epoch: 19688 \tTraining Loss: 1.570614 \tValidation Loss: 2.394311\n",
      "Epoch: 19689 \tTraining Loss: 1.606263 \tValidation Loss: 2.393579\n",
      "Epoch: 19690 \tTraining Loss: 1.521174 \tValidation Loss: 2.393986\n",
      "Epoch: 19691 \tTraining Loss: 1.565025 \tValidation Loss: 2.394455\n",
      "Epoch: 19692 \tTraining Loss: 1.549419 \tValidation Loss: 2.394475\n",
      "Epoch: 19693 \tTraining Loss: 1.587277 \tValidation Loss: 2.395310\n",
      "Epoch: 19694 \tTraining Loss: 1.554972 \tValidation Loss: 2.394852\n",
      "Epoch: 19695 \tTraining Loss: 1.551912 \tValidation Loss: 2.394996\n",
      "Epoch: 19696 \tTraining Loss: 1.587234 \tValidation Loss: 2.394189\n",
      "Epoch: 19697 \tTraining Loss: 1.599135 \tValidation Loss: 2.394004\n",
      "Epoch: 19698 \tTraining Loss: 1.575488 \tValidation Loss: 2.394510\n",
      "Epoch: 19699 \tTraining Loss: 1.588423 \tValidation Loss: 2.393808\n",
      "Epoch: 19700 \tTraining Loss: 1.568939 \tValidation Loss: 2.393475\n",
      "Epoch: 19701 \tTraining Loss: 1.579688 \tValidation Loss: 2.394091\n",
      "Epoch: 19702 \tTraining Loss: 1.583560 \tValidation Loss: 2.393677\n",
      "Epoch: 19703 \tTraining Loss: 1.604730 \tValidation Loss: 2.393458\n",
      "Epoch: 19704 \tTraining Loss: 1.588328 \tValidation Loss: 2.394249\n",
      "Epoch: 19705 \tTraining Loss: 1.573524 \tValidation Loss: 2.394300\n",
      "Epoch: 19706 \tTraining Loss: 1.559349 \tValidation Loss: 2.395194\n",
      "Epoch: 19707 \tTraining Loss: 1.565137 \tValidation Loss: 2.393980\n",
      "Epoch: 19708 \tTraining Loss: 1.552557 \tValidation Loss: 2.394813\n",
      "Epoch: 19709 \tTraining Loss: 1.571064 \tValidation Loss: 2.394436\n",
      "Epoch: 19710 \tTraining Loss: 1.494585 \tValidation Loss: 2.395256\n",
      "Epoch: 19711 \tTraining Loss: 1.561276 \tValidation Loss: 2.394513\n",
      "Epoch: 19712 \tTraining Loss: 1.545882 \tValidation Loss: 2.394922\n",
      "Epoch: 19713 \tTraining Loss: 1.547008 \tValidation Loss: 2.394641\n",
      "Epoch: 19714 \tTraining Loss: 1.521715 \tValidation Loss: 2.395642\n",
      "Epoch: 19715 \tTraining Loss: 1.570047 \tValidation Loss: 2.394938\n",
      "Epoch: 19716 \tTraining Loss: 1.598035 \tValidation Loss: 2.395418\n",
      "Epoch: 19717 \tTraining Loss: 1.586085 \tValidation Loss: 2.394580\n",
      "Epoch: 19718 \tTraining Loss: 1.596131 \tValidation Loss: 2.394220\n",
      "Epoch: 19719 \tTraining Loss: 1.563713 \tValidation Loss: 2.394428\n",
      "Epoch: 19720 \tTraining Loss: 1.576301 \tValidation Loss: 2.394764\n",
      "Epoch: 19721 \tTraining Loss: 1.559780 \tValidation Loss: 2.395439\n",
      "Epoch: 19722 \tTraining Loss: 1.588772 \tValidation Loss: 2.395023\n",
      "Epoch: 19723 \tTraining Loss: 1.585555 \tValidation Loss: 2.394454\n",
      "Epoch: 19724 \tTraining Loss: 1.578031 \tValidation Loss: 2.394556\n",
      "Epoch: 19725 \tTraining Loss: 1.571736 \tValidation Loss: 2.394837\n",
      "Epoch: 19726 \tTraining Loss: 1.556292 \tValidation Loss: 2.394773\n",
      "Epoch: 19727 \tTraining Loss: 1.576852 \tValidation Loss: 2.394510\n",
      "Epoch: 19728 \tTraining Loss: 1.559086 \tValidation Loss: 2.394792\n",
      "Epoch: 19729 \tTraining Loss: 1.570868 \tValidation Loss: 2.395206\n",
      "Epoch: 19730 \tTraining Loss: 1.621865 \tValidation Loss: 2.394491\n",
      "Epoch: 19731 \tTraining Loss: 1.545905 \tValidation Loss: 2.394813\n",
      "Epoch: 19732 \tTraining Loss: 1.569339 \tValidation Loss: 2.395445\n",
      "Epoch: 19733 \tTraining Loss: 1.560513 \tValidation Loss: 2.395745\n",
      "Epoch: 19734 \tTraining Loss: 1.552258 \tValidation Loss: 2.394866\n",
      "Epoch: 19735 \tTraining Loss: 1.497295 \tValidation Loss: 2.395506\n",
      "Epoch: 19736 \tTraining Loss: 1.631558 \tValidation Loss: 2.395134\n",
      "Epoch: 19737 \tTraining Loss: 1.590567 \tValidation Loss: 2.394800\n",
      "Epoch: 19738 \tTraining Loss: 1.581990 \tValidation Loss: 2.395831\n",
      "Epoch: 19739 \tTraining Loss: 1.578820 \tValidation Loss: 2.395474\n",
      "Epoch: 19740 \tTraining Loss: 1.514678 \tValidation Loss: 2.395480\n",
      "Epoch: 19741 \tTraining Loss: 1.572044 \tValidation Loss: 2.395600\n",
      "Epoch: 19742 \tTraining Loss: 1.565539 \tValidation Loss: 2.395523\n",
      "Epoch: 19743 \tTraining Loss: 1.553854 \tValidation Loss: 2.394740\n",
      "Epoch: 19744 \tTraining Loss: 1.559604 \tValidation Loss: 2.395312\n",
      "Epoch: 19745 \tTraining Loss: 1.534052 \tValidation Loss: 2.395405\n",
      "Epoch: 19746 \tTraining Loss: 1.591074 \tValidation Loss: 2.394664\n",
      "Epoch: 19747 \tTraining Loss: 1.598755 \tValidation Loss: 2.395388\n",
      "Epoch: 19748 \tTraining Loss: 1.573833 \tValidation Loss: 2.395305\n",
      "Epoch: 19749 \tTraining Loss: 1.515452 \tValidation Loss: 2.395842\n",
      "Epoch: 19750 \tTraining Loss: 1.541219 \tValidation Loss: 2.396138\n",
      "Epoch: 19751 \tTraining Loss: 1.546282 \tValidation Loss: 2.395424\n",
      "Epoch: 19752 \tTraining Loss: 1.523574 \tValidation Loss: 2.396226\n",
      "Epoch: 19753 \tTraining Loss: 1.550383 \tValidation Loss: 2.395944\n",
      "Epoch: 19754 \tTraining Loss: 1.568622 \tValidation Loss: 2.396782\n",
      "Epoch: 19755 \tTraining Loss: 1.562284 \tValidation Loss: 2.395500\n",
      "Epoch: 19756 \tTraining Loss: 1.580630 \tValidation Loss: 2.396140\n",
      "Epoch: 19757 \tTraining Loss: 1.561335 \tValidation Loss: 2.395780\n",
      "Epoch: 19758 \tTraining Loss: 1.568542 \tValidation Loss: 2.396005\n",
      "Epoch: 19759 \tTraining Loss: 1.574779 \tValidation Loss: 2.395534\n",
      "Epoch: 19760 \tTraining Loss: 1.587744 \tValidation Loss: 2.395110\n",
      "Epoch: 19761 \tTraining Loss: 1.543742 \tValidation Loss: 2.395869\n",
      "Epoch: 19762 \tTraining Loss: 1.545027 \tValidation Loss: 2.395736\n",
      "Epoch: 19763 \tTraining Loss: 1.582884 \tValidation Loss: 2.395696\n",
      "Epoch: 19764 \tTraining Loss: 1.635334 \tValidation Loss: 2.395031\n",
      "Epoch: 19765 \tTraining Loss: 1.620988 \tValidation Loss: 2.395282\n",
      "Epoch: 19766 \tTraining Loss: 1.555238 \tValidation Loss: 2.395002\n",
      "Epoch: 19767 \tTraining Loss: 1.577643 \tValidation Loss: 2.395279\n",
      "Epoch: 19768 \tTraining Loss: 1.572015 \tValidation Loss: 2.396237\n",
      "Epoch: 19769 \tTraining Loss: 1.580267 \tValidation Loss: 2.396740\n",
      "Epoch: 19770 \tTraining Loss: 1.587373 \tValidation Loss: 2.395449\n",
      "Epoch: 19771 \tTraining Loss: 1.540954 \tValidation Loss: 2.395868\n",
      "Epoch: 19772 \tTraining Loss: 1.574496 \tValidation Loss: 2.396990\n",
      "Epoch: 19773 \tTraining Loss: 1.584179 \tValidation Loss: 2.396588\n",
      "Epoch: 19774 \tTraining Loss: 1.517131 \tValidation Loss: 2.396961\n",
      "Epoch: 19775 \tTraining Loss: 1.606917 \tValidation Loss: 2.396564\n",
      "Epoch: 19776 \tTraining Loss: 1.556929 \tValidation Loss: 2.395778\n",
      "Epoch: 19777 \tTraining Loss: 1.562452 \tValidation Loss: 2.395752\n",
      "Epoch: 19778 \tTraining Loss: 1.590425 \tValidation Loss: 2.395948\n",
      "Epoch: 19779 \tTraining Loss: 1.577446 \tValidation Loss: 2.395608\n",
      "Epoch: 19780 \tTraining Loss: 1.591475 \tValidation Loss: 2.395993\n",
      "Epoch: 19781 \tTraining Loss: 1.573690 \tValidation Loss: 2.396167\n",
      "Epoch: 19782 \tTraining Loss: 1.584768 \tValidation Loss: 2.395263\n",
      "Epoch: 19783 \tTraining Loss: 1.582609 \tValidation Loss: 2.395823\n",
      "Epoch: 19784 \tTraining Loss: 1.578484 \tValidation Loss: 2.395679\n",
      "Epoch: 19785 \tTraining Loss: 1.575092 \tValidation Loss: 2.395288\n",
      "Epoch: 19786 \tTraining Loss: 1.562964 \tValidation Loss: 2.395838\n",
      "Epoch: 19787 \tTraining Loss: 1.557108 \tValidation Loss: 2.396361\n",
      "Epoch: 19788 \tTraining Loss: 1.573315 \tValidation Loss: 2.395735\n",
      "Epoch: 19789 \tTraining Loss: 1.595535 \tValidation Loss: 2.395111\n",
      "Epoch: 19790 \tTraining Loss: 1.577404 \tValidation Loss: 2.395709\n",
      "Epoch: 19791 \tTraining Loss: 1.620601 \tValidation Loss: 2.394903\n",
      "Epoch: 19792 \tTraining Loss: 1.572212 \tValidation Loss: 2.395359\n",
      "Epoch: 19793 \tTraining Loss: 1.587974 \tValidation Loss: 2.395051\n",
      "Epoch: 19794 \tTraining Loss: 1.560223 \tValidation Loss: 2.395488\n",
      "Epoch: 19795 \tTraining Loss: 1.582531 \tValidation Loss: 2.394901\n",
      "Epoch: 19796 \tTraining Loss: 1.515821 \tValidation Loss: 2.395863\n",
      "Epoch: 19797 \tTraining Loss: 1.563064 \tValidation Loss: 2.395722\n",
      "Epoch: 19798 \tTraining Loss: 1.581867 \tValidation Loss: 2.395822\n",
      "Epoch: 19799 \tTraining Loss: 1.546796 \tValidation Loss: 2.395298\n",
      "Epoch: 19800 \tTraining Loss: 1.611159 \tValidation Loss: 2.395535\n",
      "Epoch: 19801 \tTraining Loss: 1.601541 \tValidation Loss: 2.395045\n",
      "Epoch: 19802 \tTraining Loss: 1.537765 \tValidation Loss: 2.395633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19803 \tTraining Loss: 1.560325 \tValidation Loss: 2.394931\n",
      "Epoch: 19804 \tTraining Loss: 1.585941 \tValidation Loss: 2.395769\n",
      "Epoch: 19805 \tTraining Loss: 1.554119 \tValidation Loss: 2.395879\n",
      "Epoch: 19806 \tTraining Loss: 1.535490 \tValidation Loss: 2.396020\n",
      "Epoch: 19807 \tTraining Loss: 1.600666 \tValidation Loss: 2.395983\n",
      "Epoch: 19808 \tTraining Loss: 1.549827 \tValidation Loss: 2.395794\n",
      "Epoch: 19809 \tTraining Loss: 1.572367 \tValidation Loss: 2.395816\n",
      "Epoch: 19810 \tTraining Loss: 1.560838 \tValidation Loss: 2.396546\n",
      "Epoch: 19811 \tTraining Loss: 1.594562 \tValidation Loss: 2.395812\n",
      "Epoch: 19812 \tTraining Loss: 1.564472 \tValidation Loss: 2.396777\n",
      "Epoch: 19813 \tTraining Loss: 1.560749 \tValidation Loss: 2.395833\n",
      "Epoch: 19814 \tTraining Loss: 1.619935 \tValidation Loss: 2.395050\n",
      "Epoch: 19815 \tTraining Loss: 1.565476 \tValidation Loss: 2.395722\n",
      "Epoch: 19816 \tTraining Loss: 1.561615 \tValidation Loss: 2.395577\n",
      "Epoch: 19817 \tTraining Loss: 1.577252 \tValidation Loss: 2.395858\n",
      "Epoch: 19818 \tTraining Loss: 1.526852 \tValidation Loss: 2.397292\n",
      "Epoch: 19819 \tTraining Loss: 1.556725 \tValidation Loss: 2.396734\n",
      "Epoch: 19820 \tTraining Loss: 1.579679 \tValidation Loss: 2.395759\n",
      "Epoch: 19821 \tTraining Loss: 1.577186 \tValidation Loss: 2.396225\n",
      "Epoch: 19822 \tTraining Loss: 1.570009 \tValidation Loss: 2.396616\n",
      "Epoch: 19823 \tTraining Loss: 1.557266 \tValidation Loss: 2.396912\n",
      "Epoch: 19824 \tTraining Loss: 1.576192 \tValidation Loss: 2.396558\n",
      "Epoch: 19825 \tTraining Loss: 1.544294 \tValidation Loss: 2.396958\n",
      "Epoch: 19826 \tTraining Loss: 1.557654 \tValidation Loss: 2.396073\n",
      "Epoch: 19827 \tTraining Loss: 1.590919 \tValidation Loss: 2.396302\n",
      "Epoch: 19828 \tTraining Loss: 1.604467 \tValidation Loss: 2.396725\n",
      "Epoch: 19829 \tTraining Loss: 1.558188 \tValidation Loss: 2.396121\n",
      "Epoch: 19830 \tTraining Loss: 1.556979 \tValidation Loss: 2.396387\n",
      "Epoch: 19831 \tTraining Loss: 1.545109 \tValidation Loss: 2.396216\n",
      "Epoch: 19832 \tTraining Loss: 1.561188 \tValidation Loss: 2.396365\n",
      "Epoch: 19833 \tTraining Loss: 1.574352 \tValidation Loss: 2.395998\n",
      "Epoch: 19834 \tTraining Loss: 1.597696 \tValidation Loss: 2.395731\n",
      "Epoch: 19835 \tTraining Loss: 1.555753 \tValidation Loss: 2.395918\n",
      "Epoch: 19836 \tTraining Loss: 1.559715 \tValidation Loss: 2.396151\n",
      "Epoch: 19837 \tTraining Loss: 1.583981 \tValidation Loss: 2.396147\n",
      "Epoch: 19838 \tTraining Loss: 1.562968 \tValidation Loss: 2.396646\n",
      "Epoch: 19839 \tTraining Loss: 1.550880 \tValidation Loss: 2.396325\n",
      "Epoch: 19840 \tTraining Loss: 1.597260 \tValidation Loss: 2.395971\n",
      "Epoch: 19841 \tTraining Loss: 1.552299 \tValidation Loss: 2.396465\n",
      "Epoch: 19842 \tTraining Loss: 1.586048 \tValidation Loss: 2.395955\n",
      "Epoch: 19843 \tTraining Loss: 1.573459 \tValidation Loss: 2.396648\n",
      "Epoch: 19844 \tTraining Loss: 1.569521 \tValidation Loss: 2.396283\n",
      "Epoch: 19845 \tTraining Loss: 1.564374 \tValidation Loss: 2.396635\n",
      "Epoch: 19846 \tTraining Loss: 1.554919 \tValidation Loss: 2.396282\n",
      "Epoch: 19847 \tTraining Loss: 1.547012 \tValidation Loss: 2.396410\n",
      "Epoch: 19848 \tTraining Loss: 1.569928 \tValidation Loss: 2.396684\n",
      "Epoch: 19849 \tTraining Loss: 1.581886 \tValidation Loss: 2.396693\n",
      "Epoch: 19850 \tTraining Loss: 1.548068 \tValidation Loss: 2.395833\n",
      "Epoch: 19851 \tTraining Loss: 1.563515 \tValidation Loss: 2.396985\n",
      "Epoch: 19852 \tTraining Loss: 1.573746 \tValidation Loss: 2.396481\n",
      "Epoch: 19853 \tTraining Loss: 1.580285 \tValidation Loss: 2.396575\n",
      "Epoch: 19854 \tTraining Loss: 1.608016 \tValidation Loss: 2.396834\n",
      "Epoch: 19855 \tTraining Loss: 1.577093 \tValidation Loss: 2.397060\n",
      "Epoch: 19856 \tTraining Loss: 1.569518 \tValidation Loss: 2.396019\n",
      "Epoch: 19857 \tTraining Loss: 1.566700 \tValidation Loss: 2.396648\n",
      "Epoch: 19858 \tTraining Loss: 1.560441 \tValidation Loss: 2.396283\n",
      "Epoch: 19859 \tTraining Loss: 1.522975 \tValidation Loss: 2.396870\n",
      "Epoch: 19860 \tTraining Loss: 1.562389 \tValidation Loss: 2.397106\n",
      "Epoch: 19861 \tTraining Loss: 1.597057 \tValidation Loss: 2.395877\n",
      "Epoch: 19862 \tTraining Loss: 1.579800 \tValidation Loss: 2.396310\n",
      "Epoch: 19863 \tTraining Loss: 1.579095 \tValidation Loss: 2.396359\n",
      "Epoch: 19864 \tTraining Loss: 1.613453 \tValidation Loss: 2.395971\n",
      "Epoch: 19865 \tTraining Loss: 1.559132 \tValidation Loss: 2.396012\n",
      "Epoch: 19866 \tTraining Loss: 1.555356 \tValidation Loss: 2.396628\n",
      "Epoch: 19867 \tTraining Loss: 1.566180 \tValidation Loss: 2.396597\n",
      "Epoch: 19868 \tTraining Loss: 1.552991 \tValidation Loss: 2.395910\n",
      "Epoch: 19869 \tTraining Loss: 1.588298 \tValidation Loss: 2.396357\n",
      "Epoch: 19870 \tTraining Loss: 1.569888 \tValidation Loss: 2.396720\n",
      "Epoch: 19871 \tTraining Loss: 1.569218 \tValidation Loss: 2.396225\n",
      "Epoch: 19872 \tTraining Loss: 1.584309 \tValidation Loss: 2.396008\n",
      "Epoch: 19873 \tTraining Loss: 1.580599 \tValidation Loss: 2.396853\n",
      "Epoch: 19874 \tTraining Loss: 1.555534 \tValidation Loss: 2.396031\n",
      "Epoch: 19875 \tTraining Loss: 1.539974 \tValidation Loss: 2.396400\n",
      "Epoch: 19876 \tTraining Loss: 1.610274 \tValidation Loss: 2.395985\n",
      "Epoch: 19877 \tTraining Loss: 1.554180 \tValidation Loss: 2.396672\n",
      "Epoch: 19878 \tTraining Loss: 1.553782 \tValidation Loss: 2.397037\n",
      "Epoch: 19879 \tTraining Loss: 1.561109 \tValidation Loss: 2.397105\n",
      "Epoch: 19880 \tTraining Loss: 1.584526 \tValidation Loss: 2.397575\n",
      "Epoch: 19881 \tTraining Loss: 1.548542 \tValidation Loss: 2.397147\n",
      "Epoch: 19882 \tTraining Loss: 1.554440 \tValidation Loss: 2.397406\n",
      "Epoch: 19883 \tTraining Loss: 1.579609 \tValidation Loss: 2.396237\n",
      "Epoch: 19884 \tTraining Loss: 1.557899 \tValidation Loss: 2.396422\n",
      "Epoch: 19885 \tTraining Loss: 1.551189 \tValidation Loss: 2.396958\n",
      "Epoch: 19886 \tTraining Loss: 1.557711 \tValidation Loss: 2.396452\n",
      "Epoch: 19887 \tTraining Loss: 1.533611 \tValidation Loss: 2.396800\n",
      "Epoch: 19888 \tTraining Loss: 1.552300 \tValidation Loss: 2.397768\n",
      "Epoch: 19889 \tTraining Loss: 1.552717 \tValidation Loss: 2.397007\n",
      "Epoch: 19890 \tTraining Loss: 1.596808 \tValidation Loss: 2.396632\n",
      "Epoch: 19891 \tTraining Loss: 1.563624 \tValidation Loss: 2.396297\n",
      "Epoch: 19892 \tTraining Loss: 1.540296 \tValidation Loss: 2.396676\n",
      "Epoch: 19893 \tTraining Loss: 1.589689 \tValidation Loss: 2.396675\n",
      "Epoch: 19894 \tTraining Loss: 1.558965 \tValidation Loss: 2.396778\n",
      "Epoch: 19895 \tTraining Loss: 1.592666 \tValidation Loss: 2.396300\n",
      "Epoch: 19896 \tTraining Loss: 1.579150 \tValidation Loss: 2.396805\n",
      "Epoch: 19897 \tTraining Loss: 1.570053 \tValidation Loss: 2.396786\n",
      "Epoch: 19898 \tTraining Loss: 1.547824 \tValidation Loss: 2.397013\n",
      "Epoch: 19899 \tTraining Loss: 1.549463 \tValidation Loss: 2.397705\n",
      "Epoch: 19900 \tTraining Loss: 1.587584 \tValidation Loss: 2.397078\n",
      "Epoch: 19901 \tTraining Loss: 1.542433 \tValidation Loss: 2.397339\n",
      "Epoch: 19902 \tTraining Loss: 1.592606 \tValidation Loss: 2.397114\n",
      "Epoch: 19903 \tTraining Loss: 1.559521 \tValidation Loss: 2.396492\n",
      "Epoch: 19904 \tTraining Loss: 1.570914 \tValidation Loss: 2.397253\n",
      "Epoch: 19905 \tTraining Loss: 1.542303 \tValidation Loss: 2.397108\n",
      "Epoch: 19906 \tTraining Loss: 1.582914 \tValidation Loss: 2.397483\n",
      "Epoch: 19907 \tTraining Loss: 1.598883 \tValidation Loss: 2.397694\n",
      "Epoch: 19908 \tTraining Loss: 1.538939 \tValidation Loss: 2.397033\n",
      "Epoch: 19909 \tTraining Loss: 1.587219 \tValidation Loss: 2.397304\n",
      "Epoch: 19910 \tTraining Loss: 1.540551 \tValidation Loss: 2.398318\n",
      "Epoch: 19911 \tTraining Loss: 1.582518 \tValidation Loss: 2.398131\n",
      "Epoch: 19912 \tTraining Loss: 1.566542 \tValidation Loss: 2.397809\n",
      "Epoch: 19913 \tTraining Loss: 1.540352 \tValidation Loss: 2.397746\n",
      "Epoch: 19914 \tTraining Loss: 1.582020 \tValidation Loss: 2.397234\n",
      "Epoch: 19915 \tTraining Loss: 1.609235 \tValidation Loss: 2.396897\n",
      "Epoch: 19916 \tTraining Loss: 1.596763 \tValidation Loss: 2.397323\n",
      "Epoch: 19917 \tTraining Loss: 1.596421 \tValidation Loss: 2.396956\n",
      "Epoch: 19918 \tTraining Loss: 1.562574 \tValidation Loss: 2.398135\n",
      "Epoch: 19919 \tTraining Loss: 1.578426 \tValidation Loss: 2.397728\n",
      "Epoch: 19920 \tTraining Loss: 1.554938 \tValidation Loss: 2.398107\n",
      "Epoch: 19921 \tTraining Loss: 1.588666 \tValidation Loss: 2.397631\n",
      "Epoch: 19922 \tTraining Loss: 1.550171 \tValidation Loss: 2.398046\n",
      "Epoch: 19923 \tTraining Loss: 1.552175 \tValidation Loss: 2.397912\n",
      "Epoch: 19924 \tTraining Loss: 1.575063 \tValidation Loss: 2.397751\n",
      "Epoch: 19925 \tTraining Loss: 1.558924 \tValidation Loss: 2.397091\n",
      "Epoch: 19926 \tTraining Loss: 1.569175 \tValidation Loss: 2.397405\n",
      "Epoch: 19927 \tTraining Loss: 1.558354 \tValidation Loss: 2.397377\n",
      "Epoch: 19928 \tTraining Loss: 1.561447 \tValidation Loss: 2.397449\n",
      "Epoch: 19929 \tTraining Loss: 1.571344 \tValidation Loss: 2.397525\n",
      "Epoch: 19930 \tTraining Loss: 1.573861 \tValidation Loss: 2.397078\n",
      "Epoch: 19931 \tTraining Loss: 1.538949 \tValidation Loss: 2.397696\n",
      "Epoch: 19932 \tTraining Loss: 1.577089 \tValidation Loss: 2.397425\n",
      "Epoch: 19933 \tTraining Loss: 1.521449 \tValidation Loss: 2.396882\n",
      "Epoch: 19934 \tTraining Loss: 1.567940 \tValidation Loss: 2.396873\n",
      "Epoch: 19935 \tTraining Loss: 1.547473 \tValidation Loss: 2.397131\n",
      "Epoch: 19936 \tTraining Loss: 1.586624 \tValidation Loss: 2.397436\n",
      "Epoch: 19937 \tTraining Loss: 1.576790 \tValidation Loss: 2.396542\n",
      "Epoch: 19938 \tTraining Loss: 1.547008 \tValidation Loss: 2.397093\n",
      "Epoch: 19939 \tTraining Loss: 1.570090 \tValidation Loss: 2.397038\n",
      "Epoch: 19940 \tTraining Loss: 1.558719 \tValidation Loss: 2.397139\n",
      "Epoch: 19941 \tTraining Loss: 1.576460 \tValidation Loss: 2.397395\n",
      "Epoch: 19942 \tTraining Loss: 1.562798 \tValidation Loss: 2.397800\n",
      "Epoch: 19943 \tTraining Loss: 1.551493 \tValidation Loss: 2.397898\n",
      "Epoch: 19944 \tTraining Loss: 1.574315 \tValidation Loss: 2.396880\n",
      "Epoch: 19945 \tTraining Loss: 1.571104 \tValidation Loss: 2.398090\n",
      "Epoch: 19946 \tTraining Loss: 1.558910 \tValidation Loss: 2.398762\n",
      "Epoch: 19947 \tTraining Loss: 1.573476 \tValidation Loss: 2.398068\n",
      "Epoch: 19948 \tTraining Loss: 1.590369 \tValidation Loss: 2.397418\n",
      "Epoch: 19949 \tTraining Loss: 1.565357 \tValidation Loss: 2.397451\n",
      "Epoch: 19950 \tTraining Loss: 1.581128 \tValidation Loss: 2.398139\n",
      "Epoch: 19951 \tTraining Loss: 1.581357 \tValidation Loss: 2.397640\n",
      "Epoch: 19952 \tTraining Loss: 1.572422 \tValidation Loss: 2.397710\n",
      "Epoch: 19953 \tTraining Loss: 1.569600 \tValidation Loss: 2.397873\n",
      "Epoch: 19954 \tTraining Loss: 1.607446 \tValidation Loss: 2.397377\n",
      "Epoch: 19955 \tTraining Loss: 1.564310 \tValidation Loss: 2.397638\n",
      "Epoch: 19956 \tTraining Loss: 1.575563 \tValidation Loss: 2.397434\n",
      "Epoch: 19957 \tTraining Loss: 1.527528 \tValidation Loss: 2.397795\n",
      "Epoch: 19958 \tTraining Loss: 1.524232 \tValidation Loss: 2.398548\n",
      "Epoch: 19959 \tTraining Loss: 1.562376 \tValidation Loss: 2.397519\n",
      "Epoch: 19960 \tTraining Loss: 1.537025 \tValidation Loss: 2.397984\n",
      "Epoch: 19961 \tTraining Loss: 1.556708 \tValidation Loss: 2.398091\n",
      "Epoch: 19962 \tTraining Loss: 1.546349 \tValidation Loss: 2.397993\n",
      "Epoch: 19963 \tTraining Loss: 1.546908 \tValidation Loss: 2.398235\n",
      "Epoch: 19964 \tTraining Loss: 1.536453 \tValidation Loss: 2.398561\n",
      "Epoch: 19965 \tTraining Loss: 1.553847 \tValidation Loss: 2.398763\n",
      "Epoch: 19966 \tTraining Loss: 1.551852 \tValidation Loss: 2.398469\n",
      "Epoch: 19967 \tTraining Loss: 1.580494 \tValidation Loss: 2.398979\n",
      "Epoch: 19968 \tTraining Loss: 1.545758 \tValidation Loss: 2.398035\n",
      "Epoch: 19969 \tTraining Loss: 1.548346 \tValidation Loss: 2.398374\n",
      "Epoch: 19970 \tTraining Loss: 1.584237 \tValidation Loss: 2.398389\n",
      "Epoch: 19971 \tTraining Loss: 1.588848 \tValidation Loss: 2.397395\n",
      "Epoch: 19972 \tTraining Loss: 1.580309 \tValidation Loss: 2.397502\n",
      "Epoch: 19973 \tTraining Loss: 1.530012 \tValidation Loss: 2.397477\n",
      "Epoch: 19974 \tTraining Loss: 1.586172 \tValidation Loss: 2.398160\n",
      "Epoch: 19975 \tTraining Loss: 1.559178 \tValidation Loss: 2.398506\n",
      "Epoch: 19976 \tTraining Loss: 1.570156 \tValidation Loss: 2.398548\n",
      "Epoch: 19977 \tTraining Loss: 1.543661 \tValidation Loss: 2.398442\n",
      "Epoch: 19978 \tTraining Loss: 1.551329 \tValidation Loss: 2.398471\n",
      "Epoch: 19979 \tTraining Loss: 1.535987 \tValidation Loss: 2.398775\n",
      "Epoch: 19980 \tTraining Loss: 1.567027 \tValidation Loss: 2.398630\n",
      "Epoch: 19981 \tTraining Loss: 1.578795 \tValidation Loss: 2.398607\n",
      "Epoch: 19982 \tTraining Loss: 1.565502 \tValidation Loss: 2.398500\n",
      "Epoch: 19983 \tTraining Loss: 1.608160 \tValidation Loss: 2.398485\n",
      "Epoch: 19984 \tTraining Loss: 1.557257 \tValidation Loss: 2.397979\n",
      "Epoch: 19985 \tTraining Loss: 1.571378 \tValidation Loss: 2.397820\n",
      "Epoch: 19986 \tTraining Loss: 1.565726 \tValidation Loss: 2.398058\n",
      "Epoch: 19987 \tTraining Loss: 1.561745 \tValidation Loss: 2.398178\n",
      "Epoch: 19988 \tTraining Loss: 1.556299 \tValidation Loss: 2.398305\n",
      "Epoch: 19989 \tTraining Loss: 1.551508 \tValidation Loss: 2.397509\n",
      "Epoch: 19990 \tTraining Loss: 1.566096 \tValidation Loss: 2.397805\n",
      "Epoch: 19991 \tTraining Loss: 1.557669 \tValidation Loss: 2.398322\n",
      "Epoch: 19992 \tTraining Loss: 1.566823 \tValidation Loss: 2.398435\n",
      "Epoch: 19993 \tTraining Loss: 1.591998 \tValidation Loss: 2.397886\n",
      "Epoch: 19994 \tTraining Loss: 1.548811 \tValidation Loss: 2.398195\n",
      "Epoch: 19995 \tTraining Loss: 1.562626 \tValidation Loss: 2.398890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19996 \tTraining Loss: 1.579573 \tValidation Loss: 2.398791\n",
      "Epoch: 19997 \tTraining Loss: 1.528586 \tValidation Loss: 2.399058\n",
      "Epoch: 19998 \tTraining Loss: 1.543593 \tValidation Loss: 2.399180\n",
      "Epoch: 19999 \tTraining Loss: 1.557882 \tValidation Loss: 2.399580\n",
      "Epoch: 20000 \tTraining Loss: 1.572336 \tValidation Loss: 2.398758\n",
      "Epoch: 20001 \tTraining Loss: 1.559554 \tValidation Loss: 2.399322\n",
      "Epoch: 20002 \tTraining Loss: 1.597884 \tValidation Loss: 2.399023\n",
      "Epoch: 20003 \tTraining Loss: 1.544785 \tValidation Loss: 2.398324\n",
      "Epoch: 20004 \tTraining Loss: 1.527145 \tValidation Loss: 2.399166\n",
      "Epoch: 20005 \tTraining Loss: 1.598984 \tValidation Loss: 2.398356\n",
      "Epoch: 20006 \tTraining Loss: 1.521190 \tValidation Loss: 2.399298\n",
      "Epoch: 20007 \tTraining Loss: 1.586380 \tValidation Loss: 2.398156\n",
      "Epoch: 20008 \tTraining Loss: 1.500659 \tValidation Loss: 2.399472\n",
      "Epoch: 20009 \tTraining Loss: 1.561426 \tValidation Loss: 2.399187\n",
      "Epoch: 20010 \tTraining Loss: 1.546770 \tValidation Loss: 2.398967\n",
      "Epoch: 20011 \tTraining Loss: 1.554488 \tValidation Loss: 2.398932\n",
      "Epoch: 20012 \tTraining Loss: 1.594478 \tValidation Loss: 2.398673\n",
      "Epoch: 20013 \tTraining Loss: 1.587626 \tValidation Loss: 2.398518\n",
      "Epoch: 20014 \tTraining Loss: 1.557311 \tValidation Loss: 2.398561\n",
      "Epoch: 20015 \tTraining Loss: 1.566949 \tValidation Loss: 2.399203\n",
      "Epoch: 20016 \tTraining Loss: 1.555540 \tValidation Loss: 2.399454\n",
      "Epoch: 20017 \tTraining Loss: 1.556175 \tValidation Loss: 2.398498\n",
      "Epoch: 20018 \tTraining Loss: 1.560915 \tValidation Loss: 2.398441\n",
      "Epoch: 20019 \tTraining Loss: 1.571666 \tValidation Loss: 2.397839\n",
      "Epoch: 20020 \tTraining Loss: 1.575949 \tValidation Loss: 2.398649\n",
      "Epoch: 20021 \tTraining Loss: 1.535835 \tValidation Loss: 2.398752\n",
      "Epoch: 20022 \tTraining Loss: 1.565322 \tValidation Loss: 2.398847\n",
      "Epoch: 20023 \tTraining Loss: 1.546448 \tValidation Loss: 2.399079\n",
      "Epoch: 20024 \tTraining Loss: 1.595484 \tValidation Loss: 2.399637\n",
      "Epoch: 20025 \tTraining Loss: 1.529722 \tValidation Loss: 2.399072\n",
      "Epoch: 20026 \tTraining Loss: 1.592780 \tValidation Loss: 2.398864\n",
      "Epoch: 20027 \tTraining Loss: 1.562290 \tValidation Loss: 2.399177\n",
      "Epoch: 20028 \tTraining Loss: 1.526785 \tValidation Loss: 2.399475\n",
      "Epoch: 20029 \tTraining Loss: 1.593173 \tValidation Loss: 2.399256\n",
      "Epoch: 20030 \tTraining Loss: 1.566951 \tValidation Loss: 2.398751\n",
      "Epoch: 20031 \tTraining Loss: 1.574121 \tValidation Loss: 2.398782\n",
      "Epoch: 20032 \tTraining Loss: 1.557091 \tValidation Loss: 2.398595\n",
      "Epoch: 20033 \tTraining Loss: 1.571794 \tValidation Loss: 2.398725\n",
      "Epoch: 20034 \tTraining Loss: 1.607243 \tValidation Loss: 2.399934\n",
      "Epoch: 20035 \tTraining Loss: 1.608273 \tValidation Loss: 2.399081\n",
      "Epoch: 20036 \tTraining Loss: 1.529074 \tValidation Loss: 2.399744\n",
      "Epoch: 20037 \tTraining Loss: 1.576557 \tValidation Loss: 2.399552\n",
      "Epoch: 20038 \tTraining Loss: 1.579500 \tValidation Loss: 2.399020\n",
      "Epoch: 20039 \tTraining Loss: 1.562124 \tValidation Loss: 2.399647\n",
      "Epoch: 20040 \tTraining Loss: 1.548970 \tValidation Loss: 2.399651\n",
      "Epoch: 20041 \tTraining Loss: 1.551252 \tValidation Loss: 2.399756\n",
      "Epoch: 20042 \tTraining Loss: 1.576012 \tValidation Loss: 2.399750\n",
      "Epoch: 20043 \tTraining Loss: 1.538978 \tValidation Loss: 2.399669\n",
      "Epoch: 20044 \tTraining Loss: 1.573782 \tValidation Loss: 2.399138\n",
      "Epoch: 20045 \tTraining Loss: 1.578255 \tValidation Loss: 2.399361\n",
      "Epoch: 20046 \tTraining Loss: 1.553151 \tValidation Loss: 2.399621\n",
      "Epoch: 20047 \tTraining Loss: 1.544309 \tValidation Loss: 2.399806\n",
      "Epoch: 20048 \tTraining Loss: 1.577261 \tValidation Loss: 2.399560\n",
      "Epoch: 20049 \tTraining Loss: 1.595310 \tValidation Loss: 2.399266\n",
      "Epoch: 20050 \tTraining Loss: 1.599134 \tValidation Loss: 2.399007\n",
      "Epoch: 20051 \tTraining Loss: 1.540955 \tValidation Loss: 2.399906\n",
      "Epoch: 20052 \tTraining Loss: 1.557266 \tValidation Loss: 2.399328\n",
      "Epoch: 20053 \tTraining Loss: 1.563514 \tValidation Loss: 2.399898\n",
      "Epoch: 20054 \tTraining Loss: 1.595725 \tValidation Loss: 2.399218\n",
      "Epoch: 20055 \tTraining Loss: 1.575009 \tValidation Loss: 2.399245\n",
      "Epoch: 20056 \tTraining Loss: 1.559619 \tValidation Loss: 2.398903\n",
      "Epoch: 20057 \tTraining Loss: 1.545408 \tValidation Loss: 2.399598\n",
      "Epoch: 20058 \tTraining Loss: 1.559754 \tValidation Loss: 2.399628\n",
      "Epoch: 20059 \tTraining Loss: 1.564973 \tValidation Loss: 2.400217\n",
      "Epoch: 20060 \tTraining Loss: 1.544004 \tValidation Loss: 2.400079\n",
      "Epoch: 20061 \tTraining Loss: 1.570723 \tValidation Loss: 2.400530\n",
      "Epoch: 20062 \tTraining Loss: 1.580259 \tValidation Loss: 2.399903\n",
      "Epoch: 20063 \tTraining Loss: 1.572257 \tValidation Loss: 2.400032\n",
      "Epoch: 20064 \tTraining Loss: 1.596414 \tValidation Loss: 2.400188\n",
      "Epoch: 20065 \tTraining Loss: 1.547096 \tValidation Loss: 2.400035\n",
      "Epoch: 20066 \tTraining Loss: 1.577069 \tValidation Loss: 2.400298\n",
      "Epoch: 20067 \tTraining Loss: 1.528471 \tValidation Loss: 2.400298\n",
      "Epoch: 20068 \tTraining Loss: 1.532789 \tValidation Loss: 2.399634\n",
      "Epoch: 20069 \tTraining Loss: 1.564676 \tValidation Loss: 2.399451\n",
      "Epoch: 20070 \tTraining Loss: 1.552239 \tValidation Loss: 2.399688\n",
      "Epoch: 20071 \tTraining Loss: 1.508194 \tValidation Loss: 2.399888\n",
      "Epoch: 20072 \tTraining Loss: 1.559078 \tValidation Loss: 2.400312\n",
      "Epoch: 20073 \tTraining Loss: 1.583461 \tValidation Loss: 2.400546\n",
      "Epoch: 20074 \tTraining Loss: 1.572577 \tValidation Loss: 2.400592\n",
      "Epoch: 20075 \tTraining Loss: 1.555768 \tValidation Loss: 2.400524\n",
      "Epoch: 20076 \tTraining Loss: 1.555666 \tValidation Loss: 2.400480\n",
      "Epoch: 20077 \tTraining Loss: 1.573653 \tValidation Loss: 2.400190\n",
      "Epoch: 20078 \tTraining Loss: 1.565700 \tValidation Loss: 2.399926\n",
      "Epoch: 20079 \tTraining Loss: 1.589069 \tValidation Loss: 2.399280\n",
      "Epoch: 20080 \tTraining Loss: 1.593858 \tValidation Loss: 2.399199\n",
      "Epoch: 20081 \tTraining Loss: 1.582153 \tValidation Loss: 2.399257\n",
      "Epoch: 20082 \tTraining Loss: 1.525179 \tValidation Loss: 2.400019\n",
      "Epoch: 20083 \tTraining Loss: 1.560932 \tValidation Loss: 2.399697\n",
      "Epoch: 20084 \tTraining Loss: 1.554478 \tValidation Loss: 2.400098\n",
      "Epoch: 20085 \tTraining Loss: 1.591588 \tValidation Loss: 2.400131\n",
      "Epoch: 20086 \tTraining Loss: 1.547341 \tValidation Loss: 2.400614\n",
      "Epoch: 20087 \tTraining Loss: 1.574576 \tValidation Loss: 2.400146\n",
      "Epoch: 20088 \tTraining Loss: 1.585355 \tValidation Loss: 2.399761\n",
      "Epoch: 20089 \tTraining Loss: 1.568177 \tValidation Loss: 2.399617\n",
      "Epoch: 20090 \tTraining Loss: 1.547572 \tValidation Loss: 2.399849\n",
      "Epoch: 20091 \tTraining Loss: 1.521843 \tValidation Loss: 2.400945\n",
      "Epoch: 20092 \tTraining Loss: 1.550992 \tValidation Loss: 2.400438\n",
      "Epoch: 20093 \tTraining Loss: 1.571169 \tValidation Loss: 2.400290\n",
      "Epoch: 20094 \tTraining Loss: 1.529491 \tValidation Loss: 2.400759\n",
      "Epoch: 20095 \tTraining Loss: 1.586673 \tValidation Loss: 2.399657\n",
      "Epoch: 20096 \tTraining Loss: 1.567625 \tValidation Loss: 2.399760\n",
      "Epoch: 20097 \tTraining Loss: 1.578240 \tValidation Loss: 2.399482\n",
      "Epoch: 20098 \tTraining Loss: 1.579703 \tValidation Loss: 2.399514\n",
      "Epoch: 20099 \tTraining Loss: 1.570389 \tValidation Loss: 2.399556\n",
      "Epoch: 20100 \tTraining Loss: 1.569631 \tValidation Loss: 2.400123\n",
      "Epoch: 20101 \tTraining Loss: 1.551058 \tValidation Loss: 2.400056\n",
      "Epoch: 20102 \tTraining Loss: 1.571283 \tValidation Loss: 2.400345\n",
      "Epoch: 20103 \tTraining Loss: 1.580321 \tValidation Loss: 2.400476\n",
      "Epoch: 20104 \tTraining Loss: 1.565023 \tValidation Loss: 2.399871\n",
      "Epoch: 20105 \tTraining Loss: 1.587870 \tValidation Loss: 2.399902\n",
      "Epoch: 20106 \tTraining Loss: 1.533662 \tValidation Loss: 2.400441\n",
      "Epoch: 20107 \tTraining Loss: 1.587699 \tValidation Loss: 2.400022\n",
      "Epoch: 20108 \tTraining Loss: 1.506828 \tValidation Loss: 2.400013\n",
      "Epoch: 20109 \tTraining Loss: 1.552497 \tValidation Loss: 2.400329\n",
      "Epoch: 20110 \tTraining Loss: 1.559811 \tValidation Loss: 2.400614\n",
      "Epoch: 20111 \tTraining Loss: 1.551011 \tValidation Loss: 2.400513\n",
      "Epoch: 20112 \tTraining Loss: 1.588622 \tValidation Loss: 2.399836\n",
      "Epoch: 20113 \tTraining Loss: 1.579274 \tValidation Loss: 2.400109\n",
      "Epoch: 20114 \tTraining Loss: 1.544252 \tValidation Loss: 2.400372\n",
      "Epoch: 20115 \tTraining Loss: 1.504061 \tValidation Loss: 2.400826\n",
      "Epoch: 20116 \tTraining Loss: 1.523511 \tValidation Loss: 2.401144\n",
      "Epoch: 20117 \tTraining Loss: 1.550181 \tValidation Loss: 2.400727\n",
      "Epoch: 20118 \tTraining Loss: 1.546263 \tValidation Loss: 2.400806\n",
      "Epoch: 20119 \tTraining Loss: 1.569168 \tValidation Loss: 2.400671\n",
      "Epoch: 20120 \tTraining Loss: 1.568243 \tValidation Loss: 2.401033\n",
      "Epoch: 20121 \tTraining Loss: 1.584046 \tValidation Loss: 2.400730\n",
      "Epoch: 20122 \tTraining Loss: 1.560399 \tValidation Loss: 2.400676\n",
      "Epoch: 20123 \tTraining Loss: 1.578674 \tValidation Loss: 2.400402\n",
      "Epoch: 20124 \tTraining Loss: 1.599753 \tValidation Loss: 2.400657\n",
      "Epoch: 20125 \tTraining Loss: 1.547761 \tValidation Loss: 2.400501\n",
      "Epoch: 20126 \tTraining Loss: 1.584320 \tValidation Loss: 2.400223\n",
      "Epoch: 20127 \tTraining Loss: 1.527974 \tValidation Loss: 2.401162\n",
      "Epoch: 20128 \tTraining Loss: 1.584013 \tValidation Loss: 2.400678\n",
      "Epoch: 20129 \tTraining Loss: 1.562650 \tValidation Loss: 2.401164\n",
      "Epoch: 20130 \tTraining Loss: 1.577643 \tValidation Loss: 2.400939\n",
      "Epoch: 20131 \tTraining Loss: 1.544879 \tValidation Loss: 2.401028\n",
      "Epoch: 20132 \tTraining Loss: 1.587389 \tValidation Loss: 2.401032\n",
      "Epoch: 20133 \tTraining Loss: 1.579945 \tValidation Loss: 2.400437\n",
      "Epoch: 20134 \tTraining Loss: 1.547885 \tValidation Loss: 2.400584\n",
      "Epoch: 20135 \tTraining Loss: 1.550080 \tValidation Loss: 2.400617\n",
      "Epoch: 20136 \tTraining Loss: 1.601743 \tValidation Loss: 2.400152\n",
      "Epoch: 20137 \tTraining Loss: 1.591039 \tValidation Loss: 2.400490\n",
      "Epoch: 20138 \tTraining Loss: 1.542519 \tValidation Loss: 2.400509\n",
      "Epoch: 20139 \tTraining Loss: 1.553687 \tValidation Loss: 2.399981\n",
      "Epoch: 20140 \tTraining Loss: 1.550999 \tValidation Loss: 2.400324\n",
      "Epoch: 20141 \tTraining Loss: 1.549850 \tValidation Loss: 2.400145\n",
      "Epoch: 20142 \tTraining Loss: 1.572295 \tValidation Loss: 2.400408\n",
      "Epoch: 20143 \tTraining Loss: 1.552059 \tValidation Loss: 2.400653\n",
      "Epoch: 20144 \tTraining Loss: 1.560927 \tValidation Loss: 2.400496\n",
      "Epoch: 20145 \tTraining Loss: 1.554109 \tValidation Loss: 2.400666\n",
      "Epoch: 20146 \tTraining Loss: 1.479223 \tValidation Loss: 2.401713\n",
      "Epoch: 20147 \tTraining Loss: 1.532994 \tValidation Loss: 2.401193\n",
      "Epoch: 20148 \tTraining Loss: 1.533451 \tValidation Loss: 2.400711\n",
      "Epoch: 20149 \tTraining Loss: 1.539495 \tValidation Loss: 2.400530\n",
      "Epoch: 20150 \tTraining Loss: 1.593812 \tValidation Loss: 2.400440\n",
      "Epoch: 20151 \tTraining Loss: 1.567149 \tValidation Loss: 2.400531\n",
      "Epoch: 20152 \tTraining Loss: 1.565531 \tValidation Loss: 2.400538\n",
      "Epoch: 20153 \tTraining Loss: 1.563738 \tValidation Loss: 2.400664\n",
      "Epoch: 20154 \tTraining Loss: 1.550851 \tValidation Loss: 2.400200\n",
      "Epoch: 20155 \tTraining Loss: 1.599778 \tValidation Loss: 2.400196\n",
      "Epoch: 20156 \tTraining Loss: 1.566281 \tValidation Loss: 2.401479\n",
      "Epoch: 20157 \tTraining Loss: 1.586399 \tValidation Loss: 2.401213\n",
      "Epoch: 20158 \tTraining Loss: 1.520974 \tValidation Loss: 2.401392\n",
      "Epoch: 20159 \tTraining Loss: 1.559773 \tValidation Loss: 2.401090\n",
      "Epoch: 20160 \tTraining Loss: 1.571874 \tValidation Loss: 2.400700\n",
      "Epoch: 20161 \tTraining Loss: 1.622991 \tValidation Loss: 2.401018\n",
      "Epoch: 20162 \tTraining Loss: 1.578746 \tValidation Loss: 2.401006\n",
      "Epoch: 20163 \tTraining Loss: 1.593885 \tValidation Loss: 2.399829\n",
      "Epoch: 20164 \tTraining Loss: 1.573621 \tValidation Loss: 2.400473\n",
      "Epoch: 20165 \tTraining Loss: 1.554698 \tValidation Loss: 2.400941\n",
      "Epoch: 20166 \tTraining Loss: 1.514447 \tValidation Loss: 2.400306\n",
      "Epoch: 20167 \tTraining Loss: 1.573666 \tValidation Loss: 2.401403\n",
      "Epoch: 20168 \tTraining Loss: 1.563500 \tValidation Loss: 2.401611\n",
      "Epoch: 20169 \tTraining Loss: 1.581708 \tValidation Loss: 2.400983\n",
      "Epoch: 20170 \tTraining Loss: 1.572267 \tValidation Loss: 2.400557\n",
      "Epoch: 20171 \tTraining Loss: 1.581654 \tValidation Loss: 2.400594\n",
      "Epoch: 20172 \tTraining Loss: 1.570525 \tValidation Loss: 2.401451\n",
      "Epoch: 20173 \tTraining Loss: 1.570622 \tValidation Loss: 2.400268\n",
      "Epoch: 20174 \tTraining Loss: 1.538487 \tValidation Loss: 2.400573\n",
      "Epoch: 20175 \tTraining Loss: 1.549976 \tValidation Loss: 2.400271\n",
      "Epoch: 20176 \tTraining Loss: 1.563020 \tValidation Loss: 2.400949\n",
      "Epoch: 20177 \tTraining Loss: 1.547890 \tValidation Loss: 2.400894\n",
      "Epoch: 20178 \tTraining Loss: 1.555358 \tValidation Loss: 2.401456\n",
      "Epoch: 20179 \tTraining Loss: 1.522145 \tValidation Loss: 2.402233\n",
      "Epoch: 20180 \tTraining Loss: 1.531206 \tValidation Loss: 2.401796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20181 \tTraining Loss: 1.553686 \tValidation Loss: 2.401998\n",
      "Epoch: 20182 \tTraining Loss: 1.564785 \tValidation Loss: 2.401340\n",
      "Epoch: 20183 \tTraining Loss: 1.549114 \tValidation Loss: 2.401177\n",
      "Epoch: 20184 \tTraining Loss: 1.543103 \tValidation Loss: 2.401636\n",
      "Epoch: 20185 \tTraining Loss: 1.593666 \tValidation Loss: 2.401230\n",
      "Epoch: 20186 \tTraining Loss: 1.589454 \tValidation Loss: 2.401822\n",
      "Epoch: 20187 \tTraining Loss: 1.543476 \tValidation Loss: 2.400900\n",
      "Epoch: 20188 \tTraining Loss: 1.546107 \tValidation Loss: 2.400851\n",
      "Epoch: 20189 \tTraining Loss: 1.521735 \tValidation Loss: 2.401167\n",
      "Epoch: 20190 \tTraining Loss: 1.592596 \tValidation Loss: 2.400756\n",
      "Epoch: 20191 \tTraining Loss: 1.602607 \tValidation Loss: 2.400872\n",
      "Epoch: 20192 \tTraining Loss: 1.551596 \tValidation Loss: 2.401157\n",
      "Epoch: 20193 \tTraining Loss: 1.558101 \tValidation Loss: 2.400270\n",
      "Epoch: 20194 \tTraining Loss: 1.570622 \tValidation Loss: 2.401397\n",
      "Epoch: 20195 \tTraining Loss: 1.524673 \tValidation Loss: 2.400554\n",
      "Epoch: 20196 \tTraining Loss: 1.555721 \tValidation Loss: 2.400865\n",
      "Epoch: 20197 \tTraining Loss: 1.563873 \tValidation Loss: 2.401119\n",
      "Epoch: 20198 \tTraining Loss: 1.537147 \tValidation Loss: 2.401118\n",
      "Epoch: 20199 \tTraining Loss: 1.543003 \tValidation Loss: 2.401161\n",
      "Epoch: 20200 \tTraining Loss: 1.560676 \tValidation Loss: 2.400881\n",
      "Epoch: 20201 \tTraining Loss: 1.566365 \tValidation Loss: 2.400481\n",
      "Epoch: 20202 \tTraining Loss: 1.533094 \tValidation Loss: 2.401178\n",
      "Epoch: 20203 \tTraining Loss: 1.557105 \tValidation Loss: 2.401406\n",
      "Epoch: 20204 \tTraining Loss: 1.591357 \tValidation Loss: 2.400959\n",
      "Epoch: 20205 \tTraining Loss: 1.518770 \tValidation Loss: 2.401377\n",
      "Epoch: 20206 \tTraining Loss: 1.561015 \tValidation Loss: 2.401678\n",
      "Epoch: 20207 \tTraining Loss: 1.558937 \tValidation Loss: 2.400872\n",
      "Epoch: 20208 \tTraining Loss: 1.573335 \tValidation Loss: 2.400791\n",
      "Epoch: 20209 \tTraining Loss: 1.556256 \tValidation Loss: 2.401165\n",
      "Epoch: 20210 \tTraining Loss: 1.508506 \tValidation Loss: 2.401675\n",
      "Epoch: 20211 \tTraining Loss: 1.539104 \tValidation Loss: 2.401993\n",
      "Epoch: 20212 \tTraining Loss: 1.554943 \tValidation Loss: 2.401442\n",
      "Epoch: 20213 \tTraining Loss: 1.576022 \tValidation Loss: 2.401118\n",
      "Epoch: 20214 \tTraining Loss: 1.563855 \tValidation Loss: 2.401155\n",
      "Epoch: 20215 \tTraining Loss: 1.557287 \tValidation Loss: 2.401345\n",
      "Epoch: 20216 \tTraining Loss: 1.530963 \tValidation Loss: 2.401672\n",
      "Epoch: 20217 \tTraining Loss: 1.536487 \tValidation Loss: 2.401867\n",
      "Epoch: 20218 \tTraining Loss: 1.537082 \tValidation Loss: 2.401276\n",
      "Epoch: 20219 \tTraining Loss: 1.540776 \tValidation Loss: 2.401807\n",
      "Epoch: 20220 \tTraining Loss: 1.550518 \tValidation Loss: 2.401944\n",
      "Epoch: 20221 \tTraining Loss: 1.518648 \tValidation Loss: 2.401943\n",
      "Epoch: 20222 \tTraining Loss: 1.546606 \tValidation Loss: 2.402813\n",
      "Epoch: 20223 \tTraining Loss: 1.536170 \tValidation Loss: 2.402657\n",
      "Epoch: 20224 \tTraining Loss: 1.566808 \tValidation Loss: 2.402184\n",
      "Epoch: 20225 \tTraining Loss: 1.497454 \tValidation Loss: 2.402560\n",
      "Epoch: 20226 \tTraining Loss: 1.603611 \tValidation Loss: 2.402134\n",
      "Epoch: 20227 \tTraining Loss: 1.558832 \tValidation Loss: 2.402332\n",
      "Epoch: 20228 \tTraining Loss: 1.565075 \tValidation Loss: 2.401735\n",
      "Epoch: 20229 \tTraining Loss: 1.549258 \tValidation Loss: 2.402095\n",
      "Epoch: 20230 \tTraining Loss: 1.566346 \tValidation Loss: 2.401319\n",
      "Epoch: 20231 \tTraining Loss: 1.560288 \tValidation Loss: 2.401458\n",
      "Epoch: 20232 \tTraining Loss: 1.558378 \tValidation Loss: 2.401979\n",
      "Epoch: 20233 \tTraining Loss: 1.559570 \tValidation Loss: 2.402416\n",
      "Epoch: 20234 \tTraining Loss: 1.571844 \tValidation Loss: 2.401938\n",
      "Epoch: 20235 \tTraining Loss: 1.541430 \tValidation Loss: 2.402242\n",
      "Epoch: 20236 \tTraining Loss: 1.571837 \tValidation Loss: 2.401957\n",
      "Epoch: 20237 \tTraining Loss: 1.575636 \tValidation Loss: 2.401989\n",
      "Epoch: 20238 \tTraining Loss: 1.547127 \tValidation Loss: 2.402334\n",
      "Epoch: 20239 \tTraining Loss: 1.573141 \tValidation Loss: 2.401136\n",
      "Epoch: 20240 \tTraining Loss: 1.569196 \tValidation Loss: 2.402048\n",
      "Epoch: 20241 \tTraining Loss: 1.562286 \tValidation Loss: 2.402128\n",
      "Epoch: 20242 \tTraining Loss: 1.534376 \tValidation Loss: 2.402651\n",
      "Epoch: 20243 \tTraining Loss: 1.560710 \tValidation Loss: 2.401531\n",
      "Epoch: 20244 \tTraining Loss: 1.557511 \tValidation Loss: 2.401806\n",
      "Epoch: 20245 \tTraining Loss: 1.525274 \tValidation Loss: 2.401672\n",
      "Epoch: 20246 \tTraining Loss: 1.528401 \tValidation Loss: 2.402158\n",
      "Epoch: 20247 \tTraining Loss: 1.573602 \tValidation Loss: 2.402016\n",
      "Epoch: 20248 \tTraining Loss: 1.547651 \tValidation Loss: 2.402029\n",
      "Epoch: 20249 \tTraining Loss: 1.603709 \tValidation Loss: 2.401761\n",
      "Epoch: 20250 \tTraining Loss: 1.528302 \tValidation Loss: 2.401870\n",
      "Epoch: 20251 \tTraining Loss: 1.543263 \tValidation Loss: 2.401879\n",
      "Epoch: 20252 \tTraining Loss: 1.570333 \tValidation Loss: 2.402220\n",
      "Epoch: 20253 \tTraining Loss: 1.541852 \tValidation Loss: 2.403006\n",
      "Epoch: 20254 \tTraining Loss: 1.528707 \tValidation Loss: 2.403220\n",
      "Epoch: 20255 \tTraining Loss: 1.516727 \tValidation Loss: 2.402658\n",
      "Epoch: 20256 \tTraining Loss: 1.565270 \tValidation Loss: 2.402318\n",
      "Epoch: 20257 \tTraining Loss: 1.551969 \tValidation Loss: 2.402312\n",
      "Epoch: 20258 \tTraining Loss: 1.588492 \tValidation Loss: 2.402132\n",
      "Epoch: 20259 \tTraining Loss: 1.527920 \tValidation Loss: 2.401481\n",
      "Epoch: 20260 \tTraining Loss: 1.543312 \tValidation Loss: 2.402494\n",
      "Epoch: 20261 \tTraining Loss: 1.583167 \tValidation Loss: 2.402255\n",
      "Epoch: 20262 \tTraining Loss: 1.578431 \tValidation Loss: 2.402578\n",
      "Epoch: 20263 \tTraining Loss: 1.553590 \tValidation Loss: 2.402472\n",
      "Epoch: 20264 \tTraining Loss: 1.497050 \tValidation Loss: 2.402707\n",
      "Epoch: 20265 \tTraining Loss: 1.558501 \tValidation Loss: 2.403092\n",
      "Epoch: 20266 \tTraining Loss: 1.555397 \tValidation Loss: 2.402379\n",
      "Epoch: 20267 \tTraining Loss: 1.539960 \tValidation Loss: 2.402576\n",
      "Epoch: 20268 \tTraining Loss: 1.516481 \tValidation Loss: 2.402854\n",
      "Epoch: 20269 \tTraining Loss: 1.574854 \tValidation Loss: 2.402010\n",
      "Epoch: 20270 \tTraining Loss: 1.576719 \tValidation Loss: 2.402414\n",
      "Epoch: 20271 \tTraining Loss: 1.533569 \tValidation Loss: 2.403091\n",
      "Epoch: 20272 \tTraining Loss: 1.566865 \tValidation Loss: 2.402317\n",
      "Epoch: 20273 \tTraining Loss: 1.529090 \tValidation Loss: 2.402825\n",
      "Epoch: 20274 \tTraining Loss: 1.544702 \tValidation Loss: 2.402752\n",
      "Epoch: 20275 \tTraining Loss: 1.587306 \tValidation Loss: 2.402426\n",
      "Epoch: 20276 \tTraining Loss: 1.560948 \tValidation Loss: 2.401836\n",
      "Epoch: 20277 \tTraining Loss: 1.568613 \tValidation Loss: 2.402311\n",
      "Epoch: 20278 \tTraining Loss: 1.577757 \tValidation Loss: 2.401589\n",
      "Epoch: 20279 \tTraining Loss: 1.586010 \tValidation Loss: 2.402058\n",
      "Epoch: 20280 \tTraining Loss: 1.580496 \tValidation Loss: 2.402328\n",
      "Epoch: 20281 \tTraining Loss: 1.579742 \tValidation Loss: 2.401940\n",
      "Epoch: 20282 \tTraining Loss: 1.518531 \tValidation Loss: 2.403036\n",
      "Epoch: 20283 \tTraining Loss: 1.571525 \tValidation Loss: 2.401571\n",
      "Epoch: 20284 \tTraining Loss: 1.552559 \tValidation Loss: 2.402248\n",
      "Epoch: 20285 \tTraining Loss: 1.547395 \tValidation Loss: 2.402766\n",
      "Epoch: 20286 \tTraining Loss: 1.548868 \tValidation Loss: 2.402456\n",
      "Epoch: 20287 \tTraining Loss: 1.602723 \tValidation Loss: 2.402511\n",
      "Epoch: 20288 \tTraining Loss: 1.562894 \tValidation Loss: 2.402035\n",
      "Epoch: 20289 \tTraining Loss: 1.530044 \tValidation Loss: 2.402355\n",
      "Epoch: 20290 \tTraining Loss: 1.543071 \tValidation Loss: 2.402413\n",
      "Epoch: 20291 \tTraining Loss: 1.535757 \tValidation Loss: 2.403196\n",
      "Epoch: 20292 \tTraining Loss: 1.528437 \tValidation Loss: 2.402536\n",
      "Epoch: 20293 \tTraining Loss: 1.609297 \tValidation Loss: 2.402415\n",
      "Epoch: 20294 \tTraining Loss: 1.523455 \tValidation Loss: 2.403064\n",
      "Epoch: 20295 \tTraining Loss: 1.530003 \tValidation Loss: 2.402718\n",
      "Epoch: 20296 \tTraining Loss: 1.565882 \tValidation Loss: 2.402877\n",
      "Epoch: 20297 \tTraining Loss: 1.541381 \tValidation Loss: 2.402765\n",
      "Epoch: 20298 \tTraining Loss: 1.536731 \tValidation Loss: 2.402305\n",
      "Epoch: 20299 \tTraining Loss: 1.577335 \tValidation Loss: 2.402175\n",
      "Epoch: 20300 \tTraining Loss: 1.527464 \tValidation Loss: 2.402564\n",
      "Epoch: 20301 \tTraining Loss: 1.516212 \tValidation Loss: 2.402678\n",
      "Epoch: 20302 \tTraining Loss: 1.594108 \tValidation Loss: 2.402321\n",
      "Epoch: 20303 \tTraining Loss: 1.565932 \tValidation Loss: 2.402359\n",
      "Epoch: 20304 \tTraining Loss: 1.531996 \tValidation Loss: 2.403292\n",
      "Epoch: 20305 \tTraining Loss: 1.525025 \tValidation Loss: 2.403539\n",
      "Epoch: 20306 \tTraining Loss: 1.561141 \tValidation Loss: 2.402290\n",
      "Epoch: 20307 \tTraining Loss: 1.581215 \tValidation Loss: 2.402632\n",
      "Epoch: 20308 \tTraining Loss: 1.499542 \tValidation Loss: 2.403361\n",
      "Epoch: 20309 \tTraining Loss: 1.544679 \tValidation Loss: 2.402987\n",
      "Epoch: 20310 \tTraining Loss: 1.560617 \tValidation Loss: 2.402338\n",
      "Epoch: 20311 \tTraining Loss: 1.557235 \tValidation Loss: 2.402896\n",
      "Epoch: 20312 \tTraining Loss: 1.544271 \tValidation Loss: 2.403075\n",
      "Epoch: 20313 \tTraining Loss: 1.617589 \tValidation Loss: 2.402706\n",
      "Epoch: 20314 \tTraining Loss: 1.531551 \tValidation Loss: 2.403059\n",
      "Epoch: 20315 \tTraining Loss: 1.555964 \tValidation Loss: 2.402975\n",
      "Epoch: 20316 \tTraining Loss: 1.512127 \tValidation Loss: 2.402401\n",
      "Epoch: 20317 \tTraining Loss: 1.554810 \tValidation Loss: 2.402791\n",
      "Epoch: 20318 \tTraining Loss: 1.618179 \tValidation Loss: 2.402841\n",
      "Epoch: 20319 \tTraining Loss: 1.540710 \tValidation Loss: 2.403172\n",
      "Epoch: 20320 \tTraining Loss: 1.522974 \tValidation Loss: 2.403087\n",
      "Epoch: 20321 \tTraining Loss: 1.554998 \tValidation Loss: 2.403457\n",
      "Epoch: 20322 \tTraining Loss: 1.520283 \tValidation Loss: 2.403615\n",
      "Epoch: 20323 \tTraining Loss: 1.594395 \tValidation Loss: 2.402565\n",
      "Epoch: 20324 \tTraining Loss: 1.521000 \tValidation Loss: 2.402955\n",
      "Epoch: 20325 \tTraining Loss: 1.539847 \tValidation Loss: 2.403124\n",
      "Epoch: 20326 \tTraining Loss: 1.571048 \tValidation Loss: 2.403224\n",
      "Epoch: 20327 \tTraining Loss: 1.534007 \tValidation Loss: 2.403303\n",
      "Epoch: 20328 \tTraining Loss: 1.563243 \tValidation Loss: 2.403486\n",
      "Epoch: 20329 \tTraining Loss: 1.521068 \tValidation Loss: 2.402677\n",
      "Epoch: 20330 \tTraining Loss: 1.573690 \tValidation Loss: 2.403120\n",
      "Epoch: 20331 \tTraining Loss: 1.579985 \tValidation Loss: 2.403265\n",
      "Epoch: 20332 \tTraining Loss: 1.542745 \tValidation Loss: 2.402530\n",
      "Epoch: 20333 \tTraining Loss: 1.553481 \tValidation Loss: 2.402485\n",
      "Epoch: 20334 \tTraining Loss: 1.562932 \tValidation Loss: 2.403292\n",
      "Epoch: 20335 \tTraining Loss: 1.549225 \tValidation Loss: 2.403551\n",
      "Epoch: 20336 \tTraining Loss: 1.559624 \tValidation Loss: 2.403431\n",
      "Epoch: 20337 \tTraining Loss: 1.582002 \tValidation Loss: 2.403488\n",
      "Epoch: 20338 \tTraining Loss: 1.510551 \tValidation Loss: 2.403700\n",
      "Epoch: 20339 \tTraining Loss: 1.538479 \tValidation Loss: 2.404221\n",
      "Epoch: 20340 \tTraining Loss: 1.595267 \tValidation Loss: 2.403491\n",
      "Epoch: 20341 \tTraining Loss: 1.532599 \tValidation Loss: 2.403811\n",
      "Epoch: 20342 \tTraining Loss: 1.522783 \tValidation Loss: 2.403824\n",
      "Epoch: 20343 \tTraining Loss: 1.556914 \tValidation Loss: 2.404133\n",
      "Epoch: 20344 \tTraining Loss: 1.580038 \tValidation Loss: 2.402899\n",
      "Epoch: 20345 \tTraining Loss: 1.547229 \tValidation Loss: 2.402212\n",
      "Epoch: 20346 \tTraining Loss: 1.510949 \tValidation Loss: 2.402608\n",
      "Epoch: 20347 \tTraining Loss: 1.553493 \tValidation Loss: 2.402263\n",
      "Epoch: 20348 \tTraining Loss: 1.512781 \tValidation Loss: 2.403594\n",
      "Epoch: 20349 \tTraining Loss: 1.555520 \tValidation Loss: 2.403214\n",
      "Epoch: 20350 \tTraining Loss: 1.531617 \tValidation Loss: 2.403439\n",
      "Epoch: 20351 \tTraining Loss: 1.558665 \tValidation Loss: 2.403222\n",
      "Epoch: 20352 \tTraining Loss: 1.571411 \tValidation Loss: 2.403207\n",
      "Epoch: 20353 \tTraining Loss: 1.541084 \tValidation Loss: 2.403466\n",
      "Epoch: 20354 \tTraining Loss: 1.584648 \tValidation Loss: 2.402851\n",
      "Epoch: 20355 \tTraining Loss: 1.573478 \tValidation Loss: 2.403053\n",
      "Epoch: 20356 \tTraining Loss: 1.532378 \tValidation Loss: 2.403445\n",
      "Epoch: 20357 \tTraining Loss: 1.510820 \tValidation Loss: 2.404183\n",
      "Epoch: 20358 \tTraining Loss: 1.522820 \tValidation Loss: 2.404197\n",
      "Epoch: 20359 \tTraining Loss: 1.573607 \tValidation Loss: 2.403683\n",
      "Epoch: 20360 \tTraining Loss: 1.533431 \tValidation Loss: 2.403475\n",
      "Epoch: 20361 \tTraining Loss: 1.534020 \tValidation Loss: 2.404137\n",
      "Epoch: 20362 \tTraining Loss: 1.564201 \tValidation Loss: 2.403639\n",
      "Epoch: 20363 \tTraining Loss: 1.571171 \tValidation Loss: 2.403258\n",
      "Epoch: 20364 \tTraining Loss: 1.555941 \tValidation Loss: 2.403805\n",
      "Epoch: 20365 \tTraining Loss: 1.560235 \tValidation Loss: 2.404388\n",
      "Epoch: 20366 \tTraining Loss: 1.535523 \tValidation Loss: 2.403609\n",
      "Epoch: 20367 \tTraining Loss: 1.562228 \tValidation Loss: 2.404242\n",
      "Epoch: 20368 \tTraining Loss: 1.556860 \tValidation Loss: 2.403731\n",
      "Epoch: 20369 \tTraining Loss: 1.525366 \tValidation Loss: 2.403978\n",
      "Epoch: 20370 \tTraining Loss: 1.530763 \tValidation Loss: 2.404017\n",
      "Epoch: 20371 \tTraining Loss: 1.590840 \tValidation Loss: 2.403748\n",
      "Epoch: 20372 \tTraining Loss: 1.531716 \tValidation Loss: 2.404043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20373 \tTraining Loss: 1.499046 \tValidation Loss: 2.403989\n",
      "Epoch: 20374 \tTraining Loss: 1.565592 \tValidation Loss: 2.404100\n",
      "Epoch: 20375 \tTraining Loss: 1.570004 \tValidation Loss: 2.403987\n",
      "Epoch: 20376 \tTraining Loss: 1.554193 \tValidation Loss: 2.404006\n",
      "Epoch: 20377 \tTraining Loss: 1.560622 \tValidation Loss: 2.403861\n",
      "Epoch: 20378 \tTraining Loss: 1.582123 \tValidation Loss: 2.403692\n",
      "Epoch: 20379 \tTraining Loss: 1.532473 \tValidation Loss: 2.403296\n",
      "Epoch: 20380 \tTraining Loss: 1.568536 \tValidation Loss: 2.403431\n",
      "Epoch: 20381 \tTraining Loss: 1.537336 \tValidation Loss: 2.403767\n",
      "Epoch: 20382 \tTraining Loss: 1.583124 \tValidation Loss: 2.402895\n",
      "Epoch: 20383 \tTraining Loss: 1.520250 \tValidation Loss: 2.403547\n",
      "Epoch: 20384 \tTraining Loss: 1.559669 \tValidation Loss: 2.403731\n",
      "Epoch: 20385 \tTraining Loss: 1.580144 \tValidation Loss: 2.403529\n",
      "Epoch: 20386 \tTraining Loss: 1.546200 \tValidation Loss: 2.403980\n",
      "Epoch: 20387 \tTraining Loss: 1.560601 \tValidation Loss: 2.403320\n",
      "Epoch: 20388 \tTraining Loss: 1.536237 \tValidation Loss: 2.403712\n",
      "Epoch: 20389 \tTraining Loss: 1.568888 \tValidation Loss: 2.402858\n",
      "Epoch: 20390 \tTraining Loss: 1.542431 \tValidation Loss: 2.404236\n",
      "Epoch: 20391 \tTraining Loss: 1.577685 \tValidation Loss: 2.403909\n",
      "Epoch: 20392 \tTraining Loss: 1.581642 \tValidation Loss: 2.403515\n",
      "Epoch: 20393 \tTraining Loss: 1.540030 \tValidation Loss: 2.403508\n",
      "Epoch: 20394 \tTraining Loss: 1.586729 \tValidation Loss: 2.403308\n",
      "Epoch: 20395 \tTraining Loss: 1.575315 \tValidation Loss: 2.402886\n",
      "Epoch: 20396 \tTraining Loss: 1.567491 \tValidation Loss: 2.403586\n",
      "Epoch: 20397 \tTraining Loss: 1.579483 \tValidation Loss: 2.403202\n",
      "Epoch: 20398 \tTraining Loss: 1.544063 \tValidation Loss: 2.403754\n",
      "Epoch: 20399 \tTraining Loss: 1.501320 \tValidation Loss: 2.404262\n",
      "Epoch: 20400 \tTraining Loss: 1.554679 \tValidation Loss: 2.403449\n",
      "Epoch: 20401 \tTraining Loss: 1.563138 \tValidation Loss: 2.403727\n",
      "Epoch: 20402 \tTraining Loss: 1.578812 \tValidation Loss: 2.403872\n",
      "Epoch: 20403 \tTraining Loss: 1.540463 \tValidation Loss: 2.403801\n",
      "Epoch: 20404 \tTraining Loss: 1.525543 \tValidation Loss: 2.404541\n",
      "Epoch: 20405 \tTraining Loss: 1.609604 \tValidation Loss: 2.404819\n",
      "Epoch: 20406 \tTraining Loss: 1.554731 \tValidation Loss: 2.403515\n",
      "Epoch: 20407 \tTraining Loss: 1.562279 \tValidation Loss: 2.403964\n",
      "Epoch: 20408 \tTraining Loss: 1.505226 \tValidation Loss: 2.404669\n",
      "Epoch: 20409 \tTraining Loss: 1.590283 \tValidation Loss: 2.403689\n",
      "Epoch: 20410 \tTraining Loss: 1.528221 \tValidation Loss: 2.405074\n",
      "Epoch: 20411 \tTraining Loss: 1.532975 \tValidation Loss: 2.404693\n",
      "Epoch: 20412 \tTraining Loss: 1.537458 \tValidation Loss: 2.404559\n",
      "Epoch: 20413 \tTraining Loss: 1.523677 \tValidation Loss: 2.403770\n",
      "Epoch: 20414 \tTraining Loss: 1.569148 \tValidation Loss: 2.404324\n",
      "Epoch: 20415 \tTraining Loss: 1.566421 \tValidation Loss: 2.404103\n",
      "Epoch: 20416 \tTraining Loss: 1.557810 \tValidation Loss: 2.404376\n",
      "Epoch: 20417 \tTraining Loss: 1.544897 \tValidation Loss: 2.404370\n",
      "Epoch: 20418 \tTraining Loss: 1.531878 \tValidation Loss: 2.404568\n",
      "Epoch: 20419 \tTraining Loss: 1.535739 \tValidation Loss: 2.403999\n",
      "Epoch: 20420 \tTraining Loss: 1.560300 \tValidation Loss: 2.404320\n",
      "Epoch: 20421 \tTraining Loss: 1.568511 \tValidation Loss: 2.404369\n",
      "Epoch: 20422 \tTraining Loss: 1.509706 \tValidation Loss: 2.404133\n",
      "Epoch: 20423 \tTraining Loss: 1.569619 \tValidation Loss: 2.404468\n",
      "Epoch: 20424 \tTraining Loss: 1.555064 \tValidation Loss: 2.404390\n",
      "Epoch: 20425 \tTraining Loss: 1.567132 \tValidation Loss: 2.403937\n",
      "Epoch: 20426 \tTraining Loss: 1.564001 \tValidation Loss: 2.403184\n",
      "Epoch: 20427 \tTraining Loss: 1.564685 \tValidation Loss: 2.404807\n",
      "Epoch: 20428 \tTraining Loss: 1.534927 \tValidation Loss: 2.403171\n",
      "Epoch: 20429 \tTraining Loss: 1.585405 \tValidation Loss: 2.403494\n",
      "Epoch: 20430 \tTraining Loss: 1.551657 \tValidation Loss: 2.404426\n",
      "Epoch: 20431 \tTraining Loss: 1.552240 \tValidation Loss: 2.404210\n",
      "Epoch: 20432 \tTraining Loss: 1.531574 \tValidation Loss: 2.404099\n",
      "Epoch: 20433 \tTraining Loss: 1.625669 \tValidation Loss: 2.403727\n",
      "Epoch: 20434 \tTraining Loss: 1.534811 \tValidation Loss: 2.404522\n",
      "Epoch: 20435 \tTraining Loss: 1.556689 \tValidation Loss: 2.404653\n",
      "Epoch: 20436 \tTraining Loss: 1.531099 \tValidation Loss: 2.404588\n",
      "Epoch: 20437 \tTraining Loss: 1.550821 \tValidation Loss: 2.404336\n",
      "Epoch: 20438 \tTraining Loss: 1.532171 \tValidation Loss: 2.404938\n",
      "Epoch: 20439 \tTraining Loss: 1.557983 \tValidation Loss: 2.404773\n",
      "Epoch: 20440 \tTraining Loss: 1.549825 \tValidation Loss: 2.403563\n",
      "Epoch: 20441 \tTraining Loss: 1.555686 \tValidation Loss: 2.404213\n",
      "Epoch: 20442 \tTraining Loss: 1.546330 \tValidation Loss: 2.403825\n",
      "Epoch: 20443 \tTraining Loss: 1.559786 \tValidation Loss: 2.403693\n",
      "Epoch: 20444 \tTraining Loss: 1.568651 \tValidation Loss: 2.403736\n",
      "Epoch: 20445 \tTraining Loss: 1.544998 \tValidation Loss: 2.404460\n",
      "Epoch: 20446 \tTraining Loss: 1.534663 \tValidation Loss: 2.404787\n",
      "Epoch: 20447 \tTraining Loss: 1.547163 \tValidation Loss: 2.404466\n",
      "Epoch: 20448 \tTraining Loss: 1.575824 \tValidation Loss: 2.404629\n",
      "Epoch: 20449 \tTraining Loss: 1.559832 \tValidation Loss: 2.404398\n",
      "Epoch: 20450 \tTraining Loss: 1.568932 \tValidation Loss: 2.403760\n",
      "Epoch: 20451 \tTraining Loss: 1.527968 \tValidation Loss: 2.404487\n",
      "Epoch: 20452 \tTraining Loss: 1.517760 \tValidation Loss: 2.405487\n",
      "Epoch: 20453 \tTraining Loss: 1.544023 \tValidation Loss: 2.404513\n",
      "Epoch: 20454 \tTraining Loss: 1.558556 \tValidation Loss: 2.405284\n",
      "Epoch: 20455 \tTraining Loss: 1.526787 \tValidation Loss: 2.405659\n",
      "Epoch: 20456 \tTraining Loss: 1.567330 \tValidation Loss: 2.406061\n",
      "Epoch: 20457 \tTraining Loss: 1.549528 \tValidation Loss: 2.404897\n",
      "Epoch: 20458 \tTraining Loss: 1.508966 \tValidation Loss: 2.405323\n",
      "Epoch: 20459 \tTraining Loss: 1.560582 \tValidation Loss: 2.405021\n",
      "Epoch: 20460 \tTraining Loss: 1.550161 \tValidation Loss: 2.404989\n",
      "Epoch: 20461 \tTraining Loss: 1.522397 \tValidation Loss: 2.405166\n",
      "Epoch: 20462 \tTraining Loss: 1.571400 \tValidation Loss: 2.404919\n",
      "Epoch: 20463 \tTraining Loss: 1.551808 \tValidation Loss: 2.405365\n",
      "Epoch: 20464 \tTraining Loss: 1.571612 \tValidation Loss: 2.404633\n",
      "Epoch: 20465 \tTraining Loss: 1.511054 \tValidation Loss: 2.404930\n",
      "Epoch: 20466 \tTraining Loss: 1.546486 \tValidation Loss: 2.404813\n",
      "Epoch: 20467 \tTraining Loss: 1.558292 \tValidation Loss: 2.404410\n",
      "Epoch: 20468 \tTraining Loss: 1.550435 \tValidation Loss: 2.403930\n",
      "Epoch: 20469 \tTraining Loss: 1.549400 \tValidation Loss: 2.404366\n",
      "Epoch: 20470 \tTraining Loss: 1.542329 \tValidation Loss: 2.404415\n",
      "Epoch: 20471 \tTraining Loss: 1.534334 \tValidation Loss: 2.404936\n",
      "Epoch: 20472 \tTraining Loss: 1.543749 \tValidation Loss: 2.404680\n",
      "Epoch: 20473 \tTraining Loss: 1.493815 \tValidation Loss: 2.405221\n",
      "Epoch: 20474 \tTraining Loss: 1.591756 \tValidation Loss: 2.403900\n",
      "Epoch: 20475 \tTraining Loss: 1.577361 \tValidation Loss: 2.404432\n",
      "Epoch: 20476 \tTraining Loss: 1.555173 \tValidation Loss: 2.404518\n",
      "Epoch: 20477 \tTraining Loss: 1.542858 \tValidation Loss: 2.404591\n",
      "Epoch: 20478 \tTraining Loss: 1.538408 \tValidation Loss: 2.404930\n",
      "Epoch: 20479 \tTraining Loss: 1.543398 \tValidation Loss: 2.405140\n",
      "Epoch: 20480 \tTraining Loss: 1.526666 \tValidation Loss: 2.405492\n",
      "Epoch: 20481 \tTraining Loss: 1.519552 \tValidation Loss: 2.405672\n",
      "Epoch: 20482 \tTraining Loss: 1.584507 \tValidation Loss: 2.404475\n",
      "Epoch: 20483 \tTraining Loss: 1.511681 \tValidation Loss: 2.405435\n",
      "Epoch: 20484 \tTraining Loss: 1.537957 \tValidation Loss: 2.405645\n",
      "Epoch: 20485 \tTraining Loss: 1.556866 \tValidation Loss: 2.405312\n",
      "Epoch: 20486 \tTraining Loss: 1.556092 \tValidation Loss: 2.404819\n",
      "Epoch: 20487 \tTraining Loss: 1.540467 \tValidation Loss: 2.404692\n",
      "Epoch: 20488 \tTraining Loss: 1.569072 \tValidation Loss: 2.404760\n",
      "Epoch: 20489 \tTraining Loss: 1.526501 \tValidation Loss: 2.405358\n",
      "Epoch: 20490 \tTraining Loss: 1.523953 \tValidation Loss: 2.405022\n",
      "Epoch: 20491 \tTraining Loss: 1.556447 \tValidation Loss: 2.405405\n",
      "Epoch: 20492 \tTraining Loss: 1.550293 \tValidation Loss: 2.404373\n",
      "Epoch: 20493 \tTraining Loss: 1.524285 \tValidation Loss: 2.404348\n",
      "Epoch: 20494 \tTraining Loss: 1.534747 \tValidation Loss: 2.405253\n",
      "Epoch: 20495 \tTraining Loss: 1.543311 \tValidation Loss: 2.405856\n",
      "Epoch: 20496 \tTraining Loss: 1.560588 \tValidation Loss: 2.404573\n",
      "Epoch: 20497 \tTraining Loss: 1.575904 \tValidation Loss: 2.405039\n",
      "Epoch: 20498 \tTraining Loss: 1.499769 \tValidation Loss: 2.404594\n",
      "Epoch: 20499 \tTraining Loss: 1.550900 \tValidation Loss: 2.405349\n",
      "Epoch: 20500 \tTraining Loss: 1.569530 \tValidation Loss: 2.405185\n",
      "Epoch: 20501 \tTraining Loss: 1.568376 \tValidation Loss: 2.403350\n",
      "Epoch: 20502 \tTraining Loss: 1.528844 \tValidation Loss: 2.404330\n",
      "Epoch: 20503 \tTraining Loss: 1.545851 \tValidation Loss: 2.404564\n",
      "Epoch: 20504 \tTraining Loss: 1.550495 \tValidation Loss: 2.404703\n",
      "Epoch: 20505 \tTraining Loss: 1.544766 \tValidation Loss: 2.405941\n",
      "Epoch: 20506 \tTraining Loss: 1.537367 \tValidation Loss: 2.405793\n",
      "Epoch: 20507 \tTraining Loss: 1.495442 \tValidation Loss: 2.406013\n",
      "Epoch: 20508 \tTraining Loss: 1.540562 \tValidation Loss: 2.405932\n",
      "Epoch: 20509 \tTraining Loss: 1.569310 \tValidation Loss: 2.405660\n",
      "Epoch: 20510 \tTraining Loss: 1.538312 \tValidation Loss: 2.405316\n",
      "Epoch: 20511 \tTraining Loss: 1.555544 \tValidation Loss: 2.404881\n",
      "Epoch: 20512 \tTraining Loss: 1.490708 \tValidation Loss: 2.404947\n",
      "Epoch: 20513 \tTraining Loss: 1.558996 \tValidation Loss: 2.406002\n",
      "Epoch: 20514 \tTraining Loss: 1.559384 \tValidation Loss: 2.405848\n",
      "Epoch: 20515 \tTraining Loss: 1.537839 \tValidation Loss: 2.406621\n",
      "Epoch: 20516 \tTraining Loss: 1.525572 \tValidation Loss: 2.405880\n",
      "Epoch: 20517 \tTraining Loss: 1.578680 \tValidation Loss: 2.405164\n",
      "Epoch: 20518 \tTraining Loss: 1.498792 \tValidation Loss: 2.405578\n",
      "Epoch: 20519 \tTraining Loss: 1.554177 \tValidation Loss: 2.405768\n",
      "Epoch: 20520 \tTraining Loss: 1.524181 \tValidation Loss: 2.405947\n",
      "Epoch: 20521 \tTraining Loss: 1.531784 \tValidation Loss: 2.406110\n",
      "Epoch: 20522 \tTraining Loss: 1.549545 \tValidation Loss: 2.406241\n",
      "Epoch: 20523 \tTraining Loss: 1.483615 \tValidation Loss: 2.405773\n",
      "Epoch: 20524 \tTraining Loss: 1.565401 \tValidation Loss: 2.406267\n",
      "Epoch: 20525 \tTraining Loss: 1.537505 \tValidation Loss: 2.406159\n",
      "Epoch: 20526 \tTraining Loss: 1.577062 \tValidation Loss: 2.405839\n",
      "Epoch: 20527 \tTraining Loss: 1.545297 \tValidation Loss: 2.404821\n",
      "Epoch: 20528 \tTraining Loss: 1.579436 \tValidation Loss: 2.405188\n",
      "Epoch: 20529 \tTraining Loss: 1.560205 \tValidation Loss: 2.404811\n",
      "Epoch: 20530 \tTraining Loss: 1.540258 \tValidation Loss: 2.405799\n",
      "Epoch: 20531 \tTraining Loss: 1.515771 \tValidation Loss: 2.405723\n",
      "Epoch: 20532 \tTraining Loss: 1.541890 \tValidation Loss: 2.406290\n",
      "Epoch: 20533 \tTraining Loss: 1.535808 \tValidation Loss: 2.405887\n",
      "Epoch: 20534 \tTraining Loss: 1.548195 \tValidation Loss: 2.406075\n",
      "Epoch: 20535 \tTraining Loss: 1.531122 \tValidation Loss: 2.406250\n",
      "Epoch: 20536 \tTraining Loss: 1.541370 \tValidation Loss: 2.406618\n",
      "Epoch: 20537 \tTraining Loss: 1.563094 \tValidation Loss: 2.405628\n",
      "Epoch: 20538 \tTraining Loss: 1.561674 \tValidation Loss: 2.405916\n",
      "Epoch: 20539 \tTraining Loss: 1.545437 \tValidation Loss: 2.406153\n",
      "Epoch: 20540 \tTraining Loss: 1.561155 \tValidation Loss: 2.405507\n",
      "Epoch: 20541 \tTraining Loss: 1.553608 \tValidation Loss: 2.405668\n",
      "Epoch: 20542 \tTraining Loss: 1.518938 \tValidation Loss: 2.406677\n",
      "Epoch: 20543 \tTraining Loss: 1.550441 \tValidation Loss: 2.405581\n",
      "Epoch: 20544 \tTraining Loss: 1.578909 \tValidation Loss: 2.405538\n",
      "Epoch: 20545 \tTraining Loss: 1.546985 \tValidation Loss: 2.405890\n",
      "Epoch: 20546 \tTraining Loss: 1.539298 \tValidation Loss: 2.406535\n",
      "Epoch: 20547 \tTraining Loss: 1.523196 \tValidation Loss: 2.405860\n",
      "Epoch: 20548 \tTraining Loss: 1.552210 \tValidation Loss: 2.405170\n",
      "Epoch: 20549 \tTraining Loss: 1.522331 \tValidation Loss: 2.406286\n",
      "Epoch: 20550 \tTraining Loss: 1.553742 \tValidation Loss: 2.405592\n",
      "Epoch: 20551 \tTraining Loss: 1.578847 \tValidation Loss: 2.405769\n",
      "Epoch: 20552 \tTraining Loss: 1.542845 \tValidation Loss: 2.406216\n",
      "Epoch: 20553 \tTraining Loss: 1.596954 \tValidation Loss: 2.404811\n",
      "Epoch: 20554 \tTraining Loss: 1.545592 \tValidation Loss: 2.405535\n",
      "Epoch: 20555 \tTraining Loss: 1.574228 \tValidation Loss: 2.405705\n",
      "Epoch: 20556 \tTraining Loss: 1.528740 \tValidation Loss: 2.406380\n",
      "Epoch: 20557 \tTraining Loss: 1.564699 \tValidation Loss: 2.405693\n",
      "Epoch: 20558 \tTraining Loss: 1.480081 \tValidation Loss: 2.406583\n",
      "Epoch: 20559 \tTraining Loss: 1.575310 \tValidation Loss: 2.405837\n",
      "Epoch: 20560 \tTraining Loss: 1.552990 \tValidation Loss: 2.405949\n",
      "Epoch: 20561 \tTraining Loss: 1.578437 \tValidation Loss: 2.406110\n",
      "Epoch: 20562 \tTraining Loss: 1.534940 \tValidation Loss: 2.406248\n",
      "Epoch: 20563 \tTraining Loss: 1.529485 \tValidation Loss: 2.406035\n",
      "Epoch: 20564 \tTraining Loss: 1.577098 \tValidation Loss: 2.405460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20565 \tTraining Loss: 1.576537 \tValidation Loss: 2.405725\n",
      "Epoch: 20566 \tTraining Loss: 1.532929 \tValidation Loss: 2.406301\n",
      "Epoch: 20567 \tTraining Loss: 1.510668 \tValidation Loss: 2.406277\n",
      "Epoch: 20568 \tTraining Loss: 1.513178 \tValidation Loss: 2.406870\n",
      "Epoch: 20569 \tTraining Loss: 1.525504 \tValidation Loss: 2.406738\n",
      "Epoch: 20570 \tTraining Loss: 1.530109 \tValidation Loss: 2.407345\n",
      "Epoch: 20571 \tTraining Loss: 1.583656 \tValidation Loss: 2.406195\n",
      "Epoch: 20572 \tTraining Loss: 1.521695 \tValidation Loss: 2.406317\n",
      "Epoch: 20573 \tTraining Loss: 1.471539 \tValidation Loss: 2.407013\n",
      "Epoch: 20574 \tTraining Loss: 1.567423 \tValidation Loss: 2.407074\n",
      "Epoch: 20575 \tTraining Loss: 1.526245 \tValidation Loss: 2.407300\n",
      "Epoch: 20576 \tTraining Loss: 1.536741 \tValidation Loss: 2.407605\n",
      "Epoch: 20577 \tTraining Loss: 1.553858 \tValidation Loss: 2.406208\n",
      "Epoch: 20578 \tTraining Loss: 1.529116 \tValidation Loss: 2.406555\n",
      "Epoch: 20579 \tTraining Loss: 1.527970 \tValidation Loss: 2.406345\n",
      "Epoch: 20580 \tTraining Loss: 1.538434 \tValidation Loss: 2.406609\n",
      "Epoch: 20581 \tTraining Loss: 1.539475 \tValidation Loss: 2.406663\n",
      "Epoch: 20582 \tTraining Loss: 1.573623 \tValidation Loss: 2.406522\n",
      "Epoch: 20583 \tTraining Loss: 1.542978 \tValidation Loss: 2.406355\n",
      "Epoch: 20584 \tTraining Loss: 1.556476 \tValidation Loss: 2.406373\n",
      "Epoch: 20585 \tTraining Loss: 1.504407 \tValidation Loss: 2.406707\n",
      "Epoch: 20586 \tTraining Loss: 1.569960 \tValidation Loss: 2.407007\n",
      "Epoch: 20587 \tTraining Loss: 1.530356 \tValidation Loss: 2.407425\n",
      "Epoch: 20588 \tTraining Loss: 1.538741 \tValidation Loss: 2.406577\n",
      "Epoch: 20589 \tTraining Loss: 1.550926 \tValidation Loss: 2.406418\n",
      "Epoch: 20590 \tTraining Loss: 1.540102 \tValidation Loss: 2.405678\n",
      "Epoch: 20591 \tTraining Loss: 1.635865 \tValidation Loss: 2.405569\n",
      "Epoch: 20592 \tTraining Loss: 1.545032 \tValidation Loss: 2.406182\n",
      "Epoch: 20593 \tTraining Loss: 1.524142 \tValidation Loss: 2.406688\n",
      "Epoch: 20594 \tTraining Loss: 1.555580 \tValidation Loss: 2.406662\n",
      "Epoch: 20595 \tTraining Loss: 1.546104 \tValidation Loss: 2.405937\n",
      "Epoch: 20596 \tTraining Loss: 1.534016 \tValidation Loss: 2.406010\n",
      "Epoch: 20597 \tTraining Loss: 1.504566 \tValidation Loss: 2.407133\n",
      "Epoch: 20598 \tTraining Loss: 1.546389 \tValidation Loss: 2.406595\n",
      "Epoch: 20599 \tTraining Loss: 1.544411 \tValidation Loss: 2.406887\n",
      "Epoch: 20600 \tTraining Loss: 1.518725 \tValidation Loss: 2.407552\n",
      "Epoch: 20601 \tTraining Loss: 1.571693 \tValidation Loss: 2.407205\n",
      "Epoch: 20602 \tTraining Loss: 1.508818 \tValidation Loss: 2.406955\n",
      "Epoch: 20603 \tTraining Loss: 1.528798 \tValidation Loss: 2.406964\n",
      "Epoch: 20604 \tTraining Loss: 1.540725 \tValidation Loss: 2.406492\n",
      "Epoch: 20605 \tTraining Loss: 1.561086 \tValidation Loss: 2.406498\n",
      "Epoch: 20606 \tTraining Loss: 1.529887 \tValidation Loss: 2.406930\n",
      "Epoch: 20607 \tTraining Loss: 1.578969 \tValidation Loss: 2.405640\n",
      "Epoch: 20608 \tTraining Loss: 1.564705 \tValidation Loss: 2.406767\n",
      "Epoch: 20609 \tTraining Loss: 1.536030 \tValidation Loss: 2.406546\n",
      "Epoch: 20610 \tTraining Loss: 1.562062 \tValidation Loss: 2.406886\n",
      "Epoch: 20611 \tTraining Loss: 1.548364 \tValidation Loss: 2.406441\n",
      "Epoch: 20612 \tTraining Loss: 1.553351 \tValidation Loss: 2.406288\n",
      "Epoch: 20613 \tTraining Loss: 1.539709 \tValidation Loss: 2.405735\n",
      "Epoch: 20614 \tTraining Loss: 1.556235 \tValidation Loss: 2.406510\n",
      "Epoch: 20615 \tTraining Loss: 1.541859 \tValidation Loss: 2.406395\n",
      "Epoch: 20616 \tTraining Loss: 1.510598 \tValidation Loss: 2.406620\n",
      "Epoch: 20617 \tTraining Loss: 1.544085 \tValidation Loss: 2.406479\n",
      "Epoch: 20618 \tTraining Loss: 1.616804 \tValidation Loss: 2.406102\n",
      "Epoch: 20619 \tTraining Loss: 1.528608 \tValidation Loss: 2.406880\n",
      "Epoch: 20620 \tTraining Loss: 1.526066 \tValidation Loss: 2.407065\n",
      "Epoch: 20621 \tTraining Loss: 1.542099 \tValidation Loss: 2.407329\n",
      "Epoch: 20622 \tTraining Loss: 1.550324 \tValidation Loss: 2.406282\n",
      "Epoch: 20623 \tTraining Loss: 1.525050 \tValidation Loss: 2.407109\n",
      "Epoch: 20624 \tTraining Loss: 1.528064 \tValidation Loss: 2.406585\n",
      "Epoch: 20625 \tTraining Loss: 1.544457 \tValidation Loss: 2.406652\n",
      "Epoch: 20626 \tTraining Loss: 1.582600 \tValidation Loss: 2.406787\n",
      "Epoch: 20627 \tTraining Loss: 1.564546 \tValidation Loss: 2.405763\n",
      "Epoch: 20628 \tTraining Loss: 1.523192 \tValidation Loss: 2.407087\n",
      "Epoch: 20629 \tTraining Loss: 1.554307 \tValidation Loss: 2.406468\n",
      "Epoch: 20630 \tTraining Loss: 1.513072 \tValidation Loss: 2.406807\n",
      "Epoch: 20631 \tTraining Loss: 1.534552 \tValidation Loss: 2.406896\n",
      "Epoch: 20632 \tTraining Loss: 1.578024 \tValidation Loss: 2.406280\n",
      "Epoch: 20633 \tTraining Loss: 1.534117 \tValidation Loss: 2.407481\n",
      "Epoch: 20634 \tTraining Loss: 1.570027 \tValidation Loss: 2.405844\n",
      "Epoch: 20635 \tTraining Loss: 1.550307 \tValidation Loss: 2.406216\n",
      "Epoch: 20636 \tTraining Loss: 1.542307 \tValidation Loss: 2.406020\n",
      "Epoch: 20637 \tTraining Loss: 1.561470 \tValidation Loss: 2.406581\n",
      "Epoch: 20638 \tTraining Loss: 1.540200 \tValidation Loss: 2.406621\n",
      "Epoch: 20639 \tTraining Loss: 1.537335 \tValidation Loss: 2.406419\n",
      "Epoch: 20640 \tTraining Loss: 1.569458 \tValidation Loss: 2.405807\n",
      "Epoch: 20641 \tTraining Loss: 1.540707 \tValidation Loss: 2.405994\n",
      "Epoch: 20642 \tTraining Loss: 1.534082 \tValidation Loss: 2.406563\n",
      "Epoch: 20643 \tTraining Loss: 1.537242 \tValidation Loss: 2.405869\n",
      "Epoch: 20644 \tTraining Loss: 1.569256 \tValidation Loss: 2.405445\n",
      "Epoch: 20645 \tTraining Loss: 1.534381 \tValidation Loss: 2.406247\n",
      "Epoch: 20646 \tTraining Loss: 1.497816 \tValidation Loss: 2.406549\n",
      "Epoch: 20647 \tTraining Loss: 1.562894 \tValidation Loss: 2.406461\n",
      "Epoch: 20648 \tTraining Loss: 1.556572 \tValidation Loss: 2.406897\n",
      "Epoch: 20649 \tTraining Loss: 1.520257 \tValidation Loss: 2.408068\n",
      "Epoch: 20650 \tTraining Loss: 1.542773 \tValidation Loss: 2.407689\n",
      "Epoch: 20651 \tTraining Loss: 1.498582 \tValidation Loss: 2.407485\n",
      "Epoch: 20652 \tTraining Loss: 1.536057 \tValidation Loss: 2.407139\n",
      "Epoch: 20653 \tTraining Loss: 1.523983 \tValidation Loss: 2.407555\n",
      "Epoch: 20654 \tTraining Loss: 1.548648 \tValidation Loss: 2.408115\n",
      "Epoch: 20655 \tTraining Loss: 1.554982 \tValidation Loss: 2.406731\n",
      "Epoch: 20656 \tTraining Loss: 1.563785 \tValidation Loss: 2.406825\n",
      "Epoch: 20657 \tTraining Loss: 1.516350 \tValidation Loss: 2.407112\n",
      "Epoch: 20658 \tTraining Loss: 1.535610 \tValidation Loss: 2.407746\n",
      "Epoch: 20659 \tTraining Loss: 1.536964 \tValidation Loss: 2.406871\n",
      "Epoch: 20660 \tTraining Loss: 1.523550 \tValidation Loss: 2.408071\n",
      "Epoch: 20661 \tTraining Loss: 1.572062 \tValidation Loss: 2.407588\n",
      "Epoch: 20662 \tTraining Loss: 1.559115 \tValidation Loss: 2.406815\n",
      "Epoch: 20663 \tTraining Loss: 1.525760 \tValidation Loss: 2.407253\n",
      "Epoch: 20664 \tTraining Loss: 1.514755 \tValidation Loss: 2.406842\n",
      "Epoch: 20665 \tTraining Loss: 1.519513 \tValidation Loss: 2.407274\n",
      "Epoch: 20666 \tTraining Loss: 1.531807 \tValidation Loss: 2.406752\n",
      "Epoch: 20667 \tTraining Loss: 1.562036 \tValidation Loss: 2.407277\n",
      "Epoch: 20668 \tTraining Loss: 1.581677 \tValidation Loss: 2.406779\n",
      "Epoch: 20669 \tTraining Loss: 1.540510 \tValidation Loss: 2.406930\n",
      "Epoch: 20670 \tTraining Loss: 1.518703 \tValidation Loss: 2.406510\n",
      "Epoch: 20671 \tTraining Loss: 1.558675 \tValidation Loss: 2.406411\n",
      "Epoch: 20672 \tTraining Loss: 1.520665 \tValidation Loss: 2.406925\n",
      "Epoch: 20673 \tTraining Loss: 1.537903 \tValidation Loss: 2.407370\n",
      "Epoch: 20674 \tTraining Loss: 1.554779 \tValidation Loss: 2.406747\n",
      "Epoch: 20675 \tTraining Loss: 1.577861 \tValidation Loss: 2.407411\n",
      "Epoch: 20676 \tTraining Loss: 1.534910 \tValidation Loss: 2.407315\n",
      "Epoch: 20677 \tTraining Loss: 1.520778 \tValidation Loss: 2.407608\n",
      "Epoch: 20678 \tTraining Loss: 1.526360 \tValidation Loss: 2.407578\n",
      "Epoch: 20679 \tTraining Loss: 1.536349 \tValidation Loss: 2.407534\n",
      "Epoch: 20680 \tTraining Loss: 1.531101 \tValidation Loss: 2.407746\n",
      "Epoch: 20681 \tTraining Loss: 1.534504 \tValidation Loss: 2.407115\n",
      "Epoch: 20682 \tTraining Loss: 1.538619 \tValidation Loss: 2.407048\n",
      "Epoch: 20683 \tTraining Loss: 1.505954 \tValidation Loss: 2.407975\n",
      "Epoch: 20684 \tTraining Loss: 1.517498 \tValidation Loss: 2.408403\n",
      "Epoch: 20685 \tTraining Loss: 1.576571 \tValidation Loss: 2.407541\n",
      "Epoch: 20686 \tTraining Loss: 1.550649 \tValidation Loss: 2.407277\n",
      "Epoch: 20687 \tTraining Loss: 1.479806 \tValidation Loss: 2.407931\n",
      "Epoch: 20688 \tTraining Loss: 1.555049 \tValidation Loss: 2.407854\n",
      "Epoch: 20689 \tTraining Loss: 1.564811 \tValidation Loss: 2.407542\n",
      "Epoch: 20690 \tTraining Loss: 1.551388 \tValidation Loss: 2.407525\n",
      "Epoch: 20691 \tTraining Loss: 1.555454 \tValidation Loss: 2.407984\n",
      "Epoch: 20692 \tTraining Loss: 1.572611 \tValidation Loss: 2.406862\n",
      "Epoch: 20693 \tTraining Loss: 1.510187 \tValidation Loss: 2.407452\n",
      "Epoch: 20694 \tTraining Loss: 1.535441 \tValidation Loss: 2.408106\n",
      "Epoch: 20695 \tTraining Loss: 1.509325 \tValidation Loss: 2.408195\n",
      "Epoch: 20696 \tTraining Loss: 1.531022 \tValidation Loss: 2.408029\n",
      "Epoch: 20697 \tTraining Loss: 1.566661 \tValidation Loss: 2.407612\n",
      "Epoch: 20698 \tTraining Loss: 1.504577 \tValidation Loss: 2.407631\n",
      "Epoch: 20699 \tTraining Loss: 1.530123 \tValidation Loss: 2.408228\n",
      "Epoch: 20700 \tTraining Loss: 1.505491 \tValidation Loss: 2.408242\n",
      "Epoch: 20701 \tTraining Loss: 1.544243 \tValidation Loss: 2.408482\n",
      "Epoch: 20702 \tTraining Loss: 1.533880 \tValidation Loss: 2.408289\n",
      "Epoch: 20703 \tTraining Loss: 1.519715 \tValidation Loss: 2.408803\n",
      "Epoch: 20704 \tTraining Loss: 1.571477 \tValidation Loss: 2.408190\n",
      "Epoch: 20705 \tTraining Loss: 1.571272 \tValidation Loss: 2.408008\n",
      "Epoch: 20706 \tTraining Loss: 1.516147 \tValidation Loss: 2.408668\n",
      "Epoch: 20707 \tTraining Loss: 1.536029 \tValidation Loss: 2.408253\n",
      "Epoch: 20708 \tTraining Loss: 1.546841 \tValidation Loss: 2.407562\n",
      "Epoch: 20709 \tTraining Loss: 1.560881 \tValidation Loss: 2.407448\n",
      "Epoch: 20710 \tTraining Loss: 1.518557 \tValidation Loss: 2.407597\n",
      "Epoch: 20711 \tTraining Loss: 1.539917 \tValidation Loss: 2.407994\n",
      "Epoch: 20712 \tTraining Loss: 1.546684 \tValidation Loss: 2.407592\n",
      "Epoch: 20713 \tTraining Loss: 1.509349 \tValidation Loss: 2.408021\n",
      "Epoch: 20714 \tTraining Loss: 1.540994 \tValidation Loss: 2.407313\n",
      "Epoch: 20715 \tTraining Loss: 1.574721 \tValidation Loss: 2.407556\n",
      "Epoch: 20716 \tTraining Loss: 1.548944 \tValidation Loss: 2.407997\n",
      "Epoch: 20717 \tTraining Loss: 1.580176 \tValidation Loss: 2.406610\n",
      "Epoch: 20718 \tTraining Loss: 1.538411 \tValidation Loss: 2.406740\n",
      "Epoch: 20719 \tTraining Loss: 1.528911 \tValidation Loss: 2.408031\n",
      "Epoch: 20720 \tTraining Loss: 1.541276 \tValidation Loss: 2.407729\n",
      "Epoch: 20721 \tTraining Loss: 1.536319 \tValidation Loss: 2.407156\n",
      "Epoch: 20722 \tTraining Loss: 1.585673 \tValidation Loss: 2.406433\n",
      "Epoch: 20723 \tTraining Loss: 1.535587 \tValidation Loss: 2.407633\n",
      "Epoch: 20724 \tTraining Loss: 1.529136 \tValidation Loss: 2.408313\n",
      "Epoch: 20725 \tTraining Loss: 1.534221 \tValidation Loss: 2.408079\n",
      "Epoch: 20726 \tTraining Loss: 1.502801 \tValidation Loss: 2.408745\n",
      "Epoch: 20727 \tTraining Loss: 1.565216 \tValidation Loss: 2.407916\n",
      "Epoch: 20728 \tTraining Loss: 1.498884 \tValidation Loss: 2.407774\n",
      "Epoch: 20729 \tTraining Loss: 1.521280 \tValidation Loss: 2.407642\n",
      "Epoch: 20730 \tTraining Loss: 1.518949 \tValidation Loss: 2.407434\n",
      "Epoch: 20731 \tTraining Loss: 1.588025 \tValidation Loss: 2.407261\n",
      "Epoch: 20732 \tTraining Loss: 1.518855 \tValidation Loss: 2.408064\n",
      "Epoch: 20733 \tTraining Loss: 1.536430 \tValidation Loss: 2.408626\n",
      "Epoch: 20734 \tTraining Loss: 1.508638 \tValidation Loss: 2.408571\n",
      "Epoch: 20735 \tTraining Loss: 1.567006 \tValidation Loss: 2.408397\n",
      "Epoch: 20736 \tTraining Loss: 1.554367 \tValidation Loss: 2.407734\n",
      "Epoch: 20737 \tTraining Loss: 1.521101 \tValidation Loss: 2.408116\n",
      "Epoch: 20738 \tTraining Loss: 1.570844 \tValidation Loss: 2.407987\n",
      "Epoch: 20739 \tTraining Loss: 1.568306 \tValidation Loss: 2.408095\n",
      "Epoch: 20740 \tTraining Loss: 1.520676 \tValidation Loss: 2.408512\n",
      "Epoch: 20741 \tTraining Loss: 1.528557 \tValidation Loss: 2.408854\n",
      "Epoch: 20742 \tTraining Loss: 1.521457 \tValidation Loss: 2.408598\n",
      "Epoch: 20743 \tTraining Loss: 1.551288 \tValidation Loss: 2.407641\n",
      "Epoch: 20744 \tTraining Loss: 1.578776 \tValidation Loss: 2.408905\n",
      "Epoch: 20745 \tTraining Loss: 1.563460 \tValidation Loss: 2.408962\n",
      "Epoch: 20746 \tTraining Loss: 1.518298 \tValidation Loss: 2.409420\n",
      "Epoch: 20747 \tTraining Loss: 1.548117 \tValidation Loss: 2.408423\n",
      "Epoch: 20748 \tTraining Loss: 1.574106 \tValidation Loss: 2.408102\n",
      "Epoch: 20749 \tTraining Loss: 1.563832 \tValidation Loss: 2.407537\n",
      "Epoch: 20750 \tTraining Loss: 1.549537 \tValidation Loss: 2.408463\n",
      "Epoch: 20751 \tTraining Loss: 1.532276 \tValidation Loss: 2.408752\n",
      "Epoch: 20752 \tTraining Loss: 1.517560 \tValidation Loss: 2.408391\n",
      "Epoch: 20753 \tTraining Loss: 1.502468 \tValidation Loss: 2.408466\n",
      "Epoch: 20754 \tTraining Loss: 1.542555 \tValidation Loss: 2.409102\n",
      "Epoch: 20755 \tTraining Loss: 1.511503 \tValidation Loss: 2.408528\n",
      "Epoch: 20756 \tTraining Loss: 1.560199 \tValidation Loss: 2.408178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20757 \tTraining Loss: 1.531726 \tValidation Loss: 2.407692\n",
      "Epoch: 20758 \tTraining Loss: 1.560515 \tValidation Loss: 2.407668\n",
      "Epoch: 20759 \tTraining Loss: 1.526243 \tValidation Loss: 2.408696\n",
      "Epoch: 20760 \tTraining Loss: 1.554336 \tValidation Loss: 2.407623\n",
      "Epoch: 20761 \tTraining Loss: 1.545497 \tValidation Loss: 2.408141\n",
      "Epoch: 20762 \tTraining Loss: 1.543686 \tValidation Loss: 2.407676\n",
      "Epoch: 20763 \tTraining Loss: 1.543276 \tValidation Loss: 2.407615\n",
      "Epoch: 20764 \tTraining Loss: 1.562018 \tValidation Loss: 2.408016\n",
      "Epoch: 20765 \tTraining Loss: 1.498345 \tValidation Loss: 2.408605\n",
      "Epoch: 20766 \tTraining Loss: 1.511202 \tValidation Loss: 2.408782\n",
      "Epoch: 20767 \tTraining Loss: 1.551945 \tValidation Loss: 2.409182\n",
      "Epoch: 20768 \tTraining Loss: 1.552690 \tValidation Loss: 2.408681\n",
      "Epoch: 20769 \tTraining Loss: 1.507769 \tValidation Loss: 2.409050\n",
      "Epoch: 20770 \tTraining Loss: 1.522132 \tValidation Loss: 2.408959\n",
      "Epoch: 20771 \tTraining Loss: 1.549572 \tValidation Loss: 2.407861\n",
      "Epoch: 20772 \tTraining Loss: 1.565676 \tValidation Loss: 2.408811\n",
      "Epoch: 20773 \tTraining Loss: 1.567990 \tValidation Loss: 2.408053\n",
      "Epoch: 20774 \tTraining Loss: 1.510954 \tValidation Loss: 2.407981\n",
      "Epoch: 20775 \tTraining Loss: 1.535780 \tValidation Loss: 2.408350\n",
      "Epoch: 20776 \tTraining Loss: 1.555921 \tValidation Loss: 2.408684\n",
      "Epoch: 20777 \tTraining Loss: 1.514384 \tValidation Loss: 2.408807\n",
      "Epoch: 20778 \tTraining Loss: 1.566302 \tValidation Loss: 2.408607\n",
      "Epoch: 20779 \tTraining Loss: 1.505319 \tValidation Loss: 2.408655\n",
      "Epoch: 20780 \tTraining Loss: 1.527444 \tValidation Loss: 2.408298\n",
      "Epoch: 20781 \tTraining Loss: 1.538068 \tValidation Loss: 2.408848\n",
      "Epoch: 20782 \tTraining Loss: 1.559766 \tValidation Loss: 2.407910\n",
      "Epoch: 20783 \tTraining Loss: 1.538800 \tValidation Loss: 2.408360\n",
      "Epoch: 20784 \tTraining Loss: 1.516886 \tValidation Loss: 2.408046\n",
      "Epoch: 20785 \tTraining Loss: 1.557559 \tValidation Loss: 2.408555\n",
      "Epoch: 20786 \tTraining Loss: 1.536504 \tValidation Loss: 2.408820\n",
      "Epoch: 20787 \tTraining Loss: 1.548899 \tValidation Loss: 2.408354\n",
      "Epoch: 20788 \tTraining Loss: 1.534861 \tValidation Loss: 2.408982\n",
      "Epoch: 20789 \tTraining Loss: 1.537349 \tValidation Loss: 2.408803\n",
      "Epoch: 20790 \tTraining Loss: 1.526388 \tValidation Loss: 2.409170\n",
      "Epoch: 20791 \tTraining Loss: 1.521414 \tValidation Loss: 2.408539\n",
      "Epoch: 20792 \tTraining Loss: 1.514872 \tValidation Loss: 2.409854\n",
      "Epoch: 20793 \tTraining Loss: 1.510204 \tValidation Loss: 2.409189\n",
      "Epoch: 20794 \tTraining Loss: 1.570161 \tValidation Loss: 2.408760\n",
      "Epoch: 20795 \tTraining Loss: 1.569651 \tValidation Loss: 2.408671\n",
      "Epoch: 20796 \tTraining Loss: 1.538422 \tValidation Loss: 2.408602\n",
      "Epoch: 20797 \tTraining Loss: 1.587111 \tValidation Loss: 2.408733\n",
      "Epoch: 20798 \tTraining Loss: 1.521975 \tValidation Loss: 2.408605\n",
      "Epoch: 20799 \tTraining Loss: 1.554635 \tValidation Loss: 2.408434\n",
      "Epoch: 20800 \tTraining Loss: 1.512307 \tValidation Loss: 2.409547\n",
      "Epoch: 20801 \tTraining Loss: 1.522161 \tValidation Loss: 2.409314\n",
      "Epoch: 20802 \tTraining Loss: 1.548169 \tValidation Loss: 2.409739\n",
      "Epoch: 20803 \tTraining Loss: 1.560658 \tValidation Loss: 2.409319\n",
      "Epoch: 20804 \tTraining Loss: 1.536782 \tValidation Loss: 2.409627\n",
      "Epoch: 20805 \tTraining Loss: 1.548054 \tValidation Loss: 2.409042\n",
      "Epoch: 20806 \tTraining Loss: 1.536764 \tValidation Loss: 2.409351\n",
      "Epoch: 20807 \tTraining Loss: 1.548512 \tValidation Loss: 2.409856\n",
      "Epoch: 20808 \tTraining Loss: 1.572914 \tValidation Loss: 2.409361\n",
      "Epoch: 20809 \tTraining Loss: 1.477217 \tValidation Loss: 2.409590\n",
      "Epoch: 20810 \tTraining Loss: 1.553589 \tValidation Loss: 2.409436\n",
      "Epoch: 20811 \tTraining Loss: 1.460150 \tValidation Loss: 2.409768\n",
      "Epoch: 20812 \tTraining Loss: 1.509794 \tValidation Loss: 2.409016\n",
      "Epoch: 20813 \tTraining Loss: 1.527787 \tValidation Loss: 2.409398\n",
      "Epoch: 20814 \tTraining Loss: 1.557128 \tValidation Loss: 2.409612\n",
      "Epoch: 20815 \tTraining Loss: 1.538214 \tValidation Loss: 2.408494\n",
      "Epoch: 20816 \tTraining Loss: 1.493605 \tValidation Loss: 2.408923\n",
      "Epoch: 20817 \tTraining Loss: 1.514892 \tValidation Loss: 2.409525\n",
      "Epoch: 20818 \tTraining Loss: 1.517414 \tValidation Loss: 2.409250\n",
      "Epoch: 20819 \tTraining Loss: 1.515251 \tValidation Loss: 2.409152\n",
      "Epoch: 20820 \tTraining Loss: 1.541301 \tValidation Loss: 2.409414\n",
      "Epoch: 20821 \tTraining Loss: 1.542189 \tValidation Loss: 2.409531\n",
      "Epoch: 20822 \tTraining Loss: 1.556839 \tValidation Loss: 2.408931\n",
      "Epoch: 20823 \tTraining Loss: 1.559903 \tValidation Loss: 2.409001\n",
      "Epoch: 20824 \tTraining Loss: 1.553580 \tValidation Loss: 2.408912\n",
      "Epoch: 20825 \tTraining Loss: 1.538063 \tValidation Loss: 2.409274\n",
      "Epoch: 20826 \tTraining Loss: 1.553401 \tValidation Loss: 2.408614\n",
      "Epoch: 20827 \tTraining Loss: 1.539972 \tValidation Loss: 2.408888\n",
      "Epoch: 20828 \tTraining Loss: 1.505769 \tValidation Loss: 2.409212\n",
      "Epoch: 20829 \tTraining Loss: 1.533893 \tValidation Loss: 2.409037\n",
      "Epoch: 20830 \tTraining Loss: 1.540964 \tValidation Loss: 2.409077\n",
      "Epoch: 20831 \tTraining Loss: 1.532774 \tValidation Loss: 2.409429\n",
      "Epoch: 20832 \tTraining Loss: 1.529909 \tValidation Loss: 2.409175\n",
      "Epoch: 20833 \tTraining Loss: 1.527702 \tValidation Loss: 2.409515\n",
      "Epoch: 20834 \tTraining Loss: 1.554054 \tValidation Loss: 2.409045\n",
      "Epoch: 20835 \tTraining Loss: 1.515053 \tValidation Loss: 2.409858\n",
      "Epoch: 20836 \tTraining Loss: 1.551143 \tValidation Loss: 2.409493\n",
      "Epoch: 20837 \tTraining Loss: 1.473510 \tValidation Loss: 2.410487\n",
      "Epoch: 20838 \tTraining Loss: 1.507762 \tValidation Loss: 2.410151\n",
      "Epoch: 20839 \tTraining Loss: 1.550815 \tValidation Loss: 2.409793\n",
      "Epoch: 20840 \tTraining Loss: 1.505457 \tValidation Loss: 2.409376\n",
      "Epoch: 20841 \tTraining Loss: 1.511318 \tValidation Loss: 2.409654\n",
      "Epoch: 20842 \tTraining Loss: 1.553857 \tValidation Loss: 2.410130\n",
      "Epoch: 20843 \tTraining Loss: 1.563919 \tValidation Loss: 2.410112\n",
      "Epoch: 20844 \tTraining Loss: 1.557307 \tValidation Loss: 2.410340\n",
      "Epoch: 20845 \tTraining Loss: 1.510785 \tValidation Loss: 2.410017\n",
      "Epoch: 20846 \tTraining Loss: 1.552565 \tValidation Loss: 2.408935\n",
      "Epoch: 20847 \tTraining Loss: 1.559317 \tValidation Loss: 2.409182\n",
      "Epoch: 20848 \tTraining Loss: 1.532966 \tValidation Loss: 2.409220\n",
      "Epoch: 20849 \tTraining Loss: 1.509855 \tValidation Loss: 2.409893\n",
      "Epoch: 20850 \tTraining Loss: 1.515843 \tValidation Loss: 2.409710\n",
      "Epoch: 20851 \tTraining Loss: 1.546989 \tValidation Loss: 2.409228\n",
      "Epoch: 20852 \tTraining Loss: 1.596070 \tValidation Loss: 2.409670\n",
      "Epoch: 20853 \tTraining Loss: 1.486668 \tValidation Loss: 2.410016\n",
      "Epoch: 20854 \tTraining Loss: 1.542410 \tValidation Loss: 2.409682\n",
      "Epoch: 20855 \tTraining Loss: 1.555111 \tValidation Loss: 2.409683\n",
      "Epoch: 20856 \tTraining Loss: 1.628763 \tValidation Loss: 2.408766\n",
      "Epoch: 20857 \tTraining Loss: 1.529762 \tValidation Loss: 2.409519\n",
      "Epoch: 20858 \tTraining Loss: 1.536006 \tValidation Loss: 2.409502\n",
      "Epoch: 20859 \tTraining Loss: 1.569005 \tValidation Loss: 2.409637\n",
      "Epoch: 20860 \tTraining Loss: 1.515857 \tValidation Loss: 2.410645\n",
      "Epoch: 20861 \tTraining Loss: 1.533056 \tValidation Loss: 2.409503\n",
      "Epoch: 20862 \tTraining Loss: 1.527316 \tValidation Loss: 2.410026\n",
      "Epoch: 20863 \tTraining Loss: 1.531887 \tValidation Loss: 2.410359\n",
      "Epoch: 20864 \tTraining Loss: 1.534063 \tValidation Loss: 2.410214\n",
      "Epoch: 20865 \tTraining Loss: 1.549627 \tValidation Loss: 2.411155\n",
      "Epoch: 20866 \tTraining Loss: 1.533080 \tValidation Loss: 2.410463\n",
      "Epoch: 20867 \tTraining Loss: 1.550811 \tValidation Loss: 2.410028\n",
      "Epoch: 20868 \tTraining Loss: 1.533096 \tValidation Loss: 2.410659\n",
      "Epoch: 20869 \tTraining Loss: 1.502311 \tValidation Loss: 2.410414\n",
      "Epoch: 20870 \tTraining Loss: 1.536981 \tValidation Loss: 2.410403\n",
      "Epoch: 20871 \tTraining Loss: 1.499462 \tValidation Loss: 2.410313\n",
      "Epoch: 20872 \tTraining Loss: 1.542872 \tValidation Loss: 2.410107\n",
      "Epoch: 20873 \tTraining Loss: 1.492023 \tValidation Loss: 2.410513\n",
      "Epoch: 20874 \tTraining Loss: 1.512650 \tValidation Loss: 2.410752\n",
      "Epoch: 20875 \tTraining Loss: 1.550604 \tValidation Loss: 2.410071\n",
      "Epoch: 20876 \tTraining Loss: 1.529955 \tValidation Loss: 2.410354\n",
      "Epoch: 20877 \tTraining Loss: 1.518889 \tValidation Loss: 2.410292\n",
      "Epoch: 20878 \tTraining Loss: 1.522487 \tValidation Loss: 2.410629\n",
      "Epoch: 20879 \tTraining Loss: 1.588019 \tValidation Loss: 2.408932\n",
      "Epoch: 20880 \tTraining Loss: 1.502237 \tValidation Loss: 2.410218\n",
      "Epoch: 20881 \tTraining Loss: 1.560088 \tValidation Loss: 2.409675\n",
      "Epoch: 20882 \tTraining Loss: 1.522304 \tValidation Loss: 2.410700\n",
      "Epoch: 20883 \tTraining Loss: 1.554423 \tValidation Loss: 2.409583\n",
      "Epoch: 20884 \tTraining Loss: 1.541040 \tValidation Loss: 2.409753\n",
      "Epoch: 20885 \tTraining Loss: 1.516225 \tValidation Loss: 2.409800\n",
      "Epoch: 20886 \tTraining Loss: 1.555866 \tValidation Loss: 2.410047\n",
      "Epoch: 20887 \tTraining Loss: 1.581527 \tValidation Loss: 2.409638\n",
      "Epoch: 20888 \tTraining Loss: 1.537897 \tValidation Loss: 2.411079\n",
      "Epoch: 20889 \tTraining Loss: 1.514995 \tValidation Loss: 2.410970\n",
      "Epoch: 20890 \tTraining Loss: 1.550163 \tValidation Loss: 2.410533\n",
      "Epoch: 20891 \tTraining Loss: 1.487077 \tValidation Loss: 2.412034\n",
      "Epoch: 20892 \tTraining Loss: 1.518917 \tValidation Loss: 2.410942\n",
      "Epoch: 20893 \tTraining Loss: 1.544975 \tValidation Loss: 2.409775\n",
      "Epoch: 20894 \tTraining Loss: 1.550695 \tValidation Loss: 2.409904\n",
      "Epoch: 20895 \tTraining Loss: 1.549719 \tValidation Loss: 2.409353\n",
      "Epoch: 20896 \tTraining Loss: 1.511359 \tValidation Loss: 2.409659\n",
      "Epoch: 20897 \tTraining Loss: 1.525799 \tValidation Loss: 2.409919\n",
      "Epoch: 20898 \tTraining Loss: 1.537157 \tValidation Loss: 2.410364\n",
      "Epoch: 20899 \tTraining Loss: 1.583635 \tValidation Loss: 2.409961\n",
      "Epoch: 20900 \tTraining Loss: 1.546222 \tValidation Loss: 2.410462\n",
      "Epoch: 20901 \tTraining Loss: 1.485617 \tValidation Loss: 2.410768\n",
      "Epoch: 20902 \tTraining Loss: 1.542822 \tValidation Loss: 2.410482\n",
      "Epoch: 20903 \tTraining Loss: 1.544382 \tValidation Loss: 2.409935\n",
      "Epoch: 20904 \tTraining Loss: 1.512878 \tValidation Loss: 2.410156\n",
      "Epoch: 20905 \tTraining Loss: 1.491605 \tValidation Loss: 2.410898\n",
      "Epoch: 20906 \tTraining Loss: 1.554628 \tValidation Loss: 2.410223\n",
      "Epoch: 20907 \tTraining Loss: 1.517513 \tValidation Loss: 2.410464\n",
      "Epoch: 20908 \tTraining Loss: 1.531211 \tValidation Loss: 2.410065\n",
      "Epoch: 20909 \tTraining Loss: 1.506436 \tValidation Loss: 2.410277\n",
      "Epoch: 20910 \tTraining Loss: 1.576631 \tValidation Loss: 2.410165\n",
      "Epoch: 20911 \tTraining Loss: 1.521789 \tValidation Loss: 2.410378\n",
      "Epoch: 20912 \tTraining Loss: 1.516050 \tValidation Loss: 2.411273\n",
      "Epoch: 20913 \tTraining Loss: 1.564363 \tValidation Loss: 2.410746\n",
      "Epoch: 20914 \tTraining Loss: 1.534913 \tValidation Loss: 2.410504\n",
      "Epoch: 20915 \tTraining Loss: 1.530348 \tValidation Loss: 2.410588\n",
      "Epoch: 20916 \tTraining Loss: 1.524819 \tValidation Loss: 2.410808\n",
      "Epoch: 20917 \tTraining Loss: 1.483001 \tValidation Loss: 2.411603\n",
      "Epoch: 20918 \tTraining Loss: 1.566091 \tValidation Loss: 2.409931\n",
      "Epoch: 20919 \tTraining Loss: 1.548697 \tValidation Loss: 2.410028\n",
      "Epoch: 20920 \tTraining Loss: 1.594150 \tValidation Loss: 2.409791\n",
      "Epoch: 20921 \tTraining Loss: 1.498724 \tValidation Loss: 2.411219\n",
      "Epoch: 20922 \tTraining Loss: 1.554050 \tValidation Loss: 2.409802\n",
      "Epoch: 20923 \tTraining Loss: 1.541542 \tValidation Loss: 2.410333\n",
      "Epoch: 20924 \tTraining Loss: 1.466725 \tValidation Loss: 2.410627\n",
      "Epoch: 20925 \tTraining Loss: 1.554186 \tValidation Loss: 2.410070\n",
      "Epoch: 20926 \tTraining Loss: 1.529697 \tValidation Loss: 2.410004\n",
      "Epoch: 20927 \tTraining Loss: 1.533245 \tValidation Loss: 2.410738\n",
      "Epoch: 20928 \tTraining Loss: 1.586123 \tValidation Loss: 2.410114\n",
      "Epoch: 20929 \tTraining Loss: 1.533393 \tValidation Loss: 2.410677\n",
      "Epoch: 20930 \tTraining Loss: 1.525092 \tValidation Loss: 2.410671\n",
      "Epoch: 20931 \tTraining Loss: 1.534467 \tValidation Loss: 2.410881\n",
      "Epoch: 20932 \tTraining Loss: 1.533219 \tValidation Loss: 2.411007\n",
      "Epoch: 20933 \tTraining Loss: 1.522987 \tValidation Loss: 2.411353\n",
      "Epoch: 20934 \tTraining Loss: 1.547787 \tValidation Loss: 2.410280\n",
      "Epoch: 20935 \tTraining Loss: 1.549357 \tValidation Loss: 2.410394\n",
      "Epoch: 20936 \tTraining Loss: 1.494333 \tValidation Loss: 2.410670\n",
      "Epoch: 20937 \tTraining Loss: 1.511207 \tValidation Loss: 2.411339\n",
      "Epoch: 20938 \tTraining Loss: 1.549372 \tValidation Loss: 2.411145\n",
      "Epoch: 20939 \tTraining Loss: 1.530453 \tValidation Loss: 2.410291\n",
      "Epoch: 20940 \tTraining Loss: 1.554193 \tValidation Loss: 2.410406\n",
      "Epoch: 20941 \tTraining Loss: 1.540862 \tValidation Loss: 2.410299\n",
      "Epoch: 20942 \tTraining Loss: 1.577342 \tValidation Loss: 2.409807\n",
      "Epoch: 20943 \tTraining Loss: 1.520159 \tValidation Loss: 2.411150\n",
      "Epoch: 20944 \tTraining Loss: 1.550206 \tValidation Loss: 2.410955\n",
      "Epoch: 20945 \tTraining Loss: 1.578230 \tValidation Loss: 2.410470\n",
      "Epoch: 20946 \tTraining Loss: 1.524027 \tValidation Loss: 2.411193\n",
      "Epoch: 20947 \tTraining Loss: 1.522194 \tValidation Loss: 2.411515\n",
      "Epoch: 20948 \tTraining Loss: 1.563260 \tValidation Loss: 2.410998\n",
      "Epoch: 20949 \tTraining Loss: 1.496307 \tValidation Loss: 2.411503\n",
      "Epoch: 20950 \tTraining Loss: 1.536672 \tValidation Loss: 2.410972\n",
      "Epoch: 20951 \tTraining Loss: 1.536414 \tValidation Loss: 2.411094\n",
      "Epoch: 20952 \tTraining Loss: 1.529323 \tValidation Loss: 2.411624\n",
      "Epoch: 20953 \tTraining Loss: 1.590432 \tValidation Loss: 2.409860\n",
      "Epoch: 20954 \tTraining Loss: 1.533264 \tValidation Loss: 2.410483\n",
      "Epoch: 20955 \tTraining Loss: 1.541377 \tValidation Loss: 2.411144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20956 \tTraining Loss: 1.541286 \tValidation Loss: 2.410356\n",
      "Epoch: 20957 \tTraining Loss: 1.516701 \tValidation Loss: 2.410965\n",
      "Epoch: 20958 \tTraining Loss: 1.543251 \tValidation Loss: 2.410242\n",
      "Epoch: 20959 \tTraining Loss: 1.540854 \tValidation Loss: 2.409804\n",
      "Epoch: 20960 \tTraining Loss: 1.536881 \tValidation Loss: 2.411017\n",
      "Epoch: 20961 \tTraining Loss: 1.537345 \tValidation Loss: 2.411342\n",
      "Epoch: 20962 \tTraining Loss: 1.545013 \tValidation Loss: 2.410210\n",
      "Epoch: 20963 \tTraining Loss: 1.518990 \tValidation Loss: 2.410856\n",
      "Epoch: 20964 \tTraining Loss: 1.543393 \tValidation Loss: 2.411375\n",
      "Epoch: 20965 \tTraining Loss: 1.543566 \tValidation Loss: 2.409796\n",
      "Epoch: 20966 \tTraining Loss: 1.515444 \tValidation Loss: 2.411082\n",
      "Epoch: 20967 \tTraining Loss: 1.515618 \tValidation Loss: 2.410950\n",
      "Epoch: 20968 \tTraining Loss: 1.504399 \tValidation Loss: 2.411266\n",
      "Epoch: 20969 \tTraining Loss: 1.516909 \tValidation Loss: 2.411198\n",
      "Epoch: 20970 \tTraining Loss: 1.515515 \tValidation Loss: 2.411536\n",
      "Epoch: 20971 \tTraining Loss: 1.499223 \tValidation Loss: 2.411678\n",
      "Epoch: 20972 \tTraining Loss: 1.503282 \tValidation Loss: 2.411233\n",
      "Epoch: 20973 \tTraining Loss: 1.531920 \tValidation Loss: 2.411557\n",
      "Epoch: 20974 \tTraining Loss: 1.525519 \tValidation Loss: 2.411482\n",
      "Epoch: 20975 \tTraining Loss: 1.542828 \tValidation Loss: 2.411866\n",
      "Epoch: 20976 \tTraining Loss: 1.519703 \tValidation Loss: 2.411036\n",
      "Epoch: 20977 \tTraining Loss: 1.534194 \tValidation Loss: 2.410842\n",
      "Epoch: 20978 \tTraining Loss: 1.528717 \tValidation Loss: 2.410985\n",
      "Epoch: 20979 \tTraining Loss: 1.527243 \tValidation Loss: 2.411596\n",
      "Epoch: 20980 \tTraining Loss: 1.512753 \tValidation Loss: 2.411738\n",
      "Epoch: 20981 \tTraining Loss: 1.527643 \tValidation Loss: 2.411927\n",
      "Epoch: 20982 \tTraining Loss: 1.516358 \tValidation Loss: 2.412355\n",
      "Epoch: 20983 \tTraining Loss: 1.516151 \tValidation Loss: 2.412316\n",
      "Epoch: 20984 \tTraining Loss: 1.542180 \tValidation Loss: 2.411619\n",
      "Epoch: 20985 \tTraining Loss: 1.572130 \tValidation Loss: 2.411103\n",
      "Epoch: 20986 \tTraining Loss: 1.557343 \tValidation Loss: 2.411084\n",
      "Epoch: 20987 \tTraining Loss: 1.537279 \tValidation Loss: 2.411364\n",
      "Epoch: 20988 \tTraining Loss: 1.558249 \tValidation Loss: 2.410925\n",
      "Epoch: 20989 \tTraining Loss: 1.477552 \tValidation Loss: 2.411652\n",
      "Epoch: 20990 \tTraining Loss: 1.578490 \tValidation Loss: 2.411331\n",
      "Epoch: 20991 \tTraining Loss: 1.546320 \tValidation Loss: 2.411364\n",
      "Epoch: 20992 \tTraining Loss: 1.568587 \tValidation Loss: 2.411431\n",
      "Epoch: 20993 \tTraining Loss: 1.551126 \tValidation Loss: 2.411006\n",
      "Epoch: 20994 \tTraining Loss: 1.517979 \tValidation Loss: 2.411409\n",
      "Epoch: 20995 \tTraining Loss: 1.517931 \tValidation Loss: 2.411561\n",
      "Epoch: 20996 \tTraining Loss: 1.550243 \tValidation Loss: 2.411523\n",
      "Epoch: 20997 \tTraining Loss: 1.533303 \tValidation Loss: 2.411195\n",
      "Epoch: 20998 \tTraining Loss: 1.521109 \tValidation Loss: 2.411762\n",
      "Epoch: 20999 \tTraining Loss: 1.543170 \tValidation Loss: 2.410368\n",
      "Epoch: 21000 \tTraining Loss: 1.509105 \tValidation Loss: 2.411198\n",
      "Epoch: 21001 \tTraining Loss: 1.513194 \tValidation Loss: 2.410850\n",
      "Epoch: 21002 \tTraining Loss: 1.550538 \tValidation Loss: 2.411958\n",
      "Epoch: 21003 \tTraining Loss: 1.538194 \tValidation Loss: 2.411119\n",
      "Epoch: 21004 \tTraining Loss: 1.547842 \tValidation Loss: 2.411446\n",
      "Epoch: 21005 \tTraining Loss: 1.526828 \tValidation Loss: 2.411707\n",
      "Epoch: 21006 \tTraining Loss: 1.503726 \tValidation Loss: 2.411481\n",
      "Epoch: 21007 \tTraining Loss: 1.564872 \tValidation Loss: 2.411478\n",
      "Epoch: 21008 \tTraining Loss: 1.550753 \tValidation Loss: 2.411478\n",
      "Epoch: 21009 \tTraining Loss: 1.520576 \tValidation Loss: 2.412246\n",
      "Epoch: 21010 \tTraining Loss: 1.456213 \tValidation Loss: 2.412880\n",
      "Epoch: 21011 \tTraining Loss: 1.534114 \tValidation Loss: 2.412280\n",
      "Epoch: 21012 \tTraining Loss: 1.476948 \tValidation Loss: 2.412322\n",
      "Epoch: 21013 \tTraining Loss: 1.499899 \tValidation Loss: 2.412317\n",
      "Epoch: 21014 \tTraining Loss: 1.581756 \tValidation Loss: 2.411903\n",
      "Epoch: 21015 \tTraining Loss: 1.588969 \tValidation Loss: 2.412066\n",
      "Epoch: 21016 \tTraining Loss: 1.550856 \tValidation Loss: 2.412063\n",
      "Epoch: 21017 \tTraining Loss: 1.540144 \tValidation Loss: 2.411462\n",
      "Epoch: 21018 \tTraining Loss: 1.542904 \tValidation Loss: 2.411124\n",
      "Epoch: 21019 \tTraining Loss: 1.546147 \tValidation Loss: 2.410756\n",
      "Epoch: 21020 \tTraining Loss: 1.580634 \tValidation Loss: 2.409476\n",
      "Epoch: 21021 \tTraining Loss: 1.541010 \tValidation Loss: 2.409562\n",
      "Epoch: 21022 \tTraining Loss: 1.505840 \tValidation Loss: 2.410918\n",
      "Epoch: 21023 \tTraining Loss: 1.526062 \tValidation Loss: 2.411010\n",
      "Epoch: 21024 \tTraining Loss: 1.540524 \tValidation Loss: 2.411417\n",
      "Epoch: 21025 \tTraining Loss: 1.513131 \tValidation Loss: 2.410628\n",
      "Epoch: 21026 \tTraining Loss: 1.507089 \tValidation Loss: 2.411008\n",
      "Epoch: 21027 \tTraining Loss: 1.547004 \tValidation Loss: 2.411246\n",
      "Epoch: 21028 \tTraining Loss: 1.558830 \tValidation Loss: 2.411589\n",
      "Epoch: 21029 \tTraining Loss: 1.536965 \tValidation Loss: 2.411683\n",
      "Epoch: 21030 \tTraining Loss: 1.522459 \tValidation Loss: 2.411225\n",
      "Epoch: 21031 \tTraining Loss: 1.552593 \tValidation Loss: 2.411242\n",
      "Epoch: 21032 \tTraining Loss: 1.515638 \tValidation Loss: 2.410633\n",
      "Epoch: 21033 \tTraining Loss: 1.536394 \tValidation Loss: 2.411212\n",
      "Epoch: 21034 \tTraining Loss: 1.511865 \tValidation Loss: 2.412035\n",
      "Epoch: 21035 \tTraining Loss: 1.511913 \tValidation Loss: 2.412052\n",
      "Epoch: 21036 \tTraining Loss: 1.506912 \tValidation Loss: 2.412252\n",
      "Epoch: 21037 \tTraining Loss: 1.503878 \tValidation Loss: 2.411705\n",
      "Epoch: 21038 \tTraining Loss: 1.537188 \tValidation Loss: 2.411380\n",
      "Epoch: 21039 \tTraining Loss: 1.545086 \tValidation Loss: 2.412188\n",
      "Epoch: 21040 \tTraining Loss: 1.550480 \tValidation Loss: 2.411243\n",
      "Epoch: 21041 \tTraining Loss: 1.554283 \tValidation Loss: 2.411047\n",
      "Epoch: 21042 \tTraining Loss: 1.524242 \tValidation Loss: 2.411462\n",
      "Epoch: 21043 \tTraining Loss: 1.547960 \tValidation Loss: 2.411070\n",
      "Epoch: 21044 \tTraining Loss: 1.579939 \tValidation Loss: 2.411272\n",
      "Epoch: 21045 \tTraining Loss: 1.564599 \tValidation Loss: 2.410921\n",
      "Epoch: 21046 \tTraining Loss: 1.522257 \tValidation Loss: 2.412155\n",
      "Epoch: 21047 \tTraining Loss: 1.535737 \tValidation Loss: 2.411848\n",
      "Epoch: 21048 \tTraining Loss: 1.505647 \tValidation Loss: 2.412396\n",
      "Epoch: 21049 \tTraining Loss: 1.511716 \tValidation Loss: 2.411908\n",
      "Epoch: 21050 \tTraining Loss: 1.542441 \tValidation Loss: 2.411584\n",
      "Epoch: 21051 \tTraining Loss: 1.476582 \tValidation Loss: 2.412214\n",
      "Epoch: 21052 \tTraining Loss: 1.533792 \tValidation Loss: 2.411787\n",
      "Epoch: 21053 \tTraining Loss: 1.536144 \tValidation Loss: 2.412317\n",
      "Epoch: 21054 \tTraining Loss: 1.522849 \tValidation Loss: 2.411959\n",
      "Epoch: 21055 \tTraining Loss: 1.530955 \tValidation Loss: 2.412757\n",
      "Epoch: 21056 \tTraining Loss: 1.523928 \tValidation Loss: 2.411929\n",
      "Epoch: 21057 \tTraining Loss: 1.521097 \tValidation Loss: 2.412668\n",
      "Epoch: 21058 \tTraining Loss: 1.581747 \tValidation Loss: 2.412420\n",
      "Epoch: 21059 \tTraining Loss: 1.545156 \tValidation Loss: 2.412741\n",
      "Epoch: 21060 \tTraining Loss: 1.547645 \tValidation Loss: 2.411690\n",
      "Epoch: 21061 \tTraining Loss: 1.531423 \tValidation Loss: 2.411636\n",
      "Epoch: 21062 \tTraining Loss: 1.561611 \tValidation Loss: 2.412328\n",
      "Epoch: 21063 \tTraining Loss: 1.522746 \tValidation Loss: 2.412464\n",
      "Epoch: 21064 \tTraining Loss: 1.530718 \tValidation Loss: 2.412632\n",
      "Epoch: 21065 \tTraining Loss: 1.560840 \tValidation Loss: 2.412308\n",
      "Epoch: 21066 \tTraining Loss: 1.516773 \tValidation Loss: 2.412420\n",
      "Epoch: 21067 \tTraining Loss: 1.541606 \tValidation Loss: 2.411878\n",
      "Epoch: 21068 \tTraining Loss: 1.541163 \tValidation Loss: 2.411529\n",
      "Epoch: 21069 \tTraining Loss: 1.537744 \tValidation Loss: 2.411583\n",
      "Epoch: 21070 \tTraining Loss: 1.530921 \tValidation Loss: 2.411807\n",
      "Epoch: 21071 \tTraining Loss: 1.508840 \tValidation Loss: 2.412391\n",
      "Epoch: 21072 \tTraining Loss: 1.513302 \tValidation Loss: 2.411958\n",
      "Epoch: 21073 \tTraining Loss: 1.547852 \tValidation Loss: 2.411804\n",
      "Epoch: 21074 \tTraining Loss: 1.506507 \tValidation Loss: 2.411647\n",
      "Epoch: 21075 \tTraining Loss: 1.507300 \tValidation Loss: 2.412263\n",
      "Epoch: 21076 \tTraining Loss: 1.500590 \tValidation Loss: 2.411854\n",
      "Epoch: 21077 \tTraining Loss: 1.536291 \tValidation Loss: 2.412148\n",
      "Epoch: 21078 \tTraining Loss: 1.513229 \tValidation Loss: 2.411815\n",
      "Epoch: 21079 \tTraining Loss: 1.515785 \tValidation Loss: 2.410601\n",
      "Epoch: 21080 \tTraining Loss: 1.504121 \tValidation Loss: 2.411867\n",
      "Epoch: 21081 \tTraining Loss: 1.590097 \tValidation Loss: 2.411539\n",
      "Epoch: 21082 \tTraining Loss: 1.537642 \tValidation Loss: 2.411816\n",
      "Epoch: 21083 \tTraining Loss: 1.512621 \tValidation Loss: 2.412150\n",
      "Epoch: 21084 \tTraining Loss: 1.531384 \tValidation Loss: 2.411936\n",
      "Epoch: 21085 \tTraining Loss: 1.530556 \tValidation Loss: 2.411803\n",
      "Epoch: 21086 \tTraining Loss: 1.537127 \tValidation Loss: 2.412119\n",
      "Epoch: 21087 \tTraining Loss: 1.530487 \tValidation Loss: 2.411863\n",
      "Epoch: 21088 \tTraining Loss: 1.504112 \tValidation Loss: 2.412215\n",
      "Epoch: 21089 \tTraining Loss: 1.572917 \tValidation Loss: 2.411793\n",
      "Epoch: 21090 \tTraining Loss: 1.524104 \tValidation Loss: 2.411859\n",
      "Epoch: 21091 \tTraining Loss: 1.549081 \tValidation Loss: 2.412297\n",
      "Epoch: 21092 \tTraining Loss: 1.543494 \tValidation Loss: 2.411681\n",
      "Epoch: 21093 \tTraining Loss: 1.504699 \tValidation Loss: 2.412198\n",
      "Epoch: 21094 \tTraining Loss: 1.581391 \tValidation Loss: 2.412142\n",
      "Epoch: 21095 \tTraining Loss: 1.511086 \tValidation Loss: 2.413074\n",
      "Epoch: 21096 \tTraining Loss: 1.554390 \tValidation Loss: 2.411732\n",
      "Epoch: 21097 \tTraining Loss: 1.525638 \tValidation Loss: 2.411567\n",
      "Epoch: 21098 \tTraining Loss: 1.501118 \tValidation Loss: 2.412904\n",
      "Epoch: 21099 \tTraining Loss: 1.535520 \tValidation Loss: 2.413168\n",
      "Epoch: 21100 \tTraining Loss: 1.551230 \tValidation Loss: 2.412838\n",
      "Epoch: 21101 \tTraining Loss: 1.539443 \tValidation Loss: 2.412417\n",
      "Epoch: 21102 \tTraining Loss: 1.545465 \tValidation Loss: 2.412900\n",
      "Epoch: 21103 \tTraining Loss: 1.524526 \tValidation Loss: 2.413095\n",
      "Epoch: 21104 \tTraining Loss: 1.490353 \tValidation Loss: 2.412736\n",
      "Epoch: 21105 \tTraining Loss: 1.536610 \tValidation Loss: 2.412577\n",
      "Epoch: 21106 \tTraining Loss: 1.542581 \tValidation Loss: 2.412049\n",
      "Epoch: 21107 \tTraining Loss: 1.531560 \tValidation Loss: 2.412488\n",
      "Epoch: 21108 \tTraining Loss: 1.491809 \tValidation Loss: 2.412534\n",
      "Epoch: 21109 \tTraining Loss: 1.560929 \tValidation Loss: 2.412687\n",
      "Epoch: 21110 \tTraining Loss: 1.521782 \tValidation Loss: 2.411692\n",
      "Epoch: 21111 \tTraining Loss: 1.545276 \tValidation Loss: 2.411657\n",
      "Epoch: 21112 \tTraining Loss: 1.465050 \tValidation Loss: 2.412726\n",
      "Epoch: 21113 \tTraining Loss: 1.519665 \tValidation Loss: 2.412593\n",
      "Epoch: 21114 \tTraining Loss: 1.513662 \tValidation Loss: 2.412785\n",
      "Epoch: 21115 \tTraining Loss: 1.501332 \tValidation Loss: 2.412292\n",
      "Epoch: 21116 \tTraining Loss: 1.518990 \tValidation Loss: 2.412166\n",
      "Epoch: 21117 \tTraining Loss: 1.516992 \tValidation Loss: 2.412718\n",
      "Epoch: 21118 \tTraining Loss: 1.509191 \tValidation Loss: 2.412379\n",
      "Epoch: 21119 \tTraining Loss: 1.494189 \tValidation Loss: 2.412845\n",
      "Epoch: 21120 \tTraining Loss: 1.552454 \tValidation Loss: 2.412482\n",
      "Epoch: 21121 \tTraining Loss: 1.538700 \tValidation Loss: 2.413048\n",
      "Epoch: 21122 \tTraining Loss: 1.529010 \tValidation Loss: 2.413043\n",
      "Epoch: 21123 \tTraining Loss: 1.503186 \tValidation Loss: 2.413556\n",
      "Epoch: 21124 \tTraining Loss: 1.512791 \tValidation Loss: 2.413488\n",
      "Epoch: 21125 \tTraining Loss: 1.518870 \tValidation Loss: 2.412835\n",
      "Epoch: 21126 \tTraining Loss: 1.531732 \tValidation Loss: 2.412477\n",
      "Epoch: 21127 \tTraining Loss: 1.536314 \tValidation Loss: 2.411556\n",
      "Epoch: 21128 \tTraining Loss: 1.536678 \tValidation Loss: 2.412218\n",
      "Epoch: 21129 \tTraining Loss: 1.534583 \tValidation Loss: 2.412905\n",
      "Epoch: 21130 \tTraining Loss: 1.485069 \tValidation Loss: 2.413152\n",
      "Epoch: 21131 \tTraining Loss: 1.512184 \tValidation Loss: 2.412983\n",
      "Epoch: 21132 \tTraining Loss: 1.579102 \tValidation Loss: 2.412121\n",
      "Epoch: 21133 \tTraining Loss: 1.530548 \tValidation Loss: 2.412947\n",
      "Epoch: 21134 \tTraining Loss: 1.503494 \tValidation Loss: 2.413311\n",
      "Epoch: 21135 \tTraining Loss: 1.559172 \tValidation Loss: 2.413086\n",
      "Epoch: 21136 \tTraining Loss: 1.563471 \tValidation Loss: 2.412688\n",
      "Epoch: 21137 \tTraining Loss: 1.536649 \tValidation Loss: 2.412710\n",
      "Epoch: 21138 \tTraining Loss: 1.534306 \tValidation Loss: 2.412677\n",
      "Epoch: 21139 \tTraining Loss: 1.554413 \tValidation Loss: 2.413046\n",
      "Epoch: 21140 \tTraining Loss: 1.521363 \tValidation Loss: 2.412646\n",
      "Epoch: 21141 \tTraining Loss: 1.522660 \tValidation Loss: 2.413078\n",
      "Epoch: 21142 \tTraining Loss: 1.546489 \tValidation Loss: 2.413157\n",
      "Epoch: 21143 \tTraining Loss: 1.497025 \tValidation Loss: 2.413541\n",
      "Epoch: 21144 \tTraining Loss: 1.528868 \tValidation Loss: 2.413492\n",
      "Epoch: 21145 \tTraining Loss: 1.514848 \tValidation Loss: 2.412980\n",
      "Epoch: 21146 \tTraining Loss: 1.520715 \tValidation Loss: 2.413258\n",
      "Epoch: 21147 \tTraining Loss: 1.556114 \tValidation Loss: 2.413457\n",
      "Epoch: 21148 \tTraining Loss: 1.522355 \tValidation Loss: 2.412892\n",
      "Epoch: 21149 \tTraining Loss: 1.543056 \tValidation Loss: 2.413372\n",
      "Epoch: 21150 \tTraining Loss: 1.519350 \tValidation Loss: 2.413137\n",
      "Epoch: 21151 \tTraining Loss: 1.524558 \tValidation Loss: 2.413469\n",
      "Epoch: 21152 \tTraining Loss: 1.554672 \tValidation Loss: 2.413004\n",
      "Epoch: 21153 \tTraining Loss: 1.548828 \tValidation Loss: 2.412660\n",
      "Epoch: 21154 \tTraining Loss: 1.583560 \tValidation Loss: 2.413272\n",
      "Epoch: 21155 \tTraining Loss: 1.506166 \tValidation Loss: 2.412754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21156 \tTraining Loss: 1.547347 \tValidation Loss: 2.412785\n",
      "Epoch: 21157 \tTraining Loss: 1.546165 \tValidation Loss: 2.412562\n",
      "Epoch: 21158 \tTraining Loss: 1.513337 \tValidation Loss: 2.412434\n",
      "Epoch: 21159 \tTraining Loss: 1.519968 \tValidation Loss: 2.413261\n",
      "Epoch: 21160 \tTraining Loss: 1.533392 \tValidation Loss: 2.412420\n",
      "Epoch: 21161 \tTraining Loss: 1.512811 \tValidation Loss: 2.414306\n",
      "Epoch: 21162 \tTraining Loss: 1.541694 \tValidation Loss: 2.413018\n",
      "Epoch: 21163 \tTraining Loss: 1.542550 \tValidation Loss: 2.413138\n",
      "Epoch: 21164 \tTraining Loss: 1.519782 \tValidation Loss: 2.413263\n",
      "Epoch: 21165 \tTraining Loss: 1.518276 \tValidation Loss: 2.412930\n",
      "Epoch: 21166 \tTraining Loss: 1.515217 \tValidation Loss: 2.414411\n",
      "Epoch: 21167 \tTraining Loss: 1.514906 \tValidation Loss: 2.413813\n",
      "Epoch: 21168 \tTraining Loss: 1.542300 \tValidation Loss: 2.413751\n",
      "Epoch: 21169 \tTraining Loss: 1.502432 \tValidation Loss: 2.413560\n",
      "Epoch: 21170 \tTraining Loss: 1.493230 \tValidation Loss: 2.413455\n",
      "Epoch: 21171 \tTraining Loss: 1.532211 \tValidation Loss: 2.413352\n",
      "Epoch: 21172 \tTraining Loss: 1.543564 \tValidation Loss: 2.413170\n",
      "Epoch: 21173 \tTraining Loss: 1.536031 \tValidation Loss: 2.413262\n",
      "Epoch: 21174 \tTraining Loss: 1.544900 \tValidation Loss: 2.413208\n",
      "Epoch: 21175 \tTraining Loss: 1.548782 \tValidation Loss: 2.413205\n",
      "Epoch: 21176 \tTraining Loss: 1.524836 \tValidation Loss: 2.413255\n",
      "Epoch: 21177 \tTraining Loss: 1.569543 \tValidation Loss: 2.413196\n",
      "Epoch: 21178 \tTraining Loss: 1.527514 \tValidation Loss: 2.413478\n",
      "Epoch: 21179 \tTraining Loss: 1.493147 \tValidation Loss: 2.413674\n",
      "Epoch: 21180 \tTraining Loss: 1.537999 \tValidation Loss: 2.414076\n",
      "Epoch: 21181 \tTraining Loss: 1.545642 \tValidation Loss: 2.413917\n",
      "Epoch: 21182 \tTraining Loss: 1.526873 \tValidation Loss: 2.413731\n",
      "Epoch: 21183 \tTraining Loss: 1.574732 \tValidation Loss: 2.413011\n",
      "Epoch: 21184 \tTraining Loss: 1.513678 \tValidation Loss: 2.413765\n",
      "Epoch: 21185 \tTraining Loss: 1.548968 \tValidation Loss: 2.414073\n",
      "Epoch: 21186 \tTraining Loss: 1.551521 \tValidation Loss: 2.414510\n",
      "Epoch: 21187 \tTraining Loss: 1.520108 \tValidation Loss: 2.413708\n",
      "Epoch: 21188 \tTraining Loss: 1.530455 \tValidation Loss: 2.413163\n",
      "Epoch: 21189 \tTraining Loss: 1.520779 \tValidation Loss: 2.413218\n",
      "Epoch: 21190 \tTraining Loss: 1.501174 \tValidation Loss: 2.413904\n",
      "Epoch: 21191 \tTraining Loss: 1.511743 \tValidation Loss: 2.413352\n",
      "Epoch: 21192 \tTraining Loss: 1.499512 \tValidation Loss: 2.414442\n",
      "Epoch: 21193 \tTraining Loss: 1.527329 \tValidation Loss: 2.413990\n",
      "Epoch: 21194 \tTraining Loss: 1.528071 \tValidation Loss: 2.414151\n",
      "Epoch: 21195 \tTraining Loss: 1.522211 \tValidation Loss: 2.413383\n",
      "Epoch: 21196 \tTraining Loss: 1.567421 \tValidation Loss: 2.414187\n",
      "Epoch: 21197 \tTraining Loss: 1.520676 \tValidation Loss: 2.413502\n",
      "Epoch: 21198 \tTraining Loss: 1.472738 \tValidation Loss: 2.414791\n",
      "Epoch: 21199 \tTraining Loss: 1.554370 \tValidation Loss: 2.413934\n",
      "Epoch: 21200 \tTraining Loss: 1.517556 \tValidation Loss: 2.414427\n",
      "Epoch: 21201 \tTraining Loss: 1.524047 \tValidation Loss: 2.414640\n",
      "Epoch: 21202 \tTraining Loss: 1.498800 \tValidation Loss: 2.415062\n",
      "Epoch: 21203 \tTraining Loss: 1.509106 \tValidation Loss: 2.415054\n",
      "Epoch: 21204 \tTraining Loss: 1.526506 \tValidation Loss: 2.414424\n",
      "Epoch: 21205 \tTraining Loss: 1.538733 \tValidation Loss: 2.414964\n",
      "Epoch: 21206 \tTraining Loss: 1.528504 \tValidation Loss: 2.415158\n",
      "Epoch: 21207 \tTraining Loss: 1.550724 \tValidation Loss: 2.414931\n",
      "Epoch: 21208 \tTraining Loss: 1.510012 \tValidation Loss: 2.414351\n",
      "Epoch: 21209 \tTraining Loss: 1.492147 \tValidation Loss: 2.414665\n",
      "Epoch: 21210 \tTraining Loss: 1.511684 \tValidation Loss: 2.414104\n",
      "Epoch: 21211 \tTraining Loss: 1.501003 \tValidation Loss: 2.414890\n",
      "Epoch: 21212 \tTraining Loss: 1.535419 \tValidation Loss: 2.413874\n",
      "Epoch: 21213 \tTraining Loss: 1.497002 \tValidation Loss: 2.415421\n",
      "Epoch: 21214 \tTraining Loss: 1.534705 \tValidation Loss: 2.414950\n",
      "Epoch: 21215 \tTraining Loss: 1.513771 \tValidation Loss: 2.415637\n",
      "Epoch: 21216 \tTraining Loss: 1.512465 \tValidation Loss: 2.414083\n",
      "Epoch: 21217 \tTraining Loss: 1.522181 \tValidation Loss: 2.413952\n",
      "Epoch: 21218 \tTraining Loss: 1.559266 \tValidation Loss: 2.414688\n",
      "Epoch: 21219 \tTraining Loss: 1.520624 \tValidation Loss: 2.414826\n",
      "Epoch: 21220 \tTraining Loss: 1.538914 \tValidation Loss: 2.413754\n",
      "Epoch: 21221 \tTraining Loss: 1.515006 \tValidation Loss: 2.414365\n",
      "Epoch: 21222 \tTraining Loss: 1.525269 \tValidation Loss: 2.414061\n",
      "Epoch: 21223 \tTraining Loss: 1.507825 \tValidation Loss: 2.413468\n",
      "Epoch: 21224 \tTraining Loss: 1.570299 \tValidation Loss: 2.413760\n",
      "Epoch: 21225 \tTraining Loss: 1.520183 \tValidation Loss: 2.413898\n",
      "Epoch: 21226 \tTraining Loss: 1.516855 \tValidation Loss: 2.414635\n",
      "Epoch: 21227 \tTraining Loss: 1.538985 \tValidation Loss: 2.414402\n",
      "Epoch: 21228 \tTraining Loss: 1.498467 \tValidation Loss: 2.414432\n",
      "Epoch: 21229 \tTraining Loss: 1.504008 \tValidation Loss: 2.415264\n",
      "Epoch: 21230 \tTraining Loss: 1.546359 \tValidation Loss: 2.414474\n",
      "Epoch: 21231 \tTraining Loss: 1.506238 \tValidation Loss: 2.414063\n",
      "Epoch: 21232 \tTraining Loss: 1.530195 \tValidation Loss: 2.414197\n",
      "Epoch: 21233 \tTraining Loss: 1.550325 \tValidation Loss: 2.414368\n",
      "Epoch: 21234 \tTraining Loss: 1.515042 \tValidation Loss: 2.414678\n",
      "Epoch: 21235 \tTraining Loss: 1.522371 \tValidation Loss: 2.414897\n",
      "Epoch: 21236 \tTraining Loss: 1.531788 \tValidation Loss: 2.415200\n",
      "Epoch: 21237 \tTraining Loss: 1.474395 \tValidation Loss: 2.414824\n",
      "Epoch: 21238 \tTraining Loss: 1.509088 \tValidation Loss: 2.414434\n",
      "Epoch: 21239 \tTraining Loss: 1.510857 \tValidation Loss: 2.415115\n",
      "Epoch: 21240 \tTraining Loss: 1.538037 \tValidation Loss: 2.414540\n",
      "Epoch: 21241 \tTraining Loss: 1.529528 \tValidation Loss: 2.414673\n",
      "Epoch: 21242 \tTraining Loss: 1.517238 \tValidation Loss: 2.413973\n",
      "Epoch: 21243 \tTraining Loss: 1.497841 \tValidation Loss: 2.414978\n",
      "Epoch: 21244 \tTraining Loss: 1.582041 \tValidation Loss: 2.414874\n",
      "Epoch: 21245 \tTraining Loss: 1.541639 \tValidation Loss: 2.414273\n",
      "Epoch: 21246 \tTraining Loss: 1.493514 \tValidation Loss: 2.414340\n",
      "Epoch: 21247 \tTraining Loss: 1.500946 \tValidation Loss: 2.414896\n",
      "Epoch: 21248 \tTraining Loss: 1.541985 \tValidation Loss: 2.414810\n",
      "Epoch: 21249 \tTraining Loss: 1.554613 \tValidation Loss: 2.414927\n",
      "Epoch: 21250 \tTraining Loss: 1.527706 \tValidation Loss: 2.415020\n",
      "Epoch: 21251 \tTraining Loss: 1.569293 \tValidation Loss: 2.414316\n",
      "Epoch: 21252 \tTraining Loss: 1.529780 \tValidation Loss: 2.414908\n",
      "Epoch: 21253 \tTraining Loss: 1.494925 \tValidation Loss: 2.414706\n",
      "Epoch: 21254 \tTraining Loss: 1.537715 \tValidation Loss: 2.414573\n",
      "Epoch: 21255 \tTraining Loss: 1.533284 \tValidation Loss: 2.414193\n",
      "Epoch: 21256 \tTraining Loss: 1.517243 \tValidation Loss: 2.413886\n",
      "Epoch: 21257 \tTraining Loss: 1.466630 \tValidation Loss: 2.414377\n",
      "Epoch: 21258 \tTraining Loss: 1.531121 \tValidation Loss: 2.414649\n",
      "Epoch: 21259 \tTraining Loss: 1.475478 \tValidation Loss: 2.415034\n",
      "Epoch: 21260 \tTraining Loss: 1.527346 \tValidation Loss: 2.413889\n",
      "Epoch: 21261 \tTraining Loss: 1.542400 \tValidation Loss: 2.414309\n",
      "Epoch: 21262 \tTraining Loss: 1.521231 \tValidation Loss: 2.414969\n",
      "Epoch: 21263 \tTraining Loss: 1.540643 \tValidation Loss: 2.414453\n",
      "Epoch: 21264 \tTraining Loss: 1.553365 \tValidation Loss: 2.414781\n",
      "Epoch: 21265 \tTraining Loss: 1.546408 \tValidation Loss: 2.414706\n",
      "Epoch: 21266 \tTraining Loss: 1.539678 \tValidation Loss: 2.414821\n",
      "Epoch: 21267 \tTraining Loss: 1.554653 \tValidation Loss: 2.414304\n",
      "Epoch: 21268 \tTraining Loss: 1.502233 \tValidation Loss: 2.414013\n",
      "Epoch: 21269 \tTraining Loss: 1.521095 \tValidation Loss: 2.415213\n",
      "Epoch: 21270 \tTraining Loss: 1.527317 \tValidation Loss: 2.415023\n",
      "Epoch: 21271 \tTraining Loss: 1.544370 \tValidation Loss: 2.413544\n",
      "Epoch: 21272 \tTraining Loss: 1.519769 \tValidation Loss: 2.414428\n",
      "Epoch: 21273 \tTraining Loss: 1.523816 \tValidation Loss: 2.414519\n",
      "Epoch: 21274 \tTraining Loss: 1.518047 \tValidation Loss: 2.414505\n",
      "Epoch: 21275 \tTraining Loss: 1.537293 \tValidation Loss: 2.414261\n",
      "Epoch: 21276 \tTraining Loss: 1.545806 \tValidation Loss: 2.414113\n",
      "Epoch: 21277 \tTraining Loss: 1.523546 \tValidation Loss: 2.414922\n",
      "Epoch: 21278 \tTraining Loss: 1.553854 \tValidation Loss: 2.414319\n",
      "Epoch: 21279 \tTraining Loss: 1.510343 \tValidation Loss: 2.414212\n",
      "Epoch: 21280 \tTraining Loss: 1.514692 \tValidation Loss: 2.413827\n",
      "Epoch: 21281 \tTraining Loss: 1.516644 \tValidation Loss: 2.414802\n",
      "Epoch: 21282 \tTraining Loss: 1.518137 \tValidation Loss: 2.414500\n",
      "Epoch: 21283 \tTraining Loss: 1.521292 \tValidation Loss: 2.415233\n",
      "Epoch: 21284 \tTraining Loss: 1.532897 \tValidation Loss: 2.415762\n",
      "Epoch: 21285 \tTraining Loss: 1.569770 \tValidation Loss: 2.415299\n",
      "Epoch: 21286 \tTraining Loss: 1.570825 \tValidation Loss: 2.415157\n",
      "Epoch: 21287 \tTraining Loss: 1.527552 \tValidation Loss: 2.415777\n",
      "Epoch: 21288 \tTraining Loss: 1.538724 \tValidation Loss: 2.414832\n",
      "Epoch: 21289 \tTraining Loss: 1.540785 \tValidation Loss: 2.414598\n",
      "Epoch: 21290 \tTraining Loss: 1.486951 \tValidation Loss: 2.415248\n",
      "Epoch: 21291 \tTraining Loss: 1.513637 \tValidation Loss: 2.415519\n",
      "Epoch: 21292 \tTraining Loss: 1.530165 \tValidation Loss: 2.414836\n",
      "Epoch: 21293 \tTraining Loss: 1.514240 \tValidation Loss: 2.414997\n",
      "Epoch: 21294 \tTraining Loss: 1.525436 \tValidation Loss: 2.415397\n",
      "Epoch: 21295 \tTraining Loss: 1.542152 \tValidation Loss: 2.414805\n",
      "Epoch: 21296 \tTraining Loss: 1.517291 \tValidation Loss: 2.415329\n",
      "Epoch: 21297 \tTraining Loss: 1.536409 \tValidation Loss: 2.415093\n",
      "Epoch: 21298 \tTraining Loss: 1.517200 \tValidation Loss: 2.416087\n",
      "Epoch: 21299 \tTraining Loss: 1.512976 \tValidation Loss: 2.414923\n",
      "Epoch: 21300 \tTraining Loss: 1.534099 \tValidation Loss: 2.415392\n",
      "Epoch: 21301 \tTraining Loss: 1.535831 \tValidation Loss: 2.414844\n",
      "Epoch: 21302 \tTraining Loss: 1.551764 \tValidation Loss: 2.414927\n",
      "Epoch: 21303 \tTraining Loss: 1.554634 \tValidation Loss: 2.414649\n",
      "Epoch: 21304 \tTraining Loss: 1.526806 \tValidation Loss: 2.415355\n",
      "Epoch: 21305 \tTraining Loss: 1.516074 \tValidation Loss: 2.414738\n",
      "Epoch: 21306 \tTraining Loss: 1.523071 \tValidation Loss: 2.415558\n",
      "Epoch: 21307 \tTraining Loss: 1.531636 \tValidation Loss: 2.415917\n",
      "Epoch: 21308 \tTraining Loss: 1.505073 \tValidation Loss: 2.415312\n",
      "Epoch: 21309 \tTraining Loss: 1.475487 \tValidation Loss: 2.415813\n",
      "Epoch: 21310 \tTraining Loss: 1.538253 \tValidation Loss: 2.415964\n",
      "Epoch: 21311 \tTraining Loss: 1.571758 \tValidation Loss: 2.414662\n",
      "Epoch: 21312 \tTraining Loss: 1.533489 \tValidation Loss: 2.415775\n",
      "Epoch: 21313 \tTraining Loss: 1.446135 \tValidation Loss: 2.415883\n",
      "Epoch: 21314 \tTraining Loss: 1.549068 \tValidation Loss: 2.415030\n",
      "Epoch: 21315 \tTraining Loss: 1.554552 \tValidation Loss: 2.415622\n",
      "Epoch: 21316 \tTraining Loss: 1.548630 \tValidation Loss: 2.415215\n",
      "Epoch: 21317 \tTraining Loss: 1.540841 \tValidation Loss: 2.414819\n",
      "Epoch: 21318 \tTraining Loss: 1.517730 \tValidation Loss: 2.415592\n",
      "Epoch: 21319 \tTraining Loss: 1.537496 \tValidation Loss: 2.415274\n",
      "Epoch: 21320 \tTraining Loss: 1.570097 \tValidation Loss: 2.414848\n",
      "Epoch: 21321 \tTraining Loss: 1.584466 \tValidation Loss: 2.414385\n",
      "Epoch: 21322 \tTraining Loss: 1.525921 \tValidation Loss: 2.414491\n",
      "Epoch: 21323 \tTraining Loss: 1.501190 \tValidation Loss: 2.415264\n",
      "Epoch: 21324 \tTraining Loss: 1.535996 \tValidation Loss: 2.415048\n",
      "Epoch: 21325 \tTraining Loss: 1.523759 \tValidation Loss: 2.414784\n",
      "Epoch: 21326 \tTraining Loss: 1.544601 \tValidation Loss: 2.415002\n",
      "Epoch: 21327 \tTraining Loss: 1.515121 \tValidation Loss: 2.415698\n",
      "Epoch: 21328 \tTraining Loss: 1.562811 \tValidation Loss: 2.415148\n",
      "Epoch: 21329 \tTraining Loss: 1.487306 \tValidation Loss: 2.415900\n",
      "Epoch: 21330 \tTraining Loss: 1.521228 \tValidation Loss: 2.415394\n",
      "Epoch: 21331 \tTraining Loss: 1.555170 \tValidation Loss: 2.415439\n",
      "Epoch: 21332 \tTraining Loss: 1.555717 \tValidation Loss: 2.415131\n",
      "Epoch: 21333 \tTraining Loss: 1.516112 \tValidation Loss: 2.415404\n",
      "Epoch: 21334 \tTraining Loss: 1.481717 \tValidation Loss: 2.415819\n",
      "Epoch: 21335 \tTraining Loss: 1.521381 \tValidation Loss: 2.415811\n",
      "Epoch: 21336 \tTraining Loss: 1.540008 \tValidation Loss: 2.414877\n",
      "Epoch: 21337 \tTraining Loss: 1.505987 \tValidation Loss: 2.415400\n",
      "Epoch: 21338 \tTraining Loss: 1.501661 \tValidation Loss: 2.415636\n",
      "Epoch: 21339 \tTraining Loss: 1.521582 \tValidation Loss: 2.415487\n",
      "Epoch: 21340 \tTraining Loss: 1.530065 \tValidation Loss: 2.416440\n",
      "Epoch: 21341 \tTraining Loss: 1.501506 \tValidation Loss: 2.416395\n",
      "Epoch: 21342 \tTraining Loss: 1.510412 \tValidation Loss: 2.415805\n",
      "Epoch: 21343 \tTraining Loss: 1.542082 \tValidation Loss: 2.415865\n",
      "Epoch: 21344 \tTraining Loss: 1.546947 \tValidation Loss: 2.415824\n",
      "Epoch: 21345 \tTraining Loss: 1.537421 \tValidation Loss: 2.415454\n",
      "Epoch: 21346 \tTraining Loss: 1.552874 \tValidation Loss: 2.414949\n",
      "Epoch: 21347 \tTraining Loss: 1.509829 \tValidation Loss: 2.415645\n",
      "Epoch: 21348 \tTraining Loss: 1.535300 \tValidation Loss: 2.416142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21349 \tTraining Loss: 1.487599 \tValidation Loss: 2.416219\n",
      "Epoch: 21350 \tTraining Loss: 1.524967 \tValidation Loss: 2.415952\n",
      "Epoch: 21351 \tTraining Loss: 1.557958 \tValidation Loss: 2.416551\n",
      "Epoch: 21352 \tTraining Loss: 1.543764 \tValidation Loss: 2.417194\n",
      "Epoch: 21353 \tTraining Loss: 1.509535 \tValidation Loss: 2.416123\n",
      "Epoch: 21354 \tTraining Loss: 1.518071 \tValidation Loss: 2.416153\n",
      "Epoch: 21355 \tTraining Loss: 1.502199 \tValidation Loss: 2.416250\n",
      "Epoch: 21356 \tTraining Loss: 1.558738 \tValidation Loss: 2.416344\n",
      "Epoch: 21357 \tTraining Loss: 1.505022 \tValidation Loss: 2.416179\n",
      "Epoch: 21358 \tTraining Loss: 1.583871 \tValidation Loss: 2.416317\n",
      "Epoch: 21359 \tTraining Loss: 1.525982 \tValidation Loss: 2.415861\n",
      "Epoch: 21360 \tTraining Loss: 1.495901 \tValidation Loss: 2.415920\n",
      "Epoch: 21361 \tTraining Loss: 1.541065 \tValidation Loss: 2.416197\n",
      "Epoch: 21362 \tTraining Loss: 1.530675 \tValidation Loss: 2.415632\n",
      "Epoch: 21363 \tTraining Loss: 1.547704 \tValidation Loss: 2.415735\n",
      "Epoch: 21364 \tTraining Loss: 1.576091 \tValidation Loss: 2.415196\n",
      "Epoch: 21365 \tTraining Loss: 1.490884 \tValidation Loss: 2.415980\n",
      "Epoch: 21366 \tTraining Loss: 1.498317 \tValidation Loss: 2.416719\n",
      "Epoch: 21367 \tTraining Loss: 1.438222 \tValidation Loss: 2.416932\n",
      "Epoch: 21368 \tTraining Loss: 1.514328 \tValidation Loss: 2.416466\n",
      "Epoch: 21369 \tTraining Loss: 1.525624 \tValidation Loss: 2.416642\n",
      "Epoch: 21370 \tTraining Loss: 1.512433 \tValidation Loss: 2.416549\n",
      "Epoch: 21371 \tTraining Loss: 1.519053 \tValidation Loss: 2.416338\n",
      "Epoch: 21372 \tTraining Loss: 1.511097 \tValidation Loss: 2.415935\n",
      "Epoch: 21373 \tTraining Loss: 1.510414 \tValidation Loss: 2.416921\n",
      "Epoch: 21374 \tTraining Loss: 1.521616 \tValidation Loss: 2.416330\n",
      "Epoch: 21375 \tTraining Loss: 1.537061 \tValidation Loss: 2.416351\n",
      "Epoch: 21376 \tTraining Loss: 1.506992 \tValidation Loss: 2.416278\n",
      "Epoch: 21377 \tTraining Loss: 1.505925 \tValidation Loss: 2.416551\n",
      "Epoch: 21378 \tTraining Loss: 1.516186 \tValidation Loss: 2.416445\n",
      "Epoch: 21379 \tTraining Loss: 1.506839 \tValidation Loss: 2.415924\n",
      "Epoch: 21380 \tTraining Loss: 1.552016 \tValidation Loss: 2.416484\n",
      "Epoch: 21381 \tTraining Loss: 1.514680 \tValidation Loss: 2.416289\n",
      "Epoch: 21382 \tTraining Loss: 1.522278 \tValidation Loss: 2.416214\n",
      "Epoch: 21383 \tTraining Loss: 1.577736 \tValidation Loss: 2.416326\n",
      "Epoch: 21384 \tTraining Loss: 1.537364 \tValidation Loss: 2.416135\n",
      "Epoch: 21385 \tTraining Loss: 1.542360 \tValidation Loss: 2.416428\n",
      "Epoch: 21386 \tTraining Loss: 1.536107 \tValidation Loss: 2.416834\n",
      "Epoch: 21387 \tTraining Loss: 1.514562 \tValidation Loss: 2.415557\n",
      "Epoch: 21388 \tTraining Loss: 1.520351 \tValidation Loss: 2.415893\n",
      "Epoch: 21389 \tTraining Loss: 1.490909 \tValidation Loss: 2.416636\n",
      "Epoch: 21390 \tTraining Loss: 1.502404 \tValidation Loss: 2.416505\n",
      "Epoch: 21391 \tTraining Loss: 1.519385 \tValidation Loss: 2.416855\n",
      "Epoch: 21392 \tTraining Loss: 1.540270 \tValidation Loss: 2.416585\n",
      "Epoch: 21393 \tTraining Loss: 1.536006 \tValidation Loss: 2.416374\n",
      "Epoch: 21394 \tTraining Loss: 1.522288 \tValidation Loss: 2.416446\n",
      "Epoch: 21395 \tTraining Loss: 1.526965 \tValidation Loss: 2.416543\n",
      "Epoch: 21396 \tTraining Loss: 1.546509 \tValidation Loss: 2.415077\n",
      "Epoch: 21397 \tTraining Loss: 1.537818 \tValidation Loss: 2.415680\n",
      "Epoch: 21398 \tTraining Loss: 1.559176 \tValidation Loss: 2.416053\n",
      "Epoch: 21399 \tTraining Loss: 1.517506 \tValidation Loss: 2.417093\n",
      "Epoch: 21400 \tTraining Loss: 1.485245 \tValidation Loss: 2.416814\n",
      "Epoch: 21401 \tTraining Loss: 1.491190 \tValidation Loss: 2.417674\n",
      "Epoch: 21402 \tTraining Loss: 1.505679 \tValidation Loss: 2.417678\n",
      "Epoch: 21403 \tTraining Loss: 1.501575 \tValidation Loss: 2.416950\n",
      "Epoch: 21404 \tTraining Loss: 1.526162 \tValidation Loss: 2.416493\n",
      "Epoch: 21405 \tTraining Loss: 1.527363 \tValidation Loss: 2.415664\n",
      "Epoch: 21406 \tTraining Loss: 1.526621 \tValidation Loss: 2.416446\n",
      "Epoch: 21407 \tTraining Loss: 1.536262 \tValidation Loss: 2.416788\n",
      "Epoch: 21408 \tTraining Loss: 1.515000 \tValidation Loss: 2.416673\n",
      "Epoch: 21409 \tTraining Loss: 1.519807 \tValidation Loss: 2.417305\n",
      "Epoch: 21410 \tTraining Loss: 1.570308 \tValidation Loss: 2.415892\n",
      "Epoch: 21411 \tTraining Loss: 1.524735 \tValidation Loss: 2.415970\n",
      "Epoch: 21412 \tTraining Loss: 1.557095 \tValidation Loss: 2.415663\n",
      "Epoch: 21413 \tTraining Loss: 1.542744 \tValidation Loss: 2.416026\n",
      "Epoch: 21414 \tTraining Loss: 1.511957 \tValidation Loss: 2.416416\n",
      "Epoch: 21415 \tTraining Loss: 1.509794 \tValidation Loss: 2.416093\n",
      "Epoch: 21416 \tTraining Loss: 1.520105 \tValidation Loss: 2.416564\n",
      "Epoch: 21417 \tTraining Loss: 1.495579 \tValidation Loss: 2.417403\n",
      "Epoch: 21418 \tTraining Loss: 1.547748 \tValidation Loss: 2.416731\n",
      "Epoch: 21419 \tTraining Loss: 1.520643 \tValidation Loss: 2.416534\n",
      "Epoch: 21420 \tTraining Loss: 1.541857 \tValidation Loss: 2.416694\n",
      "Epoch: 21421 \tTraining Loss: 1.541783 \tValidation Loss: 2.416202\n",
      "Epoch: 21422 \tTraining Loss: 1.570136 \tValidation Loss: 2.416049\n",
      "Epoch: 21423 \tTraining Loss: 1.519866 \tValidation Loss: 2.417050\n",
      "Epoch: 21424 \tTraining Loss: 1.518571 \tValidation Loss: 2.417106\n",
      "Epoch: 21425 \tTraining Loss: 1.515575 \tValidation Loss: 2.416375\n",
      "Epoch: 21426 \tTraining Loss: 1.518146 \tValidation Loss: 2.416325\n",
      "Epoch: 21427 \tTraining Loss: 1.509174 \tValidation Loss: 2.416624\n",
      "Epoch: 21428 \tTraining Loss: 1.524604 \tValidation Loss: 2.416367\n",
      "Epoch: 21429 \tTraining Loss: 1.543822 \tValidation Loss: 2.416160\n",
      "Epoch: 21430 \tTraining Loss: 1.557128 \tValidation Loss: 2.416490\n",
      "Epoch: 21431 \tTraining Loss: 1.531934 \tValidation Loss: 2.417056\n",
      "Epoch: 21432 \tTraining Loss: 1.538057 \tValidation Loss: 2.417153\n",
      "Epoch: 21433 \tTraining Loss: 1.540048 \tValidation Loss: 2.416729\n",
      "Epoch: 21434 \tTraining Loss: 1.525163 \tValidation Loss: 2.417104\n",
      "Epoch: 21435 \tTraining Loss: 1.490664 \tValidation Loss: 2.417452\n",
      "Epoch: 21436 \tTraining Loss: 1.492079 \tValidation Loss: 2.417490\n",
      "Epoch: 21437 \tTraining Loss: 1.493655 \tValidation Loss: 2.417969\n",
      "Epoch: 21438 \tTraining Loss: 1.550804 \tValidation Loss: 2.416839\n",
      "Epoch: 21439 \tTraining Loss: 1.548447 \tValidation Loss: 2.416836\n",
      "Epoch: 21440 \tTraining Loss: 1.509433 \tValidation Loss: 2.416979\n",
      "Epoch: 21441 \tTraining Loss: 1.536944 \tValidation Loss: 2.417406\n",
      "Epoch: 21442 \tTraining Loss: 1.512345 \tValidation Loss: 2.417053\n",
      "Epoch: 21443 \tTraining Loss: 1.501359 \tValidation Loss: 2.416709\n",
      "Epoch: 21444 \tTraining Loss: 1.502727 \tValidation Loss: 2.417620\n",
      "Epoch: 21445 \tTraining Loss: 1.525956 \tValidation Loss: 2.417626\n",
      "Epoch: 21446 \tTraining Loss: 1.524035 \tValidation Loss: 2.417041\n",
      "Epoch: 21447 \tTraining Loss: 1.556581 \tValidation Loss: 2.417150\n",
      "Epoch: 21448 \tTraining Loss: 1.511538 \tValidation Loss: 2.417567\n",
      "Epoch: 21449 \tTraining Loss: 1.490302 \tValidation Loss: 2.417647\n",
      "Epoch: 21450 \tTraining Loss: 1.510575 \tValidation Loss: 2.417100\n",
      "Epoch: 21451 \tTraining Loss: 1.515768 \tValidation Loss: 2.416843\n",
      "Epoch: 21452 \tTraining Loss: 1.509891 \tValidation Loss: 2.417471\n",
      "Epoch: 21453 \tTraining Loss: 1.561550 \tValidation Loss: 2.416572\n",
      "Epoch: 21454 \tTraining Loss: 1.499150 \tValidation Loss: 2.417250\n",
      "Epoch: 21455 \tTraining Loss: 1.493563 \tValidation Loss: 2.417894\n",
      "Epoch: 21456 \tTraining Loss: 1.495797 \tValidation Loss: 2.417154\n",
      "Epoch: 21457 \tTraining Loss: 1.535992 \tValidation Loss: 2.417773\n",
      "Epoch: 21458 \tTraining Loss: 1.475031 \tValidation Loss: 2.417461\n",
      "Epoch: 21459 \tTraining Loss: 1.517788 \tValidation Loss: 2.417693\n",
      "Epoch: 21460 \tTraining Loss: 1.483674 \tValidation Loss: 2.418033\n",
      "Epoch: 21461 \tTraining Loss: 1.509170 \tValidation Loss: 2.416803\n",
      "Epoch: 21462 \tTraining Loss: 1.493906 \tValidation Loss: 2.418037\n",
      "Epoch: 21463 \tTraining Loss: 1.524291 \tValidation Loss: 2.418647\n",
      "Epoch: 21464 \tTraining Loss: 1.515251 \tValidation Loss: 2.418193\n",
      "Epoch: 21465 \tTraining Loss: 1.505210 \tValidation Loss: 2.418248\n",
      "Epoch: 21466 \tTraining Loss: 1.508078 \tValidation Loss: 2.417949\n",
      "Epoch: 21467 \tTraining Loss: 1.504307 \tValidation Loss: 2.417730\n",
      "Epoch: 21468 \tTraining Loss: 1.519760 \tValidation Loss: 2.417547\n",
      "Epoch: 21469 \tTraining Loss: 1.570911 \tValidation Loss: 2.416868\n",
      "Epoch: 21470 \tTraining Loss: 1.554476 \tValidation Loss: 2.416724\n",
      "Epoch: 21471 \tTraining Loss: 1.532060 \tValidation Loss: 2.417604\n",
      "Epoch: 21472 \tTraining Loss: 1.524390 \tValidation Loss: 2.417359\n",
      "Epoch: 21473 \tTraining Loss: 1.544103 \tValidation Loss: 2.417804\n",
      "Epoch: 21474 \tTraining Loss: 1.514661 \tValidation Loss: 2.417290\n",
      "Epoch: 21475 \tTraining Loss: 1.515470 \tValidation Loss: 2.417444\n",
      "Epoch: 21476 \tTraining Loss: 1.508859 \tValidation Loss: 2.417305\n",
      "Epoch: 21477 \tTraining Loss: 1.509731 \tValidation Loss: 2.417261\n",
      "Epoch: 21478 \tTraining Loss: 1.479908 \tValidation Loss: 2.417765\n",
      "Epoch: 21479 \tTraining Loss: 1.547737 \tValidation Loss: 2.417163\n",
      "Epoch: 21480 \tTraining Loss: 1.518000 \tValidation Loss: 2.417105\n",
      "Epoch: 21481 \tTraining Loss: 1.532353 \tValidation Loss: 2.416920\n",
      "Epoch: 21482 \tTraining Loss: 1.508636 \tValidation Loss: 2.417545\n",
      "Epoch: 21483 \tTraining Loss: 1.537464 \tValidation Loss: 2.416992\n",
      "Epoch: 21484 \tTraining Loss: 1.524394 \tValidation Loss: 2.417293\n",
      "Epoch: 21485 \tTraining Loss: 1.488565 \tValidation Loss: 2.418550\n",
      "Epoch: 21486 \tTraining Loss: 1.551784 \tValidation Loss: 2.418014\n",
      "Epoch: 21487 \tTraining Loss: 1.550455 \tValidation Loss: 2.417761\n",
      "Epoch: 21488 \tTraining Loss: 1.542490 \tValidation Loss: 2.417365\n",
      "Epoch: 21489 \tTraining Loss: 1.555829 \tValidation Loss: 2.417171\n",
      "Epoch: 21490 \tTraining Loss: 1.496721 \tValidation Loss: 2.417621\n",
      "Epoch: 21491 \tTraining Loss: 1.558817 \tValidation Loss: 2.417475\n",
      "Epoch: 21492 \tTraining Loss: 1.537008 \tValidation Loss: 2.418337\n",
      "Epoch: 21493 \tTraining Loss: 1.509951 \tValidation Loss: 2.417339\n",
      "Epoch: 21494 \tTraining Loss: 1.508187 \tValidation Loss: 2.417051\n",
      "Epoch: 21495 \tTraining Loss: 1.550160 \tValidation Loss: 2.417272\n",
      "Epoch: 21496 \tTraining Loss: 1.505340 \tValidation Loss: 2.417676\n",
      "Epoch: 21497 \tTraining Loss: 1.526928 \tValidation Loss: 2.418596\n",
      "Epoch: 21498 \tTraining Loss: 1.507825 \tValidation Loss: 2.418840\n",
      "Epoch: 21499 \tTraining Loss: 1.496513 \tValidation Loss: 2.418459\n",
      "Epoch: 21500 \tTraining Loss: 1.515103 \tValidation Loss: 2.418869\n",
      "Epoch: 21501 \tTraining Loss: 1.489857 \tValidation Loss: 2.418452\n",
      "Epoch: 21502 \tTraining Loss: 1.543820 \tValidation Loss: 2.418315\n",
      "Epoch: 21503 \tTraining Loss: 1.508116 \tValidation Loss: 2.419077\n",
      "Epoch: 21504 \tTraining Loss: 1.538189 \tValidation Loss: 2.419088\n",
      "Epoch: 21505 \tTraining Loss: 1.528991 \tValidation Loss: 2.418864\n",
      "Epoch: 21506 \tTraining Loss: 1.522510 \tValidation Loss: 2.418569\n",
      "Epoch: 21507 \tTraining Loss: 1.516268 \tValidation Loss: 2.418778\n",
      "Epoch: 21508 \tTraining Loss: 1.532758 \tValidation Loss: 2.418974\n",
      "Epoch: 21509 \tTraining Loss: 1.515316 \tValidation Loss: 2.417998\n",
      "Epoch: 21510 \tTraining Loss: 1.492139 \tValidation Loss: 2.418716\n",
      "Epoch: 21511 \tTraining Loss: 1.535886 \tValidation Loss: 2.418220\n",
      "Epoch: 21512 \tTraining Loss: 1.520850 \tValidation Loss: 2.418217\n",
      "Epoch: 21513 \tTraining Loss: 1.498100 \tValidation Loss: 2.418925\n",
      "Epoch: 21514 \tTraining Loss: 1.482112 \tValidation Loss: 2.418590\n",
      "Epoch: 21515 \tTraining Loss: 1.530189 \tValidation Loss: 2.418882\n",
      "Epoch: 21516 \tTraining Loss: 1.524033 \tValidation Loss: 2.418968\n",
      "Epoch: 21517 \tTraining Loss: 1.495484 \tValidation Loss: 2.418807\n",
      "Epoch: 21518 \tTraining Loss: 1.527771 \tValidation Loss: 2.418155\n",
      "Epoch: 21519 \tTraining Loss: 1.554053 \tValidation Loss: 2.418538\n",
      "Epoch: 21520 \tTraining Loss: 1.510887 \tValidation Loss: 2.417675\n",
      "Epoch: 21521 \tTraining Loss: 1.522078 \tValidation Loss: 2.417804\n",
      "Epoch: 21522 \tTraining Loss: 1.488852 \tValidation Loss: 2.418675\n",
      "Epoch: 21523 \tTraining Loss: 1.520198 \tValidation Loss: 2.418307\n",
      "Epoch: 21524 \tTraining Loss: 1.535352 \tValidation Loss: 2.418775\n",
      "Epoch: 21525 \tTraining Loss: 1.509469 \tValidation Loss: 2.418738\n",
      "Epoch: 21526 \tTraining Loss: 1.514822 \tValidation Loss: 2.418618\n",
      "Epoch: 21527 \tTraining Loss: 1.525758 \tValidation Loss: 2.418849\n",
      "Epoch: 21528 \tTraining Loss: 1.523473 \tValidation Loss: 2.418872\n",
      "Epoch: 21529 \tTraining Loss: 1.522316 \tValidation Loss: 2.419082\n",
      "Epoch: 21530 \tTraining Loss: 1.520069 \tValidation Loss: 2.419451\n",
      "Epoch: 21531 \tTraining Loss: 1.505987 \tValidation Loss: 2.418964\n",
      "Epoch: 21532 \tTraining Loss: 1.528670 \tValidation Loss: 2.418451\n",
      "Epoch: 21533 \tTraining Loss: 1.516739 \tValidation Loss: 2.418392\n",
      "Epoch: 21534 \tTraining Loss: 1.512547 \tValidation Loss: 2.418768\n",
      "Epoch: 21535 \tTraining Loss: 1.484467 \tValidation Loss: 2.418176\n",
      "Epoch: 21536 \tTraining Loss: 1.534538 \tValidation Loss: 2.417543\n",
      "Epoch: 21537 \tTraining Loss: 1.508361 \tValidation Loss: 2.417929\n",
      "Epoch: 21538 \tTraining Loss: 1.528412 \tValidation Loss: 2.417757\n",
      "Epoch: 21539 \tTraining Loss: 1.551781 \tValidation Loss: 2.417982\n",
      "Epoch: 21540 \tTraining Loss: 1.469402 \tValidation Loss: 2.418800\n",
      "Epoch: 21541 \tTraining Loss: 1.482869 \tValidation Loss: 2.419016\n",
      "Epoch: 21542 \tTraining Loss: 1.519921 \tValidation Loss: 2.418211\n",
      "Epoch: 21543 \tTraining Loss: 1.563860 \tValidation Loss: 2.417688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21544 \tTraining Loss: 1.506354 \tValidation Loss: 2.418117\n",
      "Epoch: 21545 \tTraining Loss: 1.546861 \tValidation Loss: 2.418528\n",
      "Epoch: 21546 \tTraining Loss: 1.544609 \tValidation Loss: 2.419164\n",
      "Epoch: 21547 \tTraining Loss: 1.499819 \tValidation Loss: 2.419368\n",
      "Epoch: 21548 \tTraining Loss: 1.514724 \tValidation Loss: 2.418983\n",
      "Epoch: 21549 \tTraining Loss: 1.502238 \tValidation Loss: 2.419110\n",
      "Epoch: 21550 \tTraining Loss: 1.528081 \tValidation Loss: 2.418377\n",
      "Epoch: 21551 \tTraining Loss: 1.469763 \tValidation Loss: 2.419614\n",
      "Epoch: 21552 \tTraining Loss: 1.489208 \tValidation Loss: 2.418866\n",
      "Epoch: 21553 \tTraining Loss: 1.501267 \tValidation Loss: 2.418859\n",
      "Epoch: 21554 \tTraining Loss: 1.538212 \tValidation Loss: 2.418612\n",
      "Epoch: 21555 \tTraining Loss: 1.477330 \tValidation Loss: 2.419151\n",
      "Epoch: 21556 \tTraining Loss: 1.523901 \tValidation Loss: 2.419406\n",
      "Epoch: 21557 \tTraining Loss: 1.530813 \tValidation Loss: 2.419346\n",
      "Epoch: 21558 \tTraining Loss: 1.520830 \tValidation Loss: 2.417965\n",
      "Epoch: 21559 \tTraining Loss: 1.522657 \tValidation Loss: 2.418795\n",
      "Epoch: 21560 \tTraining Loss: 1.508530 \tValidation Loss: 2.418463\n",
      "Epoch: 21561 \tTraining Loss: 1.553004 \tValidation Loss: 2.418053\n",
      "Epoch: 21562 \tTraining Loss: 1.532378 \tValidation Loss: 2.418638\n",
      "Epoch: 21563 \tTraining Loss: 1.494714 \tValidation Loss: 2.418827\n",
      "Epoch: 21564 \tTraining Loss: 1.489444 \tValidation Loss: 2.418907\n",
      "Epoch: 21565 \tTraining Loss: 1.534727 \tValidation Loss: 2.419483\n",
      "Epoch: 21566 \tTraining Loss: 1.543110 \tValidation Loss: 2.419473\n",
      "Epoch: 21567 \tTraining Loss: 1.477462 \tValidation Loss: 2.420548\n",
      "Epoch: 21568 \tTraining Loss: 1.496605 \tValidation Loss: 2.419811\n",
      "Epoch: 21569 \tTraining Loss: 1.463123 \tValidation Loss: 2.420415\n",
      "Epoch: 21570 \tTraining Loss: 1.545484 \tValidation Loss: 2.418159\n",
      "Epoch: 21571 \tTraining Loss: 1.461389 \tValidation Loss: 2.419564\n",
      "Epoch: 21572 \tTraining Loss: 1.510613 \tValidation Loss: 2.418798\n",
      "Epoch: 21573 \tTraining Loss: 1.555219 \tValidation Loss: 2.417848\n",
      "Epoch: 21574 \tTraining Loss: 1.455455 \tValidation Loss: 2.418983\n",
      "Epoch: 21575 \tTraining Loss: 1.501915 \tValidation Loss: 2.419428\n",
      "Epoch: 21576 \tTraining Loss: 1.521088 \tValidation Loss: 2.419290\n",
      "Epoch: 21577 \tTraining Loss: 1.503929 \tValidation Loss: 2.420179\n",
      "Epoch: 21578 \tTraining Loss: 1.514321 \tValidation Loss: 2.418991\n",
      "Epoch: 21579 \tTraining Loss: 1.484541 \tValidation Loss: 2.419732\n",
      "Epoch: 21580 \tTraining Loss: 1.525126 \tValidation Loss: 2.419684\n",
      "Epoch: 21581 \tTraining Loss: 1.480844 \tValidation Loss: 2.419273\n",
      "Epoch: 21582 \tTraining Loss: 1.501761 \tValidation Loss: 2.419465\n",
      "Epoch: 21583 \tTraining Loss: 1.491189 \tValidation Loss: 2.419348\n",
      "Epoch: 21584 \tTraining Loss: 1.518087 \tValidation Loss: 2.419240\n",
      "Epoch: 21585 \tTraining Loss: 1.515565 \tValidation Loss: 2.418646\n",
      "Epoch: 21586 \tTraining Loss: 1.520703 \tValidation Loss: 2.418540\n",
      "Epoch: 21587 \tTraining Loss: 1.533183 \tValidation Loss: 2.418308\n",
      "Epoch: 21588 \tTraining Loss: 1.490657 \tValidation Loss: 2.418387\n",
      "Epoch: 21589 \tTraining Loss: 1.520577 \tValidation Loss: 2.418340\n",
      "Epoch: 21590 \tTraining Loss: 1.491870 \tValidation Loss: 2.419252\n",
      "Epoch: 21591 \tTraining Loss: 1.531527 \tValidation Loss: 2.419547\n",
      "Epoch: 21592 \tTraining Loss: 1.537076 \tValidation Loss: 2.419403\n",
      "Epoch: 21593 \tTraining Loss: 1.510897 \tValidation Loss: 2.419266\n",
      "Epoch: 21594 \tTraining Loss: 1.522299 \tValidation Loss: 2.419879\n",
      "Epoch: 21595 \tTraining Loss: 1.518411 \tValidation Loss: 2.419830\n",
      "Epoch: 21596 \tTraining Loss: 1.522394 \tValidation Loss: 2.419001\n",
      "Epoch: 21597 \tTraining Loss: 1.515786 \tValidation Loss: 2.419145\n",
      "Epoch: 21598 \tTraining Loss: 1.545206 \tValidation Loss: 2.419133\n",
      "Epoch: 21599 \tTraining Loss: 1.519991 \tValidation Loss: 2.419141\n",
      "Epoch: 21600 \tTraining Loss: 1.519855 \tValidation Loss: 2.419649\n",
      "Epoch: 21601 \tTraining Loss: 1.511117 \tValidation Loss: 2.419710\n",
      "Epoch: 21602 \tTraining Loss: 1.520716 \tValidation Loss: 2.420026\n",
      "Epoch: 21603 \tTraining Loss: 1.542502 \tValidation Loss: 2.419432\n",
      "Epoch: 21604 \tTraining Loss: 1.500479 \tValidation Loss: 2.419557\n",
      "Epoch: 21605 \tTraining Loss: 1.534303 \tValidation Loss: 2.419706\n",
      "Epoch: 21606 \tTraining Loss: 1.480801 \tValidation Loss: 2.419978\n",
      "Epoch: 21607 \tTraining Loss: 1.511715 \tValidation Loss: 2.419783\n",
      "Epoch: 21608 \tTraining Loss: 1.475888 \tValidation Loss: 2.420124\n",
      "Epoch: 21609 \tTraining Loss: 1.497541 \tValidation Loss: 2.420374\n",
      "Epoch: 21610 \tTraining Loss: 1.518327 \tValidation Loss: 2.420012\n",
      "Epoch: 21611 \tTraining Loss: 1.484425 \tValidation Loss: 2.420216\n",
      "Epoch: 21612 \tTraining Loss: 1.528839 \tValidation Loss: 2.419666\n",
      "Epoch: 21613 \tTraining Loss: 1.540469 \tValidation Loss: 2.419800\n",
      "Epoch: 21614 \tTraining Loss: 1.563094 \tValidation Loss: 2.419296\n",
      "Epoch: 21615 \tTraining Loss: 1.495136 \tValidation Loss: 2.419859\n",
      "Epoch: 21616 \tTraining Loss: 1.534394 \tValidation Loss: 2.420234\n",
      "Epoch: 21617 \tTraining Loss: 1.519541 \tValidation Loss: 2.419329\n",
      "Epoch: 21618 \tTraining Loss: 1.506467 \tValidation Loss: 2.419137\n",
      "Epoch: 21619 \tTraining Loss: 1.483327 \tValidation Loss: 2.418767\n",
      "Epoch: 21620 \tTraining Loss: 1.548602 \tValidation Loss: 2.419160\n",
      "Epoch: 21621 \tTraining Loss: 1.539714 \tValidation Loss: 2.419286\n",
      "Epoch: 21622 \tTraining Loss: 1.506402 \tValidation Loss: 2.420199\n",
      "Epoch: 21623 \tTraining Loss: 1.479039 \tValidation Loss: 2.420211\n",
      "Epoch: 21624 \tTraining Loss: 1.492867 \tValidation Loss: 2.420103\n",
      "Epoch: 21625 \tTraining Loss: 1.470697 \tValidation Loss: 2.420542\n",
      "Epoch: 21626 \tTraining Loss: 1.516141 \tValidation Loss: 2.420639\n",
      "Epoch: 21627 \tTraining Loss: 1.519990 \tValidation Loss: 2.419897\n",
      "Epoch: 21628 \tTraining Loss: 1.537530 \tValidation Loss: 2.419976\n",
      "Epoch: 21629 \tTraining Loss: 1.513471 \tValidation Loss: 2.419842\n",
      "Epoch: 21630 \tTraining Loss: 1.484051 \tValidation Loss: 2.420322\n",
      "Epoch: 21631 \tTraining Loss: 1.520679 \tValidation Loss: 2.419811\n",
      "Epoch: 21632 \tTraining Loss: 1.485389 \tValidation Loss: 2.419605\n",
      "Epoch: 21633 \tTraining Loss: 1.508942 \tValidation Loss: 2.419802\n",
      "Epoch: 21634 \tTraining Loss: 1.490198 \tValidation Loss: 2.420807\n",
      "Epoch: 21635 \tTraining Loss: 1.549117 \tValidation Loss: 2.420055\n",
      "Epoch: 21636 \tTraining Loss: 1.525055 \tValidation Loss: 2.419924\n",
      "Epoch: 21637 \tTraining Loss: 1.509164 \tValidation Loss: 2.419639\n",
      "Epoch: 21638 \tTraining Loss: 1.489228 \tValidation Loss: 2.419891\n",
      "Epoch: 21639 \tTraining Loss: 1.529461 \tValidation Loss: 2.419469\n",
      "Epoch: 21640 \tTraining Loss: 1.508622 \tValidation Loss: 2.420203\n",
      "Epoch: 21641 \tTraining Loss: 1.521810 \tValidation Loss: 2.420104\n",
      "Epoch: 21642 \tTraining Loss: 1.545374 \tValidation Loss: 2.420105\n",
      "Epoch: 21643 \tTraining Loss: 1.540204 \tValidation Loss: 2.419873\n",
      "Epoch: 21644 \tTraining Loss: 1.482427 \tValidation Loss: 2.420700\n",
      "Epoch: 21645 \tTraining Loss: 1.519183 \tValidation Loss: 2.421034\n",
      "Epoch: 21646 \tTraining Loss: 1.456855 \tValidation Loss: 2.421613\n",
      "Epoch: 21647 \tTraining Loss: 1.525826 \tValidation Loss: 2.420738\n",
      "Epoch: 21648 \tTraining Loss: 1.475231 \tValidation Loss: 2.420479\n",
      "Epoch: 21649 \tTraining Loss: 1.530844 \tValidation Loss: 2.419894\n",
      "Epoch: 21650 \tTraining Loss: 1.517579 \tValidation Loss: 2.419624\n",
      "Epoch: 21651 \tTraining Loss: 1.511366 \tValidation Loss: 2.419988\n",
      "Epoch: 21652 \tTraining Loss: 1.533973 \tValidation Loss: 2.420354\n",
      "Epoch: 21653 \tTraining Loss: 1.511956 \tValidation Loss: 2.420106\n",
      "Epoch: 21654 \tTraining Loss: 1.490930 \tValidation Loss: 2.420717\n",
      "Epoch: 21655 \tTraining Loss: 1.560537 \tValidation Loss: 2.419888\n",
      "Epoch: 21656 \tTraining Loss: 1.547554 \tValidation Loss: 2.420053\n",
      "Epoch: 21657 \tTraining Loss: 1.506232 \tValidation Loss: 2.421413\n",
      "Epoch: 21658 \tTraining Loss: 1.526073 \tValidation Loss: 2.420382\n",
      "Epoch: 21659 \tTraining Loss: 1.491111 \tValidation Loss: 2.420732\n",
      "Epoch: 21660 \tTraining Loss: 1.464006 \tValidation Loss: 2.420265\n",
      "Epoch: 21661 \tTraining Loss: 1.508393 \tValidation Loss: 2.421285\n",
      "Epoch: 21662 \tTraining Loss: 1.524323 \tValidation Loss: 2.420357\n",
      "Epoch: 21663 \tTraining Loss: 1.575006 \tValidation Loss: 2.419801\n",
      "Epoch: 21664 \tTraining Loss: 1.503894 \tValidation Loss: 2.420386\n",
      "Epoch: 21665 \tTraining Loss: 1.528057 \tValidation Loss: 2.420754\n",
      "Epoch: 21666 \tTraining Loss: 1.490754 \tValidation Loss: 2.420771\n",
      "Epoch: 21667 \tTraining Loss: 1.511036 \tValidation Loss: 2.421331\n",
      "Epoch: 21668 \tTraining Loss: 1.489250 \tValidation Loss: 2.421675\n",
      "Epoch: 21669 \tTraining Loss: 1.481231 \tValidation Loss: 2.420930\n",
      "Epoch: 21670 \tTraining Loss: 1.524972 \tValidation Loss: 2.420592\n",
      "Epoch: 21671 \tTraining Loss: 1.514393 \tValidation Loss: 2.420217\n",
      "Epoch: 21672 \tTraining Loss: 1.524562 \tValidation Loss: 2.420011\n",
      "Epoch: 21673 \tTraining Loss: 1.516228 \tValidation Loss: 2.420833\n",
      "Epoch: 21674 \tTraining Loss: 1.497989 \tValidation Loss: 2.420712\n",
      "Epoch: 21675 \tTraining Loss: 1.503873 \tValidation Loss: 2.421776\n",
      "Epoch: 21676 \tTraining Loss: 1.533202 \tValidation Loss: 2.420477\n",
      "Epoch: 21677 \tTraining Loss: 1.498143 \tValidation Loss: 2.421190\n",
      "Epoch: 21678 \tTraining Loss: 1.564420 \tValidation Loss: 2.420212\n",
      "Epoch: 21679 \tTraining Loss: 1.514692 \tValidation Loss: 2.420307\n",
      "Epoch: 21680 \tTraining Loss: 1.493577 \tValidation Loss: 2.421134\n",
      "Epoch: 21681 \tTraining Loss: 1.521961 \tValidation Loss: 2.421014\n",
      "Epoch: 21682 \tTraining Loss: 1.544131 \tValidation Loss: 2.421307\n",
      "Epoch: 21683 \tTraining Loss: 1.509090 \tValidation Loss: 2.420652\n",
      "Epoch: 21684 \tTraining Loss: 1.471360 \tValidation Loss: 2.421121\n",
      "Epoch: 21685 \tTraining Loss: 1.513994 \tValidation Loss: 2.420763\n",
      "Epoch: 21686 \tTraining Loss: 1.490395 \tValidation Loss: 2.420993\n",
      "Epoch: 21687 \tTraining Loss: 1.517754 \tValidation Loss: 2.421383\n",
      "Epoch: 21688 \tTraining Loss: 1.493100 \tValidation Loss: 2.421232\n",
      "Epoch: 21689 \tTraining Loss: 1.494563 \tValidation Loss: 2.422332\n",
      "Epoch: 21690 \tTraining Loss: 1.485306 \tValidation Loss: 2.421011\n",
      "Epoch: 21691 \tTraining Loss: 1.508958 \tValidation Loss: 2.421344\n",
      "Epoch: 21692 \tTraining Loss: 1.500473 \tValidation Loss: 2.421250\n",
      "Epoch: 21693 \tTraining Loss: 1.514217 \tValidation Loss: 2.421309\n",
      "Epoch: 21694 \tTraining Loss: 1.514693 \tValidation Loss: 2.420293\n",
      "Epoch: 21695 \tTraining Loss: 1.557860 \tValidation Loss: 2.420301\n",
      "Epoch: 21696 \tTraining Loss: 1.510654 \tValidation Loss: 2.421368\n",
      "Epoch: 21697 \tTraining Loss: 1.500369 \tValidation Loss: 2.421316\n",
      "Epoch: 21698 \tTraining Loss: 1.499617 \tValidation Loss: 2.420363\n",
      "Epoch: 21699 \tTraining Loss: 1.487908 \tValidation Loss: 2.420638\n",
      "Epoch: 21700 \tTraining Loss: 1.453331 \tValidation Loss: 2.420988\n",
      "Epoch: 21701 \tTraining Loss: 1.510748 \tValidation Loss: 2.420779\n",
      "Epoch: 21702 \tTraining Loss: 1.528422 \tValidation Loss: 2.421110\n",
      "Epoch: 21703 \tTraining Loss: 1.532396 \tValidation Loss: 2.420646\n",
      "Epoch: 21704 \tTraining Loss: 1.474842 \tValidation Loss: 2.421985\n",
      "Epoch: 21705 \tTraining Loss: 1.511219 \tValidation Loss: 2.420933\n",
      "Epoch: 21706 \tTraining Loss: 1.512573 \tValidation Loss: 2.421016\n",
      "Epoch: 21707 \tTraining Loss: 1.499428 \tValidation Loss: 2.421417\n",
      "Epoch: 21708 \tTraining Loss: 1.521417 \tValidation Loss: 2.420680\n",
      "Epoch: 21709 \tTraining Loss: 1.513225 \tValidation Loss: 2.421171\n",
      "Epoch: 21710 \tTraining Loss: 1.502261 \tValidation Loss: 2.421860\n",
      "Epoch: 21711 \tTraining Loss: 1.460615 \tValidation Loss: 2.422363\n",
      "Epoch: 21712 \tTraining Loss: 1.490861 \tValidation Loss: 2.421891\n",
      "Epoch: 21713 \tTraining Loss: 1.521158 \tValidation Loss: 2.422054\n",
      "Epoch: 21714 \tTraining Loss: 1.520696 \tValidation Loss: 2.421454\n",
      "Epoch: 21715 \tTraining Loss: 1.507192 \tValidation Loss: 2.421493\n",
      "Epoch: 21716 \tTraining Loss: 1.510177 \tValidation Loss: 2.421893\n",
      "Epoch: 21717 \tTraining Loss: 1.511578 \tValidation Loss: 2.421673\n",
      "Epoch: 21718 \tTraining Loss: 1.489063 \tValidation Loss: 2.421235\n",
      "Epoch: 21719 \tTraining Loss: 1.522671 \tValidation Loss: 2.421414\n",
      "Epoch: 21720 \tTraining Loss: 1.528094 \tValidation Loss: 2.421707\n",
      "Epoch: 21721 \tTraining Loss: 1.490933 \tValidation Loss: 2.421506\n",
      "Epoch: 21722 \tTraining Loss: 1.561937 \tValidation Loss: 2.420749\n",
      "Epoch: 21723 \tTraining Loss: 1.491793 \tValidation Loss: 2.420342\n",
      "Epoch: 21724 \tTraining Loss: 1.489209 \tValidation Loss: 2.420497\n",
      "Epoch: 21725 \tTraining Loss: 1.451560 \tValidation Loss: 2.421585\n",
      "Epoch: 21726 \tTraining Loss: 1.429415 \tValidation Loss: 2.422729\n",
      "Epoch: 21727 \tTraining Loss: 1.514827 \tValidation Loss: 2.421548\n",
      "Epoch: 21728 \tTraining Loss: 1.506594 \tValidation Loss: 2.421673\n",
      "Epoch: 21729 \tTraining Loss: 1.510211 \tValidation Loss: 2.421794\n",
      "Epoch: 21730 \tTraining Loss: 1.478403 \tValidation Loss: 2.421466\n",
      "Epoch: 21731 \tTraining Loss: 1.489666 \tValidation Loss: 2.420883\n",
      "Epoch: 21732 \tTraining Loss: 1.510470 \tValidation Loss: 2.421371\n",
      "Epoch: 21733 \tTraining Loss: 1.497295 \tValidation Loss: 2.422148\n",
      "Epoch: 21734 \tTraining Loss: 1.526397 \tValidation Loss: 2.421908\n",
      "Epoch: 21735 \tTraining Loss: 1.508026 \tValidation Loss: 2.421949\n",
      "Epoch: 21736 \tTraining Loss: 1.512606 \tValidation Loss: 2.422058\n",
      "Epoch: 21737 \tTraining Loss: 1.500956 \tValidation Loss: 2.421781\n",
      "Epoch: 21738 \tTraining Loss: 1.508617 \tValidation Loss: 2.423358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21739 \tTraining Loss: 1.542502 \tValidation Loss: 2.422131\n",
      "Epoch: 21740 \tTraining Loss: 1.514571 \tValidation Loss: 2.422352\n",
      "Epoch: 21741 \tTraining Loss: 1.498040 \tValidation Loss: 2.422714\n",
      "Epoch: 21742 \tTraining Loss: 1.538730 \tValidation Loss: 2.421889\n",
      "Epoch: 21743 \tTraining Loss: 1.540467 \tValidation Loss: 2.421246\n",
      "Epoch: 21744 \tTraining Loss: 1.479295 \tValidation Loss: 2.422210\n",
      "Epoch: 21745 \tTraining Loss: 1.531470 \tValidation Loss: 2.421735\n",
      "Epoch: 21746 \tTraining Loss: 1.446571 \tValidation Loss: 2.422585\n",
      "Epoch: 21747 \tTraining Loss: 1.486222 \tValidation Loss: 2.422559\n",
      "Epoch: 21748 \tTraining Loss: 1.459441 \tValidation Loss: 2.422708\n",
      "Epoch: 21749 \tTraining Loss: 1.484962 \tValidation Loss: 2.422679\n",
      "Epoch: 21750 \tTraining Loss: 1.517175 \tValidation Loss: 2.422101\n",
      "Epoch: 21751 \tTraining Loss: 1.511943 \tValidation Loss: 2.421767\n",
      "Epoch: 21752 \tTraining Loss: 1.509186 \tValidation Loss: 2.422442\n",
      "Epoch: 21753 \tTraining Loss: 1.512571 \tValidation Loss: 2.422338\n",
      "Epoch: 21754 \tTraining Loss: 1.513021 \tValidation Loss: 2.421774\n",
      "Epoch: 21755 \tTraining Loss: 1.506672 \tValidation Loss: 2.422482\n",
      "Epoch: 21756 \tTraining Loss: 1.464466 \tValidation Loss: 2.423545\n",
      "Epoch: 21757 \tTraining Loss: 1.521161 \tValidation Loss: 2.422625\n",
      "Epoch: 21758 \tTraining Loss: 1.534456 \tValidation Loss: 2.422688\n",
      "Epoch: 21759 \tTraining Loss: 1.519084 \tValidation Loss: 2.422494\n",
      "Epoch: 21760 \tTraining Loss: 1.563247 \tValidation Loss: 2.422167\n",
      "Epoch: 21761 \tTraining Loss: 1.566273 \tValidation Loss: 2.422018\n",
      "Epoch: 21762 \tTraining Loss: 1.504578 \tValidation Loss: 2.422065\n",
      "Epoch: 21763 \tTraining Loss: 1.479510 \tValidation Loss: 2.421705\n",
      "Epoch: 21764 \tTraining Loss: 1.537658 \tValidation Loss: 2.421870\n",
      "Epoch: 21765 \tTraining Loss: 1.536426 \tValidation Loss: 2.422014\n",
      "Epoch: 21766 \tTraining Loss: 1.538418 \tValidation Loss: 2.421773\n",
      "Epoch: 21767 \tTraining Loss: 1.520767 \tValidation Loss: 2.422151\n",
      "Epoch: 21768 \tTraining Loss: 1.491549 \tValidation Loss: 2.423094\n",
      "Epoch: 21769 \tTraining Loss: 1.533122 \tValidation Loss: 2.422568\n",
      "Epoch: 21770 \tTraining Loss: 1.507553 \tValidation Loss: 2.423362\n",
      "Epoch: 21771 \tTraining Loss: 1.528662 \tValidation Loss: 2.422730\n",
      "Epoch: 21772 \tTraining Loss: 1.509014 \tValidation Loss: 2.422843\n",
      "Epoch: 21773 \tTraining Loss: 1.517772 \tValidation Loss: 2.422643\n",
      "Epoch: 21774 \tTraining Loss: 1.480678 \tValidation Loss: 2.422436\n",
      "Epoch: 21775 \tTraining Loss: 1.512726 \tValidation Loss: 2.422659\n",
      "Epoch: 21776 \tTraining Loss: 1.529159 \tValidation Loss: 2.421891\n",
      "Epoch: 21777 \tTraining Loss: 1.501863 \tValidation Loss: 2.421619\n",
      "Epoch: 21778 \tTraining Loss: 1.544471 \tValidation Loss: 2.422060\n",
      "Epoch: 21779 \tTraining Loss: 1.536257 \tValidation Loss: 2.421962\n",
      "Epoch: 21780 \tTraining Loss: 1.501861 \tValidation Loss: 2.421561\n",
      "Epoch: 21781 \tTraining Loss: 1.499452 \tValidation Loss: 2.422329\n",
      "Epoch: 21782 \tTraining Loss: 1.589576 \tValidation Loss: 2.420960\n",
      "Epoch: 21783 \tTraining Loss: 1.512172 \tValidation Loss: 2.422781\n",
      "Epoch: 21784 \tTraining Loss: 1.471789 \tValidation Loss: 2.422798\n",
      "Epoch: 21785 \tTraining Loss: 1.519066 \tValidation Loss: 2.422205\n",
      "Epoch: 21786 \tTraining Loss: 1.532720 \tValidation Loss: 2.421587\n",
      "Epoch: 21787 \tTraining Loss: 1.467519 \tValidation Loss: 2.422514\n",
      "Epoch: 21788 \tTraining Loss: 1.508363 \tValidation Loss: 2.422209\n",
      "Epoch: 21789 \tTraining Loss: 1.483873 \tValidation Loss: 2.422732\n",
      "Epoch: 21790 \tTraining Loss: 1.470284 \tValidation Loss: 2.422381\n",
      "Epoch: 21791 \tTraining Loss: 1.492699 \tValidation Loss: 2.422637\n",
      "Epoch: 21792 \tTraining Loss: 1.483336 \tValidation Loss: 2.422894\n",
      "Epoch: 21793 \tTraining Loss: 1.507695 \tValidation Loss: 2.422244\n",
      "Epoch: 21794 \tTraining Loss: 1.486028 \tValidation Loss: 2.422648\n",
      "Epoch: 21795 \tTraining Loss: 1.521768 \tValidation Loss: 2.422896\n",
      "Epoch: 21796 \tTraining Loss: 1.519282 \tValidation Loss: 2.423048\n",
      "Epoch: 21797 \tTraining Loss: 1.473059 \tValidation Loss: 2.423697\n",
      "Epoch: 21798 \tTraining Loss: 1.559716 \tValidation Loss: 2.421987\n",
      "Epoch: 21799 \tTraining Loss: 1.510403 \tValidation Loss: 2.422460\n",
      "Epoch: 21800 \tTraining Loss: 1.475291 \tValidation Loss: 2.422964\n",
      "Epoch: 21801 \tTraining Loss: 1.522952 \tValidation Loss: 2.422335\n",
      "Epoch: 21802 \tTraining Loss: 1.452255 \tValidation Loss: 2.422846\n",
      "Epoch: 21803 \tTraining Loss: 1.519534 \tValidation Loss: 2.422587\n",
      "Epoch: 21804 \tTraining Loss: 1.505337 \tValidation Loss: 2.422554\n",
      "Epoch: 21805 \tTraining Loss: 1.565172 \tValidation Loss: 2.421791\n",
      "Epoch: 21806 \tTraining Loss: 1.509500 \tValidation Loss: 2.423204\n",
      "Epoch: 21807 \tTraining Loss: 1.471059 \tValidation Loss: 2.423382\n",
      "Epoch: 21808 \tTraining Loss: 1.492273 \tValidation Loss: 2.422986\n",
      "Epoch: 21809 \tTraining Loss: 1.487282 \tValidation Loss: 2.423564\n",
      "Epoch: 21810 \tTraining Loss: 1.517950 \tValidation Loss: 2.424160\n",
      "Epoch: 21811 \tTraining Loss: 1.495190 \tValidation Loss: 2.423228\n",
      "Epoch: 21812 \tTraining Loss: 1.505254 \tValidation Loss: 2.422451\n",
      "Epoch: 21813 \tTraining Loss: 1.495901 \tValidation Loss: 2.423229\n",
      "Epoch: 21814 \tTraining Loss: 1.506704 \tValidation Loss: 2.423223\n",
      "Epoch: 21815 \tTraining Loss: 1.493987 \tValidation Loss: 2.423327\n",
      "Epoch: 21816 \tTraining Loss: 1.497130 \tValidation Loss: 2.423790\n",
      "Epoch: 21817 \tTraining Loss: 1.478930 \tValidation Loss: 2.423579\n",
      "Epoch: 21818 \tTraining Loss: 1.497365 \tValidation Loss: 2.422700\n",
      "Epoch: 21819 \tTraining Loss: 1.522863 \tValidation Loss: 2.422999\n",
      "Epoch: 21820 \tTraining Loss: 1.509343 \tValidation Loss: 2.422382\n",
      "Epoch: 21821 \tTraining Loss: 1.513617 \tValidation Loss: 2.422142\n",
      "Epoch: 21822 \tTraining Loss: 1.525953 \tValidation Loss: 2.421933\n",
      "Epoch: 21823 \tTraining Loss: 1.489110 \tValidation Loss: 2.422520\n",
      "Epoch: 21824 \tTraining Loss: 1.457723 \tValidation Loss: 2.423785\n",
      "Epoch: 21825 \tTraining Loss: 1.531557 \tValidation Loss: 2.422416\n",
      "Epoch: 21826 \tTraining Loss: 1.504179 \tValidation Loss: 2.422312\n",
      "Epoch: 21827 \tTraining Loss: 1.522797 \tValidation Loss: 2.422819\n",
      "Epoch: 21828 \tTraining Loss: 1.452973 \tValidation Loss: 2.422833\n",
      "Epoch: 21829 \tTraining Loss: 1.462522 \tValidation Loss: 2.423394\n",
      "Epoch: 21830 \tTraining Loss: 1.544792 \tValidation Loss: 2.422969\n",
      "Epoch: 21831 \tTraining Loss: 1.536874 \tValidation Loss: 2.423416\n",
      "Epoch: 21832 \tTraining Loss: 1.546100 \tValidation Loss: 2.422521\n",
      "Epoch: 21833 \tTraining Loss: 1.459239 \tValidation Loss: 2.423688\n",
      "Epoch: 21834 \tTraining Loss: 1.508880 \tValidation Loss: 2.423748\n",
      "Epoch: 21835 \tTraining Loss: 1.502934 \tValidation Loss: 2.423019\n",
      "Epoch: 21836 \tTraining Loss: 1.471739 \tValidation Loss: 2.424016\n",
      "Epoch: 21837 \tTraining Loss: 1.515812 \tValidation Loss: 2.423129\n",
      "Epoch: 21838 \tTraining Loss: 1.450136 \tValidation Loss: 2.424287\n",
      "Epoch: 21839 \tTraining Loss: 1.486941 \tValidation Loss: 2.423760\n",
      "Epoch: 21840 \tTraining Loss: 1.531308 \tValidation Loss: 2.423542\n",
      "Epoch: 21841 \tTraining Loss: 1.499102 \tValidation Loss: 2.424122\n",
      "Epoch: 21842 \tTraining Loss: 1.517391 \tValidation Loss: 2.423683\n",
      "Epoch: 21843 \tTraining Loss: 1.512515 \tValidation Loss: 2.423930\n",
      "Epoch: 21844 \tTraining Loss: 1.520150 \tValidation Loss: 2.423320\n",
      "Epoch: 21845 \tTraining Loss: 1.551655 \tValidation Loss: 2.423887\n",
      "Epoch: 21846 \tTraining Loss: 1.559546 \tValidation Loss: 2.423687\n",
      "Epoch: 21847 \tTraining Loss: 1.506585 \tValidation Loss: 2.422600\n",
      "Epoch: 21848 \tTraining Loss: 1.532794 \tValidation Loss: 2.423180\n",
      "Epoch: 21849 \tTraining Loss: 1.481300 \tValidation Loss: 2.423703\n",
      "Epoch: 21850 \tTraining Loss: 1.529231 \tValidation Loss: 2.423719\n",
      "Epoch: 21851 \tTraining Loss: 1.534007 \tValidation Loss: 2.423936\n",
      "Epoch: 21852 \tTraining Loss: 1.495195 \tValidation Loss: 2.424282\n",
      "Epoch: 21853 \tTraining Loss: 1.507171 \tValidation Loss: 2.422830\n",
      "Epoch: 21854 \tTraining Loss: 1.506820 \tValidation Loss: 2.422914\n",
      "Epoch: 21855 \tTraining Loss: 1.483536 \tValidation Loss: 2.424045\n",
      "Epoch: 21856 \tTraining Loss: 1.482374 \tValidation Loss: 2.423871\n",
      "Epoch: 21857 \tTraining Loss: 1.512917 \tValidation Loss: 2.423683\n",
      "Epoch: 21858 \tTraining Loss: 1.514159 \tValidation Loss: 2.423201\n",
      "Epoch: 21859 \tTraining Loss: 1.484515 \tValidation Loss: 2.423954\n",
      "Epoch: 21860 \tTraining Loss: 1.518167 \tValidation Loss: 2.424011\n",
      "Epoch: 21861 \tTraining Loss: 1.490664 \tValidation Loss: 2.423419\n",
      "Epoch: 21862 \tTraining Loss: 1.523443 \tValidation Loss: 2.423322\n",
      "Epoch: 21863 \tTraining Loss: 1.525611 \tValidation Loss: 2.423985\n",
      "Epoch: 21864 \tTraining Loss: 1.499726 \tValidation Loss: 2.423630\n",
      "Epoch: 21865 \tTraining Loss: 1.507472 \tValidation Loss: 2.423931\n",
      "Epoch: 21866 \tTraining Loss: 1.491094 \tValidation Loss: 2.424712\n",
      "Epoch: 21867 \tTraining Loss: 1.534097 \tValidation Loss: 2.423435\n",
      "Epoch: 21868 \tTraining Loss: 1.534072 \tValidation Loss: 2.422999\n",
      "Epoch: 21869 \tTraining Loss: 1.501536 \tValidation Loss: 2.423422\n",
      "Epoch: 21870 \tTraining Loss: 1.507993 \tValidation Loss: 2.424506\n",
      "Epoch: 21871 \tTraining Loss: 1.512544 \tValidation Loss: 2.423755\n",
      "Epoch: 21872 \tTraining Loss: 1.527742 \tValidation Loss: 2.423279\n",
      "Epoch: 21873 \tTraining Loss: 1.549545 \tValidation Loss: 2.423244\n",
      "Epoch: 21874 \tTraining Loss: 1.511837 \tValidation Loss: 2.423579\n",
      "Epoch: 21875 \tTraining Loss: 1.496509 \tValidation Loss: 2.424173\n",
      "Epoch: 21876 \tTraining Loss: 1.492833 \tValidation Loss: 2.423782\n",
      "Epoch: 21877 \tTraining Loss: 1.461918 \tValidation Loss: 2.424890\n",
      "Epoch: 21878 \tTraining Loss: 1.498713 \tValidation Loss: 2.423366\n",
      "Epoch: 21879 \tTraining Loss: 1.539437 \tValidation Loss: 2.423084\n",
      "Epoch: 21880 \tTraining Loss: 1.519618 \tValidation Loss: 2.423885\n",
      "Epoch: 21881 \tTraining Loss: 1.465077 \tValidation Loss: 2.423864\n",
      "Epoch: 21882 \tTraining Loss: 1.523312 \tValidation Loss: 2.424323\n",
      "Epoch: 21883 \tTraining Loss: 1.551110 \tValidation Loss: 2.423585\n",
      "Epoch: 21884 \tTraining Loss: 1.502118 \tValidation Loss: 2.423980\n",
      "Epoch: 21885 \tTraining Loss: 1.534608 \tValidation Loss: 2.424174\n",
      "Epoch: 21886 \tTraining Loss: 1.527484 \tValidation Loss: 2.423048\n",
      "Epoch: 21887 \tTraining Loss: 1.542841 \tValidation Loss: 2.423551\n",
      "Epoch: 21888 \tTraining Loss: 1.475261 \tValidation Loss: 2.424075\n",
      "Epoch: 21889 \tTraining Loss: 1.487628 \tValidation Loss: 2.423941\n",
      "Epoch: 21890 \tTraining Loss: 1.536498 \tValidation Loss: 2.423753\n",
      "Epoch: 21891 \tTraining Loss: 1.501885 \tValidation Loss: 2.422533\n",
      "Epoch: 21892 \tTraining Loss: 1.513289 \tValidation Loss: 2.423381\n",
      "Epoch: 21893 \tTraining Loss: 1.501871 \tValidation Loss: 2.423689\n",
      "Epoch: 21894 \tTraining Loss: 1.555771 \tValidation Loss: 2.423700\n",
      "Epoch: 21895 \tTraining Loss: 1.508212 \tValidation Loss: 2.423973\n",
      "Epoch: 21896 \tTraining Loss: 1.528481 \tValidation Loss: 2.423213\n",
      "Epoch: 21897 \tTraining Loss: 1.477970 \tValidation Loss: 2.424315\n",
      "Epoch: 21898 \tTraining Loss: 1.468216 \tValidation Loss: 2.424503\n",
      "Epoch: 21899 \tTraining Loss: 1.529716 \tValidation Loss: 2.424014\n",
      "Epoch: 21900 \tTraining Loss: 1.467482 \tValidation Loss: 2.424619\n",
      "Epoch: 21901 \tTraining Loss: 1.522354 \tValidation Loss: 2.424886\n",
      "Epoch: 21902 \tTraining Loss: 1.502114 \tValidation Loss: 2.424732\n",
      "Epoch: 21903 \tTraining Loss: 1.498372 \tValidation Loss: 2.424942\n",
      "Epoch: 21904 \tTraining Loss: 1.498959 \tValidation Loss: 2.425523\n",
      "Epoch: 21905 \tTraining Loss: 1.551904 \tValidation Loss: 2.424045\n",
      "Epoch: 21906 \tTraining Loss: 1.489302 \tValidation Loss: 2.424861\n",
      "Epoch: 21907 \tTraining Loss: 1.541506 \tValidation Loss: 2.425444\n",
      "Epoch: 21908 \tTraining Loss: 1.575699 \tValidation Loss: 2.424276\n",
      "Epoch: 21909 \tTraining Loss: 1.494499 \tValidation Loss: 2.424356\n",
      "Epoch: 21910 \tTraining Loss: 1.505975 \tValidation Loss: 2.424719\n",
      "Epoch: 21911 \tTraining Loss: 1.479746 \tValidation Loss: 2.424036\n",
      "Epoch: 21912 \tTraining Loss: 1.499123 \tValidation Loss: 2.424478\n",
      "Epoch: 21913 \tTraining Loss: 1.493481 \tValidation Loss: 2.425458\n",
      "Epoch: 21914 \tTraining Loss: 1.484196 \tValidation Loss: 2.424969\n",
      "Epoch: 21915 \tTraining Loss: 1.544499 \tValidation Loss: 2.424721\n",
      "Epoch: 21916 \tTraining Loss: 1.498587 \tValidation Loss: 2.424987\n",
      "Epoch: 21917 \tTraining Loss: 1.484001 \tValidation Loss: 2.425226\n",
      "Epoch: 21918 \tTraining Loss: 1.489920 \tValidation Loss: 2.424983\n",
      "Epoch: 21919 \tTraining Loss: 1.493202 \tValidation Loss: 2.425371\n",
      "Epoch: 21920 \tTraining Loss: 1.507541 \tValidation Loss: 2.425429\n",
      "Epoch: 21921 \tTraining Loss: 1.497180 \tValidation Loss: 2.424898\n",
      "Epoch: 21922 \tTraining Loss: 1.540402 \tValidation Loss: 2.426273\n",
      "Epoch: 21923 \tTraining Loss: 1.472329 \tValidation Loss: 2.425402\n",
      "Epoch: 21924 \tTraining Loss: 1.535703 \tValidation Loss: 2.424952\n",
      "Epoch: 21925 \tTraining Loss: 1.553884 \tValidation Loss: 2.424159\n",
      "Epoch: 21926 \tTraining Loss: 1.517246 \tValidation Loss: 2.424094\n",
      "Epoch: 21927 \tTraining Loss: 1.488373 \tValidation Loss: 2.424450\n",
      "Epoch: 21928 \tTraining Loss: 1.518497 \tValidation Loss: 2.424814\n",
      "Epoch: 21929 \tTraining Loss: 1.498093 \tValidation Loss: 2.424335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21930 \tTraining Loss: 1.498528 \tValidation Loss: 2.424646\n",
      "Epoch: 21931 \tTraining Loss: 1.487783 \tValidation Loss: 2.425618\n",
      "Epoch: 21932 \tTraining Loss: 1.517574 \tValidation Loss: 2.425498\n",
      "Epoch: 21933 \tTraining Loss: 1.511685 \tValidation Loss: 2.425767\n",
      "Epoch: 21934 \tTraining Loss: 1.495955 \tValidation Loss: 2.424638\n",
      "Epoch: 21935 \tTraining Loss: 1.446508 \tValidation Loss: 2.425496\n",
      "Epoch: 21936 \tTraining Loss: 1.486442 \tValidation Loss: 2.425356\n",
      "Epoch: 21937 \tTraining Loss: 1.478650 \tValidation Loss: 2.425415\n",
      "Epoch: 21938 \tTraining Loss: 1.527052 \tValidation Loss: 2.425470\n",
      "Epoch: 21939 \tTraining Loss: 1.538868 \tValidation Loss: 2.424922\n",
      "Epoch: 21940 \tTraining Loss: 1.513252 \tValidation Loss: 2.424840\n",
      "Epoch: 21941 \tTraining Loss: 1.506694 \tValidation Loss: 2.424639\n",
      "Epoch: 21942 \tTraining Loss: 1.491911 \tValidation Loss: 2.424928\n",
      "Epoch: 21943 \tTraining Loss: 1.493777 \tValidation Loss: 2.425378\n",
      "Epoch: 21944 \tTraining Loss: 1.458822 \tValidation Loss: 2.424433\n",
      "Epoch: 21945 \tTraining Loss: 1.513786 \tValidation Loss: 2.425022\n",
      "Epoch: 21946 \tTraining Loss: 1.506935 \tValidation Loss: 2.424569\n",
      "Epoch: 21947 \tTraining Loss: 1.523249 \tValidation Loss: 2.425054\n",
      "Epoch: 21948 \tTraining Loss: 1.524661 \tValidation Loss: 2.425311\n",
      "Epoch: 21949 \tTraining Loss: 1.511528 \tValidation Loss: 2.424683\n",
      "Epoch: 21950 \tTraining Loss: 1.490628 \tValidation Loss: 2.425018\n",
      "Epoch: 21951 \tTraining Loss: 1.507464 \tValidation Loss: 2.425003\n",
      "Epoch: 21952 \tTraining Loss: 1.522886 \tValidation Loss: 2.424331\n",
      "Epoch: 21953 \tTraining Loss: 1.502830 \tValidation Loss: 2.424295\n",
      "Epoch: 21954 \tTraining Loss: 1.498032 \tValidation Loss: 2.424690\n",
      "Epoch: 21955 \tTraining Loss: 1.523133 \tValidation Loss: 2.424881\n",
      "Epoch: 21956 \tTraining Loss: 1.497520 \tValidation Loss: 2.424567\n",
      "Epoch: 21957 \tTraining Loss: 1.525249 \tValidation Loss: 2.425382\n",
      "Epoch: 21958 \tTraining Loss: 1.525891 \tValidation Loss: 2.424074\n",
      "Epoch: 21959 \tTraining Loss: 1.494803 \tValidation Loss: 2.423951\n",
      "Epoch: 21960 \tTraining Loss: 1.504827 \tValidation Loss: 2.424763\n",
      "Epoch: 21961 \tTraining Loss: 1.488348 \tValidation Loss: 2.424424\n",
      "Epoch: 21962 \tTraining Loss: 1.508215 \tValidation Loss: 2.425481\n",
      "Epoch: 21963 \tTraining Loss: 1.485825 \tValidation Loss: 2.425275\n",
      "Epoch: 21964 \tTraining Loss: 1.517493 \tValidation Loss: 2.425647\n",
      "Epoch: 21965 \tTraining Loss: 1.488569 \tValidation Loss: 2.425975\n",
      "Epoch: 21966 \tTraining Loss: 1.469288 \tValidation Loss: 2.425788\n",
      "Epoch: 21967 \tTraining Loss: 1.528605 \tValidation Loss: 2.425184\n",
      "Epoch: 21968 \tTraining Loss: 1.518672 \tValidation Loss: 2.425473\n",
      "Epoch: 21969 \tTraining Loss: 1.452033 \tValidation Loss: 2.425998\n",
      "Epoch: 21970 \tTraining Loss: 1.483695 \tValidation Loss: 2.425041\n",
      "Epoch: 21971 \tTraining Loss: 1.554993 \tValidation Loss: 2.424251\n",
      "Epoch: 21972 \tTraining Loss: 1.520372 \tValidation Loss: 2.425271\n",
      "Epoch: 21973 \tTraining Loss: 1.475931 \tValidation Loss: 2.424775\n",
      "Epoch: 21974 \tTraining Loss: 1.490498 \tValidation Loss: 2.425912\n",
      "Epoch: 21975 \tTraining Loss: 1.544639 \tValidation Loss: 2.425789\n",
      "Epoch: 21976 \tTraining Loss: 1.513724 \tValidation Loss: 2.425921\n",
      "Epoch: 21977 \tTraining Loss: 1.515886 \tValidation Loss: 2.425204\n",
      "Epoch: 21978 \tTraining Loss: 1.517415 \tValidation Loss: 2.424501\n",
      "Epoch: 21979 \tTraining Loss: 1.500546 \tValidation Loss: 2.424639\n",
      "Epoch: 21980 \tTraining Loss: 1.472784 \tValidation Loss: 2.425359\n",
      "Epoch: 21981 \tTraining Loss: 1.497254 \tValidation Loss: 2.426445\n",
      "Epoch: 21982 \tTraining Loss: 1.477827 \tValidation Loss: 2.426572\n",
      "Epoch: 21983 \tTraining Loss: 1.451894 \tValidation Loss: 2.426675\n",
      "Epoch: 21984 \tTraining Loss: 1.496267 \tValidation Loss: 2.425450\n",
      "Epoch: 21985 \tTraining Loss: 1.524883 \tValidation Loss: 2.425392\n",
      "Epoch: 21986 \tTraining Loss: 1.523915 \tValidation Loss: 2.424927\n",
      "Epoch: 21987 \tTraining Loss: 1.464095 \tValidation Loss: 2.425609\n",
      "Epoch: 21988 \tTraining Loss: 1.524229 \tValidation Loss: 2.424479\n",
      "Epoch: 21989 \tTraining Loss: 1.549841 \tValidation Loss: 2.424483\n",
      "Epoch: 21990 \tTraining Loss: 1.521707 \tValidation Loss: 2.425593\n",
      "Epoch: 21991 \tTraining Loss: 1.515316 \tValidation Loss: 2.425669\n",
      "Epoch: 21992 \tTraining Loss: 1.505288 \tValidation Loss: 2.424362\n",
      "Epoch: 21993 \tTraining Loss: 1.483962 \tValidation Loss: 2.424973\n",
      "Epoch: 21994 \tTraining Loss: 1.500631 \tValidation Loss: 2.425012\n",
      "Epoch: 21995 \tTraining Loss: 1.487377 \tValidation Loss: 2.424980\n",
      "Epoch: 21996 \tTraining Loss: 1.480221 \tValidation Loss: 2.425275\n",
      "Epoch: 21997 \tTraining Loss: 1.486258 \tValidation Loss: 2.426004\n",
      "Epoch: 21998 \tTraining Loss: 1.558956 \tValidation Loss: 2.425490\n",
      "Epoch: 21999 \tTraining Loss: 1.517604 \tValidation Loss: 2.425686\n",
      "Epoch: 22000 \tTraining Loss: 1.517859 \tValidation Loss: 2.425598\n",
      "Epoch: 22001 \tTraining Loss: 1.478496 \tValidation Loss: 2.426599\n",
      "Epoch: 22002 \tTraining Loss: 1.475157 \tValidation Loss: 2.426366\n",
      "Epoch: 22003 \tTraining Loss: 1.543297 \tValidation Loss: 2.425908\n",
      "Epoch: 22004 \tTraining Loss: 1.485520 \tValidation Loss: 2.425795\n",
      "Epoch: 22005 \tTraining Loss: 1.500081 \tValidation Loss: 2.425529\n",
      "Epoch: 22006 \tTraining Loss: 1.472749 \tValidation Loss: 2.426704\n",
      "Epoch: 22007 \tTraining Loss: 1.513493 \tValidation Loss: 2.425826\n",
      "Epoch: 22008 \tTraining Loss: 1.503690 \tValidation Loss: 2.425792\n",
      "Epoch: 22009 \tTraining Loss: 1.467000 \tValidation Loss: 2.425064\n",
      "Epoch: 22010 \tTraining Loss: 1.505405 \tValidation Loss: 2.424803\n",
      "Epoch: 22011 \tTraining Loss: 1.529279 \tValidation Loss: 2.425620\n",
      "Epoch: 22012 \tTraining Loss: 1.488347 \tValidation Loss: 2.425323\n",
      "Epoch: 22013 \tTraining Loss: 1.513955 \tValidation Loss: 2.425401\n",
      "Epoch: 22014 \tTraining Loss: 1.564288 \tValidation Loss: 2.425316\n",
      "Epoch: 22015 \tTraining Loss: 1.473579 \tValidation Loss: 2.425192\n",
      "Epoch: 22016 \tTraining Loss: 1.534660 \tValidation Loss: 2.426008\n",
      "Epoch: 22017 \tTraining Loss: 1.565702 \tValidation Loss: 2.424898\n",
      "Epoch: 22018 \tTraining Loss: 1.498981 \tValidation Loss: 2.425849\n",
      "Epoch: 22019 \tTraining Loss: 1.495994 \tValidation Loss: 2.425324\n",
      "Epoch: 22020 \tTraining Loss: 1.518028 \tValidation Loss: 2.424963\n",
      "Epoch: 22021 \tTraining Loss: 1.517597 \tValidation Loss: 2.425663\n",
      "Epoch: 22022 \tTraining Loss: 1.506495 \tValidation Loss: 2.425360\n",
      "Epoch: 22023 \tTraining Loss: 1.461715 \tValidation Loss: 2.426448\n",
      "Epoch: 22024 \tTraining Loss: 1.564492 \tValidation Loss: 2.425571\n",
      "Epoch: 22025 \tTraining Loss: 1.478487 \tValidation Loss: 2.426251\n",
      "Epoch: 22026 \tTraining Loss: 1.513395 \tValidation Loss: 2.425194\n",
      "Epoch: 22027 \tTraining Loss: 1.503436 \tValidation Loss: 2.425858\n",
      "Epoch: 22028 \tTraining Loss: 1.522190 \tValidation Loss: 2.425376\n",
      "Epoch: 22029 \tTraining Loss: 1.489805 \tValidation Loss: 2.425817\n",
      "Epoch: 22030 \tTraining Loss: 1.506922 \tValidation Loss: 2.426363\n",
      "Epoch: 22031 \tTraining Loss: 1.468963 \tValidation Loss: 2.425547\n",
      "Epoch: 22032 \tTraining Loss: 1.501151 \tValidation Loss: 2.425935\n",
      "Epoch: 22033 \tTraining Loss: 1.474945 \tValidation Loss: 2.426219\n",
      "Epoch: 22034 \tTraining Loss: 1.481476 \tValidation Loss: 2.426311\n",
      "Epoch: 22035 \tTraining Loss: 1.477383 \tValidation Loss: 2.425610\n",
      "Epoch: 22036 \tTraining Loss: 1.497341 \tValidation Loss: 2.426582\n",
      "Epoch: 22037 \tTraining Loss: 1.497007 \tValidation Loss: 2.426764\n",
      "Epoch: 22038 \tTraining Loss: 1.524301 \tValidation Loss: 2.426347\n",
      "Epoch: 22039 \tTraining Loss: 1.531814 \tValidation Loss: 2.425491\n",
      "Epoch: 22040 \tTraining Loss: 1.559558 \tValidation Loss: 2.425865\n",
      "Epoch: 22041 \tTraining Loss: 1.482047 \tValidation Loss: 2.426260\n",
      "Epoch: 22042 \tTraining Loss: 1.545793 \tValidation Loss: 2.426195\n",
      "Epoch: 22043 \tTraining Loss: 1.504662 \tValidation Loss: 2.425300\n",
      "Epoch: 22044 \tTraining Loss: 1.493312 \tValidation Loss: 2.426288\n",
      "Epoch: 22045 \tTraining Loss: 1.521650 \tValidation Loss: 2.426294\n",
      "Epoch: 22046 \tTraining Loss: 1.487260 \tValidation Loss: 2.425989\n",
      "Epoch: 22047 \tTraining Loss: 1.454863 \tValidation Loss: 2.425922\n",
      "Epoch: 22048 \tTraining Loss: 1.501206 \tValidation Loss: 2.425956\n",
      "Epoch: 22049 \tTraining Loss: 1.483500 \tValidation Loss: 2.427084\n",
      "Epoch: 22050 \tTraining Loss: 1.482413 \tValidation Loss: 2.426842\n",
      "Epoch: 22051 \tTraining Loss: 1.504456 \tValidation Loss: 2.427104\n",
      "Epoch: 22052 \tTraining Loss: 1.500559 \tValidation Loss: 2.426578\n",
      "Epoch: 22053 \tTraining Loss: 1.509321 \tValidation Loss: 2.426626\n",
      "Epoch: 22054 \tTraining Loss: 1.518729 \tValidation Loss: 2.425609\n",
      "Epoch: 22055 \tTraining Loss: 1.502712 \tValidation Loss: 2.426352\n",
      "Epoch: 22056 \tTraining Loss: 1.455578 \tValidation Loss: 2.426173\n",
      "Epoch: 22057 \tTraining Loss: 1.466996 \tValidation Loss: 2.426681\n",
      "Epoch: 22058 \tTraining Loss: 1.505804 \tValidation Loss: 2.426933\n",
      "Epoch: 22059 \tTraining Loss: 1.537311 \tValidation Loss: 2.426382\n",
      "Epoch: 22060 \tTraining Loss: 1.471086 \tValidation Loss: 2.427146\n",
      "Epoch: 22061 \tTraining Loss: 1.448658 \tValidation Loss: 2.426708\n",
      "Epoch: 22062 \tTraining Loss: 1.529293 \tValidation Loss: 2.426907\n",
      "Epoch: 22063 \tTraining Loss: 1.510381 \tValidation Loss: 2.427464\n",
      "Epoch: 22064 \tTraining Loss: 1.515401 \tValidation Loss: 2.427376\n",
      "Epoch: 22065 \tTraining Loss: 1.497929 \tValidation Loss: 2.427327\n",
      "Epoch: 22066 \tTraining Loss: 1.493850 \tValidation Loss: 2.427127\n",
      "Epoch: 22067 \tTraining Loss: 1.507234 \tValidation Loss: 2.427279\n",
      "Epoch: 22068 \tTraining Loss: 1.520278 \tValidation Loss: 2.426401\n",
      "Epoch: 22069 \tTraining Loss: 1.513307 \tValidation Loss: 2.426588\n",
      "Epoch: 22070 \tTraining Loss: 1.524908 \tValidation Loss: 2.426922\n",
      "Epoch: 22071 \tTraining Loss: 1.477094 \tValidation Loss: 2.427975\n",
      "Epoch: 22072 \tTraining Loss: 1.490825 \tValidation Loss: 2.427661\n",
      "Epoch: 22073 \tTraining Loss: 1.495273 \tValidation Loss: 2.426818\n",
      "Epoch: 22074 \tTraining Loss: 1.485057 \tValidation Loss: 2.427210\n",
      "Epoch: 22075 \tTraining Loss: 1.475251 \tValidation Loss: 2.427608\n",
      "Epoch: 22076 \tTraining Loss: 1.549292 \tValidation Loss: 2.426302\n",
      "Epoch: 22077 \tTraining Loss: 1.474482 \tValidation Loss: 2.426525\n",
      "Epoch: 22078 \tTraining Loss: 1.541394 \tValidation Loss: 2.426799\n",
      "Epoch: 22079 \tTraining Loss: 1.470197 \tValidation Loss: 2.426887\n",
      "Epoch: 22080 \tTraining Loss: 1.528096 \tValidation Loss: 2.427310\n",
      "Epoch: 22081 \tTraining Loss: 1.501094 \tValidation Loss: 2.427259\n",
      "Epoch: 22082 \tTraining Loss: 1.485928 \tValidation Loss: 2.427745\n",
      "Epoch: 22083 \tTraining Loss: 1.524321 \tValidation Loss: 2.427561\n",
      "Epoch: 22084 \tTraining Loss: 1.507396 \tValidation Loss: 2.427136\n",
      "Epoch: 22085 \tTraining Loss: 1.499459 \tValidation Loss: 2.427639\n",
      "Epoch: 22086 \tTraining Loss: 1.515076 \tValidation Loss: 2.428026\n",
      "Epoch: 22087 \tTraining Loss: 1.514323 \tValidation Loss: 2.427750\n",
      "Epoch: 22088 \tTraining Loss: 1.468152 \tValidation Loss: 2.427570\n",
      "Epoch: 22089 \tTraining Loss: 1.508142 \tValidation Loss: 2.427201\n",
      "Epoch: 22090 \tTraining Loss: 1.507851 \tValidation Loss: 2.426092\n",
      "Epoch: 22091 \tTraining Loss: 1.477552 \tValidation Loss: 2.427637\n",
      "Epoch: 22092 \tTraining Loss: 1.468470 \tValidation Loss: 2.428652\n",
      "Epoch: 22093 \tTraining Loss: 1.530544 \tValidation Loss: 2.427085\n",
      "Epoch: 22094 \tTraining Loss: 1.483555 \tValidation Loss: 2.428025\n",
      "Epoch: 22095 \tTraining Loss: 1.495093 \tValidation Loss: 2.428023\n",
      "Epoch: 22096 \tTraining Loss: 1.523987 \tValidation Loss: 2.427624\n",
      "Epoch: 22097 \tTraining Loss: 1.536761 \tValidation Loss: 2.427634\n",
      "Epoch: 22098 \tTraining Loss: 1.510207 \tValidation Loss: 2.427685\n",
      "Epoch: 22099 \tTraining Loss: 1.509642 \tValidation Loss: 2.427629\n",
      "Epoch: 22100 \tTraining Loss: 1.553540 \tValidation Loss: 2.426448\n",
      "Epoch: 22101 \tTraining Loss: 1.456316 \tValidation Loss: 2.427833\n",
      "Epoch: 22102 \tTraining Loss: 1.497450 \tValidation Loss: 2.427039\n",
      "Epoch: 22103 \tTraining Loss: 1.471578 \tValidation Loss: 2.427201\n",
      "Epoch: 22104 \tTraining Loss: 1.486906 \tValidation Loss: 2.427547\n",
      "Epoch: 22105 \tTraining Loss: 1.466147 \tValidation Loss: 2.427895\n",
      "Epoch: 22106 \tTraining Loss: 1.465593 \tValidation Loss: 2.429534\n",
      "Epoch: 22107 \tTraining Loss: 1.491759 \tValidation Loss: 2.428593\n",
      "Epoch: 22108 \tTraining Loss: 1.496877 \tValidation Loss: 2.428461\n",
      "Epoch: 22109 \tTraining Loss: 1.493052 \tValidation Loss: 2.428270\n",
      "Epoch: 22110 \tTraining Loss: 1.496906 \tValidation Loss: 2.428195\n",
      "Epoch: 22111 \tTraining Loss: 1.499952 \tValidation Loss: 2.428221\n",
      "Epoch: 22112 \tTraining Loss: 1.485788 \tValidation Loss: 2.427130\n",
      "Epoch: 22113 \tTraining Loss: 1.553696 \tValidation Loss: 2.426740\n",
      "Epoch: 22114 \tTraining Loss: 1.493594 \tValidation Loss: 2.428104\n",
      "Epoch: 22115 \tTraining Loss: 1.505253 \tValidation Loss: 2.427184\n",
      "Epoch: 22116 \tTraining Loss: 1.497947 \tValidation Loss: 2.427142\n",
      "Epoch: 22117 \tTraining Loss: 1.511283 \tValidation Loss: 2.427361\n",
      "Epoch: 22118 \tTraining Loss: 1.498752 \tValidation Loss: 2.427178\n",
      "Epoch: 22119 \tTraining Loss: 1.466159 \tValidation Loss: 2.428036\n",
      "Epoch: 22120 \tTraining Loss: 1.481988 \tValidation Loss: 2.428558\n",
      "Epoch: 22121 \tTraining Loss: 1.527735 \tValidation Loss: 2.428009\n",
      "Epoch: 22122 \tTraining Loss: 1.484122 \tValidation Loss: 2.427422\n",
      "Epoch: 22123 \tTraining Loss: 1.453556 \tValidation Loss: 2.428076\n",
      "Epoch: 22124 \tTraining Loss: 1.500352 \tValidation Loss: 2.428856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22125 \tTraining Loss: 1.505157 \tValidation Loss: 2.428520\n",
      "Epoch: 22126 \tTraining Loss: 1.486932 \tValidation Loss: 2.428941\n",
      "Epoch: 22127 \tTraining Loss: 1.482563 \tValidation Loss: 2.427891\n",
      "Epoch: 22128 \tTraining Loss: 1.475786 \tValidation Loss: 2.428264\n",
      "Epoch: 22129 \tTraining Loss: 1.503428 \tValidation Loss: 2.428516\n",
      "Epoch: 22130 \tTraining Loss: 1.524285 \tValidation Loss: 2.428598\n",
      "Epoch: 22131 \tTraining Loss: 1.497120 \tValidation Loss: 2.429275\n",
      "Epoch: 22132 \tTraining Loss: 1.582620 \tValidation Loss: 2.427856\n",
      "Epoch: 22133 \tTraining Loss: 1.509695 \tValidation Loss: 2.428084\n",
      "Epoch: 22134 \tTraining Loss: 1.514938 \tValidation Loss: 2.427646\n",
      "Epoch: 22135 \tTraining Loss: 1.516299 \tValidation Loss: 2.427747\n",
      "Epoch: 22136 \tTraining Loss: 1.490603 \tValidation Loss: 2.427824\n",
      "Epoch: 22137 \tTraining Loss: 1.493471 \tValidation Loss: 2.428347\n",
      "Epoch: 22138 \tTraining Loss: 1.508812 \tValidation Loss: 2.428424\n",
      "Epoch: 22139 \tTraining Loss: 1.504389 \tValidation Loss: 2.427971\n",
      "Epoch: 22140 \tTraining Loss: 1.473238 \tValidation Loss: 2.428134\n",
      "Epoch: 22141 \tTraining Loss: 1.474927 \tValidation Loss: 2.428272\n",
      "Epoch: 22142 \tTraining Loss: 1.544203 \tValidation Loss: 2.428259\n",
      "Epoch: 22143 \tTraining Loss: 1.560409 \tValidation Loss: 2.427781\n",
      "Epoch: 22144 \tTraining Loss: 1.512840 \tValidation Loss: 2.427778\n",
      "Epoch: 22145 \tTraining Loss: 1.543867 \tValidation Loss: 2.428104\n",
      "Epoch: 22146 \tTraining Loss: 1.445634 \tValidation Loss: 2.428699\n",
      "Epoch: 22147 \tTraining Loss: 1.515489 \tValidation Loss: 2.427509\n",
      "Epoch: 22148 \tTraining Loss: 1.485737 \tValidation Loss: 2.428107\n",
      "Epoch: 22149 \tTraining Loss: 1.477493 \tValidation Loss: 2.428927\n",
      "Epoch: 22150 \tTraining Loss: 1.508077 \tValidation Loss: 2.428351\n",
      "Epoch: 22151 \tTraining Loss: 1.488686 \tValidation Loss: 2.428762\n",
      "Epoch: 22152 \tTraining Loss: 1.508174 \tValidation Loss: 2.428589\n",
      "Epoch: 22153 \tTraining Loss: 1.461877 \tValidation Loss: 2.428838\n",
      "Epoch: 22154 \tTraining Loss: 1.516038 \tValidation Loss: 2.428023\n",
      "Epoch: 22155 \tTraining Loss: 1.461862 \tValidation Loss: 2.428753\n",
      "Epoch: 22156 \tTraining Loss: 1.466202 \tValidation Loss: 2.429310\n",
      "Epoch: 22157 \tTraining Loss: 1.511235 \tValidation Loss: 2.428488\n",
      "Epoch: 22158 \tTraining Loss: 1.503443 \tValidation Loss: 2.428380\n",
      "Epoch: 22159 \tTraining Loss: 1.517640 \tValidation Loss: 2.428835\n",
      "Epoch: 22160 \tTraining Loss: 1.543100 \tValidation Loss: 2.428137\n",
      "Epoch: 22161 \tTraining Loss: 1.475407 \tValidation Loss: 2.429214\n",
      "Epoch: 22162 \tTraining Loss: 1.522231 \tValidation Loss: 2.429092\n",
      "Epoch: 22163 \tTraining Loss: 1.514737 \tValidation Loss: 2.428495\n",
      "Epoch: 22164 \tTraining Loss: 1.541337 \tValidation Loss: 2.428502\n",
      "Epoch: 22165 \tTraining Loss: 1.486147 \tValidation Loss: 2.428797\n",
      "Epoch: 22166 \tTraining Loss: 1.464458 \tValidation Loss: 2.428803\n",
      "Epoch: 22167 \tTraining Loss: 1.504166 \tValidation Loss: 2.428668\n",
      "Epoch: 22168 \tTraining Loss: 1.504201 \tValidation Loss: 2.428650\n",
      "Epoch: 22169 \tTraining Loss: 1.487312 \tValidation Loss: 2.428241\n",
      "Epoch: 22170 \tTraining Loss: 1.484352 \tValidation Loss: 2.427939\n",
      "Epoch: 22171 \tTraining Loss: 1.490236 \tValidation Loss: 2.428190\n",
      "Epoch: 22172 \tTraining Loss: 1.470438 \tValidation Loss: 2.428597\n",
      "Epoch: 22173 \tTraining Loss: 1.510170 \tValidation Loss: 2.428410\n",
      "Epoch: 22174 \tTraining Loss: 1.480606 \tValidation Loss: 2.428588\n",
      "Epoch: 22175 \tTraining Loss: 1.506201 \tValidation Loss: 2.428816\n",
      "Epoch: 22176 \tTraining Loss: 1.483406 \tValidation Loss: 2.429147\n",
      "Epoch: 22177 \tTraining Loss: 1.480087 \tValidation Loss: 2.428989\n",
      "Epoch: 22178 \tTraining Loss: 1.535088 \tValidation Loss: 2.428216\n",
      "Epoch: 22179 \tTraining Loss: 1.479927 \tValidation Loss: 2.428705\n",
      "Epoch: 22180 \tTraining Loss: 1.509083 \tValidation Loss: 2.429301\n",
      "Epoch: 22181 \tTraining Loss: 1.482523 \tValidation Loss: 2.429450\n",
      "Epoch: 22182 \tTraining Loss: 1.484006 \tValidation Loss: 2.430018\n",
      "Epoch: 22183 \tTraining Loss: 1.481761 \tValidation Loss: 2.429079\n",
      "Epoch: 22184 \tTraining Loss: 1.492530 \tValidation Loss: 2.428643\n",
      "Epoch: 22185 \tTraining Loss: 1.486818 \tValidation Loss: 2.429562\n",
      "Epoch: 22186 \tTraining Loss: 1.470538 \tValidation Loss: 2.429908\n",
      "Epoch: 22187 \tTraining Loss: 1.499161 \tValidation Loss: 2.429024\n",
      "Epoch: 22188 \tTraining Loss: 1.523593 \tValidation Loss: 2.428719\n",
      "Epoch: 22189 \tTraining Loss: 1.441651 \tValidation Loss: 2.429612\n",
      "Epoch: 22190 \tTraining Loss: 1.506414 \tValidation Loss: 2.428595\n",
      "Epoch: 22191 \tTraining Loss: 1.454179 \tValidation Loss: 2.429157\n",
      "Epoch: 22192 \tTraining Loss: 1.465334 \tValidation Loss: 2.428782\n",
      "Epoch: 22193 \tTraining Loss: 1.516577 \tValidation Loss: 2.428999\n",
      "Epoch: 22194 \tTraining Loss: 1.492406 \tValidation Loss: 2.428938\n",
      "Epoch: 22195 \tTraining Loss: 1.503554 \tValidation Loss: 2.429569\n",
      "Epoch: 22196 \tTraining Loss: 1.484506 \tValidation Loss: 2.429382\n",
      "Epoch: 22197 \tTraining Loss: 1.501998 \tValidation Loss: 2.429262\n",
      "Epoch: 22198 \tTraining Loss: 1.514716 \tValidation Loss: 2.428517\n",
      "Epoch: 22199 \tTraining Loss: 1.550981 \tValidation Loss: 2.428407\n",
      "Epoch: 22200 \tTraining Loss: 1.483933 \tValidation Loss: 2.428737\n",
      "Epoch: 22201 \tTraining Loss: 1.544448 \tValidation Loss: 2.428458\n",
      "Epoch: 22202 \tTraining Loss: 1.489375 \tValidation Loss: 2.429319\n",
      "Epoch: 22203 \tTraining Loss: 1.498157 \tValidation Loss: 2.429125\n",
      "Epoch: 22204 \tTraining Loss: 1.532490 \tValidation Loss: 2.428964\n",
      "Epoch: 22205 \tTraining Loss: 1.494499 \tValidation Loss: 2.428318\n",
      "Epoch: 22206 \tTraining Loss: 1.494888 \tValidation Loss: 2.429615\n",
      "Epoch: 22207 \tTraining Loss: 1.485018 \tValidation Loss: 2.429818\n",
      "Epoch: 22208 \tTraining Loss: 1.502977 \tValidation Loss: 2.428958\n",
      "Epoch: 22209 \tTraining Loss: 1.479095 \tValidation Loss: 2.429078\n",
      "Epoch: 22210 \tTraining Loss: 1.519418 \tValidation Loss: 2.429134\n",
      "Epoch: 22211 \tTraining Loss: 1.515106 \tValidation Loss: 2.428178\n",
      "Epoch: 22212 \tTraining Loss: 1.536673 \tValidation Loss: 2.428196\n",
      "Epoch: 22213 \tTraining Loss: 1.509251 \tValidation Loss: 2.428781\n",
      "Epoch: 22214 \tTraining Loss: 1.518642 \tValidation Loss: 2.429471\n",
      "Epoch: 22215 \tTraining Loss: 1.484877 \tValidation Loss: 2.429411\n",
      "Epoch: 22216 \tTraining Loss: 1.483238 \tValidation Loss: 2.429802\n",
      "Epoch: 22217 \tTraining Loss: 1.526850 \tValidation Loss: 2.429789\n",
      "Epoch: 22218 \tTraining Loss: 1.511610 \tValidation Loss: 2.429301\n",
      "Epoch: 22219 \tTraining Loss: 1.469198 \tValidation Loss: 2.429479\n",
      "Epoch: 22220 \tTraining Loss: 1.524142 \tValidation Loss: 2.429973\n",
      "Epoch: 22221 \tTraining Loss: 1.542009 \tValidation Loss: 2.429505\n",
      "Epoch: 22222 \tTraining Loss: 1.517606 \tValidation Loss: 2.429652\n",
      "Epoch: 22223 \tTraining Loss: 1.462546 \tValidation Loss: 2.430071\n",
      "Epoch: 22224 \tTraining Loss: 1.497357 \tValidation Loss: 2.428964\n",
      "Epoch: 22225 \tTraining Loss: 1.480300 \tValidation Loss: 2.429205\n",
      "Epoch: 22226 \tTraining Loss: 1.492436 \tValidation Loss: 2.429579\n",
      "Epoch: 22227 \tTraining Loss: 1.484942 \tValidation Loss: 2.429137\n",
      "Epoch: 22228 \tTraining Loss: 1.461033 \tValidation Loss: 2.428869\n",
      "Epoch: 22229 \tTraining Loss: 1.476726 \tValidation Loss: 2.429533\n",
      "Epoch: 22230 \tTraining Loss: 1.508474 \tValidation Loss: 2.429227\n",
      "Epoch: 22231 \tTraining Loss: 1.514536 \tValidation Loss: 2.429307\n",
      "Epoch: 22232 \tTraining Loss: 1.540164 \tValidation Loss: 2.429465\n",
      "Epoch: 22233 \tTraining Loss: 1.473897 \tValidation Loss: 2.429598\n",
      "Epoch: 22234 \tTraining Loss: 1.504739 \tValidation Loss: 2.430631\n",
      "Epoch: 22235 \tTraining Loss: 1.514948 \tValidation Loss: 2.430341\n",
      "Epoch: 22236 \tTraining Loss: 1.479941 \tValidation Loss: 2.430131\n",
      "Epoch: 22237 \tTraining Loss: 1.474630 \tValidation Loss: 2.429666\n",
      "Epoch: 22238 \tTraining Loss: 1.466484 \tValidation Loss: 2.430394\n",
      "Epoch: 22239 \tTraining Loss: 1.477842 \tValidation Loss: 2.430028\n",
      "Epoch: 22240 \tTraining Loss: 1.544348 \tValidation Loss: 2.429024\n",
      "Epoch: 22241 \tTraining Loss: 1.495357 \tValidation Loss: 2.429159\n",
      "Epoch: 22242 \tTraining Loss: 1.531524 \tValidation Loss: 2.429060\n",
      "Epoch: 22243 \tTraining Loss: 1.516577 \tValidation Loss: 2.429364\n",
      "Epoch: 22244 \tTraining Loss: 1.500097 \tValidation Loss: 2.429387\n",
      "Epoch: 22245 \tTraining Loss: 1.528804 \tValidation Loss: 2.429322\n",
      "Epoch: 22246 \tTraining Loss: 1.510811 \tValidation Loss: 2.428846\n",
      "Epoch: 22247 \tTraining Loss: 1.493502 \tValidation Loss: 2.427961\n",
      "Epoch: 22248 \tTraining Loss: 1.488365 \tValidation Loss: 2.428558\n",
      "Epoch: 22249 \tTraining Loss: 1.496509 \tValidation Loss: 2.429349\n",
      "Epoch: 22250 \tTraining Loss: 1.494739 \tValidation Loss: 2.428620\n",
      "Epoch: 22251 \tTraining Loss: 1.522084 \tValidation Loss: 2.429441\n",
      "Epoch: 22252 \tTraining Loss: 1.454578 \tValidation Loss: 2.430329\n",
      "Epoch: 22253 \tTraining Loss: 1.532894 \tValidation Loss: 2.429657\n",
      "Epoch: 22254 \tTraining Loss: 1.491001 \tValidation Loss: 2.429673\n",
      "Epoch: 22255 \tTraining Loss: 1.513496 \tValidation Loss: 2.429924\n",
      "Epoch: 22256 \tTraining Loss: 1.519504 \tValidation Loss: 2.430240\n",
      "Epoch: 22257 \tTraining Loss: 1.505485 \tValidation Loss: 2.429210\n",
      "Epoch: 22258 \tTraining Loss: 1.450908 \tValidation Loss: 2.430208\n",
      "Epoch: 22259 \tTraining Loss: 1.470192 \tValidation Loss: 2.430357\n",
      "Epoch: 22260 \tTraining Loss: 1.526591 \tValidation Loss: 2.429979\n",
      "Epoch: 22261 \tTraining Loss: 1.549856 \tValidation Loss: 2.428985\n",
      "Epoch: 22262 \tTraining Loss: 1.505749 \tValidation Loss: 2.430018\n",
      "Epoch: 22263 \tTraining Loss: 1.496950 \tValidation Loss: 2.430180\n",
      "Epoch: 22264 \tTraining Loss: 1.458073 \tValidation Loss: 2.430775\n",
      "Epoch: 22265 \tTraining Loss: 1.499565 \tValidation Loss: 2.429880\n",
      "Epoch: 22266 \tTraining Loss: 1.455496 \tValidation Loss: 2.430567\n",
      "Epoch: 22267 \tTraining Loss: 1.451669 \tValidation Loss: 2.430547\n",
      "Epoch: 22268 \tTraining Loss: 1.486472 \tValidation Loss: 2.431372\n",
      "Epoch: 22269 \tTraining Loss: 1.444940 \tValidation Loss: 2.430931\n",
      "Epoch: 22270 \tTraining Loss: 1.496610 \tValidation Loss: 2.430949\n",
      "Epoch: 22271 \tTraining Loss: 1.493225 \tValidation Loss: 2.430309\n",
      "Epoch: 22272 \tTraining Loss: 1.518908 \tValidation Loss: 2.430435\n",
      "Epoch: 22273 \tTraining Loss: 1.494043 \tValidation Loss: 2.429668\n",
      "Epoch: 22274 \tTraining Loss: 1.505012 \tValidation Loss: 2.430201\n",
      "Epoch: 22275 \tTraining Loss: 1.500913 \tValidation Loss: 2.430512\n",
      "Epoch: 22276 \tTraining Loss: 1.504471 \tValidation Loss: 2.430397\n",
      "Epoch: 22277 \tTraining Loss: 1.478548 \tValidation Loss: 2.429755\n",
      "Epoch: 22278 \tTraining Loss: 1.492871 \tValidation Loss: 2.429947\n",
      "Epoch: 22279 \tTraining Loss: 1.483315 \tValidation Loss: 2.430642\n",
      "Epoch: 22280 \tTraining Loss: 1.455620 \tValidation Loss: 2.430704\n",
      "Epoch: 22281 \tTraining Loss: 1.504130 \tValidation Loss: 2.429395\n",
      "Epoch: 22282 \tTraining Loss: 1.466601 \tValidation Loss: 2.430467\n",
      "Epoch: 22283 \tTraining Loss: 1.460854 \tValidation Loss: 2.430481\n",
      "Epoch: 22284 \tTraining Loss: 1.469092 \tValidation Loss: 2.431261\n",
      "Epoch: 22285 \tTraining Loss: 1.494304 \tValidation Loss: 2.430980\n",
      "Epoch: 22286 \tTraining Loss: 1.512214 \tValidation Loss: 2.430630\n",
      "Epoch: 22287 \tTraining Loss: 1.495737 \tValidation Loss: 2.430316\n",
      "Epoch: 22288 \tTraining Loss: 1.516215 \tValidation Loss: 2.430423\n",
      "Epoch: 22289 \tTraining Loss: 1.460522 \tValidation Loss: 2.430420\n",
      "Epoch: 22290 \tTraining Loss: 1.490522 \tValidation Loss: 2.430434\n",
      "Epoch: 22291 \tTraining Loss: 1.507941 \tValidation Loss: 2.430522\n",
      "Epoch: 22292 \tTraining Loss: 1.468511 \tValidation Loss: 2.430808\n",
      "Epoch: 22293 \tTraining Loss: 1.486966 \tValidation Loss: 2.431063\n",
      "Epoch: 22294 \tTraining Loss: 1.469279 \tValidation Loss: 2.429796\n",
      "Epoch: 22295 \tTraining Loss: 1.507190 \tValidation Loss: 2.429373\n",
      "Epoch: 22296 \tTraining Loss: 1.503178 \tValidation Loss: 2.430171\n",
      "Epoch: 22297 \tTraining Loss: 1.480520 \tValidation Loss: 2.431171\n",
      "Epoch: 22298 \tTraining Loss: 1.493815 \tValidation Loss: 2.431349\n",
      "Epoch: 22299 \tTraining Loss: 1.512456 \tValidation Loss: 2.430075\n",
      "Epoch: 22300 \tTraining Loss: 1.496600 \tValidation Loss: 2.429948\n",
      "Epoch: 22301 \tTraining Loss: 1.508227 \tValidation Loss: 2.430590\n",
      "Epoch: 22302 \tTraining Loss: 1.465133 \tValidation Loss: 2.429922\n",
      "Epoch: 22303 \tTraining Loss: 1.462042 \tValidation Loss: 2.431361\n",
      "Epoch: 22304 \tTraining Loss: 1.467206 \tValidation Loss: 2.430953\n",
      "Epoch: 22305 \tTraining Loss: 1.490283 \tValidation Loss: 2.430998\n",
      "Epoch: 22306 \tTraining Loss: 1.471088 \tValidation Loss: 2.431264\n",
      "Epoch: 22307 \tTraining Loss: 1.466594 \tValidation Loss: 2.431077\n",
      "Epoch: 22308 \tTraining Loss: 1.481829 \tValidation Loss: 2.431535\n",
      "Epoch: 22309 \tTraining Loss: 1.483656 \tValidation Loss: 2.431179\n",
      "Epoch: 22310 \tTraining Loss: 1.523974 \tValidation Loss: 2.430768\n",
      "Epoch: 22311 \tTraining Loss: 1.481095 \tValidation Loss: 2.431280\n",
      "Epoch: 22312 \tTraining Loss: 1.516392 \tValidation Loss: 2.431312\n",
      "Epoch: 22313 \tTraining Loss: 1.481395 \tValidation Loss: 2.431022\n",
      "Epoch: 22314 \tTraining Loss: 1.509771 \tValidation Loss: 2.431093\n",
      "Epoch: 22315 \tTraining Loss: 1.513088 \tValidation Loss: 2.430444\n",
      "Epoch: 22316 \tTraining Loss: 1.530640 \tValidation Loss: 2.430522\n",
      "Epoch: 22317 \tTraining Loss: 1.467481 \tValidation Loss: 2.431736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22318 \tTraining Loss: 1.478014 \tValidation Loss: 2.431586\n",
      "Epoch: 22319 \tTraining Loss: 1.487350 \tValidation Loss: 2.431330\n",
      "Epoch: 22320 \tTraining Loss: 1.498506 \tValidation Loss: 2.431031\n",
      "Epoch: 22321 \tTraining Loss: 1.490514 \tValidation Loss: 2.430701\n",
      "Epoch: 22322 \tTraining Loss: 1.478057 \tValidation Loss: 2.432034\n",
      "Epoch: 22323 \tTraining Loss: 1.458390 \tValidation Loss: 2.431307\n",
      "Epoch: 22324 \tTraining Loss: 1.501604 \tValidation Loss: 2.430892\n",
      "Epoch: 22325 \tTraining Loss: 1.468312 \tValidation Loss: 2.430860\n",
      "Epoch: 22326 \tTraining Loss: 1.489712 \tValidation Loss: 2.431445\n",
      "Epoch: 22327 \tTraining Loss: 1.465932 \tValidation Loss: 2.430593\n",
      "Epoch: 22328 \tTraining Loss: 1.474352 \tValidation Loss: 2.431051\n",
      "Epoch: 22329 \tTraining Loss: 1.473409 \tValidation Loss: 2.431618\n",
      "Epoch: 22330 \tTraining Loss: 1.511697 \tValidation Loss: 2.430955\n",
      "Epoch: 22331 \tTraining Loss: 1.502454 \tValidation Loss: 2.431452\n",
      "Epoch: 22332 \tTraining Loss: 1.484371 \tValidation Loss: 2.431178\n",
      "Epoch: 22333 \tTraining Loss: 1.516605 \tValidation Loss: 2.430911\n",
      "Epoch: 22334 \tTraining Loss: 1.470976 \tValidation Loss: 2.431860\n",
      "Epoch: 22335 \tTraining Loss: 1.491344 \tValidation Loss: 2.432121\n",
      "Epoch: 22336 \tTraining Loss: 1.484233 \tValidation Loss: 2.431762\n",
      "Epoch: 22337 \tTraining Loss: 1.506537 \tValidation Loss: 2.430670\n",
      "Epoch: 22338 \tTraining Loss: 1.507744 \tValidation Loss: 2.430969\n",
      "Epoch: 22339 \tTraining Loss: 1.466247 \tValidation Loss: 2.430896\n",
      "Epoch: 22340 \tTraining Loss: 1.513555 \tValidation Loss: 2.430939\n",
      "Epoch: 22341 \tTraining Loss: 1.483282 \tValidation Loss: 2.431757\n",
      "Epoch: 22342 \tTraining Loss: 1.482586 \tValidation Loss: 2.431547\n",
      "Epoch: 22343 \tTraining Loss: 1.472611 \tValidation Loss: 2.431589\n",
      "Epoch: 22344 \tTraining Loss: 1.458875 \tValidation Loss: 2.432189\n",
      "Epoch: 22345 \tTraining Loss: 1.471843 \tValidation Loss: 2.431255\n",
      "Epoch: 22346 \tTraining Loss: 1.462482 \tValidation Loss: 2.432572\n",
      "Epoch: 22347 \tTraining Loss: 1.513745 \tValidation Loss: 2.431364\n",
      "Epoch: 22348 \tTraining Loss: 1.492360 \tValidation Loss: 2.431914\n",
      "Epoch: 22349 \tTraining Loss: 1.476995 \tValidation Loss: 2.431878\n",
      "Epoch: 22350 \tTraining Loss: 1.485335 \tValidation Loss: 2.432211\n",
      "Epoch: 22351 \tTraining Loss: 1.448694 \tValidation Loss: 2.431852\n",
      "Epoch: 22352 \tTraining Loss: 1.526444 \tValidation Loss: 2.431420\n",
      "Epoch: 22353 \tTraining Loss: 1.537241 \tValidation Loss: 2.430959\n",
      "Epoch: 22354 \tTraining Loss: 1.465948 \tValidation Loss: 2.431218\n",
      "Epoch: 22355 \tTraining Loss: 1.513757 \tValidation Loss: 2.431488\n",
      "Epoch: 22356 \tTraining Loss: 1.484743 \tValidation Loss: 2.432173\n",
      "Epoch: 22357 \tTraining Loss: 1.480779 \tValidation Loss: 2.431543\n",
      "Epoch: 22358 \tTraining Loss: 1.487079 \tValidation Loss: 2.431964\n",
      "Epoch: 22359 \tTraining Loss: 1.529580 \tValidation Loss: 2.432082\n",
      "Epoch: 22360 \tTraining Loss: 1.508899 \tValidation Loss: 2.431805\n",
      "Epoch: 22361 \tTraining Loss: 1.467959 \tValidation Loss: 2.431595\n",
      "Epoch: 22362 \tTraining Loss: 1.485040 \tValidation Loss: 2.431981\n",
      "Epoch: 22363 \tTraining Loss: 1.516061 \tValidation Loss: 2.431869\n",
      "Epoch: 22364 \tTraining Loss: 1.522033 \tValidation Loss: 2.431556\n",
      "Epoch: 22365 \tTraining Loss: 1.506091 \tValidation Loss: 2.431909\n",
      "Epoch: 22366 \tTraining Loss: 1.487385 \tValidation Loss: 2.432670\n",
      "Epoch: 22367 \tTraining Loss: 1.455354 \tValidation Loss: 2.432715\n",
      "Epoch: 22368 \tTraining Loss: 1.463853 \tValidation Loss: 2.432125\n",
      "Epoch: 22369 \tTraining Loss: 1.505447 \tValidation Loss: 2.431857\n",
      "Epoch: 22370 \tTraining Loss: 1.497468 \tValidation Loss: 2.432271\n",
      "Epoch: 22371 \tTraining Loss: 1.457676 \tValidation Loss: 2.432343\n",
      "Epoch: 22372 \tTraining Loss: 1.477659 \tValidation Loss: 2.432182\n",
      "Epoch: 22373 \tTraining Loss: 1.480373 \tValidation Loss: 2.432446\n",
      "Epoch: 22374 \tTraining Loss: 1.500579 \tValidation Loss: 2.431784\n",
      "Epoch: 22375 \tTraining Loss: 1.511412 \tValidation Loss: 2.432171\n",
      "Epoch: 22376 \tTraining Loss: 1.478112 \tValidation Loss: 2.431859\n",
      "Epoch: 22377 \tTraining Loss: 1.501582 \tValidation Loss: 2.432044\n",
      "Epoch: 22378 \tTraining Loss: 1.486613 \tValidation Loss: 2.432086\n",
      "Epoch: 22379 \tTraining Loss: 1.486819 \tValidation Loss: 2.432450\n",
      "Epoch: 22380 \tTraining Loss: 1.551128 \tValidation Loss: 2.431866\n",
      "Epoch: 22381 \tTraining Loss: 1.492761 \tValidation Loss: 2.431151\n",
      "Epoch: 22382 \tTraining Loss: 1.510337 \tValidation Loss: 2.432100\n",
      "Epoch: 22383 \tTraining Loss: 1.456426 \tValidation Loss: 2.432994\n",
      "Epoch: 22384 \tTraining Loss: 1.512518 \tValidation Loss: 2.432001\n",
      "Epoch: 22385 \tTraining Loss: 1.499323 \tValidation Loss: 2.432577\n",
      "Epoch: 22386 \tTraining Loss: 1.449911 \tValidation Loss: 2.433289\n",
      "Epoch: 22387 \tTraining Loss: 1.471784 \tValidation Loss: 2.432058\n",
      "Epoch: 22388 \tTraining Loss: 1.453875 \tValidation Loss: 2.432652\n",
      "Epoch: 22389 \tTraining Loss: 1.496522 \tValidation Loss: 2.431471\n",
      "Epoch: 22390 \tTraining Loss: 1.461195 \tValidation Loss: 2.431854\n",
      "Epoch: 22391 \tTraining Loss: 1.506106 \tValidation Loss: 2.432187\n",
      "Epoch: 22392 \tTraining Loss: 1.487165 \tValidation Loss: 2.432303\n",
      "Epoch: 22393 \tTraining Loss: 1.481871 \tValidation Loss: 2.431825\n",
      "Epoch: 22394 \tTraining Loss: 1.484548 \tValidation Loss: 2.432145\n",
      "Epoch: 22395 \tTraining Loss: 1.489383 \tValidation Loss: 2.432009\n",
      "Epoch: 22396 \tTraining Loss: 1.465014 \tValidation Loss: 2.432464\n",
      "Epoch: 22397 \tTraining Loss: 1.525734 \tValidation Loss: 2.431231\n",
      "Epoch: 22398 \tTraining Loss: 1.469944 \tValidation Loss: 2.432609\n",
      "Epoch: 22399 \tTraining Loss: 1.486700 \tValidation Loss: 2.432816\n",
      "Epoch: 22400 \tTraining Loss: 1.458466 \tValidation Loss: 2.432960\n",
      "Epoch: 22401 \tTraining Loss: 1.487921 \tValidation Loss: 2.433169\n",
      "Epoch: 22402 \tTraining Loss: 1.506824 \tValidation Loss: 2.432466\n",
      "Epoch: 22403 \tTraining Loss: 1.466068 \tValidation Loss: 2.432815\n",
      "Epoch: 22404 \tTraining Loss: 1.495911 \tValidation Loss: 2.433291\n",
      "Epoch: 22405 \tTraining Loss: 1.480347 \tValidation Loss: 2.432915\n",
      "Epoch: 22406 \tTraining Loss: 1.490538 \tValidation Loss: 2.432912\n",
      "Epoch: 22407 \tTraining Loss: 1.479219 \tValidation Loss: 2.432386\n",
      "Epoch: 22408 \tTraining Loss: 1.486485 \tValidation Loss: 2.432881\n",
      "Epoch: 22409 \tTraining Loss: 1.498252 \tValidation Loss: 2.432650\n",
      "Epoch: 22410 \tTraining Loss: 1.523929 \tValidation Loss: 2.432410\n",
      "Epoch: 22411 \tTraining Loss: 1.484313 \tValidation Loss: 2.431719\n",
      "Epoch: 22412 \tTraining Loss: 1.482925 \tValidation Loss: 2.432639\n",
      "Epoch: 22413 \tTraining Loss: 1.517712 \tValidation Loss: 2.431927\n",
      "Epoch: 22414 \tTraining Loss: 1.522843 \tValidation Loss: 2.432374\n",
      "Epoch: 22415 \tTraining Loss: 1.478887 \tValidation Loss: 2.432755\n",
      "Epoch: 22416 \tTraining Loss: 1.469267 \tValidation Loss: 2.433197\n",
      "Epoch: 22417 \tTraining Loss: 1.497829 \tValidation Loss: 2.432724\n",
      "Epoch: 22418 \tTraining Loss: 1.494508 \tValidation Loss: 2.432905\n",
      "Epoch: 22419 \tTraining Loss: 1.497973 \tValidation Loss: 2.432436\n",
      "Epoch: 22420 \tTraining Loss: 1.473669 \tValidation Loss: 2.432832\n",
      "Epoch: 22421 \tTraining Loss: 1.528480 \tValidation Loss: 2.432151\n",
      "Epoch: 22422 \tTraining Loss: 1.504022 \tValidation Loss: 2.433193\n",
      "Epoch: 22423 \tTraining Loss: 1.473076 \tValidation Loss: 2.433449\n",
      "Epoch: 22424 \tTraining Loss: 1.489864 \tValidation Loss: 2.432687\n",
      "Epoch: 22425 \tTraining Loss: 1.477161 \tValidation Loss: 2.432724\n",
      "Epoch: 22426 \tTraining Loss: 1.504458 \tValidation Loss: 2.431954\n",
      "Epoch: 22427 \tTraining Loss: 1.477767 \tValidation Loss: 2.432653\n",
      "Epoch: 22428 \tTraining Loss: 1.506694 \tValidation Loss: 2.432732\n",
      "Epoch: 22429 \tTraining Loss: 1.500742 \tValidation Loss: 2.432091\n",
      "Epoch: 22430 \tTraining Loss: 1.517372 \tValidation Loss: 2.433085\n",
      "Epoch: 22431 \tTraining Loss: 1.475159 \tValidation Loss: 2.432220\n",
      "Epoch: 22432 \tTraining Loss: 1.484246 \tValidation Loss: 2.432342\n",
      "Epoch: 22433 \tTraining Loss: 1.511568 \tValidation Loss: 2.432308\n",
      "Epoch: 22434 \tTraining Loss: 1.511679 \tValidation Loss: 2.432562\n",
      "Epoch: 22435 \tTraining Loss: 1.519329 \tValidation Loss: 2.432650\n",
      "Epoch: 22436 \tTraining Loss: 1.501930 \tValidation Loss: 2.432451\n",
      "Epoch: 22437 \tTraining Loss: 1.517754 \tValidation Loss: 2.432466\n",
      "Epoch: 22438 \tTraining Loss: 1.496596 \tValidation Loss: 2.432803\n",
      "Epoch: 22439 \tTraining Loss: 1.475995 \tValidation Loss: 2.432989\n",
      "Epoch: 22440 \tTraining Loss: 1.484264 \tValidation Loss: 2.434040\n",
      "Epoch: 22441 \tTraining Loss: 1.461381 \tValidation Loss: 2.433775\n",
      "Epoch: 22442 \tTraining Loss: 1.475149 \tValidation Loss: 2.433506\n",
      "Epoch: 22443 \tTraining Loss: 1.499727 \tValidation Loss: 2.432600\n",
      "Epoch: 22444 \tTraining Loss: 1.508291 \tValidation Loss: 2.432774\n",
      "Epoch: 22445 \tTraining Loss: 1.438118 \tValidation Loss: 2.432900\n",
      "Epoch: 22446 \tTraining Loss: 1.480620 \tValidation Loss: 2.432428\n",
      "Epoch: 22447 \tTraining Loss: 1.482617 \tValidation Loss: 2.433257\n",
      "Epoch: 22448 \tTraining Loss: 1.487092 \tValidation Loss: 2.433204\n",
      "Epoch: 22449 \tTraining Loss: 1.509175 \tValidation Loss: 2.432820\n",
      "Epoch: 22450 \tTraining Loss: 1.460353 \tValidation Loss: 2.434108\n",
      "Epoch: 22451 \tTraining Loss: 1.486149 \tValidation Loss: 2.433596\n",
      "Epoch: 22452 \tTraining Loss: 1.467777 \tValidation Loss: 2.433606\n",
      "Epoch: 22453 \tTraining Loss: 1.483483 \tValidation Loss: 2.433565\n",
      "Epoch: 22454 \tTraining Loss: 1.484498 \tValidation Loss: 2.433522\n",
      "Epoch: 22455 \tTraining Loss: 1.517270 \tValidation Loss: 2.432950\n",
      "Epoch: 22456 \tTraining Loss: 1.494047 \tValidation Loss: 2.433166\n",
      "Epoch: 22457 \tTraining Loss: 1.469880 \tValidation Loss: 2.433630\n",
      "Epoch: 22458 \tTraining Loss: 1.516975 \tValidation Loss: 2.432539\n",
      "Epoch: 22459 \tTraining Loss: 1.437936 \tValidation Loss: 2.433069\n",
      "Epoch: 22460 \tTraining Loss: 1.496507 \tValidation Loss: 2.433354\n",
      "Epoch: 22461 \tTraining Loss: 1.477288 \tValidation Loss: 2.433455\n",
      "Epoch: 22462 \tTraining Loss: 1.514856 \tValidation Loss: 2.432448\n",
      "Epoch: 22463 \tTraining Loss: 1.518260 \tValidation Loss: 2.433415\n",
      "Epoch: 22464 \tTraining Loss: 1.518501 \tValidation Loss: 2.433172\n",
      "Epoch: 22465 \tTraining Loss: 1.474866 \tValidation Loss: 2.433187\n",
      "Epoch: 22466 \tTraining Loss: 1.520478 \tValidation Loss: 2.432158\n",
      "Epoch: 22467 \tTraining Loss: 1.518618 \tValidation Loss: 2.432570\n",
      "Epoch: 22468 \tTraining Loss: 1.490195 \tValidation Loss: 2.432808\n",
      "Epoch: 22469 \tTraining Loss: 1.495168 \tValidation Loss: 2.432764\n",
      "Epoch: 22470 \tTraining Loss: 1.490366 \tValidation Loss: 2.433352\n",
      "Epoch: 22471 \tTraining Loss: 1.470886 \tValidation Loss: 2.433509\n",
      "Epoch: 22472 \tTraining Loss: 1.473805 \tValidation Loss: 2.433808\n",
      "Epoch: 22473 \tTraining Loss: 1.499521 \tValidation Loss: 2.433832\n",
      "Epoch: 22474 \tTraining Loss: 1.487459 \tValidation Loss: 2.433207\n",
      "Epoch: 22475 \tTraining Loss: 1.476999 \tValidation Loss: 2.433858\n",
      "Epoch: 22476 \tTraining Loss: 1.441405 \tValidation Loss: 2.433603\n",
      "Epoch: 22477 \tTraining Loss: 1.521347 \tValidation Loss: 2.433957\n",
      "Epoch: 22478 \tTraining Loss: 1.491451 \tValidation Loss: 2.432888\n",
      "Epoch: 22479 \tTraining Loss: 1.513389 \tValidation Loss: 2.432771\n",
      "Epoch: 22480 \tTraining Loss: 1.457604 \tValidation Loss: 2.433877\n",
      "Epoch: 22481 \tTraining Loss: 1.465503 \tValidation Loss: 2.434266\n",
      "Epoch: 22482 \tTraining Loss: 1.490277 \tValidation Loss: 2.434434\n",
      "Epoch: 22483 \tTraining Loss: 1.466285 \tValidation Loss: 2.434259\n",
      "Epoch: 22484 \tTraining Loss: 1.474448 \tValidation Loss: 2.435230\n",
      "Epoch: 22485 \tTraining Loss: 1.487980 \tValidation Loss: 2.433507\n",
      "Epoch: 22486 \tTraining Loss: 1.471949 \tValidation Loss: 2.434115\n",
      "Epoch: 22487 \tTraining Loss: 1.476879 \tValidation Loss: 2.433909\n",
      "Epoch: 22488 \tTraining Loss: 1.475912 \tValidation Loss: 2.434026\n",
      "Epoch: 22489 \tTraining Loss: 1.509873 \tValidation Loss: 2.433651\n",
      "Epoch: 22490 \tTraining Loss: 1.493928 \tValidation Loss: 2.433787\n",
      "Epoch: 22491 \tTraining Loss: 1.492209 \tValidation Loss: 2.433687\n",
      "Epoch: 22492 \tTraining Loss: 1.481972 \tValidation Loss: 2.433225\n",
      "Epoch: 22493 \tTraining Loss: 1.455189 \tValidation Loss: 2.433728\n",
      "Epoch: 22494 \tTraining Loss: 1.512524 \tValidation Loss: 2.433187\n",
      "Epoch: 22495 \tTraining Loss: 1.496585 \tValidation Loss: 2.433523\n",
      "Epoch: 22496 \tTraining Loss: 1.503940 \tValidation Loss: 2.433046\n",
      "Epoch: 22497 \tTraining Loss: 1.459942 \tValidation Loss: 2.434603\n",
      "Epoch: 22498 \tTraining Loss: 1.508130 \tValidation Loss: 2.434625\n",
      "Epoch: 22499 \tTraining Loss: 1.537192 \tValidation Loss: 2.433990\n",
      "Epoch: 22500 \tTraining Loss: 1.507233 \tValidation Loss: 2.433938\n",
      "Epoch: 22501 \tTraining Loss: 1.502669 \tValidation Loss: 2.433933\n",
      "Epoch: 22502 \tTraining Loss: 1.506751 \tValidation Loss: 2.434213\n",
      "Epoch: 22503 \tTraining Loss: 1.493288 \tValidation Loss: 2.434139\n",
      "Epoch: 22504 \tTraining Loss: 1.492564 \tValidation Loss: 2.434121\n",
      "Epoch: 22505 \tTraining Loss: 1.525651 \tValidation Loss: 2.433640\n",
      "Epoch: 22506 \tTraining Loss: 1.520407 \tValidation Loss: 2.433762\n",
      "Epoch: 22507 \tTraining Loss: 1.409322 \tValidation Loss: 2.434589\n",
      "Epoch: 22508 \tTraining Loss: 1.489304 \tValidation Loss: 2.433909\n",
      "Epoch: 22509 \tTraining Loss: 1.460262 \tValidation Loss: 2.434047\n",
      "Epoch: 22510 \tTraining Loss: 1.488166 \tValidation Loss: 2.434147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22511 \tTraining Loss: 1.518312 \tValidation Loss: 2.433917\n",
      "Epoch: 22512 \tTraining Loss: 1.496042 \tValidation Loss: 2.433841\n",
      "Epoch: 22513 \tTraining Loss: 1.516924 \tValidation Loss: 2.433857\n",
      "Epoch: 22514 \tTraining Loss: 1.496218 \tValidation Loss: 2.434745\n",
      "Epoch: 22515 \tTraining Loss: 1.478687 \tValidation Loss: 2.434637\n",
      "Epoch: 22516 \tTraining Loss: 1.504880 \tValidation Loss: 2.433833\n",
      "Epoch: 22517 \tTraining Loss: 1.474274 \tValidation Loss: 2.433763\n",
      "Epoch: 22518 \tTraining Loss: 1.511451 \tValidation Loss: 2.433903\n",
      "Epoch: 22519 \tTraining Loss: 1.460057 \tValidation Loss: 2.435093\n",
      "Epoch: 22520 \tTraining Loss: 1.510651 \tValidation Loss: 2.434525\n",
      "Epoch: 22521 \tTraining Loss: 1.483263 \tValidation Loss: 2.433848\n",
      "Epoch: 22522 \tTraining Loss: 1.487919 \tValidation Loss: 2.433666\n",
      "Epoch: 22523 \tTraining Loss: 1.468923 \tValidation Loss: 2.435129\n",
      "Epoch: 22524 \tTraining Loss: 1.486290 \tValidation Loss: 2.434896\n",
      "Epoch: 22525 \tTraining Loss: 1.484592 \tValidation Loss: 2.434103\n",
      "Epoch: 22526 \tTraining Loss: 1.417771 \tValidation Loss: 2.435237\n",
      "Epoch: 22527 \tTraining Loss: 1.495597 \tValidation Loss: 2.434337\n",
      "Epoch: 22528 \tTraining Loss: 1.463720 \tValidation Loss: 2.434304\n",
      "Epoch: 22529 \tTraining Loss: 1.435549 \tValidation Loss: 2.435340\n",
      "Epoch: 22530 \tTraining Loss: 1.560224 \tValidation Loss: 2.435318\n",
      "Epoch: 22531 \tTraining Loss: 1.471326 \tValidation Loss: 2.435524\n",
      "Epoch: 22532 \tTraining Loss: 1.454035 \tValidation Loss: 2.435321\n",
      "Epoch: 22533 \tTraining Loss: 1.465252 \tValidation Loss: 2.435440\n",
      "Epoch: 22534 \tTraining Loss: 1.455474 \tValidation Loss: 2.435290\n",
      "Epoch: 22535 \tTraining Loss: 1.451927 \tValidation Loss: 2.434898\n",
      "Epoch: 22536 \tTraining Loss: 1.518600 \tValidation Loss: 2.434660\n",
      "Epoch: 22537 \tTraining Loss: 1.494421 \tValidation Loss: 2.434972\n",
      "Epoch: 22538 \tTraining Loss: 1.475121 \tValidation Loss: 2.434972\n",
      "Epoch: 22539 \tTraining Loss: 1.495625 \tValidation Loss: 2.434251\n",
      "Epoch: 22540 \tTraining Loss: 1.556342 \tValidation Loss: 2.434857\n",
      "Epoch: 22541 \tTraining Loss: 1.519773 \tValidation Loss: 2.434192\n",
      "Epoch: 22542 \tTraining Loss: 1.501996 \tValidation Loss: 2.434844\n",
      "Epoch: 22543 \tTraining Loss: 1.474586 \tValidation Loss: 2.435214\n",
      "Epoch: 22544 \tTraining Loss: 1.511668 \tValidation Loss: 2.434469\n",
      "Epoch: 22545 \tTraining Loss: 1.481445 \tValidation Loss: 2.434850\n",
      "Epoch: 22546 \tTraining Loss: 1.497705 \tValidation Loss: 2.434500\n",
      "Epoch: 22547 \tTraining Loss: 1.526782 \tValidation Loss: 2.433890\n",
      "Epoch: 22548 \tTraining Loss: 1.497667 \tValidation Loss: 2.434922\n",
      "Epoch: 22549 \tTraining Loss: 1.471218 \tValidation Loss: 2.435035\n",
      "Epoch: 22550 \tTraining Loss: 1.484394 \tValidation Loss: 2.434129\n",
      "Epoch: 22551 \tTraining Loss: 1.497926 \tValidation Loss: 2.436244\n",
      "Epoch: 22552 \tTraining Loss: 1.448681 \tValidation Loss: 2.436016\n",
      "Epoch: 22553 \tTraining Loss: 1.446711 \tValidation Loss: 2.436403\n",
      "Epoch: 22554 \tTraining Loss: 1.471115 \tValidation Loss: 2.436052\n",
      "Epoch: 22555 \tTraining Loss: 1.535847 \tValidation Loss: 2.435097\n",
      "Epoch: 22556 \tTraining Loss: 1.503071 \tValidation Loss: 2.434984\n",
      "Epoch: 22557 \tTraining Loss: 1.487059 \tValidation Loss: 2.434473\n",
      "Epoch: 22558 \tTraining Loss: 1.471700 \tValidation Loss: 2.435099\n",
      "Epoch: 22559 \tTraining Loss: 1.503847 \tValidation Loss: 2.435258\n",
      "Epoch: 22560 \tTraining Loss: 1.459228 \tValidation Loss: 2.435548\n",
      "Epoch: 22561 \tTraining Loss: 1.507529 \tValidation Loss: 2.434373\n",
      "Epoch: 22562 \tTraining Loss: 1.469612 \tValidation Loss: 2.434903\n",
      "Epoch: 22563 \tTraining Loss: 1.480652 \tValidation Loss: 2.435251\n",
      "Epoch: 22564 \tTraining Loss: 1.488210 \tValidation Loss: 2.435220\n",
      "Epoch: 22565 \tTraining Loss: 1.487681 \tValidation Loss: 2.434165\n",
      "Epoch: 22566 \tTraining Loss: 1.462925 \tValidation Loss: 2.435555\n",
      "Epoch: 22567 \tTraining Loss: 1.488168 \tValidation Loss: 2.435927\n",
      "Epoch: 22568 \tTraining Loss: 1.457298 \tValidation Loss: 2.436053\n",
      "Epoch: 22569 \tTraining Loss: 1.444094 \tValidation Loss: 2.436080\n",
      "Epoch: 22570 \tTraining Loss: 1.468777 \tValidation Loss: 2.436128\n",
      "Epoch: 22571 \tTraining Loss: 1.451992 \tValidation Loss: 2.435840\n",
      "Epoch: 22572 \tTraining Loss: 1.515180 \tValidation Loss: 2.434503\n",
      "Epoch: 22573 \tTraining Loss: 1.452750 \tValidation Loss: 2.435765\n",
      "Epoch: 22574 \tTraining Loss: 1.478700 \tValidation Loss: 2.435211\n",
      "Epoch: 22575 \tTraining Loss: 1.468821 \tValidation Loss: 2.435673\n",
      "Epoch: 22576 \tTraining Loss: 1.531410 \tValidation Loss: 2.435236\n",
      "Epoch: 22577 \tTraining Loss: 1.531574 \tValidation Loss: 2.435357\n",
      "Epoch: 22578 \tTraining Loss: 1.472239 \tValidation Loss: 2.435235\n",
      "Epoch: 22579 \tTraining Loss: 1.491558 \tValidation Loss: 2.435395\n",
      "Epoch: 22580 \tTraining Loss: 1.475898 \tValidation Loss: 2.434431\n",
      "Epoch: 22581 \tTraining Loss: 1.496849 \tValidation Loss: 2.434509\n",
      "Epoch: 22582 \tTraining Loss: 1.478818 \tValidation Loss: 2.434849\n",
      "Epoch: 22583 \tTraining Loss: 1.520432 \tValidation Loss: 2.435243\n",
      "Epoch: 22584 \tTraining Loss: 1.519266 \tValidation Loss: 2.434994\n",
      "Epoch: 22585 \tTraining Loss: 1.488115 \tValidation Loss: 2.435380\n",
      "Epoch: 22586 \tTraining Loss: 1.528583 \tValidation Loss: 2.434346\n",
      "Epoch: 22587 \tTraining Loss: 1.497532 \tValidation Loss: 2.434392\n",
      "Epoch: 22588 \tTraining Loss: 1.508028 \tValidation Loss: 2.435190\n",
      "Epoch: 22589 \tTraining Loss: 1.494742 \tValidation Loss: 2.435600\n",
      "Epoch: 22590 \tTraining Loss: 1.424268 \tValidation Loss: 2.434910\n",
      "Epoch: 22591 \tTraining Loss: 1.472378 \tValidation Loss: 2.435767\n",
      "Epoch: 22592 \tTraining Loss: 1.486839 \tValidation Loss: 2.435193\n",
      "Epoch: 22593 \tTraining Loss: 1.495295 \tValidation Loss: 2.434931\n",
      "Epoch: 22594 \tTraining Loss: 1.509652 \tValidation Loss: 2.434211\n",
      "Epoch: 22595 \tTraining Loss: 1.473459 \tValidation Loss: 2.435786\n",
      "Epoch: 22596 \tTraining Loss: 1.461955 \tValidation Loss: 2.435402\n",
      "Epoch: 22597 \tTraining Loss: 1.478693 \tValidation Loss: 2.435712\n",
      "Epoch: 22598 \tTraining Loss: 1.455318 \tValidation Loss: 2.435942\n",
      "Epoch: 22599 \tTraining Loss: 1.504951 \tValidation Loss: 2.435761\n",
      "Epoch: 22600 \tTraining Loss: 1.502123 \tValidation Loss: 2.436163\n",
      "Epoch: 22601 \tTraining Loss: 1.515298 \tValidation Loss: 2.436167\n",
      "Epoch: 22602 \tTraining Loss: 1.471558 \tValidation Loss: 2.436042\n",
      "Epoch: 22603 \tTraining Loss: 1.498377 \tValidation Loss: 2.435513\n",
      "Epoch: 22604 \tTraining Loss: 1.482605 \tValidation Loss: 2.435699\n",
      "Epoch: 22605 \tTraining Loss: 1.464905 \tValidation Loss: 2.435888\n",
      "Epoch: 22606 \tTraining Loss: 1.534690 \tValidation Loss: 2.435886\n",
      "Epoch: 22607 \tTraining Loss: 1.478954 \tValidation Loss: 2.435989\n",
      "Epoch: 22608 \tTraining Loss: 1.528751 \tValidation Loss: 2.435221\n",
      "Epoch: 22609 \tTraining Loss: 1.466279 \tValidation Loss: 2.435768\n",
      "Epoch: 22610 \tTraining Loss: 1.495439 \tValidation Loss: 2.435714\n",
      "Epoch: 22611 \tTraining Loss: 1.513867 \tValidation Loss: 2.436876\n",
      "Epoch: 22612 \tTraining Loss: 1.514560 \tValidation Loss: 2.435656\n",
      "Epoch: 22613 \tTraining Loss: 1.463345 \tValidation Loss: 2.436084\n",
      "Epoch: 22614 \tTraining Loss: 1.492720 \tValidation Loss: 2.435735\n",
      "Epoch: 22615 \tTraining Loss: 1.465240 \tValidation Loss: 2.435943\n",
      "Epoch: 22616 \tTraining Loss: 1.492345 \tValidation Loss: 2.435298\n",
      "Epoch: 22617 \tTraining Loss: 1.471350 \tValidation Loss: 2.435096\n",
      "Epoch: 22618 \tTraining Loss: 1.462770 \tValidation Loss: 2.436255\n",
      "Epoch: 22619 \tTraining Loss: 1.551718 \tValidation Loss: 2.435404\n",
      "Epoch: 22620 \tTraining Loss: 1.482735 \tValidation Loss: 2.435765\n",
      "Epoch: 22621 \tTraining Loss: 1.503748 \tValidation Loss: 2.435735\n",
      "Epoch: 22622 \tTraining Loss: 1.459711 \tValidation Loss: 2.436028\n",
      "Epoch: 22623 \tTraining Loss: 1.502845 \tValidation Loss: 2.436086\n",
      "Epoch: 22624 \tTraining Loss: 1.497756 \tValidation Loss: 2.435926\n",
      "Epoch: 22625 \tTraining Loss: 1.501328 \tValidation Loss: 2.436227\n",
      "Epoch: 22626 \tTraining Loss: 1.526980 \tValidation Loss: 2.434892\n",
      "Epoch: 22627 \tTraining Loss: 1.508297 \tValidation Loss: 2.436406\n",
      "Epoch: 22628 \tTraining Loss: 1.465628 \tValidation Loss: 2.436236\n",
      "Epoch: 22629 \tTraining Loss: 1.469227 \tValidation Loss: 2.436809\n",
      "Epoch: 22630 \tTraining Loss: 1.507826 \tValidation Loss: 2.436087\n",
      "Epoch: 22631 \tTraining Loss: 1.513947 \tValidation Loss: 2.436798\n",
      "Epoch: 22632 \tTraining Loss: 1.486569 \tValidation Loss: 2.436001\n",
      "Epoch: 22633 \tTraining Loss: 1.472588 \tValidation Loss: 2.436649\n",
      "Epoch: 22634 \tTraining Loss: 1.473166 \tValidation Loss: 2.436506\n",
      "Epoch: 22635 \tTraining Loss: 1.506896 \tValidation Loss: 2.436566\n",
      "Epoch: 22636 \tTraining Loss: 1.534204 \tValidation Loss: 2.436547\n",
      "Epoch: 22637 \tTraining Loss: 1.510015 \tValidation Loss: 2.436025\n",
      "Epoch: 22638 \tTraining Loss: 1.464228 \tValidation Loss: 2.436410\n",
      "Epoch: 22639 \tTraining Loss: 1.480495 \tValidation Loss: 2.436739\n",
      "Epoch: 22640 \tTraining Loss: 1.528172 \tValidation Loss: 2.435810\n",
      "Epoch: 22641 \tTraining Loss: 1.494942 \tValidation Loss: 2.436247\n",
      "Epoch: 22642 \tTraining Loss: 1.464500 \tValidation Loss: 2.435755\n",
      "Epoch: 22643 \tTraining Loss: 1.492907 \tValidation Loss: 2.436756\n",
      "Epoch: 22644 \tTraining Loss: 1.500717 \tValidation Loss: 2.435859\n",
      "Epoch: 22645 \tTraining Loss: 1.474674 \tValidation Loss: 2.436133\n",
      "Epoch: 22646 \tTraining Loss: 1.479997 \tValidation Loss: 2.435762\n",
      "Epoch: 22647 \tTraining Loss: 1.510268 \tValidation Loss: 2.435658\n",
      "Epoch: 22648 \tTraining Loss: 1.481847 \tValidation Loss: 2.435969\n",
      "Epoch: 22649 \tTraining Loss: 1.472222 \tValidation Loss: 2.436365\n",
      "Epoch: 22650 \tTraining Loss: 1.445433 \tValidation Loss: 2.436578\n",
      "Epoch: 22651 \tTraining Loss: 1.487231 \tValidation Loss: 2.437100\n",
      "Epoch: 22652 \tTraining Loss: 1.517913 \tValidation Loss: 2.436880\n",
      "Epoch: 22653 \tTraining Loss: 1.465753 \tValidation Loss: 2.437567\n",
      "Epoch: 22654 \tTraining Loss: 1.485254 \tValidation Loss: 2.436324\n",
      "Epoch: 22655 \tTraining Loss: 1.504342 \tValidation Loss: 2.435269\n",
      "Epoch: 22656 \tTraining Loss: 1.470473 \tValidation Loss: 2.436488\n",
      "Epoch: 22657 \tTraining Loss: 1.459022 \tValidation Loss: 2.437123\n",
      "Epoch: 22658 \tTraining Loss: 1.472077 \tValidation Loss: 2.436977\n",
      "Epoch: 22659 \tTraining Loss: 1.439268 \tValidation Loss: 2.437674\n",
      "Epoch: 22660 \tTraining Loss: 1.487203 \tValidation Loss: 2.436874\n",
      "Epoch: 22661 \tTraining Loss: 1.485915 \tValidation Loss: 2.437167\n",
      "Epoch: 22662 \tTraining Loss: 1.446732 \tValidation Loss: 2.437576\n",
      "Epoch: 22663 \tTraining Loss: 1.517323 \tValidation Loss: 2.436250\n",
      "Epoch: 22664 \tTraining Loss: 1.451079 \tValidation Loss: 2.436205\n",
      "Epoch: 22665 \tTraining Loss: 1.502043 \tValidation Loss: 2.436452\n",
      "Epoch: 22666 \tTraining Loss: 1.454261 \tValidation Loss: 2.436954\n",
      "Epoch: 22667 \tTraining Loss: 1.508126 \tValidation Loss: 2.436764\n",
      "Epoch: 22668 \tTraining Loss: 1.478318 \tValidation Loss: 2.435921\n",
      "Epoch: 22669 \tTraining Loss: 1.493200 \tValidation Loss: 2.435562\n",
      "Epoch: 22670 \tTraining Loss: 1.496030 \tValidation Loss: 2.435752\n",
      "Epoch: 22671 \tTraining Loss: 1.495539 \tValidation Loss: 2.436005\n",
      "Epoch: 22672 \tTraining Loss: 1.496665 \tValidation Loss: 2.436182\n",
      "Epoch: 22673 \tTraining Loss: 1.479907 \tValidation Loss: 2.436556\n",
      "Epoch: 22674 \tTraining Loss: 1.493355 \tValidation Loss: 2.435756\n",
      "Epoch: 22675 \tTraining Loss: 1.482414 \tValidation Loss: 2.437283\n",
      "Epoch: 22676 \tTraining Loss: 1.486441 \tValidation Loss: 2.437714\n",
      "Epoch: 22677 \tTraining Loss: 1.491367 \tValidation Loss: 2.436189\n",
      "Epoch: 22678 \tTraining Loss: 1.508803 \tValidation Loss: 2.436389\n",
      "Epoch: 22679 \tTraining Loss: 1.510522 \tValidation Loss: 2.436878\n",
      "Epoch: 22680 \tTraining Loss: 1.480039 \tValidation Loss: 2.436311\n",
      "Epoch: 22681 \tTraining Loss: 1.473070 \tValidation Loss: 2.436817\n",
      "Epoch: 22682 \tTraining Loss: 1.469664 \tValidation Loss: 2.437104\n",
      "Epoch: 22683 \tTraining Loss: 1.491533 \tValidation Loss: 2.436406\n",
      "Epoch: 22684 \tTraining Loss: 1.524885 \tValidation Loss: 2.436685\n",
      "Epoch: 22685 \tTraining Loss: 1.473163 \tValidation Loss: 2.437408\n",
      "Epoch: 22686 \tTraining Loss: 1.451690 \tValidation Loss: 2.437480\n",
      "Epoch: 22687 \tTraining Loss: 1.467380 \tValidation Loss: 2.437304\n",
      "Epoch: 22688 \tTraining Loss: 1.456364 \tValidation Loss: 2.437627\n",
      "Epoch: 22689 \tTraining Loss: 1.460679 \tValidation Loss: 2.437555\n",
      "Epoch: 22690 \tTraining Loss: 1.467804 \tValidation Loss: 2.436781\n",
      "Epoch: 22691 \tTraining Loss: 1.453359 \tValidation Loss: 2.437011\n",
      "Epoch: 22692 \tTraining Loss: 1.476802 \tValidation Loss: 2.437198\n",
      "Epoch: 22693 \tTraining Loss: 1.521720 \tValidation Loss: 2.436843\n",
      "Epoch: 22694 \tTraining Loss: 1.460296 \tValidation Loss: 2.435934\n",
      "Epoch: 22695 \tTraining Loss: 1.515828 \tValidation Loss: 2.435496\n",
      "Epoch: 22696 \tTraining Loss: 1.464491 \tValidation Loss: 2.436726\n",
      "Epoch: 22697 \tTraining Loss: 1.491696 \tValidation Loss: 2.436190\n",
      "Epoch: 22698 \tTraining Loss: 1.447437 \tValidation Loss: 2.437085\n",
      "Epoch: 22699 \tTraining Loss: 1.496153 \tValidation Loss: 2.436886\n",
      "Epoch: 22700 \tTraining Loss: 1.467205 \tValidation Loss: 2.436512\n",
      "Epoch: 22701 \tTraining Loss: 1.486171 \tValidation Loss: 2.436787\n",
      "Epoch: 22702 \tTraining Loss: 1.475253 \tValidation Loss: 2.436951\n",
      "Epoch: 22703 \tTraining Loss: 1.545397 \tValidation Loss: 2.436409\n",
      "Epoch: 22704 \tTraining Loss: 1.484988 \tValidation Loss: 2.437332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22705 \tTraining Loss: 1.531056 \tValidation Loss: 2.436804\n",
      "Epoch: 22706 \tTraining Loss: 1.503036 \tValidation Loss: 2.436747\n",
      "Epoch: 22707 \tTraining Loss: 1.491590 \tValidation Loss: 2.437332\n",
      "Epoch: 22708 \tTraining Loss: 1.479307 \tValidation Loss: 2.437016\n",
      "Epoch: 22709 \tTraining Loss: 1.482392 \tValidation Loss: 2.437173\n",
      "Epoch: 22710 \tTraining Loss: 1.491648 \tValidation Loss: 2.437009\n",
      "Epoch: 22711 \tTraining Loss: 1.481804 \tValidation Loss: 2.437035\n",
      "Epoch: 22712 \tTraining Loss: 1.488274 \tValidation Loss: 2.436670\n",
      "Epoch: 22713 \tTraining Loss: 1.456209 \tValidation Loss: 2.437483\n",
      "Epoch: 22714 \tTraining Loss: 1.480732 \tValidation Loss: 2.437625\n",
      "Epoch: 22715 \tTraining Loss: 1.450847 \tValidation Loss: 2.437250\n",
      "Epoch: 22716 \tTraining Loss: 1.469003 \tValidation Loss: 2.437557\n",
      "Epoch: 22717 \tTraining Loss: 1.488555 \tValidation Loss: 2.437642\n",
      "Epoch: 22718 \tTraining Loss: 1.482360 \tValidation Loss: 2.436663\n",
      "Epoch: 22719 \tTraining Loss: 1.453058 \tValidation Loss: 2.437667\n",
      "Epoch: 22720 \tTraining Loss: 1.462897 \tValidation Loss: 2.437701\n",
      "Epoch: 22721 \tTraining Loss: 1.472584 \tValidation Loss: 2.437266\n",
      "Epoch: 22722 \tTraining Loss: 1.458898 \tValidation Loss: 2.437164\n",
      "Epoch: 22723 \tTraining Loss: 1.477707 \tValidation Loss: 2.437731\n",
      "Epoch: 22724 \tTraining Loss: 1.477255 \tValidation Loss: 2.437402\n",
      "Epoch: 22725 \tTraining Loss: 1.448071 \tValidation Loss: 2.437625\n",
      "Epoch: 22726 \tTraining Loss: 1.477467 \tValidation Loss: 2.437330\n",
      "Epoch: 22727 \tTraining Loss: 1.490067 \tValidation Loss: 2.437251\n",
      "Epoch: 22728 \tTraining Loss: 1.450144 \tValidation Loss: 2.437350\n",
      "Epoch: 22729 \tTraining Loss: 1.498056 \tValidation Loss: 2.437682\n",
      "Epoch: 22730 \tTraining Loss: 1.518245 \tValidation Loss: 2.436462\n",
      "Epoch: 22731 \tTraining Loss: 1.515354 \tValidation Loss: 2.436757\n",
      "Epoch: 22732 \tTraining Loss: 1.505594 \tValidation Loss: 2.437216\n",
      "Epoch: 22733 \tTraining Loss: 1.505608 \tValidation Loss: 2.437392\n",
      "Epoch: 22734 \tTraining Loss: 1.518453 \tValidation Loss: 2.437286\n",
      "Epoch: 22735 \tTraining Loss: 1.528666 \tValidation Loss: 2.437842\n",
      "Epoch: 22736 \tTraining Loss: 1.494320 \tValidation Loss: 2.437909\n",
      "Epoch: 22737 \tTraining Loss: 1.491456 \tValidation Loss: 2.437911\n",
      "Epoch: 22738 \tTraining Loss: 1.488511 \tValidation Loss: 2.437581\n",
      "Epoch: 22739 \tTraining Loss: 1.512291 \tValidation Loss: 2.437696\n",
      "Epoch: 22740 \tTraining Loss: 1.486455 \tValidation Loss: 2.437303\n",
      "Epoch: 22741 \tTraining Loss: 1.516628 \tValidation Loss: 2.436625\n",
      "Epoch: 22742 \tTraining Loss: 1.523095 \tValidation Loss: 2.436729\n",
      "Epoch: 22743 \tTraining Loss: 1.512393 \tValidation Loss: 2.436871\n",
      "Epoch: 22744 \tTraining Loss: 1.503202 \tValidation Loss: 2.436238\n",
      "Epoch: 22745 \tTraining Loss: 1.480496 \tValidation Loss: 2.435648\n",
      "Epoch: 22746 \tTraining Loss: 1.462273 \tValidation Loss: 2.436679\n",
      "Epoch: 22747 \tTraining Loss: 1.484194 \tValidation Loss: 2.437847\n",
      "Epoch: 22748 \tTraining Loss: 1.481705 \tValidation Loss: 2.437722\n",
      "Epoch: 22749 \tTraining Loss: 1.453522 \tValidation Loss: 2.437761\n",
      "Epoch: 22750 \tTraining Loss: 1.478495 \tValidation Loss: 2.437685\n",
      "Epoch: 22751 \tTraining Loss: 1.456461 \tValidation Loss: 2.438293\n",
      "Epoch: 22752 \tTraining Loss: 1.475075 \tValidation Loss: 2.437990\n",
      "Epoch: 22753 \tTraining Loss: 1.502679 \tValidation Loss: 2.438168\n",
      "Epoch: 22754 \tTraining Loss: 1.474804 \tValidation Loss: 2.438293\n",
      "Epoch: 22755 \tTraining Loss: 1.479419 \tValidation Loss: 2.438554\n",
      "Epoch: 22756 \tTraining Loss: 1.481156 \tValidation Loss: 2.437599\n",
      "Epoch: 22757 \tTraining Loss: 1.479204 \tValidation Loss: 2.438290\n",
      "Epoch: 22758 \tTraining Loss: 1.485456 \tValidation Loss: 2.438534\n",
      "Epoch: 22759 \tTraining Loss: 1.493297 \tValidation Loss: 2.438924\n",
      "Epoch: 22760 \tTraining Loss: 1.507440 \tValidation Loss: 2.437843\n",
      "Epoch: 22761 \tTraining Loss: 1.511379 \tValidation Loss: 2.438154\n",
      "Epoch: 22762 \tTraining Loss: 1.497435 \tValidation Loss: 2.438116\n",
      "Epoch: 22763 \tTraining Loss: 1.449860 \tValidation Loss: 2.438724\n",
      "Epoch: 22764 \tTraining Loss: 1.469210 \tValidation Loss: 2.438037\n",
      "Epoch: 22765 \tTraining Loss: 1.520056 \tValidation Loss: 2.437622\n",
      "Epoch: 22766 \tTraining Loss: 1.477446 \tValidation Loss: 2.437385\n",
      "Epoch: 22767 \tTraining Loss: 1.489541 \tValidation Loss: 2.438470\n",
      "Epoch: 22768 \tTraining Loss: 1.482837 \tValidation Loss: 2.438150\n",
      "Epoch: 22769 \tTraining Loss: 1.469399 \tValidation Loss: 2.437645\n",
      "Epoch: 22770 \tTraining Loss: 1.477823 \tValidation Loss: 2.437942\n",
      "Epoch: 22771 \tTraining Loss: 1.497059 \tValidation Loss: 2.438232\n",
      "Epoch: 22772 \tTraining Loss: 1.497266 \tValidation Loss: 2.438308\n",
      "Epoch: 22773 \tTraining Loss: 1.450912 \tValidation Loss: 2.438522\n",
      "Epoch: 22774 \tTraining Loss: 1.476040 \tValidation Loss: 2.438355\n",
      "Epoch: 22775 \tTraining Loss: 1.468846 \tValidation Loss: 2.438780\n",
      "Epoch: 22776 \tTraining Loss: 1.492183 \tValidation Loss: 2.438015\n",
      "Epoch: 22777 \tTraining Loss: 1.476273 \tValidation Loss: 2.437234\n",
      "Epoch: 22778 \tTraining Loss: 1.505690 \tValidation Loss: 2.437398\n",
      "Epoch: 22779 \tTraining Loss: 1.475403 \tValidation Loss: 2.437481\n",
      "Epoch: 22780 \tTraining Loss: 1.477464 \tValidation Loss: 2.437540\n",
      "Epoch: 22781 \tTraining Loss: 1.442643 \tValidation Loss: 2.439093\n",
      "Epoch: 22782 \tTraining Loss: 1.464744 \tValidation Loss: 2.438458\n",
      "Epoch: 22783 \tTraining Loss: 1.455105 \tValidation Loss: 2.438683\n",
      "Epoch: 22784 \tTraining Loss: 1.454815 \tValidation Loss: 2.438532\n",
      "Epoch: 22785 \tTraining Loss: 1.515011 \tValidation Loss: 2.438261\n",
      "Epoch: 22786 \tTraining Loss: 1.486873 \tValidation Loss: 2.438023\n",
      "Epoch: 22787 \tTraining Loss: 1.473474 \tValidation Loss: 2.438083\n",
      "Epoch: 22788 \tTraining Loss: 1.463328 \tValidation Loss: 2.438506\n",
      "Epoch: 22789 \tTraining Loss: 1.497348 \tValidation Loss: 2.439478\n",
      "Epoch: 22790 \tTraining Loss: 1.461232 \tValidation Loss: 2.438162\n",
      "Epoch: 22791 \tTraining Loss: 1.506110 \tValidation Loss: 2.437325\n",
      "Epoch: 22792 \tTraining Loss: 1.405103 \tValidation Loss: 2.439225\n",
      "Epoch: 22793 \tTraining Loss: 1.453621 \tValidation Loss: 2.439130\n",
      "Epoch: 22794 \tTraining Loss: 1.472439 \tValidation Loss: 2.438308\n",
      "Epoch: 22795 \tTraining Loss: 1.463840 \tValidation Loss: 2.439145\n",
      "Epoch: 22796 \tTraining Loss: 1.502687 \tValidation Loss: 2.438131\n",
      "Epoch: 22797 \tTraining Loss: 1.494827 \tValidation Loss: 2.438619\n",
      "Epoch: 22798 \tTraining Loss: 1.500463 \tValidation Loss: 2.438211\n",
      "Epoch: 22799 \tTraining Loss: 1.517488 \tValidation Loss: 2.437454\n",
      "Epoch: 22800 \tTraining Loss: 1.500655 \tValidation Loss: 2.438368\n",
      "Epoch: 22801 \tTraining Loss: 1.468557 \tValidation Loss: 2.438292\n",
      "Epoch: 22802 \tTraining Loss: 1.496096 \tValidation Loss: 2.439331\n",
      "Epoch: 22803 \tTraining Loss: 1.497904 \tValidation Loss: 2.438993\n",
      "Epoch: 22804 \tTraining Loss: 1.461672 \tValidation Loss: 2.439125\n",
      "Epoch: 22805 \tTraining Loss: 1.433176 \tValidation Loss: 2.439357\n",
      "Epoch: 22806 \tTraining Loss: 1.438489 \tValidation Loss: 2.439900\n",
      "Epoch: 22807 \tTraining Loss: 1.458420 \tValidation Loss: 2.440480\n",
      "Epoch: 22808 \tTraining Loss: 1.493017 \tValidation Loss: 2.439400\n",
      "Epoch: 22809 \tTraining Loss: 1.520170 \tValidation Loss: 2.438728\n",
      "Epoch: 22810 \tTraining Loss: 1.528081 \tValidation Loss: 2.438435\n",
      "Epoch: 22811 \tTraining Loss: 1.508748 \tValidation Loss: 2.438680\n",
      "Epoch: 22812 \tTraining Loss: 1.479416 \tValidation Loss: 2.438713\n",
      "Epoch: 22813 \tTraining Loss: 1.464913 \tValidation Loss: 2.438692\n",
      "Epoch: 22814 \tTraining Loss: 1.443316 \tValidation Loss: 2.439504\n",
      "Epoch: 22815 \tTraining Loss: 1.523021 \tValidation Loss: 2.438642\n",
      "Epoch: 22816 \tTraining Loss: 1.503505 \tValidation Loss: 2.438173\n",
      "Epoch: 22817 \tTraining Loss: 1.475330 \tValidation Loss: 2.438458\n",
      "Epoch: 22818 \tTraining Loss: 1.501570 \tValidation Loss: 2.438694\n",
      "Epoch: 22819 \tTraining Loss: 1.511893 \tValidation Loss: 2.438453\n",
      "Epoch: 22820 \tTraining Loss: 1.455505 \tValidation Loss: 2.438913\n",
      "Epoch: 22821 \tTraining Loss: 1.447617 \tValidation Loss: 2.439221\n",
      "Epoch: 22822 \tTraining Loss: 1.471929 \tValidation Loss: 2.439415\n",
      "Epoch: 22823 \tTraining Loss: 1.450511 \tValidation Loss: 2.439537\n",
      "Epoch: 22824 \tTraining Loss: 1.490746 \tValidation Loss: 2.439372\n",
      "Epoch: 22825 \tTraining Loss: 1.498657 \tValidation Loss: 2.439163\n",
      "Epoch: 22826 \tTraining Loss: 1.468868 \tValidation Loss: 2.439856\n",
      "Epoch: 22827 \tTraining Loss: 1.467484 \tValidation Loss: 2.439587\n",
      "Epoch: 22828 \tTraining Loss: 1.501920 \tValidation Loss: 2.439152\n",
      "Epoch: 22829 \tTraining Loss: 1.458238 \tValidation Loss: 2.439184\n",
      "Epoch: 22830 \tTraining Loss: 1.489355 \tValidation Loss: 2.439509\n",
      "Epoch: 22831 \tTraining Loss: 1.505245 \tValidation Loss: 2.440066\n",
      "Epoch: 22832 \tTraining Loss: 1.494078 \tValidation Loss: 2.440188\n",
      "Epoch: 22833 \tTraining Loss: 1.478024 \tValidation Loss: 2.439914\n",
      "Epoch: 22834 \tTraining Loss: 1.475329 \tValidation Loss: 2.439036\n",
      "Epoch: 22835 \tTraining Loss: 1.485923 \tValidation Loss: 2.439085\n",
      "Epoch: 22836 \tTraining Loss: 1.509177 \tValidation Loss: 2.438824\n",
      "Epoch: 22837 \tTraining Loss: 1.496600 \tValidation Loss: 2.439854\n",
      "Epoch: 22838 \tTraining Loss: 1.472493 \tValidation Loss: 2.439392\n",
      "Epoch: 22839 \tTraining Loss: 1.501993 \tValidation Loss: 2.438986\n",
      "Epoch: 22840 \tTraining Loss: 1.517103 \tValidation Loss: 2.438641\n",
      "Epoch: 22841 \tTraining Loss: 1.499738 \tValidation Loss: 2.438739\n",
      "Epoch: 22842 \tTraining Loss: 1.471824 \tValidation Loss: 2.439128\n",
      "Epoch: 22843 \tTraining Loss: 1.528733 \tValidation Loss: 2.438755\n",
      "Epoch: 22844 \tTraining Loss: 1.460593 \tValidation Loss: 2.439816\n",
      "Epoch: 22845 \tTraining Loss: 1.439026 \tValidation Loss: 2.439601\n",
      "Epoch: 22846 \tTraining Loss: 1.454862 \tValidation Loss: 2.439909\n",
      "Epoch: 22847 \tTraining Loss: 1.505056 \tValidation Loss: 2.440110\n",
      "Epoch: 22848 \tTraining Loss: 1.478282 \tValidation Loss: 2.439114\n",
      "Epoch: 22849 \tTraining Loss: 1.475519 \tValidation Loss: 2.439878\n",
      "Epoch: 22850 \tTraining Loss: 1.492336 \tValidation Loss: 2.439333\n",
      "Epoch: 22851 \tTraining Loss: 1.482462 \tValidation Loss: 2.439885\n",
      "Epoch: 22852 \tTraining Loss: 1.494436 \tValidation Loss: 2.439713\n",
      "Epoch: 22853 \tTraining Loss: 1.465093 \tValidation Loss: 2.439590\n",
      "Epoch: 22854 \tTraining Loss: 1.447408 \tValidation Loss: 2.439431\n",
      "Epoch: 22855 \tTraining Loss: 1.418800 \tValidation Loss: 2.440441\n",
      "Epoch: 22856 \tTraining Loss: 1.485857 \tValidation Loss: 2.440140\n",
      "Epoch: 22857 \tTraining Loss: 1.472138 \tValidation Loss: 2.439732\n",
      "Epoch: 22858 \tTraining Loss: 1.494392 \tValidation Loss: 2.439853\n",
      "Epoch: 22859 \tTraining Loss: 1.446953 \tValidation Loss: 2.440342\n",
      "Epoch: 22860 \tTraining Loss: 1.462237 \tValidation Loss: 2.439987\n",
      "Epoch: 22861 \tTraining Loss: 1.463922 \tValidation Loss: 2.440400\n",
      "Epoch: 22862 \tTraining Loss: 1.533999 \tValidation Loss: 2.438943\n",
      "Epoch: 22863 \tTraining Loss: 1.486078 \tValidation Loss: 2.438905\n",
      "Epoch: 22864 \tTraining Loss: 1.440739 \tValidation Loss: 2.440641\n",
      "Epoch: 22865 \tTraining Loss: 1.474219 \tValidation Loss: 2.439862\n",
      "Epoch: 22866 \tTraining Loss: 1.460615 \tValidation Loss: 2.440271\n",
      "Epoch: 22867 \tTraining Loss: 1.469756 \tValidation Loss: 2.440239\n",
      "Epoch: 22868 \tTraining Loss: 1.453318 \tValidation Loss: 2.440766\n",
      "Epoch: 22869 \tTraining Loss: 1.496595 \tValidation Loss: 2.440834\n",
      "Epoch: 22870 \tTraining Loss: 1.447585 \tValidation Loss: 2.440125\n",
      "Epoch: 22871 \tTraining Loss: 1.472817 \tValidation Loss: 2.440919\n",
      "Epoch: 22872 \tTraining Loss: 1.471688 \tValidation Loss: 2.440317\n",
      "Epoch: 22873 \tTraining Loss: 1.478202 \tValidation Loss: 2.440214\n",
      "Epoch: 22874 \tTraining Loss: 1.472734 \tValidation Loss: 2.440782\n",
      "Epoch: 22875 \tTraining Loss: 1.513607 \tValidation Loss: 2.439941\n",
      "Epoch: 22876 \tTraining Loss: 1.433268 \tValidation Loss: 2.439946\n",
      "Epoch: 22877 \tTraining Loss: 1.445998 \tValidation Loss: 2.440405\n",
      "Epoch: 22878 \tTraining Loss: 1.498499 \tValidation Loss: 2.439463\n",
      "Epoch: 22879 \tTraining Loss: 1.464759 \tValidation Loss: 2.440627\n",
      "Epoch: 22880 \tTraining Loss: 1.484393 \tValidation Loss: 2.440224\n",
      "Epoch: 22881 \tTraining Loss: 1.474573 \tValidation Loss: 2.440258\n",
      "Epoch: 22882 \tTraining Loss: 1.465517 \tValidation Loss: 2.440650\n",
      "Epoch: 22883 \tTraining Loss: 1.467473 \tValidation Loss: 2.440781\n",
      "Epoch: 22884 \tTraining Loss: 1.425411 \tValidation Loss: 2.441731\n",
      "Epoch: 22885 \tTraining Loss: 1.455131 \tValidation Loss: 2.441906\n",
      "Epoch: 22886 \tTraining Loss: 1.462903 \tValidation Loss: 2.440961\n",
      "Epoch: 22887 \tTraining Loss: 1.480627 \tValidation Loss: 2.440265\n",
      "Epoch: 22888 \tTraining Loss: 1.453938 \tValidation Loss: 2.440631\n",
      "Epoch: 22889 \tTraining Loss: 1.502292 \tValidation Loss: 2.440284\n",
      "Epoch: 22890 \tTraining Loss: 1.473289 \tValidation Loss: 2.441326\n",
      "Epoch: 22891 \tTraining Loss: 1.460483 \tValidation Loss: 2.440788\n",
      "Epoch: 22892 \tTraining Loss: 1.459422 \tValidation Loss: 2.441175\n",
      "Epoch: 22893 \tTraining Loss: 1.471551 \tValidation Loss: 2.441266\n",
      "Epoch: 22894 \tTraining Loss: 1.464968 \tValidation Loss: 2.440486\n",
      "Epoch: 22895 \tTraining Loss: 1.460003 \tValidation Loss: 2.440776\n",
      "Epoch: 22896 \tTraining Loss: 1.450158 \tValidation Loss: 2.441386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22897 \tTraining Loss: 1.524610 \tValidation Loss: 2.439668\n",
      "Epoch: 22898 \tTraining Loss: 1.488809 \tValidation Loss: 2.439668\n",
      "Epoch: 22899 \tTraining Loss: 1.453566 \tValidation Loss: 2.440501\n",
      "Epoch: 22900 \tTraining Loss: 1.463667 \tValidation Loss: 2.439956\n",
      "Epoch: 22901 \tTraining Loss: 1.452047 \tValidation Loss: 2.440641\n",
      "Epoch: 22902 \tTraining Loss: 1.479638 \tValidation Loss: 2.441006\n",
      "Epoch: 22903 \tTraining Loss: 1.525507 \tValidation Loss: 2.439804\n",
      "Epoch: 22904 \tTraining Loss: 1.470395 \tValidation Loss: 2.440163\n",
      "Epoch: 22905 \tTraining Loss: 1.451450 \tValidation Loss: 2.441344\n",
      "Epoch: 22906 \tTraining Loss: 1.466130 \tValidation Loss: 2.440085\n",
      "Epoch: 22907 \tTraining Loss: 1.483548 \tValidation Loss: 2.440046\n",
      "Epoch: 22908 \tTraining Loss: 1.473799 \tValidation Loss: 2.440685\n",
      "Epoch: 22909 \tTraining Loss: 1.476833 \tValidation Loss: 2.440595\n",
      "Epoch: 22910 \tTraining Loss: 1.443513 \tValidation Loss: 2.440410\n",
      "Epoch: 22911 \tTraining Loss: 1.479030 \tValidation Loss: 2.441220\n",
      "Epoch: 22912 \tTraining Loss: 1.482300 \tValidation Loss: 2.441466\n",
      "Epoch: 22913 \tTraining Loss: 1.479346 \tValidation Loss: 2.440969\n",
      "Epoch: 22914 \tTraining Loss: 1.466941 \tValidation Loss: 2.441357\n",
      "Epoch: 22915 \tTraining Loss: 1.512390 \tValidation Loss: 2.440225\n",
      "Epoch: 22916 \tTraining Loss: 1.439856 \tValidation Loss: 2.441248\n",
      "Epoch: 22917 \tTraining Loss: 1.494015 \tValidation Loss: 2.440440\n",
      "Epoch: 22918 \tTraining Loss: 1.449941 \tValidation Loss: 2.440616\n",
      "Epoch: 22919 \tTraining Loss: 1.508152 \tValidation Loss: 2.441312\n",
      "Epoch: 22920 \tTraining Loss: 1.434553 \tValidation Loss: 2.441277\n",
      "Epoch: 22921 \tTraining Loss: 1.473364 \tValidation Loss: 2.441152\n",
      "Epoch: 22922 \tTraining Loss: 1.454135 \tValidation Loss: 2.441408\n",
      "Epoch: 22923 \tTraining Loss: 1.465961 \tValidation Loss: 2.441818\n",
      "Epoch: 22924 \tTraining Loss: 1.457705 \tValidation Loss: 2.442344\n",
      "Epoch: 22925 \tTraining Loss: 1.484733 \tValidation Loss: 2.441490\n",
      "Epoch: 22926 \tTraining Loss: 1.474346 \tValidation Loss: 2.441633\n",
      "Epoch: 22927 \tTraining Loss: 1.489172 \tValidation Loss: 2.441239\n",
      "Epoch: 22928 \tTraining Loss: 1.458387 \tValidation Loss: 2.441494\n",
      "Epoch: 22929 \tTraining Loss: 1.472189 \tValidation Loss: 2.442118\n",
      "Epoch: 22930 \tTraining Loss: 1.447911 \tValidation Loss: 2.442496\n",
      "Epoch: 22931 \tTraining Loss: 1.478132 \tValidation Loss: 2.443217\n",
      "Epoch: 22932 \tTraining Loss: 1.473753 \tValidation Loss: 2.442188\n",
      "Epoch: 22933 \tTraining Loss: 1.426212 \tValidation Loss: 2.443081\n",
      "Epoch: 22934 \tTraining Loss: 1.490720 \tValidation Loss: 2.441916\n",
      "Epoch: 22935 \tTraining Loss: 1.495520 \tValidation Loss: 2.440647\n",
      "Epoch: 22936 \tTraining Loss: 1.474827 \tValidation Loss: 2.440509\n",
      "Epoch: 22937 \tTraining Loss: 1.464019 \tValidation Loss: 2.441396\n",
      "Epoch: 22938 \tTraining Loss: 1.444388 \tValidation Loss: 2.442072\n",
      "Epoch: 22939 \tTraining Loss: 1.459632 \tValidation Loss: 2.440796\n",
      "Epoch: 22940 \tTraining Loss: 1.462785 \tValidation Loss: 2.441261\n",
      "Epoch: 22941 \tTraining Loss: 1.445565 \tValidation Loss: 2.441730\n",
      "Epoch: 22942 \tTraining Loss: 1.497951 \tValidation Loss: 2.441460\n",
      "Epoch: 22943 \tTraining Loss: 1.508718 \tValidation Loss: 2.441235\n",
      "Epoch: 22944 \tTraining Loss: 1.462305 \tValidation Loss: 2.441833\n",
      "Epoch: 22945 \tTraining Loss: 1.475722 \tValidation Loss: 2.441813\n",
      "Epoch: 22946 \tTraining Loss: 1.489330 \tValidation Loss: 2.441564\n",
      "Epoch: 22947 \tTraining Loss: 1.494995 \tValidation Loss: 2.442127\n",
      "Epoch: 22948 \tTraining Loss: 1.509300 \tValidation Loss: 2.441253\n",
      "Epoch: 22949 \tTraining Loss: 1.459705 \tValidation Loss: 2.441835\n",
      "Epoch: 22950 \tTraining Loss: 1.468230 \tValidation Loss: 2.442016\n",
      "Epoch: 22951 \tTraining Loss: 1.462925 \tValidation Loss: 2.441310\n",
      "Epoch: 22952 \tTraining Loss: 1.475344 \tValidation Loss: 2.441293\n",
      "Epoch: 22953 \tTraining Loss: 1.477162 \tValidation Loss: 2.441655\n",
      "Epoch: 22954 \tTraining Loss: 1.483847 \tValidation Loss: 2.441297\n",
      "Epoch: 22955 \tTraining Loss: 1.472553 \tValidation Loss: 2.441542\n",
      "Epoch: 22956 \tTraining Loss: 1.506834 \tValidation Loss: 2.441398\n",
      "Epoch: 22957 \tTraining Loss: 1.457390 \tValidation Loss: 2.441805\n",
      "Epoch: 22958 \tTraining Loss: 1.514076 \tValidation Loss: 2.441696\n",
      "Epoch: 22959 \tTraining Loss: 1.482446 \tValidation Loss: 2.441437\n",
      "Epoch: 22960 \tTraining Loss: 1.487811 \tValidation Loss: 2.441704\n",
      "Epoch: 22961 \tTraining Loss: 1.453422 \tValidation Loss: 2.441945\n",
      "Epoch: 22962 \tTraining Loss: 1.534348 \tValidation Loss: 2.440736\n",
      "Epoch: 22963 \tTraining Loss: 1.459579 \tValidation Loss: 2.441972\n",
      "Epoch: 22964 \tTraining Loss: 1.440121 \tValidation Loss: 2.442290\n",
      "Epoch: 22965 \tTraining Loss: 1.479946 \tValidation Loss: 2.442260\n",
      "Epoch: 22966 \tTraining Loss: 1.519375 \tValidation Loss: 2.441478\n",
      "Epoch: 22967 \tTraining Loss: 1.514200 \tValidation Loss: 2.441725\n",
      "Epoch: 22968 \tTraining Loss: 1.506387 \tValidation Loss: 2.442422\n",
      "Epoch: 22969 \tTraining Loss: 1.436708 \tValidation Loss: 2.441841\n",
      "Epoch: 22970 \tTraining Loss: 1.485193 \tValidation Loss: 2.441808\n",
      "Epoch: 22971 \tTraining Loss: 1.457204 \tValidation Loss: 2.442122\n",
      "Epoch: 22972 \tTraining Loss: 1.460013 \tValidation Loss: 2.442870\n",
      "Epoch: 22973 \tTraining Loss: 1.456613 \tValidation Loss: 2.442397\n",
      "Epoch: 22974 \tTraining Loss: 1.442541 \tValidation Loss: 2.442380\n",
      "Epoch: 22975 \tTraining Loss: 1.468171 \tValidation Loss: 2.442626\n",
      "Epoch: 22976 \tTraining Loss: 1.496886 \tValidation Loss: 2.442686\n",
      "Epoch: 22977 \tTraining Loss: 1.480787 \tValidation Loss: 2.443630\n",
      "Epoch: 22978 \tTraining Loss: 1.445886 \tValidation Loss: 2.443137\n",
      "Epoch: 22979 \tTraining Loss: 1.472292 \tValidation Loss: 2.443156\n",
      "Epoch: 22980 \tTraining Loss: 1.474662 \tValidation Loss: 2.443357\n",
      "Epoch: 22981 \tTraining Loss: 1.483976 \tValidation Loss: 2.442487\n",
      "Epoch: 22982 \tTraining Loss: 1.458380 \tValidation Loss: 2.443380\n",
      "Epoch: 22983 \tTraining Loss: 1.467265 \tValidation Loss: 2.442481\n",
      "Epoch: 22984 \tTraining Loss: 1.449680 \tValidation Loss: 2.443486\n",
      "Epoch: 22985 \tTraining Loss: 1.453814 \tValidation Loss: 2.443254\n",
      "Epoch: 22986 \tTraining Loss: 1.442922 \tValidation Loss: 2.443460\n",
      "Epoch: 22987 \tTraining Loss: 1.457811 \tValidation Loss: 2.443497\n",
      "Epoch: 22988 \tTraining Loss: 1.476216 \tValidation Loss: 2.443680\n",
      "Epoch: 22989 \tTraining Loss: 1.520538 \tValidation Loss: 2.442812\n",
      "Epoch: 22990 \tTraining Loss: 1.495795 \tValidation Loss: 2.442703\n",
      "Epoch: 22991 \tTraining Loss: 1.456350 \tValidation Loss: 2.442518\n",
      "Epoch: 22992 \tTraining Loss: 1.502280 \tValidation Loss: 2.443264\n",
      "Epoch: 22993 \tTraining Loss: 1.477636 \tValidation Loss: 2.442799\n",
      "Epoch: 22994 \tTraining Loss: 1.499388 \tValidation Loss: 2.441368\n",
      "Epoch: 22995 \tTraining Loss: 1.462919 \tValidation Loss: 2.442218\n",
      "Epoch: 22996 \tTraining Loss: 1.442690 \tValidation Loss: 2.442252\n",
      "Epoch: 22997 \tTraining Loss: 1.474197 \tValidation Loss: 2.441954\n",
      "Epoch: 22998 \tTraining Loss: 1.445109 \tValidation Loss: 2.442309\n",
      "Epoch: 22999 \tTraining Loss: 1.461960 \tValidation Loss: 2.442416\n",
      "Epoch: 23000 \tTraining Loss: 1.453238 \tValidation Loss: 2.443502\n",
      "Epoch: 23001 \tTraining Loss: 1.484593 \tValidation Loss: 2.443289\n",
      "Epoch: 23002 \tTraining Loss: 1.476656 \tValidation Loss: 2.443095\n",
      "Epoch: 23003 \tTraining Loss: 1.489397 \tValidation Loss: 2.443779\n",
      "Epoch: 23004 \tTraining Loss: 1.439793 \tValidation Loss: 2.443715\n",
      "Epoch: 23005 \tTraining Loss: 1.479123 \tValidation Loss: 2.443220\n",
      "Epoch: 23006 \tTraining Loss: 1.439449 \tValidation Loss: 2.443281\n",
      "Epoch: 23007 \tTraining Loss: 1.495728 \tValidation Loss: 2.442720\n",
      "Epoch: 23008 \tTraining Loss: 1.426952 \tValidation Loss: 2.443074\n",
      "Epoch: 23009 \tTraining Loss: 1.478708 \tValidation Loss: 2.443957\n",
      "Epoch: 23010 \tTraining Loss: 1.469812 \tValidation Loss: 2.444027\n",
      "Epoch: 23011 \tTraining Loss: 1.484619 \tValidation Loss: 2.443760\n",
      "Epoch: 23012 \tTraining Loss: 1.473663 \tValidation Loss: 2.443003\n",
      "Epoch: 23013 \tTraining Loss: 1.508041 \tValidation Loss: 2.443187\n",
      "Epoch: 23014 \tTraining Loss: 1.475757 \tValidation Loss: 2.443492\n",
      "Epoch: 23015 \tTraining Loss: 1.473616 \tValidation Loss: 2.443211\n",
      "Epoch: 23016 \tTraining Loss: 1.444239 \tValidation Loss: 2.443188\n",
      "Epoch: 23017 \tTraining Loss: 1.501263 \tValidation Loss: 2.442469\n",
      "Epoch: 23018 \tTraining Loss: 1.506894 \tValidation Loss: 2.442134\n",
      "Epoch: 23019 \tTraining Loss: 1.433375 \tValidation Loss: 2.443065\n",
      "Epoch: 23020 \tTraining Loss: 1.485857 \tValidation Loss: 2.442415\n",
      "Epoch: 23021 \tTraining Loss: 1.486155 \tValidation Loss: 2.443644\n",
      "Epoch: 23022 \tTraining Loss: 1.491525 \tValidation Loss: 2.443318\n",
      "Epoch: 23023 \tTraining Loss: 1.460070 \tValidation Loss: 2.443218\n",
      "Epoch: 23024 \tTraining Loss: 1.441244 \tValidation Loss: 2.443812\n",
      "Epoch: 23025 \tTraining Loss: 1.521313 \tValidation Loss: 2.442460\n",
      "Epoch: 23026 \tTraining Loss: 1.463773 \tValidation Loss: 2.443083\n",
      "Epoch: 23027 \tTraining Loss: 1.513106 \tValidation Loss: 2.443318\n",
      "Epoch: 23028 \tTraining Loss: 1.484661 \tValidation Loss: 2.443362\n",
      "Epoch: 23029 \tTraining Loss: 1.486088 \tValidation Loss: 2.443608\n",
      "Epoch: 23030 \tTraining Loss: 1.488697 \tValidation Loss: 2.444008\n",
      "Epoch: 23031 \tTraining Loss: 1.431416 \tValidation Loss: 2.443984\n",
      "Epoch: 23032 \tTraining Loss: 1.473237 \tValidation Loss: 2.443204\n",
      "Epoch: 23033 \tTraining Loss: 1.507010 \tValidation Loss: 2.442755\n",
      "Epoch: 23034 \tTraining Loss: 1.520122 \tValidation Loss: 2.442885\n",
      "Epoch: 23035 \tTraining Loss: 1.464614 \tValidation Loss: 2.443033\n",
      "Epoch: 23036 \tTraining Loss: 1.492456 \tValidation Loss: 2.442687\n",
      "Epoch: 23037 \tTraining Loss: 1.497569 \tValidation Loss: 2.442962\n",
      "Epoch: 23038 \tTraining Loss: 1.417425 \tValidation Loss: 2.444151\n",
      "Epoch: 23039 \tTraining Loss: 1.475056 \tValidation Loss: 2.443422\n",
      "Epoch: 23040 \tTraining Loss: 1.452365 \tValidation Loss: 2.443272\n",
      "Epoch: 23041 \tTraining Loss: 1.482952 \tValidation Loss: 2.443148\n",
      "Epoch: 23042 \tTraining Loss: 1.488511 \tValidation Loss: 2.443575\n",
      "Epoch: 23043 \tTraining Loss: 1.477139 \tValidation Loss: 2.442811\n",
      "Epoch: 23044 \tTraining Loss: 1.481095 \tValidation Loss: 2.443451\n",
      "Epoch: 23045 \tTraining Loss: 1.464536 \tValidation Loss: 2.442570\n",
      "Epoch: 23046 \tTraining Loss: 1.492070 \tValidation Loss: 2.443740\n",
      "Epoch: 23047 \tTraining Loss: 1.501909 \tValidation Loss: 2.443865\n",
      "Epoch: 23048 \tTraining Loss: 1.469743 \tValidation Loss: 2.443751\n",
      "Epoch: 23049 \tTraining Loss: 1.446993 \tValidation Loss: 2.443084\n",
      "Epoch: 23050 \tTraining Loss: 1.446804 \tValidation Loss: 2.443634\n",
      "Epoch: 23051 \tTraining Loss: 1.492208 \tValidation Loss: 2.443690\n",
      "Epoch: 23052 \tTraining Loss: 1.506382 \tValidation Loss: 2.443715\n",
      "Epoch: 23053 \tTraining Loss: 1.461237 \tValidation Loss: 2.444181\n",
      "Epoch: 23054 \tTraining Loss: 1.432017 \tValidation Loss: 2.443826\n",
      "Epoch: 23055 \tTraining Loss: 1.492912 \tValidation Loss: 2.443748\n",
      "Epoch: 23056 \tTraining Loss: 1.485341 \tValidation Loss: 2.443298\n",
      "Epoch: 23057 \tTraining Loss: 1.481304 \tValidation Loss: 2.444466\n",
      "Epoch: 23058 \tTraining Loss: 1.446761 \tValidation Loss: 2.444285\n",
      "Epoch: 23059 \tTraining Loss: 1.430277 \tValidation Loss: 2.445385\n",
      "Epoch: 23060 \tTraining Loss: 1.460802 \tValidation Loss: 2.445133\n",
      "Epoch: 23061 \tTraining Loss: 1.498328 \tValidation Loss: 2.444639\n",
      "Epoch: 23062 \tTraining Loss: 1.440043 \tValidation Loss: 2.444633\n",
      "Epoch: 23063 \tTraining Loss: 1.509311 \tValidation Loss: 2.443810\n",
      "Epoch: 23064 \tTraining Loss: 1.447504 \tValidation Loss: 2.444843\n",
      "Epoch: 23065 \tTraining Loss: 1.469659 \tValidation Loss: 2.444426\n",
      "Epoch: 23066 \tTraining Loss: 1.444204 \tValidation Loss: 2.444514\n",
      "Epoch: 23067 \tTraining Loss: 1.455374 \tValidation Loss: 2.443921\n",
      "Epoch: 23068 \tTraining Loss: 1.515666 \tValidation Loss: 2.443034\n",
      "Epoch: 23069 \tTraining Loss: 1.463205 \tValidation Loss: 2.443978\n",
      "Epoch: 23070 \tTraining Loss: 1.444535 \tValidation Loss: 2.444986\n",
      "Epoch: 23071 \tTraining Loss: 1.468378 \tValidation Loss: 2.443250\n",
      "Epoch: 23072 \tTraining Loss: 1.504126 \tValidation Loss: 2.443855\n",
      "Epoch: 23073 \tTraining Loss: 1.451210 \tValidation Loss: 2.444221\n",
      "Epoch: 23074 \tTraining Loss: 1.513819 \tValidation Loss: 2.444092\n",
      "Epoch: 23075 \tTraining Loss: 1.475587 \tValidation Loss: 2.444339\n",
      "Epoch: 23076 \tTraining Loss: 1.449712 \tValidation Loss: 2.444121\n",
      "Epoch: 23077 \tTraining Loss: 1.502156 \tValidation Loss: 2.443680\n",
      "Epoch: 23078 \tTraining Loss: 1.492234 \tValidation Loss: 2.444525\n",
      "Epoch: 23079 \tTraining Loss: 1.468613 \tValidation Loss: 2.445220\n",
      "Epoch: 23080 \tTraining Loss: 1.472529 \tValidation Loss: 2.444157\n",
      "Epoch: 23081 \tTraining Loss: 1.461994 \tValidation Loss: 2.444570\n",
      "Epoch: 23082 \tTraining Loss: 1.441100 \tValidation Loss: 2.444635\n",
      "Epoch: 23083 \tTraining Loss: 1.442992 \tValidation Loss: 2.444029\n",
      "Epoch: 23084 \tTraining Loss: 1.448182 \tValidation Loss: 2.444691\n",
      "Epoch: 23085 \tTraining Loss: 1.515427 \tValidation Loss: 2.443694\n",
      "Epoch: 23086 \tTraining Loss: 1.440304 \tValidation Loss: 2.443694\n",
      "Epoch: 23087 \tTraining Loss: 1.469147 \tValidation Loss: 2.443821\n",
      "Epoch: 23088 \tTraining Loss: 1.480677 \tValidation Loss: 2.443766\n",
      "Epoch: 23089 \tTraining Loss: 1.485583 \tValidation Loss: 2.444150\n",
      "Epoch: 23090 \tTraining Loss: 1.468687 \tValidation Loss: 2.444513\n",
      "Epoch: 23091 \tTraining Loss: 1.475834 \tValidation Loss: 2.444000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23092 \tTraining Loss: 1.472957 \tValidation Loss: 2.443973\n",
      "Epoch: 23093 \tTraining Loss: 1.488914 \tValidation Loss: 2.444093\n",
      "Epoch: 23094 \tTraining Loss: 1.494818 \tValidation Loss: 2.444085\n",
      "Epoch: 23095 \tTraining Loss: 1.422024 \tValidation Loss: 2.444332\n",
      "Epoch: 23096 \tTraining Loss: 1.456390 \tValidation Loss: 2.444060\n",
      "Epoch: 23097 \tTraining Loss: 1.542081 \tValidation Loss: 2.443977\n",
      "Epoch: 23098 \tTraining Loss: 1.471111 \tValidation Loss: 2.444534\n",
      "Epoch: 23099 \tTraining Loss: 1.439918 \tValidation Loss: 2.445481\n",
      "Epoch: 23100 \tTraining Loss: 1.442349 \tValidation Loss: 2.445773\n",
      "Epoch: 23101 \tTraining Loss: 1.471109 \tValidation Loss: 2.444897\n",
      "Epoch: 23102 \tTraining Loss: 1.466754 \tValidation Loss: 2.443959\n",
      "Epoch: 23103 \tTraining Loss: 1.515515 \tValidation Loss: 2.444017\n",
      "Epoch: 23104 \tTraining Loss: 1.510620 \tValidation Loss: 2.443941\n",
      "Epoch: 23105 \tTraining Loss: 1.449757 \tValidation Loss: 2.443377\n",
      "Epoch: 23106 \tTraining Loss: 1.451066 \tValidation Loss: 2.444742\n",
      "Epoch: 23107 \tTraining Loss: 1.452198 \tValidation Loss: 2.445539\n",
      "Epoch: 23108 \tTraining Loss: 1.521403 \tValidation Loss: 2.444920\n",
      "Epoch: 23109 \tTraining Loss: 1.471065 \tValidation Loss: 2.444485\n",
      "Epoch: 23110 \tTraining Loss: 1.506710 \tValidation Loss: 2.443702\n",
      "Epoch: 23111 \tTraining Loss: 1.486092 \tValidation Loss: 2.443517\n",
      "Epoch: 23112 \tTraining Loss: 1.489155 \tValidation Loss: 2.443586\n",
      "Epoch: 23113 \tTraining Loss: 1.481083 \tValidation Loss: 2.443895\n",
      "Epoch: 23114 \tTraining Loss: 1.462119 \tValidation Loss: 2.444824\n",
      "Epoch: 23115 \tTraining Loss: 1.450026 \tValidation Loss: 2.445250\n",
      "Epoch: 23116 \tTraining Loss: 1.434542 \tValidation Loss: 2.444926\n",
      "Epoch: 23117 \tTraining Loss: 1.525162 \tValidation Loss: 2.443774\n",
      "Epoch: 23118 \tTraining Loss: 1.452942 \tValidation Loss: 2.444770\n",
      "Epoch: 23119 \tTraining Loss: 1.437378 \tValidation Loss: 2.444869\n",
      "Epoch: 23120 \tTraining Loss: 1.422025 \tValidation Loss: 2.445336\n",
      "Epoch: 23121 \tTraining Loss: 1.450487 \tValidation Loss: 2.444555\n",
      "Epoch: 23122 \tTraining Loss: 1.504737 \tValidation Loss: 2.443888\n",
      "Epoch: 23123 \tTraining Loss: 1.522826 \tValidation Loss: 2.443870\n",
      "Epoch: 23124 \tTraining Loss: 1.477396 \tValidation Loss: 2.444599\n",
      "Epoch: 23125 \tTraining Loss: 1.492805 \tValidation Loss: 2.444198\n",
      "Epoch: 23126 \tTraining Loss: 1.478439 \tValidation Loss: 2.444628\n",
      "Epoch: 23127 \tTraining Loss: 1.462873 \tValidation Loss: 2.445654\n",
      "Epoch: 23128 \tTraining Loss: 1.446750 \tValidation Loss: 2.445173\n",
      "Epoch: 23129 \tTraining Loss: 1.481016 \tValidation Loss: 2.445811\n",
      "Epoch: 23130 \tTraining Loss: 1.515000 \tValidation Loss: 2.445338\n",
      "Epoch: 23131 \tTraining Loss: 1.442297 \tValidation Loss: 2.445268\n",
      "Epoch: 23132 \tTraining Loss: 1.492535 \tValidation Loss: 2.445621\n",
      "Epoch: 23133 \tTraining Loss: 1.458770 \tValidation Loss: 2.446061\n",
      "Epoch: 23134 \tTraining Loss: 1.443132 \tValidation Loss: 2.445044\n",
      "Epoch: 23135 \tTraining Loss: 1.479133 \tValidation Loss: 2.445541\n",
      "Epoch: 23136 \tTraining Loss: 1.496349 \tValidation Loss: 2.444554\n",
      "Epoch: 23137 \tTraining Loss: 1.477023 \tValidation Loss: 2.445397\n",
      "Epoch: 23138 \tTraining Loss: 1.458672 \tValidation Loss: 2.446082\n",
      "Epoch: 23139 \tTraining Loss: 1.425130 \tValidation Loss: 2.444712\n",
      "Epoch: 23140 \tTraining Loss: 1.418591 \tValidation Loss: 2.445597\n",
      "Epoch: 23141 \tTraining Loss: 1.473380 \tValidation Loss: 2.445443\n",
      "Epoch: 23142 \tTraining Loss: 1.471167 \tValidation Loss: 2.445207\n",
      "Epoch: 23143 \tTraining Loss: 1.450301 \tValidation Loss: 2.445645\n",
      "Epoch: 23144 \tTraining Loss: 1.472705 \tValidation Loss: 2.444692\n",
      "Epoch: 23145 \tTraining Loss: 1.437073 \tValidation Loss: 2.445646\n",
      "Epoch: 23146 \tTraining Loss: 1.500835 \tValidation Loss: 2.445095\n",
      "Epoch: 23147 \tTraining Loss: 1.458681 \tValidation Loss: 2.444718\n",
      "Epoch: 23148 \tTraining Loss: 1.438964 \tValidation Loss: 2.445276\n",
      "Epoch: 23149 \tTraining Loss: 1.454702 \tValidation Loss: 2.444875\n",
      "Epoch: 23150 \tTraining Loss: 1.469663 \tValidation Loss: 2.445317\n",
      "Epoch: 23151 \tTraining Loss: 1.432754 \tValidation Loss: 2.445128\n",
      "Epoch: 23152 \tTraining Loss: 1.518102 \tValidation Loss: 2.445062\n",
      "Epoch: 23153 \tTraining Loss: 1.469087 \tValidation Loss: 2.444873\n",
      "Epoch: 23154 \tTraining Loss: 1.500934 \tValidation Loss: 2.444836\n",
      "Epoch: 23155 \tTraining Loss: 1.466209 \tValidation Loss: 2.445684\n",
      "Epoch: 23156 \tTraining Loss: 1.479563 \tValidation Loss: 2.444582\n",
      "Epoch: 23157 \tTraining Loss: 1.468047 \tValidation Loss: 2.445760\n",
      "Epoch: 23158 \tTraining Loss: 1.460867 \tValidation Loss: 2.445232\n",
      "Epoch: 23159 \tTraining Loss: 1.439834 \tValidation Loss: 2.445983\n",
      "Epoch: 23160 \tTraining Loss: 1.469082 \tValidation Loss: 2.445069\n",
      "Epoch: 23161 \tTraining Loss: 1.463510 \tValidation Loss: 2.445578\n",
      "Epoch: 23162 \tTraining Loss: 1.463729 \tValidation Loss: 2.445863\n",
      "Epoch: 23163 \tTraining Loss: 1.520362 \tValidation Loss: 2.445760\n",
      "Epoch: 23164 \tTraining Loss: 1.489140 \tValidation Loss: 2.446232\n",
      "Epoch: 23165 \tTraining Loss: 1.475850 \tValidation Loss: 2.446548\n",
      "Epoch: 23166 \tTraining Loss: 1.508363 \tValidation Loss: 2.445744\n",
      "Epoch: 23167 \tTraining Loss: 1.436155 \tValidation Loss: 2.445817\n",
      "Epoch: 23168 \tTraining Loss: 1.490234 \tValidation Loss: 2.445918\n",
      "Epoch: 23169 \tTraining Loss: 1.477588 \tValidation Loss: 2.445548\n",
      "Epoch: 23170 \tTraining Loss: 1.491762 \tValidation Loss: 2.445555\n",
      "Epoch: 23171 \tTraining Loss: 1.466915 \tValidation Loss: 2.446268\n",
      "Epoch: 23172 \tTraining Loss: 1.464676 \tValidation Loss: 2.446160\n",
      "Epoch: 23173 \tTraining Loss: 1.466580 \tValidation Loss: 2.446060\n",
      "Epoch: 23174 \tTraining Loss: 1.468194 \tValidation Loss: 2.444965\n",
      "Epoch: 23175 \tTraining Loss: 1.506739 \tValidation Loss: 2.445612\n",
      "Epoch: 23176 \tTraining Loss: 1.498293 \tValidation Loss: 2.445015\n",
      "Epoch: 23177 \tTraining Loss: 1.441889 \tValidation Loss: 2.445928\n",
      "Epoch: 23178 \tTraining Loss: 1.461346 \tValidation Loss: 2.445569\n",
      "Epoch: 23179 \tTraining Loss: 1.448391 \tValidation Loss: 2.445495\n",
      "Epoch: 23180 \tTraining Loss: 1.462677 \tValidation Loss: 2.445525\n",
      "Epoch: 23181 \tTraining Loss: 1.500214 \tValidation Loss: 2.445253\n",
      "Epoch: 23182 \tTraining Loss: 1.475674 \tValidation Loss: 2.445713\n",
      "Epoch: 23183 \tTraining Loss: 1.491173 \tValidation Loss: 2.446001\n",
      "Epoch: 23184 \tTraining Loss: 1.477360 \tValidation Loss: 2.446460\n",
      "Epoch: 23185 \tTraining Loss: 1.514891 \tValidation Loss: 2.445732\n",
      "Epoch: 23186 \tTraining Loss: 1.508853 \tValidation Loss: 2.446007\n",
      "Epoch: 23187 \tTraining Loss: 1.483434 \tValidation Loss: 2.445609\n",
      "Epoch: 23188 \tTraining Loss: 1.473731 \tValidation Loss: 2.446231\n",
      "Epoch: 23189 \tTraining Loss: 1.448202 \tValidation Loss: 2.445617\n",
      "Epoch: 23190 \tTraining Loss: 1.449679 \tValidation Loss: 2.446630\n",
      "Epoch: 23191 \tTraining Loss: 1.456285 \tValidation Loss: 2.446273\n",
      "Epoch: 23192 \tTraining Loss: 1.483498 \tValidation Loss: 2.447570\n",
      "Epoch: 23193 \tTraining Loss: 1.472068 \tValidation Loss: 2.446282\n",
      "Epoch: 23194 \tTraining Loss: 1.463350 \tValidation Loss: 2.446635\n",
      "Epoch: 23195 \tTraining Loss: 1.505823 \tValidation Loss: 2.445403\n",
      "Epoch: 23196 \tTraining Loss: 1.444579 \tValidation Loss: 2.445529\n",
      "Epoch: 23197 \tTraining Loss: 1.505808 \tValidation Loss: 2.445634\n",
      "Epoch: 23198 \tTraining Loss: 1.476795 \tValidation Loss: 2.446070\n",
      "Epoch: 23199 \tTraining Loss: 1.518759 \tValidation Loss: 2.446199\n",
      "Epoch: 23200 \tTraining Loss: 1.483937 \tValidation Loss: 2.446059\n",
      "Epoch: 23201 \tTraining Loss: 1.466751 \tValidation Loss: 2.446955\n",
      "Epoch: 23202 \tTraining Loss: 1.419348 \tValidation Loss: 2.447141\n",
      "Epoch: 23203 \tTraining Loss: 1.492649 \tValidation Loss: 2.445583\n",
      "Epoch: 23204 \tTraining Loss: 1.442640 \tValidation Loss: 2.445304\n",
      "Epoch: 23205 \tTraining Loss: 1.445655 \tValidation Loss: 2.446231\n",
      "Epoch: 23206 \tTraining Loss: 1.459015 \tValidation Loss: 2.446383\n",
      "Epoch: 23207 \tTraining Loss: 1.466741 \tValidation Loss: 2.445320\n",
      "Epoch: 23208 \tTraining Loss: 1.477311 \tValidation Loss: 2.446411\n",
      "Epoch: 23209 \tTraining Loss: 1.495653 \tValidation Loss: 2.446066\n",
      "Epoch: 23210 \tTraining Loss: 1.496585 \tValidation Loss: 2.445391\n",
      "Epoch: 23211 \tTraining Loss: 1.476737 \tValidation Loss: 2.445762\n",
      "Epoch: 23212 \tTraining Loss: 1.473282 \tValidation Loss: 2.446291\n",
      "Epoch: 23213 \tTraining Loss: 1.483324 \tValidation Loss: 2.445727\n",
      "Epoch: 23214 \tTraining Loss: 1.429372 \tValidation Loss: 2.446304\n",
      "Epoch: 23215 \tTraining Loss: 1.467904 \tValidation Loss: 2.446964\n",
      "Epoch: 23216 \tTraining Loss: 1.479765 \tValidation Loss: 2.447139\n",
      "Epoch: 23217 \tTraining Loss: 1.456543 \tValidation Loss: 2.446270\n",
      "Epoch: 23218 \tTraining Loss: 1.480914 \tValidation Loss: 2.446852\n",
      "Epoch: 23219 \tTraining Loss: 1.449281 \tValidation Loss: 2.445967\n",
      "Epoch: 23220 \tTraining Loss: 1.489877 \tValidation Loss: 2.445695\n",
      "Epoch: 23221 \tTraining Loss: 1.450008 \tValidation Loss: 2.446998\n",
      "Epoch: 23222 \tTraining Loss: 1.434754 \tValidation Loss: 2.445873\n",
      "Epoch: 23223 \tTraining Loss: 1.516308 \tValidation Loss: 2.446158\n",
      "Epoch: 23224 \tTraining Loss: 1.485840 \tValidation Loss: 2.446046\n",
      "Epoch: 23225 \tTraining Loss: 1.521457 \tValidation Loss: 2.446434\n",
      "Epoch: 23226 \tTraining Loss: 1.471933 \tValidation Loss: 2.446616\n",
      "Epoch: 23227 \tTraining Loss: 1.400918 \tValidation Loss: 2.447111\n",
      "Epoch: 23228 \tTraining Loss: 1.451883 \tValidation Loss: 2.446838\n",
      "Epoch: 23229 \tTraining Loss: 1.451753 \tValidation Loss: 2.446489\n",
      "Epoch: 23230 \tTraining Loss: 1.427649 \tValidation Loss: 2.446730\n",
      "Epoch: 23231 \tTraining Loss: 1.426677 \tValidation Loss: 2.446188\n",
      "Epoch: 23232 \tTraining Loss: 1.484230 \tValidation Loss: 2.445811\n",
      "Epoch: 23233 \tTraining Loss: 1.466518 \tValidation Loss: 2.446208\n",
      "Epoch: 23234 \tTraining Loss: 1.462342 \tValidation Loss: 2.446104\n",
      "Epoch: 23235 \tTraining Loss: 1.496070 \tValidation Loss: 2.446600\n",
      "Epoch: 23236 \tTraining Loss: 1.470851 \tValidation Loss: 2.447221\n",
      "Epoch: 23237 \tTraining Loss: 1.536588 \tValidation Loss: 2.446124\n",
      "Epoch: 23238 \tTraining Loss: 1.461735 \tValidation Loss: 2.446648\n",
      "Epoch: 23239 \tTraining Loss: 1.485335 \tValidation Loss: 2.445690\n",
      "Epoch: 23240 \tTraining Loss: 1.440079 \tValidation Loss: 2.446331\n",
      "Epoch: 23241 \tTraining Loss: 1.513236 \tValidation Loss: 2.445542\n",
      "Epoch: 23242 \tTraining Loss: 1.462250 \tValidation Loss: 2.446208\n",
      "Epoch: 23243 \tTraining Loss: 1.466514 \tValidation Loss: 2.446584\n",
      "Epoch: 23244 \tTraining Loss: 1.480687 \tValidation Loss: 2.446253\n",
      "Epoch: 23245 \tTraining Loss: 1.412167 \tValidation Loss: 2.447323\n",
      "Epoch: 23246 \tTraining Loss: 1.427428 \tValidation Loss: 2.446905\n",
      "Epoch: 23247 \tTraining Loss: 1.461607 \tValidation Loss: 2.446502\n",
      "Epoch: 23248 \tTraining Loss: 1.460687 \tValidation Loss: 2.447551\n",
      "Epoch: 23249 \tTraining Loss: 1.494969 \tValidation Loss: 2.446726\n",
      "Epoch: 23250 \tTraining Loss: 1.483740 \tValidation Loss: 2.447365\n",
      "Epoch: 23251 \tTraining Loss: 1.473114 \tValidation Loss: 2.447048\n",
      "Epoch: 23252 \tTraining Loss: 1.460943 \tValidation Loss: 2.447171\n",
      "Epoch: 23253 \tTraining Loss: 1.426804 \tValidation Loss: 2.447110\n",
      "Epoch: 23254 \tTraining Loss: 1.495441 \tValidation Loss: 2.446026\n",
      "Epoch: 23255 \tTraining Loss: 1.495062 \tValidation Loss: 2.446902\n",
      "Epoch: 23256 \tTraining Loss: 1.494038 \tValidation Loss: 2.446255\n",
      "Epoch: 23257 \tTraining Loss: 1.477643 \tValidation Loss: 2.446887\n",
      "Epoch: 23258 \tTraining Loss: 1.488842 \tValidation Loss: 2.445243\n",
      "Epoch: 23259 \tTraining Loss: 1.480407 \tValidation Loss: 2.445894\n",
      "Epoch: 23260 \tTraining Loss: 1.453304 \tValidation Loss: 2.446232\n",
      "Epoch: 23261 \tTraining Loss: 1.495683 \tValidation Loss: 2.446839\n",
      "Epoch: 23262 \tTraining Loss: 1.475402 \tValidation Loss: 2.447524\n",
      "Epoch: 23263 \tTraining Loss: 1.501382 \tValidation Loss: 2.447026\n",
      "Epoch: 23264 \tTraining Loss: 1.445630 \tValidation Loss: 2.447725\n",
      "Epoch: 23265 \tTraining Loss: 1.448023 \tValidation Loss: 2.448044\n",
      "Epoch: 23266 \tTraining Loss: 1.476069 \tValidation Loss: 2.447440\n",
      "Epoch: 23267 \tTraining Loss: 1.415128 \tValidation Loss: 2.448894\n",
      "Epoch: 23268 \tTraining Loss: 1.481400 \tValidation Loss: 2.448168\n",
      "Epoch: 23269 \tTraining Loss: 1.435238 \tValidation Loss: 2.448443\n",
      "Epoch: 23270 \tTraining Loss: 1.477857 \tValidation Loss: 2.448360\n",
      "Epoch: 23271 \tTraining Loss: 1.464161 \tValidation Loss: 2.446623\n",
      "Epoch: 23272 \tTraining Loss: 1.474174 \tValidation Loss: 2.446604\n",
      "Epoch: 23273 \tTraining Loss: 1.468087 \tValidation Loss: 2.446969\n",
      "Epoch: 23274 \tTraining Loss: 1.487287 \tValidation Loss: 2.446995\n",
      "Epoch: 23275 \tTraining Loss: 1.510025 \tValidation Loss: 2.447608\n",
      "Epoch: 23276 \tTraining Loss: 1.461765 \tValidation Loss: 2.447699\n",
      "Epoch: 23277 \tTraining Loss: 1.428373 \tValidation Loss: 2.448877\n",
      "Epoch: 23278 \tTraining Loss: 1.456535 \tValidation Loss: 2.447500\n",
      "Epoch: 23279 \tTraining Loss: 1.422652 \tValidation Loss: 2.448523\n",
      "Epoch: 23280 \tTraining Loss: 1.454809 \tValidation Loss: 2.448745\n",
      "Epoch: 23281 \tTraining Loss: 1.512198 \tValidation Loss: 2.447809\n",
      "Epoch: 23282 \tTraining Loss: 1.471977 \tValidation Loss: 2.447850\n",
      "Epoch: 23283 \tTraining Loss: 1.449350 \tValidation Loss: 2.448521\n",
      "Epoch: 23284 \tTraining Loss: 1.480236 \tValidation Loss: 2.447621\n",
      "Epoch: 23285 \tTraining Loss: 1.474669 \tValidation Loss: 2.447174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23286 \tTraining Loss: 1.477535 \tValidation Loss: 2.446875\n",
      "Epoch: 23287 \tTraining Loss: 1.436105 \tValidation Loss: 2.448025\n",
      "Epoch: 23288 \tTraining Loss: 1.440776 \tValidation Loss: 2.448044\n",
      "Epoch: 23289 \tTraining Loss: 1.435014 \tValidation Loss: 2.447876\n",
      "Epoch: 23290 \tTraining Loss: 1.450182 \tValidation Loss: 2.448369\n",
      "Epoch: 23291 \tTraining Loss: 1.432649 \tValidation Loss: 2.448536\n",
      "Epoch: 23292 \tTraining Loss: 1.471114 \tValidation Loss: 2.447888\n",
      "Epoch: 23293 \tTraining Loss: 1.471253 \tValidation Loss: 2.448073\n",
      "Epoch: 23294 \tTraining Loss: 1.449769 \tValidation Loss: 2.448692\n",
      "Epoch: 23295 \tTraining Loss: 1.493589 \tValidation Loss: 2.448049\n",
      "Epoch: 23296 \tTraining Loss: 1.485052 \tValidation Loss: 2.447458\n",
      "Epoch: 23297 \tTraining Loss: 1.467394 \tValidation Loss: 2.448340\n",
      "Epoch: 23298 \tTraining Loss: 1.448207 \tValidation Loss: 2.448746\n",
      "Epoch: 23299 \tTraining Loss: 1.472880 \tValidation Loss: 2.448449\n",
      "Epoch: 23300 \tTraining Loss: 1.468506 \tValidation Loss: 2.448535\n",
      "Epoch: 23301 \tTraining Loss: 1.515655 \tValidation Loss: 2.446840\n",
      "Epoch: 23302 \tTraining Loss: 1.447641 \tValidation Loss: 2.447228\n",
      "Epoch: 23303 \tTraining Loss: 1.449195 \tValidation Loss: 2.448353\n",
      "Epoch: 23304 \tTraining Loss: 1.474721 \tValidation Loss: 2.449197\n",
      "Epoch: 23305 \tTraining Loss: 1.471655 \tValidation Loss: 2.447938\n",
      "Epoch: 23306 \tTraining Loss: 1.474486 \tValidation Loss: 2.448052\n",
      "Epoch: 23307 \tTraining Loss: 1.419491 \tValidation Loss: 2.448492\n",
      "Epoch: 23308 \tTraining Loss: 1.463471 \tValidation Loss: 2.449000\n",
      "Epoch: 23309 \tTraining Loss: 1.458765 \tValidation Loss: 2.449013\n",
      "Epoch: 23310 \tTraining Loss: 1.456184 \tValidation Loss: 2.448650\n",
      "Epoch: 23311 \tTraining Loss: 1.467186 \tValidation Loss: 2.448110\n",
      "Epoch: 23312 \tTraining Loss: 1.435403 \tValidation Loss: 2.449252\n",
      "Epoch: 23313 \tTraining Loss: 1.498994 \tValidation Loss: 2.449172\n",
      "Epoch: 23314 \tTraining Loss: 1.449341 \tValidation Loss: 2.447901\n",
      "Epoch: 23315 \tTraining Loss: 1.481070 \tValidation Loss: 2.448244\n",
      "Epoch: 23316 \tTraining Loss: 1.427271 \tValidation Loss: 2.448258\n",
      "Epoch: 23317 \tTraining Loss: 1.485558 \tValidation Loss: 2.448511\n",
      "Epoch: 23318 \tTraining Loss: 1.458959 \tValidation Loss: 2.449037\n",
      "Epoch: 23319 \tTraining Loss: 1.494848 \tValidation Loss: 2.448879\n",
      "Epoch: 23320 \tTraining Loss: 1.413901 \tValidation Loss: 2.449345\n",
      "Epoch: 23321 \tTraining Loss: 1.463394 \tValidation Loss: 2.448663\n",
      "Epoch: 23322 \tTraining Loss: 1.446572 \tValidation Loss: 2.448684\n",
      "Epoch: 23323 \tTraining Loss: 1.498604 \tValidation Loss: 2.448391\n",
      "Epoch: 23324 \tTraining Loss: 1.479489 \tValidation Loss: 2.447963\n",
      "Epoch: 23325 \tTraining Loss: 1.470791 \tValidation Loss: 2.447988\n",
      "Epoch: 23326 \tTraining Loss: 1.468313 \tValidation Loss: 2.448022\n",
      "Epoch: 23327 \tTraining Loss: 1.449535 \tValidation Loss: 2.448739\n",
      "Epoch: 23328 \tTraining Loss: 1.434960 \tValidation Loss: 2.448635\n",
      "Epoch: 23329 \tTraining Loss: 1.503326 \tValidation Loss: 2.448195\n",
      "Epoch: 23330 \tTraining Loss: 1.420369 \tValidation Loss: 2.448618\n",
      "Epoch: 23331 \tTraining Loss: 1.459032 \tValidation Loss: 2.448221\n",
      "Epoch: 23332 \tTraining Loss: 1.447189 \tValidation Loss: 2.448380\n",
      "Epoch: 23333 \tTraining Loss: 1.486783 \tValidation Loss: 2.448722\n",
      "Epoch: 23334 \tTraining Loss: 1.453457 \tValidation Loss: 2.448402\n",
      "Epoch: 23335 \tTraining Loss: 1.446882 \tValidation Loss: 2.448943\n",
      "Epoch: 23336 \tTraining Loss: 1.455153 \tValidation Loss: 2.448632\n",
      "Epoch: 23337 \tTraining Loss: 1.419926 \tValidation Loss: 2.449229\n",
      "Epoch: 23338 \tTraining Loss: 1.463622 \tValidation Loss: 2.449330\n",
      "Epoch: 23339 \tTraining Loss: 1.426726 \tValidation Loss: 2.449988\n",
      "Epoch: 23340 \tTraining Loss: 1.457062 \tValidation Loss: 2.450280\n",
      "Epoch: 23341 \tTraining Loss: 1.467014 \tValidation Loss: 2.449234\n",
      "Epoch: 23342 \tTraining Loss: 1.441985 \tValidation Loss: 2.449539\n",
      "Epoch: 23343 \tTraining Loss: 1.474642 \tValidation Loss: 2.449533\n",
      "Epoch: 23344 \tTraining Loss: 1.483567 \tValidation Loss: 2.449853\n",
      "Epoch: 23345 \tTraining Loss: 1.446536 \tValidation Loss: 2.449388\n",
      "Epoch: 23346 \tTraining Loss: 1.461846 \tValidation Loss: 2.448806\n",
      "Epoch: 23347 \tTraining Loss: 1.495674 \tValidation Loss: 2.449454\n",
      "Epoch: 23348 \tTraining Loss: 1.512416 \tValidation Loss: 2.448046\n",
      "Epoch: 23349 \tTraining Loss: 1.441103 \tValidation Loss: 2.449231\n",
      "Epoch: 23350 \tTraining Loss: 1.456828 \tValidation Loss: 2.449904\n",
      "Epoch: 23351 \tTraining Loss: 1.520537 \tValidation Loss: 2.448735\n",
      "Epoch: 23352 \tTraining Loss: 1.459237 \tValidation Loss: 2.449761\n",
      "Epoch: 23353 \tTraining Loss: 1.463782 \tValidation Loss: 2.449515\n",
      "Epoch: 23354 \tTraining Loss: 1.472562 \tValidation Loss: 2.449531\n",
      "Epoch: 23355 \tTraining Loss: 1.461140 \tValidation Loss: 2.449131\n",
      "Epoch: 23356 \tTraining Loss: 1.480945 \tValidation Loss: 2.448624\n",
      "Epoch: 23357 \tTraining Loss: 1.470047 \tValidation Loss: 2.449867\n",
      "Epoch: 23358 \tTraining Loss: 1.443621 \tValidation Loss: 2.448614\n",
      "Epoch: 23359 \tTraining Loss: 1.482432 \tValidation Loss: 2.449668\n",
      "Epoch: 23360 \tTraining Loss: 1.438036 \tValidation Loss: 2.448315\n",
      "Epoch: 23361 \tTraining Loss: 1.480526 \tValidation Loss: 2.449175\n",
      "Epoch: 23362 \tTraining Loss: 1.491489 \tValidation Loss: 2.449511\n",
      "Epoch: 23363 \tTraining Loss: 1.494967 \tValidation Loss: 2.449891\n",
      "Epoch: 23364 \tTraining Loss: 1.456847 \tValidation Loss: 2.448527\n",
      "Epoch: 23365 \tTraining Loss: 1.481881 \tValidation Loss: 2.448856\n",
      "Epoch: 23366 \tTraining Loss: 1.478048 \tValidation Loss: 2.449623\n",
      "Epoch: 23367 \tTraining Loss: 1.473865 \tValidation Loss: 2.448916\n",
      "Epoch: 23368 \tTraining Loss: 1.415586 \tValidation Loss: 2.449683\n",
      "Epoch: 23369 \tTraining Loss: 1.416775 \tValidation Loss: 2.450167\n",
      "Epoch: 23370 \tTraining Loss: 1.485994 \tValidation Loss: 2.449426\n",
      "Epoch: 23371 \tTraining Loss: 1.446566 \tValidation Loss: 2.449623\n",
      "Epoch: 23372 \tTraining Loss: 1.468768 \tValidation Loss: 2.449450\n",
      "Epoch: 23373 \tTraining Loss: 1.475211 \tValidation Loss: 2.450211\n",
      "Epoch: 23374 \tTraining Loss: 1.502915 \tValidation Loss: 2.448983\n",
      "Epoch: 23375 \tTraining Loss: 1.479215 \tValidation Loss: 2.448979\n",
      "Epoch: 23376 \tTraining Loss: 1.463594 \tValidation Loss: 2.449284\n",
      "Epoch: 23377 \tTraining Loss: 1.468743 \tValidation Loss: 2.449365\n",
      "Epoch: 23378 \tTraining Loss: 1.412860 \tValidation Loss: 2.449693\n",
      "Epoch: 23379 \tTraining Loss: 1.469538 \tValidation Loss: 2.449479\n",
      "Epoch: 23380 \tTraining Loss: 1.456048 \tValidation Loss: 2.449514\n",
      "Epoch: 23381 \tTraining Loss: 1.518237 \tValidation Loss: 2.448738\n",
      "Epoch: 23382 \tTraining Loss: 1.433458 \tValidation Loss: 2.449728\n",
      "Epoch: 23383 \tTraining Loss: 1.448479 \tValidation Loss: 2.449994\n",
      "Epoch: 23384 \tTraining Loss: 1.456251 \tValidation Loss: 2.449069\n",
      "Epoch: 23385 \tTraining Loss: 1.533182 \tValidation Loss: 2.449236\n",
      "Epoch: 23386 \tTraining Loss: 1.499090 \tValidation Loss: 2.449281\n",
      "Epoch: 23387 \tTraining Loss: 1.447557 \tValidation Loss: 2.449985\n",
      "Epoch: 23388 \tTraining Loss: 1.492191 \tValidation Loss: 2.449573\n",
      "Epoch: 23389 \tTraining Loss: 1.445079 \tValidation Loss: 2.450747\n",
      "Epoch: 23390 \tTraining Loss: 1.504566 \tValidation Loss: 2.449287\n",
      "Epoch: 23391 \tTraining Loss: 1.539669 \tValidation Loss: 2.448492\n",
      "Epoch: 23392 \tTraining Loss: 1.476705 \tValidation Loss: 2.449234\n",
      "Epoch: 23393 \tTraining Loss: 1.473101 \tValidation Loss: 2.449134\n",
      "Epoch: 23394 \tTraining Loss: 1.487273 \tValidation Loss: 2.449138\n",
      "Epoch: 23395 \tTraining Loss: 1.485437 \tValidation Loss: 2.449439\n",
      "Epoch: 23396 \tTraining Loss: 1.493005 \tValidation Loss: 2.450026\n",
      "Epoch: 23397 \tTraining Loss: 1.462966 \tValidation Loss: 2.449768\n",
      "Epoch: 23398 \tTraining Loss: 1.441746 \tValidation Loss: 2.450428\n",
      "Epoch: 23399 \tTraining Loss: 1.449255 \tValidation Loss: 2.450361\n",
      "Epoch: 23400 \tTraining Loss: 1.455027 \tValidation Loss: 2.450822\n",
      "Epoch: 23401 \tTraining Loss: 1.462253 \tValidation Loss: 2.449305\n",
      "Epoch: 23402 \tTraining Loss: 1.449205 \tValidation Loss: 2.450327\n",
      "Epoch: 23403 \tTraining Loss: 1.443150 \tValidation Loss: 2.450297\n",
      "Epoch: 23404 \tTraining Loss: 1.488240 \tValidation Loss: 2.450180\n",
      "Epoch: 23405 \tTraining Loss: 1.467814 \tValidation Loss: 2.449751\n",
      "Epoch: 23406 \tTraining Loss: 1.476750 \tValidation Loss: 2.450206\n",
      "Epoch: 23407 \tTraining Loss: 1.453322 \tValidation Loss: 2.450372\n",
      "Epoch: 23408 \tTraining Loss: 1.455705 \tValidation Loss: 2.450983\n",
      "Epoch: 23409 \tTraining Loss: 1.477669 \tValidation Loss: 2.450813\n",
      "Epoch: 23410 \tTraining Loss: 1.455130 \tValidation Loss: 2.449637\n",
      "Epoch: 23411 \tTraining Loss: 1.469229 \tValidation Loss: 2.450568\n",
      "Epoch: 23412 \tTraining Loss: 1.468818 \tValidation Loss: 2.450806\n",
      "Epoch: 23413 \tTraining Loss: 1.501318 \tValidation Loss: 2.450559\n",
      "Epoch: 23414 \tTraining Loss: 1.497283 \tValidation Loss: 2.449935\n",
      "Epoch: 23415 \tTraining Loss: 1.434115 \tValidation Loss: 2.450065\n",
      "Epoch: 23416 \tTraining Loss: 1.460609 \tValidation Loss: 2.450297\n",
      "Epoch: 23417 \tTraining Loss: 1.426406 \tValidation Loss: 2.451633\n",
      "Epoch: 23418 \tTraining Loss: 1.467889 \tValidation Loss: 2.451366\n",
      "Epoch: 23419 \tTraining Loss: 1.452950 \tValidation Loss: 2.451102\n",
      "Epoch: 23420 \tTraining Loss: 1.483832 \tValidation Loss: 2.450746\n",
      "Epoch: 23421 \tTraining Loss: 1.461010 \tValidation Loss: 2.450366\n",
      "Epoch: 23422 \tTraining Loss: 1.430166 \tValidation Loss: 2.451181\n",
      "Epoch: 23423 \tTraining Loss: 1.419110 \tValidation Loss: 2.450669\n",
      "Epoch: 23424 \tTraining Loss: 1.469702 \tValidation Loss: 2.450826\n",
      "Epoch: 23425 \tTraining Loss: 1.436802 \tValidation Loss: 2.450401\n",
      "Epoch: 23426 \tTraining Loss: 1.469196 \tValidation Loss: 2.450398\n",
      "Epoch: 23427 \tTraining Loss: 1.530001 \tValidation Loss: 2.450483\n",
      "Epoch: 23428 \tTraining Loss: 1.477347 \tValidation Loss: 2.450440\n",
      "Epoch: 23429 \tTraining Loss: 1.437965 \tValidation Loss: 2.450644\n",
      "Epoch: 23430 \tTraining Loss: 1.464549 \tValidation Loss: 2.450302\n",
      "Epoch: 23431 \tTraining Loss: 1.476091 \tValidation Loss: 2.450489\n",
      "Epoch: 23432 \tTraining Loss: 1.443820 \tValidation Loss: 2.451689\n",
      "Epoch: 23433 \tTraining Loss: 1.461135 \tValidation Loss: 2.450331\n",
      "Epoch: 23434 \tTraining Loss: 1.401383 \tValidation Loss: 2.450655\n",
      "Epoch: 23435 \tTraining Loss: 1.451922 \tValidation Loss: 2.451374\n",
      "Epoch: 23436 \tTraining Loss: 1.450314 \tValidation Loss: 2.451358\n",
      "Epoch: 23437 \tTraining Loss: 1.503876 \tValidation Loss: 2.450778\n",
      "Epoch: 23438 \tTraining Loss: 1.457597 \tValidation Loss: 2.451199\n",
      "Epoch: 23439 \tTraining Loss: 1.435424 \tValidation Loss: 2.450107\n",
      "Epoch: 23440 \tTraining Loss: 1.428277 \tValidation Loss: 2.450887\n",
      "Epoch: 23441 \tTraining Loss: 1.466688 \tValidation Loss: 2.450937\n",
      "Epoch: 23442 \tTraining Loss: 1.451461 \tValidation Loss: 2.450948\n",
      "Epoch: 23443 \tTraining Loss: 1.527421 \tValidation Loss: 2.451663\n",
      "Epoch: 23444 \tTraining Loss: 1.456804 \tValidation Loss: 2.451660\n",
      "Epoch: 23445 \tTraining Loss: 1.450075 \tValidation Loss: 2.450815\n",
      "Epoch: 23446 \tTraining Loss: 1.478927 \tValidation Loss: 2.450639\n",
      "Epoch: 23447 \tTraining Loss: 1.477345 \tValidation Loss: 2.451041\n",
      "Epoch: 23448 \tTraining Loss: 1.456202 \tValidation Loss: 2.451704\n",
      "Epoch: 23449 \tTraining Loss: 1.453525 \tValidation Loss: 2.451450\n",
      "Epoch: 23450 \tTraining Loss: 1.478075 \tValidation Loss: 2.451130\n",
      "Epoch: 23451 \tTraining Loss: 1.473286 \tValidation Loss: 2.450644\n",
      "Epoch: 23452 \tTraining Loss: 1.459707 \tValidation Loss: 2.450858\n",
      "Epoch: 23453 \tTraining Loss: 1.462109 \tValidation Loss: 2.450587\n",
      "Epoch: 23454 \tTraining Loss: 1.450653 \tValidation Loss: 2.451139\n",
      "Epoch: 23455 \tTraining Loss: 1.450917 \tValidation Loss: 2.451773\n",
      "Epoch: 23456 \tTraining Loss: 1.441540 \tValidation Loss: 2.451710\n",
      "Epoch: 23457 \tTraining Loss: 1.471989 \tValidation Loss: 2.451477\n",
      "Epoch: 23458 \tTraining Loss: 1.502579 \tValidation Loss: 2.451057\n",
      "Epoch: 23459 \tTraining Loss: 1.511593 \tValidation Loss: 2.449107\n",
      "Epoch: 23460 \tTraining Loss: 1.469865 \tValidation Loss: 2.449705\n",
      "Epoch: 23461 \tTraining Loss: 1.422800 \tValidation Loss: 2.450598\n",
      "Epoch: 23462 \tTraining Loss: 1.446535 \tValidation Loss: 2.451163\n",
      "Epoch: 23463 \tTraining Loss: 1.511078 \tValidation Loss: 2.450793\n",
      "Epoch: 23464 \tTraining Loss: 1.461756 \tValidation Loss: 2.450527\n",
      "Epoch: 23465 \tTraining Loss: 1.480178 \tValidation Loss: 2.450737\n",
      "Epoch: 23466 \tTraining Loss: 1.435655 \tValidation Loss: 2.450703\n",
      "Epoch: 23467 \tTraining Loss: 1.443667 \tValidation Loss: 2.451236\n",
      "Epoch: 23468 \tTraining Loss: 1.502581 \tValidation Loss: 2.450081\n",
      "Epoch: 23469 \tTraining Loss: 1.439453 \tValidation Loss: 2.450271\n",
      "Epoch: 23470 \tTraining Loss: 1.497523 \tValidation Loss: 2.450043\n",
      "Epoch: 23471 \tTraining Loss: 1.492787 \tValidation Loss: 2.451079\n",
      "Epoch: 23472 \tTraining Loss: 1.482903 \tValidation Loss: 2.452110\n",
      "Epoch: 23473 \tTraining Loss: 1.419131 \tValidation Loss: 2.451266\n",
      "Epoch: 23474 \tTraining Loss: 1.493523 \tValidation Loss: 2.451016\n",
      "Epoch: 23475 \tTraining Loss: 1.448307 \tValidation Loss: 2.451746\n",
      "Epoch: 23476 \tTraining Loss: 1.446799 \tValidation Loss: 2.451027\n",
      "Epoch: 23477 \tTraining Loss: 1.486232 \tValidation Loss: 2.451098\n",
      "Epoch: 23478 \tTraining Loss: 1.437119 \tValidation Loss: 2.450750\n",
      "Epoch: 23479 \tTraining Loss: 1.438373 \tValidation Loss: 2.451714\n",
      "Epoch: 23480 \tTraining Loss: 1.459258 \tValidation Loss: 2.451401\n",
      "Epoch: 23481 \tTraining Loss: 1.463829 \tValidation Loss: 2.450747\n",
      "Epoch: 23482 \tTraining Loss: 1.456132 \tValidation Loss: 2.451744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23483 \tTraining Loss: 1.468624 \tValidation Loss: 2.451044\n",
      "Epoch: 23484 \tTraining Loss: 1.466191 \tValidation Loss: 2.452030\n",
      "Epoch: 23485 \tTraining Loss: 1.433483 \tValidation Loss: 2.451924\n",
      "Epoch: 23486 \tTraining Loss: 1.450234 \tValidation Loss: 2.452396\n",
      "Epoch: 23487 \tTraining Loss: 1.468068 \tValidation Loss: 2.451313\n",
      "Epoch: 23488 \tTraining Loss: 1.457056 \tValidation Loss: 2.451660\n",
      "Epoch: 23489 \tTraining Loss: 1.471601 \tValidation Loss: 2.451986\n",
      "Epoch: 23490 \tTraining Loss: 1.430771 \tValidation Loss: 2.453038\n",
      "Epoch: 23491 \tTraining Loss: 1.424596 \tValidation Loss: 2.452572\n",
      "Epoch: 23492 \tTraining Loss: 1.473822 \tValidation Loss: 2.451973\n",
      "Epoch: 23493 \tTraining Loss: 1.483036 \tValidation Loss: 2.451468\n",
      "Epoch: 23494 \tTraining Loss: 1.504926 \tValidation Loss: 2.451385\n",
      "Epoch: 23495 \tTraining Loss: 1.473370 \tValidation Loss: 2.451705\n",
      "Epoch: 23496 \tTraining Loss: 1.492772 \tValidation Loss: 2.452089\n",
      "Epoch: 23497 \tTraining Loss: 1.450676 \tValidation Loss: 2.452394\n",
      "Epoch: 23498 \tTraining Loss: 1.419338 \tValidation Loss: 2.452497\n",
      "Epoch: 23499 \tTraining Loss: 1.432744 \tValidation Loss: 2.451571\n",
      "Epoch: 23500 \tTraining Loss: 1.436999 \tValidation Loss: 2.451652\n",
      "Epoch: 23501 \tTraining Loss: 1.463362 \tValidation Loss: 2.452435\n",
      "Epoch: 23502 \tTraining Loss: 1.465132 \tValidation Loss: 2.452317\n",
      "Epoch: 23503 \tTraining Loss: 1.449731 \tValidation Loss: 2.451774\n",
      "Epoch: 23504 \tTraining Loss: 1.436217 \tValidation Loss: 2.451519\n",
      "Epoch: 23505 \tTraining Loss: 1.452268 \tValidation Loss: 2.451641\n",
      "Epoch: 23506 \tTraining Loss: 1.453372 \tValidation Loss: 2.451533\n",
      "Epoch: 23507 \tTraining Loss: 1.471707 \tValidation Loss: 2.452052\n",
      "Epoch: 23508 \tTraining Loss: 1.452513 \tValidation Loss: 2.452084\n",
      "Epoch: 23509 \tTraining Loss: 1.485924 \tValidation Loss: 2.452017\n",
      "Epoch: 23510 \tTraining Loss: 1.442675 \tValidation Loss: 2.452071\n",
      "Epoch: 23511 \tTraining Loss: 1.441303 \tValidation Loss: 2.452078\n",
      "Epoch: 23512 \tTraining Loss: 1.464267 \tValidation Loss: 2.451594\n",
      "Epoch: 23513 \tTraining Loss: 1.438170 \tValidation Loss: 2.451637\n",
      "Epoch: 23514 \tTraining Loss: 1.423694 \tValidation Loss: 2.451212\n",
      "Epoch: 23515 \tTraining Loss: 1.473488 \tValidation Loss: 2.450917\n",
      "Epoch: 23516 \tTraining Loss: 1.451196 \tValidation Loss: 2.451874\n",
      "Epoch: 23517 \tTraining Loss: 1.484271 \tValidation Loss: 2.451971\n",
      "Epoch: 23518 \tTraining Loss: 1.446948 \tValidation Loss: 2.452834\n",
      "Epoch: 23519 \tTraining Loss: 1.463996 \tValidation Loss: 2.451823\n",
      "Epoch: 23520 \tTraining Loss: 1.485641 \tValidation Loss: 2.452516\n",
      "Epoch: 23521 \tTraining Loss: 1.441636 \tValidation Loss: 2.452544\n",
      "Epoch: 23522 \tTraining Loss: 1.405214 \tValidation Loss: 2.452427\n",
      "Epoch: 23523 \tTraining Loss: 1.420191 \tValidation Loss: 2.452628\n",
      "Epoch: 23524 \tTraining Loss: 1.404518 \tValidation Loss: 2.453442\n",
      "Epoch: 23525 \tTraining Loss: 1.485884 \tValidation Loss: 2.451296\n",
      "Epoch: 23526 \tTraining Loss: 1.417772 \tValidation Loss: 2.452155\n",
      "Epoch: 23527 \tTraining Loss: 1.466517 \tValidation Loss: 2.451937\n",
      "Epoch: 23528 \tTraining Loss: 1.510942 \tValidation Loss: 2.451838\n",
      "Epoch: 23529 \tTraining Loss: 1.473538 \tValidation Loss: 2.451909\n",
      "Epoch: 23530 \tTraining Loss: 1.476551 \tValidation Loss: 2.452851\n",
      "Epoch: 23531 \tTraining Loss: 1.436107 \tValidation Loss: 2.453959\n",
      "Epoch: 23532 \tTraining Loss: 1.430030 \tValidation Loss: 2.452847\n",
      "Epoch: 23533 \tTraining Loss: 1.500710 \tValidation Loss: 2.452318\n",
      "Epoch: 23534 \tTraining Loss: 1.441278 \tValidation Loss: 2.452245\n",
      "Epoch: 23535 \tTraining Loss: 1.448468 \tValidation Loss: 2.452652\n",
      "Epoch: 23536 \tTraining Loss: 1.492750 \tValidation Loss: 2.451901\n",
      "Epoch: 23537 \tTraining Loss: 1.437896 \tValidation Loss: 2.452339\n",
      "Epoch: 23538 \tTraining Loss: 1.463898 \tValidation Loss: 2.452564\n",
      "Epoch: 23539 \tTraining Loss: 1.466132 \tValidation Loss: 2.452401\n",
      "Epoch: 23540 \tTraining Loss: 1.479956 \tValidation Loss: 2.452240\n",
      "Epoch: 23541 \tTraining Loss: 1.495422 \tValidation Loss: 2.451543\n",
      "Epoch: 23542 \tTraining Loss: 1.459251 \tValidation Loss: 2.451452\n",
      "Epoch: 23543 \tTraining Loss: 1.441342 \tValidation Loss: 2.452795\n",
      "Epoch: 23544 \tTraining Loss: 1.441704 \tValidation Loss: 2.452975\n",
      "Epoch: 23545 \tTraining Loss: 1.439899 \tValidation Loss: 2.453172\n",
      "Epoch: 23546 \tTraining Loss: 1.455120 \tValidation Loss: 2.452024\n",
      "Epoch: 23547 \tTraining Loss: 1.474185 \tValidation Loss: 2.452535\n",
      "Epoch: 23548 \tTraining Loss: 1.448969 \tValidation Loss: 2.452611\n",
      "Epoch: 23549 \tTraining Loss: 1.449545 \tValidation Loss: 2.453445\n",
      "Epoch: 23550 \tTraining Loss: 1.484896 \tValidation Loss: 2.452651\n",
      "Epoch: 23551 \tTraining Loss: 1.501186 \tValidation Loss: 2.452921\n",
      "Epoch: 23552 \tTraining Loss: 1.455897 \tValidation Loss: 2.452966\n",
      "Epoch: 23553 \tTraining Loss: 1.458154 \tValidation Loss: 2.453474\n",
      "Epoch: 23554 \tTraining Loss: 1.463642 \tValidation Loss: 2.452561\n",
      "Epoch: 23555 \tTraining Loss: 1.467312 \tValidation Loss: 2.452492\n",
      "Epoch: 23556 \tTraining Loss: 1.441750 \tValidation Loss: 2.451985\n",
      "Epoch: 23557 \tTraining Loss: 1.438137 \tValidation Loss: 2.453196\n",
      "Epoch: 23558 \tTraining Loss: 1.444882 \tValidation Loss: 2.452983\n",
      "Epoch: 23559 \tTraining Loss: 1.447990 \tValidation Loss: 2.453694\n",
      "Epoch: 23560 \tTraining Loss: 1.459430 \tValidation Loss: 2.452286\n",
      "Epoch: 23561 \tTraining Loss: 1.483334 \tValidation Loss: 2.452436\n",
      "Epoch: 23562 \tTraining Loss: 1.448766 \tValidation Loss: 2.452030\n",
      "Epoch: 23563 \tTraining Loss: 1.455558 \tValidation Loss: 2.451792\n",
      "Epoch: 23564 \tTraining Loss: 1.458030 \tValidation Loss: 2.452811\n",
      "Epoch: 23565 \tTraining Loss: 1.437761 \tValidation Loss: 2.453276\n",
      "Epoch: 23566 \tTraining Loss: 1.467886 \tValidation Loss: 2.452794\n",
      "Epoch: 23567 \tTraining Loss: 1.455457 \tValidation Loss: 2.452731\n",
      "Epoch: 23568 \tTraining Loss: 1.443376 \tValidation Loss: 2.453205\n",
      "Epoch: 23569 \tTraining Loss: 1.443792 \tValidation Loss: 2.453672\n",
      "Epoch: 23570 \tTraining Loss: 1.450239 \tValidation Loss: 2.453099\n",
      "Epoch: 23571 \tTraining Loss: 1.488845 \tValidation Loss: 2.451766\n",
      "Epoch: 23572 \tTraining Loss: 1.480907 \tValidation Loss: 2.452767\n",
      "Epoch: 23573 \tTraining Loss: 1.471048 \tValidation Loss: 2.452512\n",
      "Epoch: 23574 \tTraining Loss: 1.466150 \tValidation Loss: 2.453064\n",
      "Epoch: 23575 \tTraining Loss: 1.488368 \tValidation Loss: 2.452629\n",
      "Epoch: 23576 \tTraining Loss: 1.502648 \tValidation Loss: 2.452895\n",
      "Epoch: 23577 \tTraining Loss: 1.438589 \tValidation Loss: 2.453314\n",
      "Epoch: 23578 \tTraining Loss: 1.469522 \tValidation Loss: 2.453023\n",
      "Epoch: 23579 \tTraining Loss: 1.481094 \tValidation Loss: 2.452267\n",
      "Epoch: 23580 \tTraining Loss: 1.477647 \tValidation Loss: 2.453119\n",
      "Epoch: 23581 \tTraining Loss: 1.478185 \tValidation Loss: 2.452351\n",
      "Epoch: 23582 \tTraining Loss: 1.472271 \tValidation Loss: 2.453117\n",
      "Epoch: 23583 \tTraining Loss: 1.463092 \tValidation Loss: 2.453029\n",
      "Epoch: 23584 \tTraining Loss: 1.438919 \tValidation Loss: 2.453376\n",
      "Epoch: 23585 \tTraining Loss: 1.463576 \tValidation Loss: 2.453737\n",
      "Epoch: 23586 \tTraining Loss: 1.435247 \tValidation Loss: 2.453306\n",
      "Epoch: 23587 \tTraining Loss: 1.459169 \tValidation Loss: 2.453578\n",
      "Epoch: 23588 \tTraining Loss: 1.465712 \tValidation Loss: 2.453287\n",
      "Epoch: 23589 \tTraining Loss: 1.462737 \tValidation Loss: 2.453410\n",
      "Epoch: 23590 \tTraining Loss: 1.475347 \tValidation Loss: 2.453166\n",
      "Epoch: 23591 \tTraining Loss: 1.460900 \tValidation Loss: 2.452925\n",
      "Epoch: 23592 \tTraining Loss: 1.454807 \tValidation Loss: 2.453474\n",
      "Epoch: 23593 \tTraining Loss: 1.455883 \tValidation Loss: 2.453599\n",
      "Epoch: 23594 \tTraining Loss: 1.469705 \tValidation Loss: 2.452682\n",
      "Epoch: 23595 \tTraining Loss: 1.455056 \tValidation Loss: 2.453352\n",
      "Epoch: 23596 \tTraining Loss: 1.476294 \tValidation Loss: 2.453338\n",
      "Epoch: 23597 \tTraining Loss: 1.501193 \tValidation Loss: 2.452505\n",
      "Epoch: 23598 \tTraining Loss: 1.456500 \tValidation Loss: 2.453565\n",
      "Epoch: 23599 \tTraining Loss: 1.479391 \tValidation Loss: 2.453132\n",
      "Epoch: 23600 \tTraining Loss: 1.460963 \tValidation Loss: 2.453422\n",
      "Epoch: 23601 \tTraining Loss: 1.462907 \tValidation Loss: 2.453847\n",
      "Epoch: 23602 \tTraining Loss: 1.497291 \tValidation Loss: 2.453097\n",
      "Epoch: 23603 \tTraining Loss: 1.477466 \tValidation Loss: 2.453614\n",
      "Epoch: 23604 \tTraining Loss: 1.471801 \tValidation Loss: 2.452429\n",
      "Epoch: 23605 \tTraining Loss: 1.464566 \tValidation Loss: 2.452423\n",
      "Epoch: 23606 \tTraining Loss: 1.437491 \tValidation Loss: 2.453070\n",
      "Epoch: 23607 \tTraining Loss: 1.451704 \tValidation Loss: 2.452696\n",
      "Epoch: 23608 \tTraining Loss: 1.444165 \tValidation Loss: 2.453707\n",
      "Epoch: 23609 \tTraining Loss: 1.468801 \tValidation Loss: 2.453678\n",
      "Epoch: 23610 \tTraining Loss: 1.458256 \tValidation Loss: 2.454064\n",
      "Epoch: 23611 \tTraining Loss: 1.505713 \tValidation Loss: 2.453569\n",
      "Epoch: 23612 \tTraining Loss: 1.479524 \tValidation Loss: 2.453253\n",
      "Epoch: 23613 \tTraining Loss: 1.472301 \tValidation Loss: 2.452893\n",
      "Epoch: 23614 \tTraining Loss: 1.439163 \tValidation Loss: 2.452314\n",
      "Epoch: 23615 \tTraining Loss: 1.493744 \tValidation Loss: 2.453040\n",
      "Epoch: 23616 \tTraining Loss: 1.460489 \tValidation Loss: 2.452977\n",
      "Epoch: 23617 \tTraining Loss: 1.430651 \tValidation Loss: 2.453374\n",
      "Epoch: 23618 \tTraining Loss: 1.476226 \tValidation Loss: 2.452870\n",
      "Epoch: 23619 \tTraining Loss: 1.448152 \tValidation Loss: 2.453097\n",
      "Epoch: 23620 \tTraining Loss: 1.459293 \tValidation Loss: 2.453828\n",
      "Epoch: 23621 \tTraining Loss: 1.457330 \tValidation Loss: 2.454213\n",
      "Epoch: 23622 \tTraining Loss: 1.435386 \tValidation Loss: 2.454170\n",
      "Epoch: 23623 \tTraining Loss: 1.479666 \tValidation Loss: 2.452772\n",
      "Epoch: 23624 \tTraining Loss: 1.494208 \tValidation Loss: 2.453127\n",
      "Epoch: 23625 \tTraining Loss: 1.407261 \tValidation Loss: 2.453472\n",
      "Epoch: 23626 \tTraining Loss: 1.458969 \tValidation Loss: 2.454070\n",
      "Epoch: 23627 \tTraining Loss: 1.435273 \tValidation Loss: 2.454838\n",
      "Epoch: 23628 \tTraining Loss: 1.490050 \tValidation Loss: 2.453935\n",
      "Epoch: 23629 \tTraining Loss: 1.454644 \tValidation Loss: 2.454293\n",
      "Epoch: 23630 \tTraining Loss: 1.441633 \tValidation Loss: 2.454107\n",
      "Epoch: 23631 \tTraining Loss: 1.469517 \tValidation Loss: 2.454182\n",
      "Epoch: 23632 \tTraining Loss: 1.447246 \tValidation Loss: 2.454491\n",
      "Epoch: 23633 \tTraining Loss: 1.449093 \tValidation Loss: 2.454016\n",
      "Epoch: 23634 \tTraining Loss: 1.449935 \tValidation Loss: 2.453542\n",
      "Epoch: 23635 \tTraining Loss: 1.435344 \tValidation Loss: 2.453943\n",
      "Epoch: 23636 \tTraining Loss: 1.484275 \tValidation Loss: 2.454728\n",
      "Epoch: 23637 \tTraining Loss: 1.482439 \tValidation Loss: 2.454177\n",
      "Epoch: 23638 \tTraining Loss: 1.433483 \tValidation Loss: 2.454150\n",
      "Epoch: 23639 \tTraining Loss: 1.418498 \tValidation Loss: 2.454803\n",
      "Epoch: 23640 \tTraining Loss: 1.460399 \tValidation Loss: 2.455272\n",
      "Epoch: 23641 \tTraining Loss: 1.457841 \tValidation Loss: 2.454312\n",
      "Epoch: 23642 \tTraining Loss: 1.474179 \tValidation Loss: 2.453763\n",
      "Epoch: 23643 \tTraining Loss: 1.458130 \tValidation Loss: 2.454361\n",
      "Epoch: 23644 \tTraining Loss: 1.452890 \tValidation Loss: 2.453686\n",
      "Epoch: 23645 \tTraining Loss: 1.464878 \tValidation Loss: 2.453436\n",
      "Epoch: 23646 \tTraining Loss: 1.486275 \tValidation Loss: 2.453727\n",
      "Epoch: 23647 \tTraining Loss: 1.475781 \tValidation Loss: 2.453916\n",
      "Epoch: 23648 \tTraining Loss: 1.446652 \tValidation Loss: 2.454197\n",
      "Epoch: 23649 \tTraining Loss: 1.399888 \tValidation Loss: 2.455435\n",
      "Epoch: 23650 \tTraining Loss: 1.462787 \tValidation Loss: 2.454738\n",
      "Epoch: 23651 \tTraining Loss: 1.453174 \tValidation Loss: 2.455223\n",
      "Epoch: 23652 \tTraining Loss: 1.435480 \tValidation Loss: 2.455262\n",
      "Epoch: 23653 \tTraining Loss: 1.471876 \tValidation Loss: 2.454872\n",
      "Epoch: 23654 \tTraining Loss: 1.499809 \tValidation Loss: 2.454222\n",
      "Epoch: 23655 \tTraining Loss: 1.405234 \tValidation Loss: 2.455305\n",
      "Epoch: 23656 \tTraining Loss: 1.463574 \tValidation Loss: 2.455252\n",
      "Epoch: 23657 \tTraining Loss: 1.460368 \tValidation Loss: 2.455214\n",
      "Epoch: 23658 \tTraining Loss: 1.424072 \tValidation Loss: 2.454599\n",
      "Epoch: 23659 \tTraining Loss: 1.443049 \tValidation Loss: 2.455441\n",
      "Epoch: 23660 \tTraining Loss: 1.466879 \tValidation Loss: 2.454416\n",
      "Epoch: 23661 \tTraining Loss: 1.464544 \tValidation Loss: 2.454703\n",
      "Epoch: 23662 \tTraining Loss: 1.493275 \tValidation Loss: 2.453862\n",
      "Epoch: 23663 \tTraining Loss: 1.455813 \tValidation Loss: 2.454505\n",
      "Epoch: 23664 \tTraining Loss: 1.467434 \tValidation Loss: 2.454680\n",
      "Epoch: 23665 \tTraining Loss: 1.475385 \tValidation Loss: 2.454790\n",
      "Epoch: 23666 \tTraining Loss: 1.461920 \tValidation Loss: 2.455042\n",
      "Epoch: 23667 \tTraining Loss: 1.445657 \tValidation Loss: 2.455110\n",
      "Epoch: 23668 \tTraining Loss: 1.462803 \tValidation Loss: 2.454815\n",
      "Epoch: 23669 \tTraining Loss: 1.412335 \tValidation Loss: 2.454839\n",
      "Epoch: 23670 \tTraining Loss: 1.462552 \tValidation Loss: 2.455112\n",
      "Epoch: 23671 \tTraining Loss: 1.442728 \tValidation Loss: 2.454019\n",
      "Epoch: 23672 \tTraining Loss: 1.445101 \tValidation Loss: 2.454413\n",
      "Epoch: 23673 \tTraining Loss: 1.448929 \tValidation Loss: 2.454576\n",
      "Epoch: 23674 \tTraining Loss: 1.490295 \tValidation Loss: 2.454267\n",
      "Epoch: 23675 \tTraining Loss: 1.467942 \tValidation Loss: 2.453760\n",
      "Epoch: 23676 \tTraining Loss: 1.470049 \tValidation Loss: 2.454061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23677 \tTraining Loss: 1.453075 \tValidation Loss: 2.454169\n",
      "Epoch: 23678 \tTraining Loss: 1.439684 \tValidation Loss: 2.454733\n",
      "Epoch: 23679 \tTraining Loss: 1.466485 \tValidation Loss: 2.454446\n",
      "Epoch: 23680 \tTraining Loss: 1.451119 \tValidation Loss: 2.454901\n",
      "Epoch: 23681 \tTraining Loss: 1.439834 \tValidation Loss: 2.455407\n",
      "Epoch: 23682 \tTraining Loss: 1.481390 \tValidation Loss: 2.454504\n",
      "Epoch: 23683 \tTraining Loss: 1.473541 \tValidation Loss: 2.455354\n",
      "Epoch: 23684 \tTraining Loss: 1.485171 \tValidation Loss: 2.454293\n",
      "Epoch: 23685 \tTraining Loss: 1.457634 \tValidation Loss: 2.454490\n",
      "Epoch: 23686 \tTraining Loss: 1.448761 \tValidation Loss: 2.454830\n",
      "Epoch: 23687 \tTraining Loss: 1.452892 \tValidation Loss: 2.456656\n",
      "Epoch: 23688 \tTraining Loss: 1.447660 \tValidation Loss: 2.455369\n",
      "Epoch: 23689 \tTraining Loss: 1.412612 \tValidation Loss: 2.455427\n",
      "Epoch: 23690 \tTraining Loss: 1.439308 \tValidation Loss: 2.454490\n",
      "Epoch: 23691 \tTraining Loss: 1.441296 \tValidation Loss: 2.455377\n",
      "Epoch: 23692 \tTraining Loss: 1.438819 \tValidation Loss: 2.454636\n",
      "Epoch: 23693 \tTraining Loss: 1.459213 \tValidation Loss: 2.455440\n",
      "Epoch: 23694 \tTraining Loss: 1.423982 \tValidation Loss: 2.454515\n",
      "Epoch: 23695 \tTraining Loss: 1.454838 \tValidation Loss: 2.455220\n",
      "Epoch: 23696 \tTraining Loss: 1.473560 \tValidation Loss: 2.455121\n",
      "Epoch: 23697 \tTraining Loss: 1.503060 \tValidation Loss: 2.454987\n",
      "Epoch: 23698 \tTraining Loss: 1.446455 \tValidation Loss: 2.455439\n",
      "Epoch: 23699 \tTraining Loss: 1.448146 \tValidation Loss: 2.455636\n",
      "Epoch: 23700 \tTraining Loss: 1.450583 \tValidation Loss: 2.455737\n",
      "Epoch: 23701 \tTraining Loss: 1.426670 \tValidation Loss: 2.456532\n",
      "Epoch: 23702 \tTraining Loss: 1.441492 \tValidation Loss: 2.455814\n",
      "Epoch: 23703 \tTraining Loss: 1.474843 \tValidation Loss: 2.455825\n",
      "Epoch: 23704 \tTraining Loss: 1.457692 \tValidation Loss: 2.455729\n",
      "Epoch: 23705 \tTraining Loss: 1.441714 \tValidation Loss: 2.455577\n",
      "Epoch: 23706 \tTraining Loss: 1.465640 \tValidation Loss: 2.455208\n",
      "Epoch: 23707 \tTraining Loss: 1.447942 \tValidation Loss: 2.454939\n",
      "Epoch: 23708 \tTraining Loss: 1.449290 \tValidation Loss: 2.455599\n",
      "Epoch: 23709 \tTraining Loss: 1.455679 \tValidation Loss: 2.455611\n",
      "Epoch: 23710 \tTraining Loss: 1.417990 \tValidation Loss: 2.455095\n",
      "Epoch: 23711 \tTraining Loss: 1.483566 \tValidation Loss: 2.455735\n",
      "Epoch: 23712 \tTraining Loss: 1.458372 \tValidation Loss: 2.455492\n",
      "Epoch: 23713 \tTraining Loss: 1.510087 \tValidation Loss: 2.455556\n",
      "Epoch: 23714 \tTraining Loss: 1.473594 \tValidation Loss: 2.454979\n",
      "Epoch: 23715 \tTraining Loss: 1.460911 \tValidation Loss: 2.455019\n",
      "Epoch: 23716 \tTraining Loss: 1.451283 \tValidation Loss: 2.455184\n",
      "Epoch: 23717 \tTraining Loss: 1.434677 \tValidation Loss: 2.455194\n",
      "Epoch: 23718 \tTraining Loss: 1.434276 \tValidation Loss: 2.456328\n",
      "Epoch: 23719 \tTraining Loss: 1.437149 \tValidation Loss: 2.456007\n",
      "Epoch: 23720 \tTraining Loss: 1.444031 \tValidation Loss: 2.455608\n",
      "Epoch: 23721 \tTraining Loss: 1.455311 \tValidation Loss: 2.455626\n",
      "Epoch: 23722 \tTraining Loss: 1.440828 \tValidation Loss: 2.455472\n",
      "Epoch: 23723 \tTraining Loss: 1.457228 \tValidation Loss: 2.455992\n",
      "Epoch: 23724 \tTraining Loss: 1.467945 \tValidation Loss: 2.455262\n",
      "Epoch: 23725 \tTraining Loss: 1.497439 \tValidation Loss: 2.456305\n",
      "Epoch: 23726 \tTraining Loss: 1.426625 \tValidation Loss: 2.456624\n",
      "Epoch: 23727 \tTraining Loss: 1.461999 \tValidation Loss: 2.456895\n",
      "Epoch: 23728 \tTraining Loss: 1.458996 \tValidation Loss: 2.456315\n",
      "Epoch: 23729 \tTraining Loss: 1.440024 \tValidation Loss: 2.456397\n",
      "Epoch: 23730 \tTraining Loss: 1.458251 \tValidation Loss: 2.456518\n",
      "Epoch: 23731 \tTraining Loss: 1.453329 \tValidation Loss: 2.456454\n",
      "Epoch: 23732 \tTraining Loss: 1.461532 \tValidation Loss: 2.456770\n",
      "Epoch: 23733 \tTraining Loss: 1.443136 \tValidation Loss: 2.456305\n",
      "Epoch: 23734 \tTraining Loss: 1.471565 \tValidation Loss: 2.455341\n",
      "Epoch: 23735 \tTraining Loss: 1.451783 \tValidation Loss: 2.456359\n",
      "Epoch: 23736 \tTraining Loss: 1.444867 \tValidation Loss: 2.456717\n",
      "Epoch: 23737 \tTraining Loss: 1.442198 \tValidation Loss: 2.457306\n",
      "Epoch: 23738 \tTraining Loss: 1.443186 \tValidation Loss: 2.457202\n",
      "Epoch: 23739 \tTraining Loss: 1.473628 \tValidation Loss: 2.457089\n",
      "Epoch: 23740 \tTraining Loss: 1.440792 \tValidation Loss: 2.456063\n",
      "Epoch: 23741 \tTraining Loss: 1.416386 \tValidation Loss: 2.456696\n",
      "Epoch: 23742 \tTraining Loss: 1.462976 \tValidation Loss: 2.456423\n",
      "Epoch: 23743 \tTraining Loss: 1.438272 \tValidation Loss: 2.456823\n",
      "Epoch: 23744 \tTraining Loss: 1.454402 \tValidation Loss: 2.456186\n",
      "Epoch: 23745 \tTraining Loss: 1.447690 \tValidation Loss: 2.455961\n",
      "Epoch: 23746 \tTraining Loss: 1.419372 \tValidation Loss: 2.456708\n",
      "Epoch: 23747 \tTraining Loss: 1.442750 \tValidation Loss: 2.457248\n",
      "Epoch: 23748 \tTraining Loss: 1.434450 \tValidation Loss: 2.457509\n",
      "Epoch: 23749 \tTraining Loss: 1.463667 \tValidation Loss: 2.456527\n",
      "Epoch: 23750 \tTraining Loss: 1.467426 \tValidation Loss: 2.456281\n",
      "Epoch: 23751 \tTraining Loss: 1.440699 \tValidation Loss: 2.456010\n",
      "Epoch: 23752 \tTraining Loss: 1.460634 \tValidation Loss: 2.455878\n",
      "Epoch: 23753 \tTraining Loss: 1.480745 \tValidation Loss: 2.455620\n",
      "Epoch: 23754 \tTraining Loss: 1.468256 \tValidation Loss: 2.456060\n",
      "Epoch: 23755 \tTraining Loss: 1.456006 \tValidation Loss: 2.456204\n",
      "Epoch: 23756 \tTraining Loss: 1.469623 \tValidation Loss: 2.455594\n",
      "Epoch: 23757 \tTraining Loss: 1.447474 \tValidation Loss: 2.455677\n",
      "Epoch: 23758 \tTraining Loss: 1.473371 \tValidation Loss: 2.455231\n",
      "Epoch: 23759 \tTraining Loss: 1.421392 \tValidation Loss: 2.456819\n",
      "Epoch: 23760 \tTraining Loss: 1.409645 \tValidation Loss: 2.456902\n",
      "Epoch: 23761 \tTraining Loss: 1.434384 \tValidation Loss: 2.457509\n",
      "Epoch: 23762 \tTraining Loss: 1.451059 \tValidation Loss: 2.456774\n",
      "Epoch: 23763 \tTraining Loss: 1.460843 \tValidation Loss: 2.456897\n",
      "Epoch: 23764 \tTraining Loss: 1.467570 \tValidation Loss: 2.456712\n",
      "Epoch: 23765 \tTraining Loss: 1.445424 \tValidation Loss: 2.456699\n",
      "Epoch: 23766 \tTraining Loss: 1.449538 \tValidation Loss: 2.455558\n",
      "Epoch: 23767 \tTraining Loss: 1.490015 \tValidation Loss: 2.455789\n",
      "Epoch: 23768 \tTraining Loss: 1.459852 \tValidation Loss: 2.455997\n",
      "Epoch: 23769 \tTraining Loss: 1.470413 \tValidation Loss: 2.456836\n",
      "Epoch: 23770 \tTraining Loss: 1.449017 \tValidation Loss: 2.456506\n",
      "Epoch: 23771 \tTraining Loss: 1.465701 \tValidation Loss: 2.456925\n",
      "Epoch: 23772 \tTraining Loss: 1.431009 \tValidation Loss: 2.456384\n",
      "Epoch: 23773 \tTraining Loss: 1.432906 \tValidation Loss: 2.457326\n",
      "Epoch: 23774 \tTraining Loss: 1.460810 \tValidation Loss: 2.457472\n",
      "Epoch: 23775 \tTraining Loss: 1.483630 \tValidation Loss: 2.456656\n",
      "Epoch: 23776 \tTraining Loss: 1.425920 \tValidation Loss: 2.457063\n",
      "Epoch: 23777 \tTraining Loss: 1.448333 \tValidation Loss: 2.458225\n",
      "Epoch: 23778 \tTraining Loss: 1.460505 \tValidation Loss: 2.457390\n",
      "Epoch: 23779 \tTraining Loss: 1.415750 \tValidation Loss: 2.457881\n",
      "Epoch: 23780 \tTraining Loss: 1.459968 \tValidation Loss: 2.456074\n",
      "Epoch: 23781 \tTraining Loss: 1.444042 \tValidation Loss: 2.456663\n",
      "Epoch: 23782 \tTraining Loss: 1.419311 \tValidation Loss: 2.457161\n",
      "Epoch: 23783 \tTraining Loss: 1.436747 \tValidation Loss: 2.457532\n",
      "Epoch: 23784 \tTraining Loss: 1.478562 \tValidation Loss: 2.457140\n",
      "Epoch: 23785 \tTraining Loss: 1.433209 \tValidation Loss: 2.456225\n",
      "Epoch: 23786 \tTraining Loss: 1.459237 \tValidation Loss: 2.457306\n",
      "Epoch: 23787 \tTraining Loss: 1.463898 \tValidation Loss: 2.457402\n",
      "Epoch: 23788 \tTraining Loss: 1.432825 \tValidation Loss: 2.457672\n",
      "Epoch: 23789 \tTraining Loss: 1.478255 \tValidation Loss: 2.457323\n",
      "Epoch: 23790 \tTraining Loss: 1.432317 \tValidation Loss: 2.458472\n",
      "Epoch: 23791 \tTraining Loss: 1.426837 \tValidation Loss: 2.457591\n",
      "Epoch: 23792 \tTraining Loss: 1.438104 \tValidation Loss: 2.457513\n",
      "Epoch: 23793 \tTraining Loss: 1.430483 \tValidation Loss: 2.457635\n",
      "Epoch: 23794 \tTraining Loss: 1.443064 \tValidation Loss: 2.457749\n",
      "Epoch: 23795 \tTraining Loss: 1.481374 \tValidation Loss: 2.457706\n",
      "Epoch: 23796 \tTraining Loss: 1.480541 \tValidation Loss: 2.456946\n",
      "Epoch: 23797 \tTraining Loss: 1.427643 \tValidation Loss: 2.458124\n",
      "Epoch: 23798 \tTraining Loss: 1.422449 \tValidation Loss: 2.457920\n",
      "Epoch: 23799 \tTraining Loss: 1.459376 \tValidation Loss: 2.458667\n",
      "Epoch: 23800 \tTraining Loss: 1.471875 \tValidation Loss: 2.457203\n",
      "Epoch: 23801 \tTraining Loss: 1.491134 \tValidation Loss: 2.457464\n",
      "Epoch: 23802 \tTraining Loss: 1.489359 \tValidation Loss: 2.457634\n",
      "Epoch: 23803 \tTraining Loss: 1.445677 \tValidation Loss: 2.457826\n",
      "Epoch: 23804 \tTraining Loss: 1.459944 \tValidation Loss: 2.457426\n",
      "Epoch: 23805 \tTraining Loss: 1.439353 \tValidation Loss: 2.457986\n",
      "Epoch: 23806 \tTraining Loss: 1.451221 \tValidation Loss: 2.457205\n",
      "Epoch: 23807 \tTraining Loss: 1.495846 \tValidation Loss: 2.457634\n",
      "Epoch: 23808 \tTraining Loss: 1.444678 \tValidation Loss: 2.457354\n",
      "Epoch: 23809 \tTraining Loss: 1.442621 \tValidation Loss: 2.456885\n",
      "Epoch: 23810 \tTraining Loss: 1.464409 \tValidation Loss: 2.457040\n",
      "Epoch: 23811 \tTraining Loss: 1.466561 \tValidation Loss: 2.457850\n",
      "Epoch: 23812 \tTraining Loss: 1.450488 \tValidation Loss: 2.457418\n",
      "Epoch: 23813 \tTraining Loss: 1.443368 \tValidation Loss: 2.458335\n",
      "Epoch: 23814 \tTraining Loss: 1.432914 \tValidation Loss: 2.458221\n",
      "Epoch: 23815 \tTraining Loss: 1.437837 \tValidation Loss: 2.458402\n",
      "Epoch: 23816 \tTraining Loss: 1.478646 \tValidation Loss: 2.457290\n",
      "Epoch: 23817 \tTraining Loss: 1.476581 \tValidation Loss: 2.456456\n",
      "Epoch: 23818 \tTraining Loss: 1.436497 \tValidation Loss: 2.457924\n",
      "Epoch: 23819 \tTraining Loss: 1.461438 \tValidation Loss: 2.456881\n",
      "Epoch: 23820 \tTraining Loss: 1.465122 \tValidation Loss: 2.458185\n",
      "Epoch: 23821 \tTraining Loss: 1.421298 \tValidation Loss: 2.457675\n",
      "Epoch: 23822 \tTraining Loss: 1.455589 \tValidation Loss: 2.457533\n",
      "Epoch: 23823 \tTraining Loss: 1.438791 \tValidation Loss: 2.457782\n",
      "Epoch: 23824 \tTraining Loss: 1.416357 \tValidation Loss: 2.458019\n",
      "Epoch: 23825 \tTraining Loss: 1.444108 \tValidation Loss: 2.457479\n",
      "Epoch: 23826 \tTraining Loss: 1.469208 \tValidation Loss: 2.458088\n",
      "Epoch: 23827 \tTraining Loss: 1.459939 \tValidation Loss: 2.458796\n",
      "Epoch: 23828 \tTraining Loss: 1.416401 \tValidation Loss: 2.457781\n",
      "Epoch: 23829 \tTraining Loss: 1.434067 \tValidation Loss: 2.458592\n",
      "Epoch: 23830 \tTraining Loss: 1.454854 \tValidation Loss: 2.457870\n",
      "Epoch: 23831 \tTraining Loss: 1.454811 \tValidation Loss: 2.458209\n",
      "Epoch: 23832 \tTraining Loss: 1.459761 \tValidation Loss: 2.458264\n",
      "Epoch: 23833 \tTraining Loss: 1.436783 \tValidation Loss: 2.458768\n",
      "Epoch: 23834 \tTraining Loss: 1.446995 \tValidation Loss: 2.458021\n",
      "Epoch: 23835 \tTraining Loss: 1.489935 \tValidation Loss: 2.458711\n",
      "Epoch: 23836 \tTraining Loss: 1.416744 \tValidation Loss: 2.458811\n",
      "Epoch: 23837 \tTraining Loss: 1.431801 \tValidation Loss: 2.459239\n",
      "Epoch: 23838 \tTraining Loss: 1.476633 \tValidation Loss: 2.458367\n",
      "Epoch: 23839 \tTraining Loss: 1.465052 \tValidation Loss: 2.457749\n",
      "Epoch: 23840 \tTraining Loss: 1.444852 \tValidation Loss: 2.458765\n",
      "Epoch: 23841 \tTraining Loss: 1.468493 \tValidation Loss: 2.456911\n",
      "Epoch: 23842 \tTraining Loss: 1.488625 \tValidation Loss: 2.457293\n",
      "Epoch: 23843 \tTraining Loss: 1.458119 \tValidation Loss: 2.457910\n",
      "Epoch: 23844 \tTraining Loss: 1.403136 \tValidation Loss: 2.459441\n",
      "Epoch: 23845 \tTraining Loss: 1.427286 \tValidation Loss: 2.458984\n",
      "Epoch: 23846 \tTraining Loss: 1.443059 \tValidation Loss: 2.458194\n",
      "Epoch: 23847 \tTraining Loss: 1.450606 \tValidation Loss: 2.459058\n",
      "Epoch: 23848 \tTraining Loss: 1.440588 \tValidation Loss: 2.459581\n",
      "Epoch: 23849 \tTraining Loss: 1.460122 \tValidation Loss: 2.458743\n",
      "Epoch: 23850 \tTraining Loss: 1.379131 \tValidation Loss: 2.459735\n",
      "Epoch: 23851 \tTraining Loss: 1.447580 \tValidation Loss: 2.458695\n",
      "Epoch: 23852 \tTraining Loss: 1.393709 \tValidation Loss: 2.458789\n",
      "Epoch: 23853 \tTraining Loss: 1.443259 \tValidation Loss: 2.460014\n",
      "Epoch: 23854 \tTraining Loss: 1.444876 \tValidation Loss: 2.459167\n",
      "Epoch: 23855 \tTraining Loss: 1.491963 \tValidation Loss: 2.458831\n",
      "Epoch: 23856 \tTraining Loss: 1.425448 \tValidation Loss: 2.459203\n",
      "Epoch: 23857 \tTraining Loss: 1.437237 \tValidation Loss: 2.458823\n",
      "Epoch: 23858 \tTraining Loss: 1.443875 \tValidation Loss: 2.458858\n",
      "Epoch: 23859 \tTraining Loss: 1.439145 \tValidation Loss: 2.458758\n",
      "Epoch: 23860 \tTraining Loss: 1.402811 \tValidation Loss: 2.459024\n",
      "Epoch: 23861 \tTraining Loss: 1.457389 \tValidation Loss: 2.459078\n",
      "Epoch: 23862 \tTraining Loss: 1.460964 \tValidation Loss: 2.458975\n",
      "Epoch: 23863 \tTraining Loss: 1.448248 \tValidation Loss: 2.459029\n",
      "Epoch: 23864 \tTraining Loss: 1.498492 \tValidation Loss: 2.458713\n",
      "Epoch: 23865 \tTraining Loss: 1.477193 \tValidation Loss: 2.458478\n",
      "Epoch: 23866 \tTraining Loss: 1.446617 \tValidation Loss: 2.458443\n",
      "Epoch: 23867 \tTraining Loss: 1.441103 \tValidation Loss: 2.458681\n",
      "Epoch: 23868 \tTraining Loss: 1.408907 \tValidation Loss: 2.459301\n",
      "Epoch: 23869 \tTraining Loss: 1.418318 \tValidation Loss: 2.458994\n",
      "Epoch: 23870 \tTraining Loss: 1.487771 \tValidation Loss: 2.457962\n",
      "Epoch: 23871 \tTraining Loss: 1.427838 \tValidation Loss: 2.458598\n",
      "Epoch: 23872 \tTraining Loss: 1.433509 \tValidation Loss: 2.458885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23873 \tTraining Loss: 1.455011 \tValidation Loss: 2.458599\n",
      "Epoch: 23874 \tTraining Loss: 1.422338 \tValidation Loss: 2.459309\n",
      "Epoch: 23875 \tTraining Loss: 1.512873 \tValidation Loss: 2.458044\n",
      "Epoch: 23876 \tTraining Loss: 1.496380 \tValidation Loss: 2.459082\n",
      "Epoch: 23877 \tTraining Loss: 1.456010 \tValidation Loss: 2.459728\n",
      "Epoch: 23878 \tTraining Loss: 1.454601 \tValidation Loss: 2.458860\n",
      "Epoch: 23879 \tTraining Loss: 1.471343 \tValidation Loss: 2.459279\n",
      "Epoch: 23880 \tTraining Loss: 1.477653 \tValidation Loss: 2.458616\n",
      "Epoch: 23881 \tTraining Loss: 1.448842 \tValidation Loss: 2.458822\n",
      "Epoch: 23882 \tTraining Loss: 1.440603 \tValidation Loss: 2.459427\n",
      "Epoch: 23883 \tTraining Loss: 1.425202 \tValidation Loss: 2.459347\n",
      "Epoch: 23884 \tTraining Loss: 1.512694 \tValidation Loss: 2.458855\n",
      "Epoch: 23885 \tTraining Loss: 1.460953 \tValidation Loss: 2.458615\n",
      "Epoch: 23886 \tTraining Loss: 1.444110 \tValidation Loss: 2.459170\n",
      "Epoch: 23887 \tTraining Loss: 1.440465 \tValidation Loss: 2.459225\n",
      "Epoch: 23888 \tTraining Loss: 1.457593 \tValidation Loss: 2.460006\n",
      "Epoch: 23889 \tTraining Loss: 1.454768 \tValidation Loss: 2.458559\n",
      "Epoch: 23890 \tTraining Loss: 1.396497 \tValidation Loss: 2.460216\n",
      "Epoch: 23891 \tTraining Loss: 1.449305 \tValidation Loss: 2.459050\n",
      "Epoch: 23892 \tTraining Loss: 1.481922 \tValidation Loss: 2.458957\n",
      "Epoch: 23893 \tTraining Loss: 1.429298 \tValidation Loss: 2.458407\n",
      "Epoch: 23894 \tTraining Loss: 1.445633 \tValidation Loss: 2.458826\n",
      "Epoch: 23895 \tTraining Loss: 1.466573 \tValidation Loss: 2.458551\n",
      "Epoch: 23896 \tTraining Loss: 1.479015 \tValidation Loss: 2.459391\n",
      "Epoch: 23897 \tTraining Loss: 1.481364 \tValidation Loss: 2.459573\n",
      "Epoch: 23898 \tTraining Loss: 1.435309 \tValidation Loss: 2.458453\n",
      "Epoch: 23899 \tTraining Loss: 1.464689 \tValidation Loss: 2.459557\n",
      "Epoch: 23900 \tTraining Loss: 1.442008 \tValidation Loss: 2.459613\n",
      "Epoch: 23901 \tTraining Loss: 1.448126 \tValidation Loss: 2.460117\n",
      "Epoch: 23902 \tTraining Loss: 1.463352 \tValidation Loss: 2.459130\n",
      "Epoch: 23903 \tTraining Loss: 1.462122 \tValidation Loss: 2.458935\n",
      "Epoch: 23904 \tTraining Loss: 1.453839 \tValidation Loss: 2.459185\n",
      "Epoch: 23905 \tTraining Loss: 1.460258 \tValidation Loss: 2.459684\n",
      "Epoch: 23906 \tTraining Loss: 1.441308 \tValidation Loss: 2.459826\n",
      "Epoch: 23907 \tTraining Loss: 1.430803 \tValidation Loss: 2.460168\n",
      "Epoch: 23908 \tTraining Loss: 1.456753 \tValidation Loss: 2.459134\n",
      "Epoch: 23909 \tTraining Loss: 1.460242 \tValidation Loss: 2.458795\n",
      "Epoch: 23910 \tTraining Loss: 1.445191 \tValidation Loss: 2.458993\n",
      "Epoch: 23911 \tTraining Loss: 1.484244 \tValidation Loss: 2.458601\n",
      "Epoch: 23912 \tTraining Loss: 1.461064 \tValidation Loss: 2.459168\n",
      "Epoch: 23913 \tTraining Loss: 1.477765 \tValidation Loss: 2.459384\n",
      "Epoch: 23914 \tTraining Loss: 1.451761 \tValidation Loss: 2.459211\n",
      "Epoch: 23915 \tTraining Loss: 1.487754 \tValidation Loss: 2.459501\n",
      "Epoch: 23916 \tTraining Loss: 1.461089 \tValidation Loss: 2.459255\n",
      "Epoch: 23917 \tTraining Loss: 1.421682 \tValidation Loss: 2.459519\n",
      "Epoch: 23918 \tTraining Loss: 1.408482 \tValidation Loss: 2.458941\n",
      "Epoch: 23919 \tTraining Loss: 1.454253 \tValidation Loss: 2.458649\n",
      "Epoch: 23920 \tTraining Loss: 1.453940 \tValidation Loss: 2.458744\n",
      "Epoch: 23921 \tTraining Loss: 1.469496 \tValidation Loss: 2.459011\n",
      "Epoch: 23922 \tTraining Loss: 1.440721 \tValidation Loss: 2.459021\n",
      "Epoch: 23923 \tTraining Loss: 1.443027 \tValidation Loss: 2.459105\n",
      "Epoch: 23924 \tTraining Loss: 1.462652 \tValidation Loss: 2.458208\n",
      "Epoch: 23925 \tTraining Loss: 1.456076 \tValidation Loss: 2.458806\n",
      "Epoch: 23926 \tTraining Loss: 1.433459 \tValidation Loss: 2.459435\n",
      "Epoch: 23927 \tTraining Loss: 1.445945 \tValidation Loss: 2.458424\n",
      "Epoch: 23928 \tTraining Loss: 1.436681 \tValidation Loss: 2.459296\n",
      "Epoch: 23929 \tTraining Loss: 1.437804 \tValidation Loss: 2.459168\n",
      "Epoch: 23930 \tTraining Loss: 1.380498 \tValidation Loss: 2.460116\n",
      "Epoch: 23931 \tTraining Loss: 1.473695 \tValidation Loss: 2.459739\n",
      "Epoch: 23932 \tTraining Loss: 1.437475 \tValidation Loss: 2.460219\n",
      "Epoch: 23933 \tTraining Loss: 1.456375 \tValidation Loss: 2.460087\n",
      "Epoch: 23934 \tTraining Loss: 1.440164 \tValidation Loss: 2.459651\n",
      "Epoch: 23935 \tTraining Loss: 1.451728 \tValidation Loss: 2.459202\n",
      "Epoch: 23936 \tTraining Loss: 1.421780 \tValidation Loss: 2.460679\n",
      "Epoch: 23937 \tTraining Loss: 1.454826 \tValidation Loss: 2.459056\n",
      "Epoch: 23938 \tTraining Loss: 1.433911 \tValidation Loss: 2.459391\n",
      "Epoch: 23939 \tTraining Loss: 1.424693 \tValidation Loss: 2.459391\n",
      "Epoch: 23940 \tTraining Loss: 1.454301 \tValidation Loss: 2.459333\n",
      "Epoch: 23941 \tTraining Loss: 1.413258 \tValidation Loss: 2.459753\n",
      "Epoch: 23942 \tTraining Loss: 1.444169 \tValidation Loss: 2.460091\n",
      "Epoch: 23943 \tTraining Loss: 1.443340 \tValidation Loss: 2.458955\n",
      "Epoch: 23944 \tTraining Loss: 1.423153 \tValidation Loss: 2.459771\n",
      "Epoch: 23945 \tTraining Loss: 1.518200 \tValidation Loss: 2.458851\n",
      "Epoch: 23946 \tTraining Loss: 1.418452 \tValidation Loss: 2.458533\n",
      "Epoch: 23947 \tTraining Loss: 1.473301 \tValidation Loss: 2.459334\n",
      "Epoch: 23948 \tTraining Loss: 1.414582 \tValidation Loss: 2.460453\n",
      "Epoch: 23949 \tTraining Loss: 1.432920 \tValidation Loss: 2.460733\n",
      "Epoch: 23950 \tTraining Loss: 1.447531 \tValidation Loss: 2.460330\n",
      "Epoch: 23951 \tTraining Loss: 1.410629 \tValidation Loss: 2.460481\n",
      "Epoch: 23952 \tTraining Loss: 1.472519 \tValidation Loss: 2.460317\n",
      "Epoch: 23953 \tTraining Loss: 1.447258 \tValidation Loss: 2.460680\n",
      "Epoch: 23954 \tTraining Loss: 1.467160 \tValidation Loss: 2.460134\n",
      "Epoch: 23955 \tTraining Loss: 1.482631 \tValidation Loss: 2.460335\n",
      "Epoch: 23956 \tTraining Loss: 1.428282 \tValidation Loss: 2.461140\n",
      "Epoch: 23957 \tTraining Loss: 1.466834 \tValidation Loss: 2.459944\n",
      "Epoch: 23958 \tTraining Loss: 1.466747 \tValidation Loss: 2.460048\n",
      "Epoch: 23959 \tTraining Loss: 1.470108 \tValidation Loss: 2.460361\n",
      "Epoch: 23960 \tTraining Loss: 1.480671 \tValidation Loss: 2.460037\n",
      "Epoch: 23961 \tTraining Loss: 1.471715 \tValidation Loss: 2.460384\n",
      "Epoch: 23962 \tTraining Loss: 1.412809 \tValidation Loss: 2.460819\n",
      "Epoch: 23963 \tTraining Loss: 1.393203 \tValidation Loss: 2.460555\n",
      "Epoch: 23964 \tTraining Loss: 1.482961 \tValidation Loss: 2.459777\n",
      "Epoch: 23965 \tTraining Loss: 1.469486 \tValidation Loss: 2.459514\n",
      "Epoch: 23966 \tTraining Loss: 1.453337 \tValidation Loss: 2.459752\n",
      "Epoch: 23967 \tTraining Loss: 1.470182 \tValidation Loss: 2.460408\n",
      "Epoch: 23968 \tTraining Loss: 1.486295 \tValidation Loss: 2.459952\n",
      "Epoch: 23969 \tTraining Loss: 1.454366 \tValidation Loss: 2.461231\n",
      "Epoch: 23970 \tTraining Loss: 1.446413 \tValidation Loss: 2.460612\n",
      "Epoch: 23971 \tTraining Loss: 1.426278 \tValidation Loss: 2.461330\n",
      "Epoch: 23972 \tTraining Loss: 1.482339 \tValidation Loss: 2.460082\n",
      "Epoch: 23973 \tTraining Loss: 1.423863 \tValidation Loss: 2.460620\n",
      "Epoch: 23974 \tTraining Loss: 1.422767 \tValidation Loss: 2.461184\n",
      "Epoch: 23975 \tTraining Loss: 1.474213 \tValidation Loss: 2.461084\n",
      "Epoch: 23976 \tTraining Loss: 1.450211 \tValidation Loss: 2.460854\n",
      "Epoch: 23977 \tTraining Loss: 1.425695 \tValidation Loss: 2.461268\n",
      "Epoch: 23978 \tTraining Loss: 1.446249 \tValidation Loss: 2.460837\n",
      "Epoch: 23979 \tTraining Loss: 1.449257 \tValidation Loss: 2.459993\n",
      "Epoch: 23980 \tTraining Loss: 1.434878 \tValidation Loss: 2.460353\n",
      "Epoch: 23981 \tTraining Loss: 1.460796 \tValidation Loss: 2.460364\n",
      "Epoch: 23982 \tTraining Loss: 1.443224 \tValidation Loss: 2.460915\n",
      "Epoch: 23983 \tTraining Loss: 1.445539 \tValidation Loss: 2.460722\n",
      "Epoch: 23984 \tTraining Loss: 1.425556 \tValidation Loss: 2.460867\n",
      "Epoch: 23985 \tTraining Loss: 1.433179 \tValidation Loss: 2.460964\n",
      "Epoch: 23986 \tTraining Loss: 1.454842 \tValidation Loss: 2.460778\n",
      "Epoch: 23987 \tTraining Loss: 1.449186 \tValidation Loss: 2.460499\n",
      "Epoch: 23988 \tTraining Loss: 1.478571 \tValidation Loss: 2.460626\n",
      "Epoch: 23989 \tTraining Loss: 1.469431 \tValidation Loss: 2.460109\n",
      "Epoch: 23990 \tTraining Loss: 1.431302 \tValidation Loss: 2.460124\n",
      "Epoch: 23991 \tTraining Loss: 1.492320 \tValidation Loss: 2.460137\n",
      "Epoch: 23992 \tTraining Loss: 1.434583 \tValidation Loss: 2.460311\n",
      "Epoch: 23993 \tTraining Loss: 1.432598 \tValidation Loss: 2.460308\n",
      "Epoch: 23994 \tTraining Loss: 1.434355 \tValidation Loss: 2.461193\n",
      "Epoch: 23995 \tTraining Loss: 1.420493 \tValidation Loss: 2.461744\n",
      "Epoch: 23996 \tTraining Loss: 1.484079 \tValidation Loss: 2.461218\n",
      "Epoch: 23997 \tTraining Loss: 1.466605 \tValidation Loss: 2.460428\n",
      "Epoch: 23998 \tTraining Loss: 1.409115 \tValidation Loss: 2.461433\n",
      "Epoch: 23999 \tTraining Loss: 1.445176 \tValidation Loss: 2.461336\n",
      "Epoch: 24000 \tTraining Loss: 1.490169 \tValidation Loss: 2.460502\n",
      "Epoch: 24001 \tTraining Loss: 1.464709 \tValidation Loss: 2.461107\n",
      "Epoch: 24002 \tTraining Loss: 1.460497 \tValidation Loss: 2.460768\n",
      "Epoch: 24003 \tTraining Loss: 1.469734 \tValidation Loss: 2.460669\n",
      "Epoch: 24004 \tTraining Loss: 1.449725 \tValidation Loss: 2.461379\n",
      "Epoch: 24005 \tTraining Loss: 1.448486 \tValidation Loss: 2.461881\n",
      "Epoch: 24006 \tTraining Loss: 1.492018 \tValidation Loss: 2.460659\n",
      "Epoch: 24007 \tTraining Loss: 1.449967 \tValidation Loss: 2.461050\n",
      "Epoch: 24008 \tTraining Loss: 1.429423 \tValidation Loss: 2.461583\n",
      "Epoch: 24009 \tTraining Loss: 1.435538 \tValidation Loss: 2.460819\n",
      "Epoch: 24010 \tTraining Loss: 1.476413 \tValidation Loss: 2.460580\n",
      "Epoch: 24011 \tTraining Loss: 1.468008 \tValidation Loss: 2.460892\n",
      "Epoch: 24012 \tTraining Loss: 1.440335 \tValidation Loss: 2.461397\n",
      "Epoch: 24013 \tTraining Loss: 1.426362 \tValidation Loss: 2.461454\n",
      "Epoch: 24014 \tTraining Loss: 1.436521 \tValidation Loss: 2.461422\n",
      "Epoch: 24015 \tTraining Loss: 1.491795 \tValidation Loss: 2.461127\n",
      "Epoch: 24016 \tTraining Loss: 1.453099 \tValidation Loss: 2.461682\n",
      "Epoch: 24017 \tTraining Loss: 1.443465 \tValidation Loss: 2.460957\n",
      "Epoch: 24018 \tTraining Loss: 1.424684 \tValidation Loss: 2.462827\n",
      "Epoch: 24019 \tTraining Loss: 1.455053 \tValidation Loss: 2.462056\n",
      "Epoch: 24020 \tTraining Loss: 1.423062 \tValidation Loss: 2.462478\n",
      "Epoch: 24021 \tTraining Loss: 1.431052 \tValidation Loss: 2.461180\n",
      "Epoch: 24022 \tTraining Loss: 1.430172 \tValidation Loss: 2.461658\n",
      "Epoch: 24023 \tTraining Loss: 1.435163 \tValidation Loss: 2.461925\n",
      "Epoch: 24024 \tTraining Loss: 1.461771 \tValidation Loss: 2.461120\n",
      "Epoch: 24025 \tTraining Loss: 1.456596 \tValidation Loss: 2.461468\n",
      "Epoch: 24026 \tTraining Loss: 1.425192 \tValidation Loss: 2.461333\n",
      "Epoch: 24027 \tTraining Loss: 1.466404 \tValidation Loss: 2.460742\n",
      "Epoch: 24028 \tTraining Loss: 1.446596 \tValidation Loss: 2.460962\n",
      "Epoch: 24029 \tTraining Loss: 1.442532 \tValidation Loss: 2.460815\n",
      "Epoch: 24030 \tTraining Loss: 1.391381 \tValidation Loss: 2.461277\n",
      "Epoch: 24031 \tTraining Loss: 1.465288 \tValidation Loss: 2.462492\n",
      "Epoch: 24032 \tTraining Loss: 1.407575 \tValidation Loss: 2.462467\n",
      "Epoch: 24033 \tTraining Loss: 1.452856 \tValidation Loss: 2.461657\n",
      "Epoch: 24034 \tTraining Loss: 1.436969 \tValidation Loss: 2.462343\n",
      "Epoch: 24035 \tTraining Loss: 1.424376 \tValidation Loss: 2.462405\n",
      "Epoch: 24036 \tTraining Loss: 1.462336 \tValidation Loss: 2.461995\n",
      "Epoch: 24037 \tTraining Loss: 1.461616 \tValidation Loss: 2.461884\n",
      "Epoch: 24038 \tTraining Loss: 1.453110 \tValidation Loss: 2.460329\n",
      "Epoch: 24039 \tTraining Loss: 1.455855 \tValidation Loss: 2.461369\n",
      "Epoch: 24040 \tTraining Loss: 1.449702 \tValidation Loss: 2.461625\n",
      "Epoch: 24041 \tTraining Loss: 1.435634 \tValidation Loss: 2.461448\n",
      "Epoch: 24042 \tTraining Loss: 1.461276 \tValidation Loss: 2.461729\n",
      "Epoch: 24043 \tTraining Loss: 1.394058 \tValidation Loss: 2.463223\n",
      "Epoch: 24044 \tTraining Loss: 1.410039 \tValidation Loss: 2.462730\n",
      "Epoch: 24045 \tTraining Loss: 1.448137 \tValidation Loss: 2.461331\n",
      "Epoch: 24046 \tTraining Loss: 1.436419 \tValidation Loss: 2.460943\n",
      "Epoch: 24047 \tTraining Loss: 1.477511 \tValidation Loss: 2.461806\n",
      "Epoch: 24048 \tTraining Loss: 1.489937 \tValidation Loss: 2.461958\n",
      "Epoch: 24049 \tTraining Loss: 1.428525 \tValidation Loss: 2.462403\n",
      "Epoch: 24050 \tTraining Loss: 1.456815 \tValidation Loss: 2.461938\n",
      "Epoch: 24051 \tTraining Loss: 1.448043 \tValidation Loss: 2.461860\n",
      "Epoch: 24052 \tTraining Loss: 1.443410 \tValidation Loss: 2.461741\n",
      "Epoch: 24053 \tTraining Loss: 1.418786 \tValidation Loss: 2.462175\n",
      "Epoch: 24054 \tTraining Loss: 1.448378 \tValidation Loss: 2.462874\n",
      "Epoch: 24055 \tTraining Loss: 1.456176 \tValidation Loss: 2.461709\n",
      "Epoch: 24056 \tTraining Loss: 1.452300 \tValidation Loss: 2.462516\n",
      "Epoch: 24057 \tTraining Loss: 1.464609 \tValidation Loss: 2.462726\n",
      "Epoch: 24058 \tTraining Loss: 1.418935 \tValidation Loss: 2.462244\n",
      "Epoch: 24059 \tTraining Loss: 1.491287 \tValidation Loss: 2.461222\n",
      "Epoch: 24060 \tTraining Loss: 1.482949 \tValidation Loss: 2.461451\n",
      "Epoch: 24061 \tTraining Loss: 1.477511 \tValidation Loss: 2.461379\n",
      "Epoch: 24062 \tTraining Loss: 1.411819 \tValidation Loss: 2.462504\n",
      "Epoch: 24063 \tTraining Loss: 1.446165 \tValidation Loss: 2.462771\n",
      "Epoch: 24064 \tTraining Loss: 1.422874 \tValidation Loss: 2.462888\n",
      "Epoch: 24065 \tTraining Loss: 1.455979 \tValidation Loss: 2.462341\n",
      "Epoch: 24066 \tTraining Loss: 1.432048 \tValidation Loss: 2.463008\n",
      "Epoch: 24067 \tTraining Loss: 1.411517 \tValidation Loss: 2.462036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24068 \tTraining Loss: 1.417007 \tValidation Loss: 2.462428\n",
      "Epoch: 24069 \tTraining Loss: 1.403527 \tValidation Loss: 2.462644\n",
      "Epoch: 24070 \tTraining Loss: 1.444826 \tValidation Loss: 2.463134\n",
      "Epoch: 24071 \tTraining Loss: 1.471048 \tValidation Loss: 2.463634\n",
      "Epoch: 24072 \tTraining Loss: 1.407744 \tValidation Loss: 2.463001\n",
      "Epoch: 24073 \tTraining Loss: 1.451239 \tValidation Loss: 2.463430\n",
      "Epoch: 24074 \tTraining Loss: 1.443264 \tValidation Loss: 2.462675\n",
      "Epoch: 24075 \tTraining Loss: 1.404993 \tValidation Loss: 2.463989\n",
      "Epoch: 24076 \tTraining Loss: 1.368920 \tValidation Loss: 2.463663\n",
      "Epoch: 24077 \tTraining Loss: 1.481737 \tValidation Loss: 2.463639\n",
      "Epoch: 24078 \tTraining Loss: 1.420742 \tValidation Loss: 2.462315\n",
      "Epoch: 24079 \tTraining Loss: 1.445701 \tValidation Loss: 2.462744\n",
      "Epoch: 24080 \tTraining Loss: 1.430656 \tValidation Loss: 2.462494\n",
      "Epoch: 24081 \tTraining Loss: 1.375504 \tValidation Loss: 2.462901\n",
      "Epoch: 24082 \tTraining Loss: 1.470307 \tValidation Loss: 2.462207\n",
      "Epoch: 24083 \tTraining Loss: 1.423907 \tValidation Loss: 2.462574\n",
      "Epoch: 24084 \tTraining Loss: 1.446697 \tValidation Loss: 2.462883\n",
      "Epoch: 24085 \tTraining Loss: 1.418345 \tValidation Loss: 2.462931\n",
      "Epoch: 24086 \tTraining Loss: 1.405480 \tValidation Loss: 2.463138\n",
      "Epoch: 24087 \tTraining Loss: 1.492291 \tValidation Loss: 2.462066\n",
      "Epoch: 24088 \tTraining Loss: 1.427800 \tValidation Loss: 2.461777\n",
      "Epoch: 24089 \tTraining Loss: 1.443850 \tValidation Loss: 2.462816\n",
      "Epoch: 24090 \tTraining Loss: 1.446880 \tValidation Loss: 2.463319\n",
      "Epoch: 24091 \tTraining Loss: 1.441282 \tValidation Loss: 2.462837\n",
      "Epoch: 24092 \tTraining Loss: 1.437353 \tValidation Loss: 2.463493\n",
      "Epoch: 24093 \tTraining Loss: 1.466550 \tValidation Loss: 2.463825\n",
      "Epoch: 24094 \tTraining Loss: 1.448182 \tValidation Loss: 2.463011\n",
      "Epoch: 24095 \tTraining Loss: 1.444555 \tValidation Loss: 2.462979\n",
      "Epoch: 24096 \tTraining Loss: 1.481494 \tValidation Loss: 2.462583\n",
      "Epoch: 24097 \tTraining Loss: 1.459484 \tValidation Loss: 2.462593\n",
      "Epoch: 24098 \tTraining Loss: 1.419997 \tValidation Loss: 2.463832\n",
      "Epoch: 24099 \tTraining Loss: 1.426125 \tValidation Loss: 2.462865\n",
      "Epoch: 24100 \tTraining Loss: 1.486183 \tValidation Loss: 2.462960\n",
      "Epoch: 24101 \tTraining Loss: 1.459333 \tValidation Loss: 2.463326\n",
      "Epoch: 24102 \tTraining Loss: 1.418791 \tValidation Loss: 2.463003\n",
      "Epoch: 24103 \tTraining Loss: 1.432535 \tValidation Loss: 2.463453\n",
      "Epoch: 24104 \tTraining Loss: 1.407745 \tValidation Loss: 2.463159\n",
      "Epoch: 24105 \tTraining Loss: 1.470969 \tValidation Loss: 2.463755\n",
      "Epoch: 24106 \tTraining Loss: 1.418112 \tValidation Loss: 2.463828\n",
      "Epoch: 24107 \tTraining Loss: 1.453464 \tValidation Loss: 2.463024\n",
      "Epoch: 24108 \tTraining Loss: 1.443740 \tValidation Loss: 2.464288\n",
      "Epoch: 24109 \tTraining Loss: 1.437532 \tValidation Loss: 2.463578\n",
      "Epoch: 24110 \tTraining Loss: 1.457632 \tValidation Loss: 2.464341\n",
      "Epoch: 24111 \tTraining Loss: 1.439941 \tValidation Loss: 2.464561\n",
      "Epoch: 24112 \tTraining Loss: 1.478797 \tValidation Loss: 2.464463\n",
      "Epoch: 24113 \tTraining Loss: 1.465392 \tValidation Loss: 2.463624\n",
      "Epoch: 24114 \tTraining Loss: 1.456045 \tValidation Loss: 2.463983\n",
      "Epoch: 24115 \tTraining Loss: 1.458690 \tValidation Loss: 2.463786\n",
      "Epoch: 24116 \tTraining Loss: 1.476939 \tValidation Loss: 2.463722\n",
      "Epoch: 24117 \tTraining Loss: 1.441501 \tValidation Loss: 2.463756\n",
      "Epoch: 24118 \tTraining Loss: 1.466918 \tValidation Loss: 2.462982\n",
      "Epoch: 24119 \tTraining Loss: 1.411337 \tValidation Loss: 2.463920\n",
      "Epoch: 24120 \tTraining Loss: 1.468877 \tValidation Loss: 2.463491\n",
      "Epoch: 24121 \tTraining Loss: 1.431920 \tValidation Loss: 2.463947\n",
      "Epoch: 24122 \tTraining Loss: 1.468354 \tValidation Loss: 2.463936\n",
      "Epoch: 24123 \tTraining Loss: 1.464727 \tValidation Loss: 2.463356\n",
      "Epoch: 24124 \tTraining Loss: 1.447162 \tValidation Loss: 2.463651\n",
      "Epoch: 24125 \tTraining Loss: 1.431921 \tValidation Loss: 2.464460\n",
      "Epoch: 24126 \tTraining Loss: 1.444620 \tValidation Loss: 2.463921\n",
      "Epoch: 24127 \tTraining Loss: 1.414976 \tValidation Loss: 2.464454\n",
      "Epoch: 24128 \tTraining Loss: 1.427932 \tValidation Loss: 2.465001\n",
      "Epoch: 24129 \tTraining Loss: 1.446460 \tValidation Loss: 2.464120\n",
      "Epoch: 24130 \tTraining Loss: 1.452898 \tValidation Loss: 2.464643\n",
      "Epoch: 24131 \tTraining Loss: 1.447727 \tValidation Loss: 2.464257\n",
      "Epoch: 24132 \tTraining Loss: 1.499964 \tValidation Loss: 2.462987\n",
      "Epoch: 24133 \tTraining Loss: 1.415897 \tValidation Loss: 2.463744\n",
      "Epoch: 24134 \tTraining Loss: 1.441725 \tValidation Loss: 2.464167\n",
      "Epoch: 24135 \tTraining Loss: 1.445830 \tValidation Loss: 2.463290\n",
      "Epoch: 24136 \tTraining Loss: 1.446825 \tValidation Loss: 2.464368\n",
      "Epoch: 24137 \tTraining Loss: 1.419418 \tValidation Loss: 2.464314\n",
      "Epoch: 24138 \tTraining Loss: 1.424933 \tValidation Loss: 2.463702\n",
      "Epoch: 24139 \tTraining Loss: 1.449368 \tValidation Loss: 2.464243\n",
      "Epoch: 24140 \tTraining Loss: 1.451687 \tValidation Loss: 2.464144\n",
      "Epoch: 24141 \tTraining Loss: 1.466389 \tValidation Loss: 2.463328\n",
      "Epoch: 24142 \tTraining Loss: 1.441802 \tValidation Loss: 2.463723\n",
      "Epoch: 24143 \tTraining Loss: 1.473086 \tValidation Loss: 2.463487\n",
      "Epoch: 24144 \tTraining Loss: 1.437073 \tValidation Loss: 2.464792\n",
      "Epoch: 24145 \tTraining Loss: 1.408781 \tValidation Loss: 2.463422\n",
      "Epoch: 24146 \tTraining Loss: 1.443777 \tValidation Loss: 2.463356\n",
      "Epoch: 24147 \tTraining Loss: 1.443621 \tValidation Loss: 2.464283\n",
      "Epoch: 24148 \tTraining Loss: 1.439728 \tValidation Loss: 2.463783\n",
      "Epoch: 24149 \tTraining Loss: 1.439199 \tValidation Loss: 2.464675\n",
      "Epoch: 24150 \tTraining Loss: 1.474084 \tValidation Loss: 2.464017\n",
      "Epoch: 24151 \tTraining Loss: 1.457126 \tValidation Loss: 2.463160\n",
      "Epoch: 24152 \tTraining Loss: 1.417415 \tValidation Loss: 2.463626\n",
      "Epoch: 24153 \tTraining Loss: 1.414776 \tValidation Loss: 2.464001\n",
      "Epoch: 24154 \tTraining Loss: 1.505936 \tValidation Loss: 2.464524\n",
      "Epoch: 24155 \tTraining Loss: 1.425373 \tValidation Loss: 2.464465\n",
      "Epoch: 24156 \tTraining Loss: 1.449456 \tValidation Loss: 2.465068\n",
      "Epoch: 24157 \tTraining Loss: 1.464291 \tValidation Loss: 2.463644\n",
      "Epoch: 24158 \tTraining Loss: 1.463731 \tValidation Loss: 2.463026\n",
      "Epoch: 24159 \tTraining Loss: 1.377203 \tValidation Loss: 2.464164\n",
      "Epoch: 24160 \tTraining Loss: 1.411913 \tValidation Loss: 2.464425\n",
      "Epoch: 24161 \tTraining Loss: 1.412500 \tValidation Loss: 2.465025\n",
      "Epoch: 24162 \tTraining Loss: 1.447596 \tValidation Loss: 2.465038\n",
      "Epoch: 24163 \tTraining Loss: 1.446136 \tValidation Loss: 2.464065\n",
      "Epoch: 24164 \tTraining Loss: 1.427080 \tValidation Loss: 2.464778\n",
      "Epoch: 24165 \tTraining Loss: 1.469560 \tValidation Loss: 2.464455\n",
      "Epoch: 24166 \tTraining Loss: 1.485636 \tValidation Loss: 2.464715\n",
      "Epoch: 24167 \tTraining Loss: 1.435583 \tValidation Loss: 2.464864\n",
      "Epoch: 24168 \tTraining Loss: 1.463291 \tValidation Loss: 2.465132\n",
      "Epoch: 24169 \tTraining Loss: 1.438509 \tValidation Loss: 2.464824\n",
      "Epoch: 24170 \tTraining Loss: 1.427047 \tValidation Loss: 2.465119\n",
      "Epoch: 24171 \tTraining Loss: 1.417359 \tValidation Loss: 2.465194\n",
      "Epoch: 24172 \tTraining Loss: 1.413646 \tValidation Loss: 2.465484\n",
      "Epoch: 24173 \tTraining Loss: 1.453281 \tValidation Loss: 2.464165\n",
      "Epoch: 24174 \tTraining Loss: 1.438210 \tValidation Loss: 2.465220\n",
      "Epoch: 24175 \tTraining Loss: 1.444707 \tValidation Loss: 2.464052\n",
      "Epoch: 24176 \tTraining Loss: 1.481651 \tValidation Loss: 2.464185\n",
      "Epoch: 24177 \tTraining Loss: 1.385505 \tValidation Loss: 2.464712\n",
      "Epoch: 24178 \tTraining Loss: 1.429539 \tValidation Loss: 2.464223\n",
      "Epoch: 24179 \tTraining Loss: 1.451956 \tValidation Loss: 2.464750\n",
      "Epoch: 24180 \tTraining Loss: 1.395205 \tValidation Loss: 2.464364\n",
      "Epoch: 24181 \tTraining Loss: 1.438855 \tValidation Loss: 2.464844\n",
      "Epoch: 24182 \tTraining Loss: 1.452390 \tValidation Loss: 2.464169\n",
      "Epoch: 24183 \tTraining Loss: 1.448737 \tValidation Loss: 2.465256\n",
      "Epoch: 24184 \tTraining Loss: 1.471937 \tValidation Loss: 2.464733\n",
      "Epoch: 24185 \tTraining Loss: 1.421874 \tValidation Loss: 2.464703\n",
      "Epoch: 24186 \tTraining Loss: 1.429518 \tValidation Loss: 2.465000\n",
      "Epoch: 24187 \tTraining Loss: 1.454421 \tValidation Loss: 2.464803\n",
      "Epoch: 24188 \tTraining Loss: 1.433788 \tValidation Loss: 2.465466\n",
      "Epoch: 24189 \tTraining Loss: 1.422967 \tValidation Loss: 2.465447\n",
      "Epoch: 24190 \tTraining Loss: 1.403612 \tValidation Loss: 2.465067\n",
      "Epoch: 24191 \tTraining Loss: 1.439175 \tValidation Loss: 2.466211\n",
      "Epoch: 24192 \tTraining Loss: 1.454669 \tValidation Loss: 2.465101\n",
      "Epoch: 24193 \tTraining Loss: 1.402175 \tValidation Loss: 2.465500\n",
      "Epoch: 24194 \tTraining Loss: 1.440172 \tValidation Loss: 2.465081\n",
      "Epoch: 24195 \tTraining Loss: 1.429588 \tValidation Loss: 2.466044\n",
      "Epoch: 24196 \tTraining Loss: 1.447706 \tValidation Loss: 2.466052\n",
      "Epoch: 24197 \tTraining Loss: 1.489692 \tValidation Loss: 2.465557\n",
      "Epoch: 24198 \tTraining Loss: 1.443746 \tValidation Loss: 2.465136\n",
      "Epoch: 24199 \tTraining Loss: 1.505110 \tValidation Loss: 2.464895\n",
      "Epoch: 24200 \tTraining Loss: 1.455901 \tValidation Loss: 2.465396\n",
      "Epoch: 24201 \tTraining Loss: 1.421456 \tValidation Loss: 2.465644\n",
      "Epoch: 24202 \tTraining Loss: 1.455548 \tValidation Loss: 2.466161\n",
      "Epoch: 24203 \tTraining Loss: 1.456012 \tValidation Loss: 2.464818\n",
      "Epoch: 24204 \tTraining Loss: 1.426591 \tValidation Loss: 2.465073\n",
      "Epoch: 24205 \tTraining Loss: 1.431910 \tValidation Loss: 2.465792\n",
      "Epoch: 24206 \tTraining Loss: 1.447338 \tValidation Loss: 2.465394\n",
      "Epoch: 24207 \tTraining Loss: 1.460534 \tValidation Loss: 2.465815\n",
      "Epoch: 24208 \tTraining Loss: 1.452164 \tValidation Loss: 2.466296\n",
      "Epoch: 24209 \tTraining Loss: 1.411230 \tValidation Loss: 2.464871\n",
      "Epoch: 24210 \tTraining Loss: 1.452828 \tValidation Loss: 2.465509\n",
      "Epoch: 24211 \tTraining Loss: 1.474590 \tValidation Loss: 2.465346\n",
      "Epoch: 24212 \tTraining Loss: 1.401380 \tValidation Loss: 2.465929\n",
      "Epoch: 24213 \tTraining Loss: 1.448658 \tValidation Loss: 2.465265\n",
      "Epoch: 24214 \tTraining Loss: 1.459691 \tValidation Loss: 2.464392\n",
      "Epoch: 24215 \tTraining Loss: 1.454362 \tValidation Loss: 2.464774\n",
      "Epoch: 24216 \tTraining Loss: 1.457597 \tValidation Loss: 2.464656\n",
      "Epoch: 24217 \tTraining Loss: 1.448861 \tValidation Loss: 2.465854\n",
      "Epoch: 24218 \tTraining Loss: 1.423675 \tValidation Loss: 2.466368\n",
      "Epoch: 24219 \tTraining Loss: 1.412304 \tValidation Loss: 2.466362\n",
      "Epoch: 24220 \tTraining Loss: 1.444342 \tValidation Loss: 2.466763\n",
      "Epoch: 24221 \tTraining Loss: 1.439494 \tValidation Loss: 2.465199\n",
      "Epoch: 24222 \tTraining Loss: 1.447135 \tValidation Loss: 2.465821\n",
      "Epoch: 24223 \tTraining Loss: 1.407364 \tValidation Loss: 2.465895\n",
      "Epoch: 24224 \tTraining Loss: 1.461539 \tValidation Loss: 2.465508\n",
      "Epoch: 24225 \tTraining Loss: 1.450221 \tValidation Loss: 2.465718\n",
      "Epoch: 24226 \tTraining Loss: 1.417408 \tValidation Loss: 2.466286\n",
      "Epoch: 24227 \tTraining Loss: 1.433364 \tValidation Loss: 2.465908\n",
      "Epoch: 24228 \tTraining Loss: 1.420130 \tValidation Loss: 2.465850\n",
      "Epoch: 24229 \tTraining Loss: 1.393348 \tValidation Loss: 2.465549\n",
      "Epoch: 24230 \tTraining Loss: 1.440549 \tValidation Loss: 2.465769\n",
      "Epoch: 24231 \tTraining Loss: 1.400528 \tValidation Loss: 2.466569\n",
      "Epoch: 24232 \tTraining Loss: 1.413514 \tValidation Loss: 2.466331\n",
      "Epoch: 24233 \tTraining Loss: 1.421397 \tValidation Loss: 2.467226\n",
      "Epoch: 24234 \tTraining Loss: 1.445270 \tValidation Loss: 2.467054\n",
      "Epoch: 24235 \tTraining Loss: 1.469505 \tValidation Loss: 2.465653\n",
      "Epoch: 24236 \tTraining Loss: 1.462929 \tValidation Loss: 2.465821\n",
      "Epoch: 24237 \tTraining Loss: 1.460294 \tValidation Loss: 2.466707\n",
      "Epoch: 24238 \tTraining Loss: 1.433192 \tValidation Loss: 2.465635\n",
      "Epoch: 24239 \tTraining Loss: 1.475240 \tValidation Loss: 2.464878\n",
      "Epoch: 24240 \tTraining Loss: 1.430028 \tValidation Loss: 2.466584\n",
      "Epoch: 24241 \tTraining Loss: 1.378915 \tValidation Loss: 2.467427\n",
      "Epoch: 24242 \tTraining Loss: 1.433310 \tValidation Loss: 2.466565\n",
      "Epoch: 24243 \tTraining Loss: 1.453889 \tValidation Loss: 2.465963\n",
      "Epoch: 24244 \tTraining Loss: 1.485075 \tValidation Loss: 2.466145\n",
      "Epoch: 24245 \tTraining Loss: 1.464687 \tValidation Loss: 2.465311\n",
      "Epoch: 24246 \tTraining Loss: 1.493007 \tValidation Loss: 2.465306\n",
      "Epoch: 24247 \tTraining Loss: 1.460679 \tValidation Loss: 2.467104\n",
      "Epoch: 24248 \tTraining Loss: 1.463797 \tValidation Loss: 2.465285\n",
      "Epoch: 24249 \tTraining Loss: 1.469625 \tValidation Loss: 2.465590\n",
      "Epoch: 24250 \tTraining Loss: 1.441018 \tValidation Loss: 2.466283\n",
      "Epoch: 24251 \tTraining Loss: 1.455391 \tValidation Loss: 2.466120\n",
      "Epoch: 24252 \tTraining Loss: 1.418984 \tValidation Loss: 2.465387\n",
      "Epoch: 24253 \tTraining Loss: 1.453610 \tValidation Loss: 2.465153\n",
      "Epoch: 24254 \tTraining Loss: 1.467412 \tValidation Loss: 2.465366\n",
      "Epoch: 24255 \tTraining Loss: 1.436287 \tValidation Loss: 2.465838\n",
      "Epoch: 24256 \tTraining Loss: 1.428673 \tValidation Loss: 2.465795\n",
      "Epoch: 24257 \tTraining Loss: 1.454909 \tValidation Loss: 2.465929\n",
      "Epoch: 24258 \tTraining Loss: 1.429399 \tValidation Loss: 2.466124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24259 \tTraining Loss: 1.445792 \tValidation Loss: 2.466559\n",
      "Epoch: 24260 \tTraining Loss: 1.420690 \tValidation Loss: 2.466616\n",
      "Epoch: 24261 \tTraining Loss: 1.443884 \tValidation Loss: 2.467107\n",
      "Epoch: 24262 \tTraining Loss: 1.407998 \tValidation Loss: 2.467113\n",
      "Epoch: 24263 \tTraining Loss: 1.483511 \tValidation Loss: 2.466423\n",
      "Epoch: 24264 \tTraining Loss: 1.454085 \tValidation Loss: 2.465892\n",
      "Epoch: 24265 \tTraining Loss: 1.428947 \tValidation Loss: 2.467827\n",
      "Epoch: 24266 \tTraining Loss: 1.458663 \tValidation Loss: 2.466260\n",
      "Epoch: 24267 \tTraining Loss: 1.426067 \tValidation Loss: 2.466450\n",
      "Epoch: 24268 \tTraining Loss: 1.419414 \tValidation Loss: 2.466651\n",
      "Epoch: 24269 \tTraining Loss: 1.493450 \tValidation Loss: 2.466295\n",
      "Epoch: 24270 \tTraining Loss: 1.443912 \tValidation Loss: 2.466846\n",
      "Epoch: 24271 \tTraining Loss: 1.442177 \tValidation Loss: 2.467657\n",
      "Epoch: 24272 \tTraining Loss: 1.475217 \tValidation Loss: 2.467138\n",
      "Epoch: 24273 \tTraining Loss: 1.468446 \tValidation Loss: 2.467338\n",
      "Epoch: 24274 \tTraining Loss: 1.474285 \tValidation Loss: 2.465987\n",
      "Epoch: 24275 \tTraining Loss: 1.428351 \tValidation Loss: 2.466787\n",
      "Epoch: 24276 \tTraining Loss: 1.453575 \tValidation Loss: 2.466839\n",
      "Epoch: 24277 \tTraining Loss: 1.480902 \tValidation Loss: 2.465998\n",
      "Epoch: 24278 \tTraining Loss: 1.427575 \tValidation Loss: 2.465622\n",
      "Epoch: 24279 \tTraining Loss: 1.431254 \tValidation Loss: 2.466299\n",
      "Epoch: 24280 \tTraining Loss: 1.420615 \tValidation Loss: 2.467057\n",
      "Epoch: 24281 \tTraining Loss: 1.399825 \tValidation Loss: 2.466170\n",
      "Epoch: 24282 \tTraining Loss: 1.452970 \tValidation Loss: 2.467432\n",
      "Epoch: 24283 \tTraining Loss: 1.386102 \tValidation Loss: 2.467761\n",
      "Epoch: 24284 \tTraining Loss: 1.381475 \tValidation Loss: 2.467555\n",
      "Epoch: 24285 \tTraining Loss: 1.448543 \tValidation Loss: 2.466830\n",
      "Epoch: 24286 \tTraining Loss: 1.459301 \tValidation Loss: 2.466877\n",
      "Epoch: 24287 \tTraining Loss: 1.452216 \tValidation Loss: 2.466213\n",
      "Epoch: 24288 \tTraining Loss: 1.490574 \tValidation Loss: 2.466712\n",
      "Epoch: 24289 \tTraining Loss: 1.435594 \tValidation Loss: 2.467321\n",
      "Epoch: 24290 \tTraining Loss: 1.408135 \tValidation Loss: 2.468047\n",
      "Epoch: 24291 \tTraining Loss: 1.435942 \tValidation Loss: 2.467570\n",
      "Epoch: 24292 \tTraining Loss: 1.437256 \tValidation Loss: 2.467259\n",
      "Epoch: 24293 \tTraining Loss: 1.444578 \tValidation Loss: 2.466167\n",
      "Epoch: 24294 \tTraining Loss: 1.432390 \tValidation Loss: 2.467086\n",
      "Epoch: 24295 \tTraining Loss: 1.441344 \tValidation Loss: 2.467293\n",
      "Epoch: 24296 \tTraining Loss: 1.424711 \tValidation Loss: 2.466121\n",
      "Epoch: 24297 \tTraining Loss: 1.474532 \tValidation Loss: 2.466276\n",
      "Epoch: 24298 \tTraining Loss: 1.430865 \tValidation Loss: 2.467832\n",
      "Epoch: 24299 \tTraining Loss: 1.440479 \tValidation Loss: 2.466723\n",
      "Epoch: 24300 \tTraining Loss: 1.469127 \tValidation Loss: 2.467468\n",
      "Epoch: 24301 \tTraining Loss: 1.401356 \tValidation Loss: 2.468224\n",
      "Epoch: 24302 \tTraining Loss: 1.448310 \tValidation Loss: 2.467647\n",
      "Epoch: 24303 \tTraining Loss: 1.436626 \tValidation Loss: 2.466534\n",
      "Epoch: 24304 \tTraining Loss: 1.419062 \tValidation Loss: 2.467059\n",
      "Epoch: 24305 \tTraining Loss: 1.466210 \tValidation Loss: 2.467202\n",
      "Epoch: 24306 \tTraining Loss: 1.437718 \tValidation Loss: 2.467267\n",
      "Epoch: 24307 \tTraining Loss: 1.446892 \tValidation Loss: 2.467588\n",
      "Epoch: 24308 \tTraining Loss: 1.432790 \tValidation Loss: 2.467583\n",
      "Epoch: 24309 \tTraining Loss: 1.473655 \tValidation Loss: 2.467877\n",
      "Epoch: 24310 \tTraining Loss: 1.467475 \tValidation Loss: 2.466933\n",
      "Epoch: 24311 \tTraining Loss: 1.421416 \tValidation Loss: 2.468195\n",
      "Epoch: 24312 \tTraining Loss: 1.433076 \tValidation Loss: 2.468117\n",
      "Epoch: 24313 \tTraining Loss: 1.448667 \tValidation Loss: 2.467953\n",
      "Epoch: 24314 \tTraining Loss: 1.418116 \tValidation Loss: 2.468144\n",
      "Epoch: 24315 \tTraining Loss: 1.435954 \tValidation Loss: 2.467801\n",
      "Epoch: 24316 \tTraining Loss: 1.471274 \tValidation Loss: 2.468064\n",
      "Epoch: 24317 \tTraining Loss: 1.407643 \tValidation Loss: 2.469481\n",
      "Epoch: 24318 \tTraining Loss: 1.438882 \tValidation Loss: 2.467882\n",
      "Epoch: 24319 \tTraining Loss: 1.444413 \tValidation Loss: 2.466972\n",
      "Epoch: 24320 \tTraining Loss: 1.394330 \tValidation Loss: 2.467461\n",
      "Epoch: 24321 \tTraining Loss: 1.421723 \tValidation Loss: 2.468314\n",
      "Epoch: 24322 \tTraining Loss: 1.446402 \tValidation Loss: 2.467124\n",
      "Epoch: 24323 \tTraining Loss: 1.457036 \tValidation Loss: 2.467008\n",
      "Epoch: 24324 \tTraining Loss: 1.421475 \tValidation Loss: 2.467153\n",
      "Epoch: 24325 \tTraining Loss: 1.459221 \tValidation Loss: 2.467813\n",
      "Epoch: 24326 \tTraining Loss: 1.448248 \tValidation Loss: 2.467805\n",
      "Epoch: 24327 \tTraining Loss: 1.426546 \tValidation Loss: 2.467458\n",
      "Epoch: 24328 \tTraining Loss: 1.434512 \tValidation Loss: 2.468694\n",
      "Epoch: 24329 \tTraining Loss: 1.436547 \tValidation Loss: 2.467158\n",
      "Epoch: 24330 \tTraining Loss: 1.378000 \tValidation Loss: 2.469450\n",
      "Epoch: 24331 \tTraining Loss: 1.420033 \tValidation Loss: 2.468769\n",
      "Epoch: 24332 \tTraining Loss: 1.440606 \tValidation Loss: 2.467993\n",
      "Epoch: 24333 \tTraining Loss: 1.413088 \tValidation Loss: 2.468213\n",
      "Epoch: 24334 \tTraining Loss: 1.489815 \tValidation Loss: 2.468546\n",
      "Epoch: 24335 \tTraining Loss: 1.436883 \tValidation Loss: 2.467757\n",
      "Epoch: 24336 \tTraining Loss: 1.445513 \tValidation Loss: 2.468334\n",
      "Epoch: 24337 \tTraining Loss: 1.439142 \tValidation Loss: 2.467691\n",
      "Epoch: 24338 \tTraining Loss: 1.396463 \tValidation Loss: 2.467662\n",
      "Epoch: 24339 \tTraining Loss: 1.394588 \tValidation Loss: 2.468527\n",
      "Epoch: 24340 \tTraining Loss: 1.426065 \tValidation Loss: 2.469003\n",
      "Epoch: 24341 \tTraining Loss: 1.445344 \tValidation Loss: 2.468162\n",
      "Epoch: 24342 \tTraining Loss: 1.416790 \tValidation Loss: 2.468057\n",
      "Epoch: 24343 \tTraining Loss: 1.435312 \tValidation Loss: 2.467992\n",
      "Epoch: 24344 \tTraining Loss: 1.423570 \tValidation Loss: 2.468213\n",
      "Epoch: 24345 \tTraining Loss: 1.424452 \tValidation Loss: 2.468774\n",
      "Epoch: 24346 \tTraining Loss: 1.429773 \tValidation Loss: 2.469128\n",
      "Epoch: 24347 \tTraining Loss: 1.439683 \tValidation Loss: 2.468360\n",
      "Epoch: 24348 \tTraining Loss: 1.463145 \tValidation Loss: 2.468361\n",
      "Epoch: 24349 \tTraining Loss: 1.470243 \tValidation Loss: 2.467804\n",
      "Epoch: 24350 \tTraining Loss: 1.468012 \tValidation Loss: 2.467948\n",
      "Epoch: 24351 \tTraining Loss: 1.422280 \tValidation Loss: 2.468159\n",
      "Epoch: 24352 \tTraining Loss: 1.391841 \tValidation Loss: 2.468121\n",
      "Epoch: 24353 \tTraining Loss: 1.444066 \tValidation Loss: 2.468245\n",
      "Epoch: 24354 \tTraining Loss: 1.450104 \tValidation Loss: 2.468419\n",
      "Epoch: 24355 \tTraining Loss: 1.442073 \tValidation Loss: 2.468534\n",
      "Epoch: 24356 \tTraining Loss: 1.436975 \tValidation Loss: 2.468098\n",
      "Epoch: 24357 \tTraining Loss: 1.389395 \tValidation Loss: 2.469126\n",
      "Epoch: 24358 \tTraining Loss: 1.422015 \tValidation Loss: 2.467978\n",
      "Epoch: 24359 \tTraining Loss: 1.445019 \tValidation Loss: 2.468443\n",
      "Epoch: 24360 \tTraining Loss: 1.468333 \tValidation Loss: 2.468290\n",
      "Epoch: 24361 \tTraining Loss: 1.449253 \tValidation Loss: 2.467548\n",
      "Epoch: 24362 \tTraining Loss: 1.457536 \tValidation Loss: 2.467633\n",
      "Epoch: 24363 \tTraining Loss: 1.429694 \tValidation Loss: 2.468916\n",
      "Epoch: 24364 \tTraining Loss: 1.461450 \tValidation Loss: 2.468114\n",
      "Epoch: 24365 \tTraining Loss: 1.446737 \tValidation Loss: 2.468188\n",
      "Epoch: 24366 \tTraining Loss: 1.468274 \tValidation Loss: 2.468090\n",
      "Epoch: 24367 \tTraining Loss: 1.439059 \tValidation Loss: 2.468350\n",
      "Epoch: 24368 \tTraining Loss: 1.444907 \tValidation Loss: 2.468304\n",
      "Epoch: 24369 \tTraining Loss: 1.446705 \tValidation Loss: 2.468141\n",
      "Epoch: 24370 \tTraining Loss: 1.452238 \tValidation Loss: 2.468050\n",
      "Epoch: 24371 \tTraining Loss: 1.480963 \tValidation Loss: 2.467285\n",
      "Epoch: 24372 \tTraining Loss: 1.432623 \tValidation Loss: 2.468138\n",
      "Epoch: 24373 \tTraining Loss: 1.398860 \tValidation Loss: 2.468592\n",
      "Epoch: 24374 \tTraining Loss: 1.441783 \tValidation Loss: 2.469284\n",
      "Epoch: 24375 \tTraining Loss: 1.443268 \tValidation Loss: 2.468465\n",
      "Epoch: 24376 \tTraining Loss: 1.448337 \tValidation Loss: 2.468409\n",
      "Epoch: 24377 \tTraining Loss: 1.410519 \tValidation Loss: 2.469243\n",
      "Epoch: 24378 \tTraining Loss: 1.414692 \tValidation Loss: 2.468667\n",
      "Epoch: 24379 \tTraining Loss: 1.437171 \tValidation Loss: 2.468143\n",
      "Epoch: 24380 \tTraining Loss: 1.468096 \tValidation Loss: 2.467455\n",
      "Epoch: 24381 \tTraining Loss: 1.463568 \tValidation Loss: 2.468007\n",
      "Epoch: 24382 \tTraining Loss: 1.433391 \tValidation Loss: 2.469109\n",
      "Epoch: 24383 \tTraining Loss: 1.453126 \tValidation Loss: 2.468464\n",
      "Epoch: 24384 \tTraining Loss: 1.420336 \tValidation Loss: 2.469015\n",
      "Epoch: 24385 \tTraining Loss: 1.430129 \tValidation Loss: 2.469319\n",
      "Epoch: 24386 \tTraining Loss: 1.436486 \tValidation Loss: 2.469019\n",
      "Epoch: 24387 \tTraining Loss: 1.424874 \tValidation Loss: 2.469315\n",
      "Epoch: 24388 \tTraining Loss: 1.452107 \tValidation Loss: 2.469418\n",
      "Epoch: 24389 \tTraining Loss: 1.436915 \tValidation Loss: 2.469045\n",
      "Epoch: 24390 \tTraining Loss: 1.444194 \tValidation Loss: 2.468616\n",
      "Epoch: 24391 \tTraining Loss: 1.416503 \tValidation Loss: 2.468875\n",
      "Epoch: 24392 \tTraining Loss: 1.394048 \tValidation Loss: 2.469023\n",
      "Epoch: 24393 \tTraining Loss: 1.473728 \tValidation Loss: 2.469724\n",
      "Epoch: 24394 \tTraining Loss: 1.451785 \tValidation Loss: 2.468691\n",
      "Epoch: 24395 \tTraining Loss: 1.460093 \tValidation Loss: 2.468895\n",
      "Epoch: 24396 \tTraining Loss: 1.432906 \tValidation Loss: 2.469646\n",
      "Epoch: 24397 \tTraining Loss: 1.426604 \tValidation Loss: 2.469127\n",
      "Epoch: 24398 \tTraining Loss: 1.466100 \tValidation Loss: 2.469067\n",
      "Epoch: 24399 \tTraining Loss: 1.410810 \tValidation Loss: 2.468655\n",
      "Epoch: 24400 \tTraining Loss: 1.438537 \tValidation Loss: 2.468353\n",
      "Epoch: 24401 \tTraining Loss: 1.421744 \tValidation Loss: 2.469319\n",
      "Epoch: 24402 \tTraining Loss: 1.448742 \tValidation Loss: 2.469413\n",
      "Epoch: 24403 \tTraining Loss: 1.474689 \tValidation Loss: 2.467717\n",
      "Epoch: 24404 \tTraining Loss: 1.434439 \tValidation Loss: 2.468223\n",
      "Epoch: 24405 \tTraining Loss: 1.428102 \tValidation Loss: 2.469160\n",
      "Epoch: 24406 \tTraining Loss: 1.398889 \tValidation Loss: 2.469121\n",
      "Epoch: 24407 \tTraining Loss: 1.477568 \tValidation Loss: 2.467682\n",
      "Epoch: 24408 \tTraining Loss: 1.389569 \tValidation Loss: 2.468740\n",
      "Epoch: 24409 \tTraining Loss: 1.440747 \tValidation Loss: 2.468379\n",
      "Epoch: 24410 \tTraining Loss: 1.440607 \tValidation Loss: 2.469141\n",
      "Epoch: 24411 \tTraining Loss: 1.412568 \tValidation Loss: 2.470283\n",
      "Epoch: 24412 \tTraining Loss: 1.465278 \tValidation Loss: 2.468801\n",
      "Epoch: 24413 \tTraining Loss: 1.449118 \tValidation Loss: 2.468995\n",
      "Epoch: 24414 \tTraining Loss: 1.436249 \tValidation Loss: 2.469970\n",
      "Epoch: 24415 \tTraining Loss: 1.446689 \tValidation Loss: 2.469475\n",
      "Epoch: 24416 \tTraining Loss: 1.428325 \tValidation Loss: 2.469136\n",
      "Epoch: 24417 \tTraining Loss: 1.422299 \tValidation Loss: 2.469925\n",
      "Epoch: 24418 \tTraining Loss: 1.453527 \tValidation Loss: 2.469090\n",
      "Epoch: 24419 \tTraining Loss: 1.477981 \tValidation Loss: 2.468164\n",
      "Epoch: 24420 \tTraining Loss: 1.401069 \tValidation Loss: 2.468793\n",
      "Epoch: 24421 \tTraining Loss: 1.478874 \tValidation Loss: 2.468973\n",
      "Epoch: 24422 \tTraining Loss: 1.406721 \tValidation Loss: 2.469134\n",
      "Epoch: 24423 \tTraining Loss: 1.449712 \tValidation Loss: 2.469567\n",
      "Epoch: 24424 \tTraining Loss: 1.418213 \tValidation Loss: 2.468926\n",
      "Epoch: 24425 \tTraining Loss: 1.389148 \tValidation Loss: 2.469415\n",
      "Epoch: 24426 \tTraining Loss: 1.410790 \tValidation Loss: 2.470080\n",
      "Epoch: 24427 \tTraining Loss: 1.428435 \tValidation Loss: 2.469892\n",
      "Epoch: 24428 \tTraining Loss: 1.458867 \tValidation Loss: 2.469461\n",
      "Epoch: 24429 \tTraining Loss: 1.415892 \tValidation Loss: 2.468997\n",
      "Epoch: 24430 \tTraining Loss: 1.419859 \tValidation Loss: 2.469712\n",
      "Epoch: 24431 \tTraining Loss: 1.414127 \tValidation Loss: 2.470285\n",
      "Epoch: 24432 \tTraining Loss: 1.420374 \tValidation Loss: 2.468930\n",
      "Epoch: 24433 \tTraining Loss: 1.429303 \tValidation Loss: 2.469321\n",
      "Epoch: 24434 \tTraining Loss: 1.440031 \tValidation Loss: 2.469975\n",
      "Epoch: 24435 \tTraining Loss: 1.455409 \tValidation Loss: 2.468907\n",
      "Epoch: 24436 \tTraining Loss: 1.412488 \tValidation Loss: 2.469227\n",
      "Epoch: 24437 \tTraining Loss: 1.400458 \tValidation Loss: 2.469832\n",
      "Epoch: 24438 \tTraining Loss: 1.492323 \tValidation Loss: 2.469059\n",
      "Epoch: 24439 \tTraining Loss: 1.446635 \tValidation Loss: 2.468682\n",
      "Epoch: 24440 \tTraining Loss: 1.423361 \tValidation Loss: 2.468550\n",
      "Epoch: 24441 \tTraining Loss: 1.412800 \tValidation Loss: 2.469875\n",
      "Epoch: 24442 \tTraining Loss: 1.460239 \tValidation Loss: 2.469393\n",
      "Epoch: 24443 \tTraining Loss: 1.466051 \tValidation Loss: 2.470226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24444 \tTraining Loss: 1.468945 \tValidation Loss: 2.468972\n",
      "Epoch: 24445 \tTraining Loss: 1.476800 \tValidation Loss: 2.469997\n",
      "Epoch: 24446 \tTraining Loss: 1.464543 \tValidation Loss: 2.469213\n",
      "Epoch: 24447 \tTraining Loss: 1.424914 \tValidation Loss: 2.469717\n",
      "Epoch: 24448 \tTraining Loss: 1.477160 \tValidation Loss: 2.468246\n",
      "Epoch: 24449 \tTraining Loss: 1.447697 \tValidation Loss: 2.469834\n",
      "Epoch: 24450 \tTraining Loss: 1.467718 \tValidation Loss: 2.468606\n",
      "Epoch: 24451 \tTraining Loss: 1.450916 \tValidation Loss: 2.468387\n",
      "Epoch: 24452 \tTraining Loss: 1.451651 \tValidation Loss: 2.469785\n",
      "Epoch: 24453 \tTraining Loss: 1.432595 \tValidation Loss: 2.470258\n",
      "Epoch: 24454 \tTraining Loss: 1.398193 \tValidation Loss: 2.469935\n",
      "Epoch: 24455 \tTraining Loss: 1.445073 \tValidation Loss: 2.470671\n",
      "Epoch: 24456 \tTraining Loss: 1.428019 \tValidation Loss: 2.471078\n",
      "Epoch: 24457 \tTraining Loss: 1.440180 \tValidation Loss: 2.471198\n",
      "Epoch: 24458 \tTraining Loss: 1.406362 \tValidation Loss: 2.471900\n",
      "Epoch: 24459 \tTraining Loss: 1.433104 \tValidation Loss: 2.470046\n",
      "Epoch: 24460 \tTraining Loss: 1.406086 \tValidation Loss: 2.470231\n",
      "Epoch: 24461 \tTraining Loss: 1.452222 \tValidation Loss: 2.469861\n",
      "Epoch: 24462 \tTraining Loss: 1.395480 \tValidation Loss: 2.471030\n",
      "Epoch: 24463 \tTraining Loss: 1.398146 \tValidation Loss: 2.470471\n",
      "Epoch: 24464 \tTraining Loss: 1.421128 \tValidation Loss: 2.470771\n",
      "Epoch: 24465 \tTraining Loss: 1.441019 \tValidation Loss: 2.469535\n",
      "Epoch: 24466 \tTraining Loss: 1.444177 \tValidation Loss: 2.469785\n",
      "Epoch: 24467 \tTraining Loss: 1.437093 \tValidation Loss: 2.470309\n",
      "Epoch: 24468 \tTraining Loss: 1.449185 \tValidation Loss: 2.469541\n",
      "Epoch: 24469 \tTraining Loss: 1.445963 \tValidation Loss: 2.470095\n",
      "Epoch: 24470 \tTraining Loss: 1.403998 \tValidation Loss: 2.470847\n",
      "Epoch: 24471 \tTraining Loss: 1.415135 \tValidation Loss: 2.471617\n",
      "Epoch: 24472 \tTraining Loss: 1.418404 \tValidation Loss: 2.471192\n",
      "Epoch: 24473 \tTraining Loss: 1.459754 \tValidation Loss: 2.470487\n",
      "Epoch: 24474 \tTraining Loss: 1.447037 \tValidation Loss: 2.470416\n",
      "Epoch: 24475 \tTraining Loss: 1.412365 \tValidation Loss: 2.469738\n",
      "Epoch: 24476 \tTraining Loss: 1.463318 \tValidation Loss: 2.470513\n",
      "Epoch: 24477 \tTraining Loss: 1.473791 \tValidation Loss: 2.469416\n",
      "Epoch: 24478 \tTraining Loss: 1.459007 \tValidation Loss: 2.470748\n",
      "Epoch: 24479 \tTraining Loss: 1.444771 \tValidation Loss: 2.469692\n",
      "Epoch: 24480 \tTraining Loss: 1.393702 \tValidation Loss: 2.470831\n",
      "Epoch: 24481 \tTraining Loss: 1.437970 \tValidation Loss: 2.470029\n",
      "Epoch: 24482 \tTraining Loss: 1.388586 \tValidation Loss: 2.470591\n",
      "Epoch: 24483 \tTraining Loss: 1.449448 \tValidation Loss: 2.469940\n",
      "Epoch: 24484 \tTraining Loss: 1.462521 \tValidation Loss: 2.471095\n",
      "Epoch: 24485 \tTraining Loss: 1.393473 \tValidation Loss: 2.471297\n",
      "Epoch: 24486 \tTraining Loss: 1.451288 \tValidation Loss: 2.470731\n",
      "Epoch: 24487 \tTraining Loss: 1.386594 \tValidation Loss: 2.471443\n",
      "Epoch: 24488 \tTraining Loss: 1.405726 \tValidation Loss: 2.472288\n",
      "Epoch: 24489 \tTraining Loss: 1.371182 \tValidation Loss: 2.471383\n",
      "Epoch: 24490 \tTraining Loss: 1.480191 \tValidation Loss: 2.471293\n",
      "Epoch: 24491 \tTraining Loss: 1.466270 \tValidation Loss: 2.471091\n",
      "Epoch: 24492 \tTraining Loss: 1.448194 \tValidation Loss: 2.470085\n",
      "Epoch: 24493 \tTraining Loss: 1.474001 \tValidation Loss: 2.469923\n",
      "Epoch: 24494 \tTraining Loss: 1.448160 \tValidation Loss: 2.470454\n",
      "Epoch: 24495 \tTraining Loss: 1.476487 \tValidation Loss: 2.470816\n",
      "Epoch: 24496 \tTraining Loss: 1.466058 \tValidation Loss: 2.471422\n",
      "Epoch: 24497 \tTraining Loss: 1.446973 \tValidation Loss: 2.471403\n",
      "Epoch: 24498 \tTraining Loss: 1.447310 \tValidation Loss: 2.470986\n",
      "Epoch: 24499 \tTraining Loss: 1.439147 \tValidation Loss: 2.471288\n",
      "Epoch: 24500 \tTraining Loss: 1.437369 \tValidation Loss: 2.471698\n",
      "Epoch: 24501 \tTraining Loss: 1.441512 \tValidation Loss: 2.471932\n",
      "Epoch: 24502 \tTraining Loss: 1.444588 \tValidation Loss: 2.471051\n",
      "Epoch: 24503 \tTraining Loss: 1.422921 \tValidation Loss: 2.470268\n",
      "Epoch: 24504 \tTraining Loss: 1.437424 \tValidation Loss: 2.470455\n",
      "Epoch: 24505 \tTraining Loss: 1.435754 \tValidation Loss: 2.470789\n",
      "Epoch: 24506 \tTraining Loss: 1.449314 \tValidation Loss: 2.471249\n",
      "Epoch: 24507 \tTraining Loss: 1.421080 \tValidation Loss: 2.472664\n",
      "Epoch: 24508 \tTraining Loss: 1.475205 \tValidation Loss: 2.470987\n",
      "Epoch: 24509 \tTraining Loss: 1.461629 \tValidation Loss: 2.470939\n",
      "Epoch: 24510 \tTraining Loss: 1.414198 \tValidation Loss: 2.470815\n",
      "Epoch: 24511 \tTraining Loss: 1.436691 \tValidation Loss: 2.471664\n",
      "Epoch: 24512 \tTraining Loss: 1.484513 \tValidation Loss: 2.470779\n",
      "Epoch: 24513 \tTraining Loss: 1.437918 \tValidation Loss: 2.471244\n",
      "Epoch: 24514 \tTraining Loss: 1.425096 \tValidation Loss: 2.471328\n",
      "Epoch: 24515 \tTraining Loss: 1.424623 \tValidation Loss: 2.471250\n",
      "Epoch: 24516 \tTraining Loss: 1.414873 \tValidation Loss: 2.470642\n",
      "Epoch: 24517 \tTraining Loss: 1.400809 \tValidation Loss: 2.471166\n",
      "Epoch: 24518 \tTraining Loss: 1.425246 \tValidation Loss: 2.471143\n",
      "Epoch: 24519 \tTraining Loss: 1.438120 \tValidation Loss: 2.470761\n",
      "Epoch: 24520 \tTraining Loss: 1.413612 \tValidation Loss: 2.470865\n",
      "Epoch: 24521 \tTraining Loss: 1.449050 \tValidation Loss: 2.471554\n",
      "Epoch: 24522 \tTraining Loss: 1.439476 \tValidation Loss: 2.472534\n",
      "Epoch: 24523 \tTraining Loss: 1.453460 \tValidation Loss: 2.471585\n",
      "Epoch: 24524 \tTraining Loss: 1.444250 \tValidation Loss: 2.471197\n",
      "Epoch: 24525 \tTraining Loss: 1.444291 \tValidation Loss: 2.471244\n",
      "Epoch: 24526 \tTraining Loss: 1.426111 \tValidation Loss: 2.472483\n",
      "Epoch: 24527 \tTraining Loss: 1.427980 \tValidation Loss: 2.471409\n",
      "Epoch: 24528 \tTraining Loss: 1.420614 \tValidation Loss: 2.472323\n",
      "Epoch: 24529 \tTraining Loss: 1.437961 \tValidation Loss: 2.471819\n",
      "Epoch: 24530 \tTraining Loss: 1.424595 \tValidation Loss: 2.470136\n",
      "Epoch: 24531 \tTraining Loss: 1.414409 \tValidation Loss: 2.471304\n",
      "Epoch: 24532 \tTraining Loss: 1.434451 \tValidation Loss: 2.471457\n",
      "Epoch: 24533 \tTraining Loss: 1.388043 \tValidation Loss: 2.471390\n",
      "Epoch: 24534 \tTraining Loss: 1.428434 \tValidation Loss: 2.471694\n",
      "Epoch: 24535 \tTraining Loss: 1.422560 \tValidation Loss: 2.471698\n",
      "Epoch: 24536 \tTraining Loss: 1.464290 \tValidation Loss: 2.471548\n",
      "Epoch: 24537 \tTraining Loss: 1.453004 \tValidation Loss: 2.471192\n",
      "Epoch: 24538 \tTraining Loss: 1.451767 \tValidation Loss: 2.471505\n",
      "Epoch: 24539 \tTraining Loss: 1.439002 \tValidation Loss: 2.472079\n",
      "Epoch: 24540 \tTraining Loss: 1.455225 \tValidation Loss: 2.471959\n",
      "Epoch: 24541 \tTraining Loss: 1.462057 \tValidation Loss: 2.471234\n",
      "Epoch: 24542 \tTraining Loss: 1.413375 \tValidation Loss: 2.471860\n",
      "Epoch: 24543 \tTraining Loss: 1.426428 \tValidation Loss: 2.470903\n",
      "Epoch: 24544 \tTraining Loss: 1.422121 \tValidation Loss: 2.471996\n",
      "Epoch: 24545 \tTraining Loss: 1.434626 \tValidation Loss: 2.471228\n",
      "Epoch: 24546 \tTraining Loss: 1.418517 \tValidation Loss: 2.471349\n",
      "Epoch: 24547 \tTraining Loss: 1.453949 \tValidation Loss: 2.471558\n",
      "Epoch: 24548 \tTraining Loss: 1.393619 \tValidation Loss: 2.471833\n",
      "Epoch: 24549 \tTraining Loss: 1.440881 \tValidation Loss: 2.471780\n",
      "Epoch: 24550 \tTraining Loss: 1.456179 \tValidation Loss: 2.472701\n",
      "Epoch: 24551 \tTraining Loss: 1.447701 \tValidation Loss: 2.472208\n",
      "Epoch: 24552 \tTraining Loss: 1.446096 \tValidation Loss: 2.471831\n",
      "Epoch: 24553 \tTraining Loss: 1.419045 \tValidation Loss: 2.472929\n",
      "Epoch: 24554 \tTraining Loss: 1.433504 \tValidation Loss: 2.472410\n",
      "Epoch: 24555 \tTraining Loss: 1.438897 \tValidation Loss: 2.472656\n",
      "Epoch: 24556 \tTraining Loss: 1.477993 \tValidation Loss: 2.471641\n",
      "Epoch: 24557 \tTraining Loss: 1.448783 \tValidation Loss: 2.471608\n",
      "Epoch: 24558 \tTraining Loss: 1.404017 \tValidation Loss: 2.473614\n",
      "Epoch: 24559 \tTraining Loss: 1.485970 \tValidation Loss: 2.472444\n",
      "Epoch: 24560 \tTraining Loss: 1.444596 \tValidation Loss: 2.471793\n",
      "Epoch: 24561 \tTraining Loss: 1.437296 \tValidation Loss: 2.470868\n",
      "Epoch: 24562 \tTraining Loss: 1.448779 \tValidation Loss: 2.471771\n",
      "Epoch: 24563 \tTraining Loss: 1.430653 \tValidation Loss: 2.471641\n",
      "Epoch: 24564 \tTraining Loss: 1.431835 \tValidation Loss: 2.473047\n",
      "Epoch: 24565 \tTraining Loss: 1.423351 \tValidation Loss: 2.471469\n",
      "Epoch: 24566 \tTraining Loss: 1.476685 \tValidation Loss: 2.470895\n",
      "Epoch: 24567 \tTraining Loss: 1.453743 \tValidation Loss: 2.472458\n",
      "Epoch: 24568 \tTraining Loss: 1.422805 \tValidation Loss: 2.473039\n",
      "Epoch: 24569 \tTraining Loss: 1.451025 \tValidation Loss: 2.472725\n",
      "Epoch: 24570 \tTraining Loss: 1.433384 \tValidation Loss: 2.472953\n",
      "Epoch: 24571 \tTraining Loss: 1.439684 \tValidation Loss: 2.471613\n",
      "Epoch: 24572 \tTraining Loss: 1.449667 \tValidation Loss: 2.472030\n",
      "Epoch: 24573 \tTraining Loss: 1.450458 \tValidation Loss: 2.472830\n",
      "Epoch: 24574 \tTraining Loss: 1.468236 \tValidation Loss: 2.471962\n",
      "Epoch: 24575 \tTraining Loss: 1.442215 \tValidation Loss: 2.472292\n",
      "Epoch: 24576 \tTraining Loss: 1.406871 \tValidation Loss: 2.473559\n",
      "Epoch: 24577 \tTraining Loss: 1.444989 \tValidation Loss: 2.473062\n",
      "Epoch: 24578 \tTraining Loss: 1.435449 \tValidation Loss: 2.473010\n",
      "Epoch: 24579 \tTraining Loss: 1.445837 \tValidation Loss: 2.472796\n",
      "Epoch: 24580 \tTraining Loss: 1.447661 \tValidation Loss: 2.472293\n",
      "Epoch: 24581 \tTraining Loss: 1.424938 \tValidation Loss: 2.472715\n",
      "Epoch: 24582 \tTraining Loss: 1.459188 \tValidation Loss: 2.472467\n",
      "Epoch: 24583 \tTraining Loss: 1.417932 \tValidation Loss: 2.472981\n",
      "Epoch: 24584 \tTraining Loss: 1.502041 \tValidation Loss: 2.471714\n",
      "Epoch: 24585 \tTraining Loss: 1.459048 \tValidation Loss: 2.471966\n",
      "Epoch: 24586 \tTraining Loss: 1.409980 \tValidation Loss: 2.473065\n",
      "Epoch: 24587 \tTraining Loss: 1.423167 \tValidation Loss: 2.472195\n",
      "Epoch: 24588 \tTraining Loss: 1.461910 \tValidation Loss: 2.472893\n",
      "Epoch: 24589 \tTraining Loss: 1.426134 \tValidation Loss: 2.472845\n",
      "Epoch: 24590 \tTraining Loss: 1.483411 \tValidation Loss: 2.472317\n",
      "Epoch: 24591 \tTraining Loss: 1.412667 \tValidation Loss: 2.473686\n",
      "Epoch: 24592 \tTraining Loss: 1.448641 \tValidation Loss: 2.473564\n",
      "Epoch: 24593 \tTraining Loss: 1.406871 \tValidation Loss: 2.473047\n",
      "Epoch: 24594 \tTraining Loss: 1.431437 \tValidation Loss: 2.472146\n",
      "Epoch: 24595 \tTraining Loss: 1.407881 \tValidation Loss: 2.472425\n",
      "Epoch: 24596 \tTraining Loss: 1.454913 \tValidation Loss: 2.472999\n",
      "Epoch: 24597 \tTraining Loss: 1.455792 \tValidation Loss: 2.472733\n",
      "Epoch: 24598 \tTraining Loss: 1.450585 \tValidation Loss: 2.472945\n",
      "Epoch: 24599 \tTraining Loss: 1.445665 \tValidation Loss: 2.473634\n",
      "Epoch: 24600 \tTraining Loss: 1.438498 \tValidation Loss: 2.473049\n",
      "Epoch: 24601 \tTraining Loss: 1.375754 \tValidation Loss: 2.473631\n",
      "Epoch: 24602 \tTraining Loss: 1.397325 \tValidation Loss: 2.472780\n",
      "Epoch: 24603 \tTraining Loss: 1.453081 \tValidation Loss: 2.472956\n",
      "Epoch: 24604 \tTraining Loss: 1.476421 \tValidation Loss: 2.473339\n",
      "Epoch: 24605 \tTraining Loss: 1.412014 \tValidation Loss: 2.472929\n",
      "Epoch: 24606 \tTraining Loss: 1.432856 \tValidation Loss: 2.473167\n",
      "Epoch: 24607 \tTraining Loss: 1.475892 \tValidation Loss: 2.472390\n",
      "Epoch: 24608 \tTraining Loss: 1.452811 \tValidation Loss: 2.472445\n",
      "Epoch: 24609 \tTraining Loss: 1.451393 \tValidation Loss: 2.472708\n",
      "Epoch: 24610 \tTraining Loss: 1.436552 \tValidation Loss: 2.473502\n",
      "Epoch: 24611 \tTraining Loss: 1.417275 \tValidation Loss: 2.472991\n",
      "Epoch: 24612 \tTraining Loss: 1.435488 \tValidation Loss: 2.472718\n",
      "Epoch: 24613 \tTraining Loss: 1.446429 \tValidation Loss: 2.472777\n",
      "Epoch: 24614 \tTraining Loss: 1.435887 \tValidation Loss: 2.473007\n",
      "Epoch: 24615 \tTraining Loss: 1.403225 \tValidation Loss: 2.472809\n",
      "Epoch: 24616 \tTraining Loss: 1.413140 \tValidation Loss: 2.473155\n",
      "Epoch: 24617 \tTraining Loss: 1.477618 \tValidation Loss: 2.473151\n",
      "Epoch: 24618 \tTraining Loss: 1.429316 \tValidation Loss: 2.472917\n",
      "Epoch: 24619 \tTraining Loss: 1.426034 \tValidation Loss: 2.472977\n",
      "Epoch: 24620 \tTraining Loss: 1.470813 \tValidation Loss: 2.472572\n",
      "Epoch: 24621 \tTraining Loss: 1.434819 \tValidation Loss: 2.473989\n",
      "Epoch: 24622 \tTraining Loss: 1.428072 \tValidation Loss: 2.473798\n",
      "Epoch: 24623 \tTraining Loss: 1.428292 \tValidation Loss: 2.473139\n",
      "Epoch: 24624 \tTraining Loss: 1.466837 \tValidation Loss: 2.472211\n",
      "Epoch: 24625 \tTraining Loss: 1.419914 \tValidation Loss: 2.473744\n",
      "Epoch: 24626 \tTraining Loss: 1.407902 \tValidation Loss: 2.473003\n",
      "Epoch: 24627 \tTraining Loss: 1.393680 \tValidation Loss: 2.473168\n",
      "Epoch: 24628 \tTraining Loss: 1.419466 \tValidation Loss: 2.474248\n",
      "Epoch: 24629 \tTraining Loss: 1.386850 \tValidation Loss: 2.473821\n",
      "Epoch: 24630 \tTraining Loss: 1.416722 \tValidation Loss: 2.474526\n",
      "Epoch: 24631 \tTraining Loss: 1.410153 \tValidation Loss: 2.473790\n",
      "Epoch: 24632 \tTraining Loss: 1.447986 \tValidation Loss: 2.473642\n",
      "Epoch: 24633 \tTraining Loss: 1.458160 \tValidation Loss: 2.473729\n",
      "Epoch: 24634 \tTraining Loss: 1.416950 \tValidation Loss: 2.474500\n",
      "Epoch: 24635 \tTraining Loss: 1.406993 \tValidation Loss: 2.474950\n",
      "Epoch: 24636 \tTraining Loss: 1.426839 \tValidation Loss: 2.474576\n",
      "Epoch: 24637 \tTraining Loss: 1.439468 \tValidation Loss: 2.474478\n",
      "Epoch: 24638 \tTraining Loss: 1.417621 \tValidation Loss: 2.473969\n",
      "Epoch: 24639 \tTraining Loss: 1.457269 \tValidation Loss: 2.473291\n",
      "Epoch: 24640 \tTraining Loss: 1.397193 \tValidation Loss: 2.474081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24641 \tTraining Loss: 1.436417 \tValidation Loss: 2.472924\n",
      "Epoch: 24642 \tTraining Loss: 1.423270 \tValidation Loss: 2.473837\n",
      "Epoch: 24643 \tTraining Loss: 1.447076 \tValidation Loss: 2.473380\n",
      "Epoch: 24644 \tTraining Loss: 1.405650 \tValidation Loss: 2.474513\n",
      "Epoch: 24645 \tTraining Loss: 1.458216 \tValidation Loss: 2.472868\n",
      "Epoch: 24646 \tTraining Loss: 1.445873 \tValidation Loss: 2.473410\n",
      "Epoch: 24647 \tTraining Loss: 1.447487 \tValidation Loss: 2.473243\n",
      "Epoch: 24648 \tTraining Loss: 1.443662 \tValidation Loss: 2.473583\n",
      "Epoch: 24649 \tTraining Loss: 1.427970 \tValidation Loss: 2.473747\n",
      "Epoch: 24650 \tTraining Loss: 1.428127 \tValidation Loss: 2.473713\n",
      "Epoch: 24651 \tTraining Loss: 1.448426 \tValidation Loss: 2.473040\n",
      "Epoch: 24652 \tTraining Loss: 1.413521 \tValidation Loss: 2.473067\n",
      "Epoch: 24653 \tTraining Loss: 1.453784 \tValidation Loss: 2.473205\n",
      "Epoch: 24654 \tTraining Loss: 1.433291 \tValidation Loss: 2.473396\n",
      "Epoch: 24655 \tTraining Loss: 1.432589 \tValidation Loss: 2.473546\n",
      "Epoch: 24656 \tTraining Loss: 1.410370 \tValidation Loss: 2.474218\n",
      "Epoch: 24657 \tTraining Loss: 1.468145 \tValidation Loss: 2.473268\n",
      "Epoch: 24658 \tTraining Loss: 1.432531 \tValidation Loss: 2.473364\n",
      "Epoch: 24659 \tTraining Loss: 1.402104 \tValidation Loss: 2.474365\n",
      "Epoch: 24660 \tTraining Loss: 1.390432 \tValidation Loss: 2.474932\n",
      "Epoch: 24661 \tTraining Loss: 1.449082 \tValidation Loss: 2.473718\n",
      "Epoch: 24662 \tTraining Loss: 1.404305 \tValidation Loss: 2.474478\n",
      "Epoch: 24663 \tTraining Loss: 1.441729 \tValidation Loss: 2.474582\n",
      "Epoch: 24664 \tTraining Loss: 1.454968 \tValidation Loss: 2.474574\n",
      "Epoch: 24665 \tTraining Loss: 1.415196 \tValidation Loss: 2.474526\n",
      "Epoch: 24666 \tTraining Loss: 1.406902 \tValidation Loss: 2.475460\n",
      "Epoch: 24667 \tTraining Loss: 1.429156 \tValidation Loss: 2.474672\n",
      "Epoch: 24668 \tTraining Loss: 1.467100 \tValidation Loss: 2.474008\n",
      "Epoch: 24669 \tTraining Loss: 1.440270 \tValidation Loss: 2.474088\n",
      "Epoch: 24670 \tTraining Loss: 1.431033 \tValidation Loss: 2.475342\n",
      "Epoch: 24671 \tTraining Loss: 1.408650 \tValidation Loss: 2.474461\n",
      "Epoch: 24672 \tTraining Loss: 1.413387 \tValidation Loss: 2.474241\n",
      "Epoch: 24673 \tTraining Loss: 1.450912 \tValidation Loss: 2.474897\n",
      "Epoch: 24674 \tTraining Loss: 1.494617 \tValidation Loss: 2.474480\n",
      "Epoch: 24675 \tTraining Loss: 1.400139 \tValidation Loss: 2.474352\n",
      "Epoch: 24676 \tTraining Loss: 1.471092 \tValidation Loss: 2.473421\n",
      "Epoch: 24677 \tTraining Loss: 1.433943 \tValidation Loss: 2.474316\n",
      "Epoch: 24678 \tTraining Loss: 1.439519 \tValidation Loss: 2.473931\n",
      "Epoch: 24679 \tTraining Loss: 1.415354 \tValidation Loss: 2.474285\n",
      "Epoch: 24680 \tTraining Loss: 1.413177 \tValidation Loss: 2.474093\n",
      "Epoch: 24681 \tTraining Loss: 1.404104 \tValidation Loss: 2.474804\n",
      "Epoch: 24682 \tTraining Loss: 1.450769 \tValidation Loss: 2.474471\n",
      "Epoch: 24683 \tTraining Loss: 1.439805 \tValidation Loss: 2.475085\n",
      "Epoch: 24684 \tTraining Loss: 1.423165 \tValidation Loss: 2.475492\n",
      "Epoch: 24685 \tTraining Loss: 1.389377 \tValidation Loss: 2.474783\n",
      "Epoch: 24686 \tTraining Loss: 1.435619 \tValidation Loss: 2.474579\n",
      "Epoch: 24687 \tTraining Loss: 1.414247 \tValidation Loss: 2.474763\n",
      "Epoch: 24688 \tTraining Loss: 1.451689 \tValidation Loss: 2.474472\n",
      "Epoch: 24689 \tTraining Loss: 1.382812 \tValidation Loss: 2.475201\n",
      "Epoch: 24690 \tTraining Loss: 1.436515 \tValidation Loss: 2.474083\n",
      "Epoch: 24691 \tTraining Loss: 1.453873 \tValidation Loss: 2.474475\n",
      "Epoch: 24692 \tTraining Loss: 1.412243 \tValidation Loss: 2.475155\n",
      "Epoch: 24693 \tTraining Loss: 1.466522 \tValidation Loss: 2.473918\n",
      "Epoch: 24694 \tTraining Loss: 1.409809 \tValidation Loss: 2.475594\n",
      "Epoch: 24695 \tTraining Loss: 1.394687 \tValidation Loss: 2.475574\n",
      "Epoch: 24696 \tTraining Loss: 1.410236 \tValidation Loss: 2.474941\n",
      "Epoch: 24697 \tTraining Loss: 1.457842 \tValidation Loss: 2.474602\n",
      "Epoch: 24698 \tTraining Loss: 1.398042 \tValidation Loss: 2.475720\n",
      "Epoch: 24699 \tTraining Loss: 1.430108 \tValidation Loss: 2.474973\n",
      "Epoch: 24700 \tTraining Loss: 1.442547 \tValidation Loss: 2.475765\n",
      "Epoch: 24701 \tTraining Loss: 1.429645 \tValidation Loss: 2.475433\n",
      "Epoch: 24702 \tTraining Loss: 1.403594 \tValidation Loss: 2.475331\n",
      "Epoch: 24703 \tTraining Loss: 1.441835 \tValidation Loss: 2.474771\n",
      "Epoch: 24704 \tTraining Loss: 1.451225 \tValidation Loss: 2.475034\n",
      "Epoch: 24705 \tTraining Loss: 1.425874 \tValidation Loss: 2.476317\n",
      "Epoch: 24706 \tTraining Loss: 1.438781 \tValidation Loss: 2.476123\n",
      "Epoch: 24707 \tTraining Loss: 1.407349 \tValidation Loss: 2.475313\n",
      "Epoch: 24708 \tTraining Loss: 1.418770 \tValidation Loss: 2.475356\n",
      "Epoch: 24709 \tTraining Loss: 1.398834 \tValidation Loss: 2.475353\n",
      "Epoch: 24710 \tTraining Loss: 1.430694 \tValidation Loss: 2.475519\n",
      "Epoch: 24711 \tTraining Loss: 1.451203 \tValidation Loss: 2.475053\n",
      "Epoch: 24712 \tTraining Loss: 1.434214 \tValidation Loss: 2.475248\n",
      "Epoch: 24713 \tTraining Loss: 1.409677 \tValidation Loss: 2.475136\n",
      "Epoch: 24714 \tTraining Loss: 1.431796 \tValidation Loss: 2.475883\n",
      "Epoch: 24715 \tTraining Loss: 1.474393 \tValidation Loss: 2.475579\n",
      "Epoch: 24716 \tTraining Loss: 1.440140 \tValidation Loss: 2.474890\n",
      "Epoch: 24717 \tTraining Loss: 1.446265 \tValidation Loss: 2.475414\n",
      "Epoch: 24718 \tTraining Loss: 1.446964 \tValidation Loss: 2.475527\n",
      "Epoch: 24719 \tTraining Loss: 1.451654 \tValidation Loss: 2.474887\n",
      "Epoch: 24720 \tTraining Loss: 1.425105 \tValidation Loss: 2.474584\n",
      "Epoch: 24721 \tTraining Loss: 1.415131 \tValidation Loss: 2.475356\n",
      "Epoch: 24722 \tTraining Loss: 1.443292 \tValidation Loss: 2.474829\n",
      "Epoch: 24723 \tTraining Loss: 1.421669 \tValidation Loss: 2.474981\n",
      "Epoch: 24724 \tTraining Loss: 1.465445 \tValidation Loss: 2.474977\n",
      "Epoch: 24725 \tTraining Loss: 1.440568 \tValidation Loss: 2.475859\n",
      "Epoch: 24726 \tTraining Loss: 1.415557 \tValidation Loss: 2.474335\n",
      "Epoch: 24727 \tTraining Loss: 1.435766 \tValidation Loss: 2.474340\n",
      "Epoch: 24728 \tTraining Loss: 1.434573 \tValidation Loss: 2.474003\n",
      "Epoch: 24729 \tTraining Loss: 1.402842 \tValidation Loss: 2.475533\n",
      "Epoch: 24730 \tTraining Loss: 1.444808 \tValidation Loss: 2.475179\n",
      "Epoch: 24731 \tTraining Loss: 1.421840 \tValidation Loss: 2.476439\n",
      "Epoch: 24732 \tTraining Loss: 1.415263 \tValidation Loss: 2.475320\n",
      "Epoch: 24733 \tTraining Loss: 1.472760 \tValidation Loss: 2.475053\n",
      "Epoch: 24734 \tTraining Loss: 1.372548 \tValidation Loss: 2.476702\n",
      "Epoch: 24735 \tTraining Loss: 1.377002 \tValidation Loss: 2.476618\n",
      "Epoch: 24736 \tTraining Loss: 1.442338 \tValidation Loss: 2.475083\n",
      "Epoch: 24737 \tTraining Loss: 1.408768 \tValidation Loss: 2.475450\n",
      "Epoch: 24738 \tTraining Loss: 1.389423 \tValidation Loss: 2.475830\n",
      "Epoch: 24739 \tTraining Loss: 1.445961 \tValidation Loss: 2.474664\n",
      "Epoch: 24740 \tTraining Loss: 1.438931 \tValidation Loss: 2.474887\n",
      "Epoch: 24741 \tTraining Loss: 1.411853 \tValidation Loss: 2.475161\n",
      "Epoch: 24742 \tTraining Loss: 1.441240 \tValidation Loss: 2.475688\n",
      "Epoch: 24743 \tTraining Loss: 1.429139 \tValidation Loss: 2.475866\n",
      "Epoch: 24744 \tTraining Loss: 1.441609 \tValidation Loss: 2.476233\n",
      "Epoch: 24745 \tTraining Loss: 1.428615 \tValidation Loss: 2.476163\n",
      "Epoch: 24746 \tTraining Loss: 1.424597 \tValidation Loss: 2.475361\n",
      "Epoch: 24747 \tTraining Loss: 1.401955 \tValidation Loss: 2.475552\n",
      "Epoch: 24748 \tTraining Loss: 1.422421 \tValidation Loss: 2.475470\n",
      "Epoch: 24749 \tTraining Loss: 1.430867 \tValidation Loss: 2.474802\n",
      "Epoch: 24750 \tTraining Loss: 1.451177 \tValidation Loss: 2.475266\n",
      "Epoch: 24751 \tTraining Loss: 1.431981 \tValidation Loss: 2.476046\n",
      "Epoch: 24752 \tTraining Loss: 1.422856 \tValidation Loss: 2.475975\n",
      "Epoch: 24753 \tTraining Loss: 1.411363 \tValidation Loss: 2.475353\n",
      "Epoch: 24754 \tTraining Loss: 1.413303 \tValidation Loss: 2.476498\n",
      "Epoch: 24755 \tTraining Loss: 1.387401 \tValidation Loss: 2.476771\n",
      "Epoch: 24756 \tTraining Loss: 1.449817 \tValidation Loss: 2.476300\n",
      "Epoch: 24757 \tTraining Loss: 1.439690 \tValidation Loss: 2.475542\n",
      "Epoch: 24758 \tTraining Loss: 1.413506 \tValidation Loss: 2.475971\n",
      "Epoch: 24759 \tTraining Loss: 1.369838 \tValidation Loss: 2.477201\n",
      "Epoch: 24760 \tTraining Loss: 1.389032 \tValidation Loss: 2.475868\n",
      "Epoch: 24761 \tTraining Loss: 1.440253 \tValidation Loss: 2.475288\n",
      "Epoch: 24762 \tTraining Loss: 1.441255 \tValidation Loss: 2.475575\n",
      "Epoch: 24763 \tTraining Loss: 1.410072 \tValidation Loss: 2.476655\n",
      "Epoch: 24764 \tTraining Loss: 1.420165 \tValidation Loss: 2.476609\n",
      "Epoch: 24765 \tTraining Loss: 1.453890 \tValidation Loss: 2.476247\n",
      "Epoch: 24766 \tTraining Loss: 1.442833 \tValidation Loss: 2.475985\n",
      "Epoch: 24767 \tTraining Loss: 1.458740 \tValidation Loss: 2.475672\n",
      "Epoch: 24768 \tTraining Loss: 1.406767 \tValidation Loss: 2.476081\n",
      "Epoch: 24769 \tTraining Loss: 1.404245 \tValidation Loss: 2.477091\n",
      "Epoch: 24770 \tTraining Loss: 1.407241 \tValidation Loss: 2.476038\n",
      "Epoch: 24771 \tTraining Loss: 1.416860 \tValidation Loss: 2.476616\n",
      "Epoch: 24772 \tTraining Loss: 1.449153 \tValidation Loss: 2.476090\n",
      "Epoch: 24773 \tTraining Loss: 1.421432 \tValidation Loss: 2.476251\n",
      "Epoch: 24774 \tTraining Loss: 1.417819 \tValidation Loss: 2.477038\n",
      "Epoch: 24775 \tTraining Loss: 1.433948 \tValidation Loss: 2.476156\n",
      "Epoch: 24776 \tTraining Loss: 1.395941 \tValidation Loss: 2.476342\n",
      "Epoch: 24777 \tTraining Loss: 1.410091 \tValidation Loss: 2.476600\n",
      "Epoch: 24778 \tTraining Loss: 1.452241 \tValidation Loss: 2.476586\n",
      "Epoch: 24779 \tTraining Loss: 1.447628 \tValidation Loss: 2.475429\n",
      "Epoch: 24780 \tTraining Loss: 1.437146 \tValidation Loss: 2.475298\n",
      "Epoch: 24781 \tTraining Loss: 1.399586 \tValidation Loss: 2.476024\n",
      "Epoch: 24782 \tTraining Loss: 1.415351 \tValidation Loss: 2.476940\n",
      "Epoch: 24783 \tTraining Loss: 1.399967 \tValidation Loss: 2.477654\n",
      "Epoch: 24784 \tTraining Loss: 1.444600 \tValidation Loss: 2.477154\n",
      "Epoch: 24785 \tTraining Loss: 1.447000 \tValidation Loss: 2.476844\n",
      "Epoch: 24786 \tTraining Loss: 1.387112 \tValidation Loss: 2.476717\n",
      "Epoch: 24787 \tTraining Loss: 1.422563 \tValidation Loss: 2.476748\n",
      "Epoch: 24788 \tTraining Loss: 1.431094 \tValidation Loss: 2.477061\n",
      "Epoch: 24789 \tTraining Loss: 1.387356 \tValidation Loss: 2.476874\n",
      "Epoch: 24790 \tTraining Loss: 1.439319 \tValidation Loss: 2.477221\n",
      "Epoch: 24791 \tTraining Loss: 1.420779 \tValidation Loss: 2.477379\n",
      "Epoch: 24792 \tTraining Loss: 1.433154 \tValidation Loss: 2.476611\n",
      "Epoch: 24793 \tTraining Loss: 1.440405 \tValidation Loss: 2.477075\n",
      "Epoch: 24794 \tTraining Loss: 1.427693 \tValidation Loss: 2.477475\n",
      "Epoch: 24795 \tTraining Loss: 1.429865 \tValidation Loss: 2.476573\n",
      "Epoch: 24796 \tTraining Loss: 1.433395 \tValidation Loss: 2.476671\n",
      "Epoch: 24797 \tTraining Loss: 1.443596 \tValidation Loss: 2.477305\n",
      "Epoch: 24798 \tTraining Loss: 1.439381 \tValidation Loss: 2.476732\n",
      "Epoch: 24799 \tTraining Loss: 1.407041 \tValidation Loss: 2.477834\n",
      "Epoch: 24800 \tTraining Loss: 1.436449 \tValidation Loss: 2.476685\n",
      "Epoch: 24801 \tTraining Loss: 1.429002 \tValidation Loss: 2.477086\n",
      "Epoch: 24802 \tTraining Loss: 1.447639 \tValidation Loss: 2.476649\n",
      "Epoch: 24803 \tTraining Loss: 1.415389 \tValidation Loss: 2.476563\n",
      "Epoch: 24804 \tTraining Loss: 1.414416 \tValidation Loss: 2.476951\n",
      "Epoch: 24805 \tTraining Loss: 1.374459 \tValidation Loss: 2.477466\n",
      "Epoch: 24806 \tTraining Loss: 1.442555 \tValidation Loss: 2.476394\n",
      "Epoch: 24807 \tTraining Loss: 1.447523 \tValidation Loss: 2.476898\n",
      "Epoch: 24808 \tTraining Loss: 1.437432 \tValidation Loss: 2.476717\n",
      "Epoch: 24809 \tTraining Loss: 1.473874 \tValidation Loss: 2.476723\n",
      "Epoch: 24810 \tTraining Loss: 1.431958 \tValidation Loss: 2.476332\n",
      "Epoch: 24811 \tTraining Loss: 1.432740 \tValidation Loss: 2.477695\n",
      "Epoch: 24812 \tTraining Loss: 1.440773 \tValidation Loss: 2.477724\n",
      "Epoch: 24813 \tTraining Loss: 1.419279 \tValidation Loss: 2.476788\n",
      "Epoch: 24814 \tTraining Loss: 1.438416 \tValidation Loss: 2.477914\n",
      "Epoch: 24815 \tTraining Loss: 1.437214 \tValidation Loss: 2.476771\n",
      "Epoch: 24816 \tTraining Loss: 1.447721 \tValidation Loss: 2.476861\n",
      "Epoch: 24817 \tTraining Loss: 1.416868 \tValidation Loss: 2.477091\n",
      "Epoch: 24818 \tTraining Loss: 1.407946 \tValidation Loss: 2.477568\n",
      "Epoch: 24819 \tTraining Loss: 1.426251 \tValidation Loss: 2.477279\n",
      "Epoch: 24820 \tTraining Loss: 1.401134 \tValidation Loss: 2.477596\n",
      "Epoch: 24821 \tTraining Loss: 1.423057 \tValidation Loss: 2.477937\n",
      "Epoch: 24822 \tTraining Loss: 1.499229 \tValidation Loss: 2.476830\n",
      "Epoch: 24823 \tTraining Loss: 1.407233 \tValidation Loss: 2.478369\n",
      "Epoch: 24824 \tTraining Loss: 1.397672 \tValidation Loss: 2.477434\n",
      "Epoch: 24825 \tTraining Loss: 1.447250 \tValidation Loss: 2.477173\n",
      "Epoch: 24826 \tTraining Loss: 1.437071 \tValidation Loss: 2.477156\n",
      "Epoch: 24827 \tTraining Loss: 1.454428 \tValidation Loss: 2.476900\n",
      "Epoch: 24828 \tTraining Loss: 1.446446 \tValidation Loss: 2.477362\n",
      "Epoch: 24829 \tTraining Loss: 1.429904 \tValidation Loss: 2.477465\n",
      "Epoch: 24830 \tTraining Loss: 1.439873 \tValidation Loss: 2.478581\n",
      "Epoch: 24831 \tTraining Loss: 1.464560 \tValidation Loss: 2.477375\n",
      "Epoch: 24832 \tTraining Loss: 1.414468 \tValidation Loss: 2.477545\n",
      "Epoch: 24833 \tTraining Loss: 1.441001 \tValidation Loss: 2.477170\n",
      "Epoch: 24834 \tTraining Loss: 1.403831 \tValidation Loss: 2.477321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24835 \tTraining Loss: 1.401020 \tValidation Loss: 2.476976\n",
      "Epoch: 24836 \tTraining Loss: 1.410537 \tValidation Loss: 2.477592\n",
      "Epoch: 24837 \tTraining Loss: 1.469578 \tValidation Loss: 2.477559\n",
      "Epoch: 24838 \tTraining Loss: 1.421912 \tValidation Loss: 2.477492\n",
      "Epoch: 24839 \tTraining Loss: 1.442398 \tValidation Loss: 2.478009\n",
      "Epoch: 24840 \tTraining Loss: 1.393660 \tValidation Loss: 2.477960\n",
      "Epoch: 24841 \tTraining Loss: 1.448625 \tValidation Loss: 2.478335\n",
      "Epoch: 24842 \tTraining Loss: 1.433692 \tValidation Loss: 2.477743\n",
      "Epoch: 24843 \tTraining Loss: 1.413127 \tValidation Loss: 2.478093\n",
      "Epoch: 24844 \tTraining Loss: 1.391527 \tValidation Loss: 2.478412\n",
      "Epoch: 24845 \tTraining Loss: 1.399315 \tValidation Loss: 2.478904\n",
      "Epoch: 24846 \tTraining Loss: 1.424754 \tValidation Loss: 2.478049\n",
      "Epoch: 24847 \tTraining Loss: 1.445851 \tValidation Loss: 2.477970\n",
      "Epoch: 24848 \tTraining Loss: 1.391562 \tValidation Loss: 2.478105\n",
      "Epoch: 24849 \tTraining Loss: 1.449501 \tValidation Loss: 2.477414\n",
      "Epoch: 24850 \tTraining Loss: 1.449187 \tValidation Loss: 2.477585\n",
      "Epoch: 24851 \tTraining Loss: 1.452928 \tValidation Loss: 2.477385\n",
      "Epoch: 24852 \tTraining Loss: 1.444552 \tValidation Loss: 2.478193\n",
      "Epoch: 24853 \tTraining Loss: 1.411134 \tValidation Loss: 2.478068\n",
      "Epoch: 24854 \tTraining Loss: 1.445274 \tValidation Loss: 2.478642\n",
      "Epoch: 24855 \tTraining Loss: 1.434802 \tValidation Loss: 2.477834\n",
      "Epoch: 24856 \tTraining Loss: 1.445722 \tValidation Loss: 2.477628\n",
      "Epoch: 24857 \tTraining Loss: 1.446638 \tValidation Loss: 2.477997\n",
      "Epoch: 24858 \tTraining Loss: 1.434786 \tValidation Loss: 2.477712\n",
      "Epoch: 24859 \tTraining Loss: 1.420308 \tValidation Loss: 2.479116\n",
      "Epoch: 24860 \tTraining Loss: 1.423030 \tValidation Loss: 2.478036\n",
      "Epoch: 24861 \tTraining Loss: 1.444701 \tValidation Loss: 2.478679\n",
      "Epoch: 24862 \tTraining Loss: 1.410359 \tValidation Loss: 2.478918\n",
      "Epoch: 24863 \tTraining Loss: 1.424757 \tValidation Loss: 2.478703\n",
      "Epoch: 24864 \tTraining Loss: 1.452186 \tValidation Loss: 2.478606\n",
      "Epoch: 24865 \tTraining Loss: 1.429667 \tValidation Loss: 2.478497\n",
      "Epoch: 24866 \tTraining Loss: 1.430442 \tValidation Loss: 2.477805\n",
      "Epoch: 24867 \tTraining Loss: 1.400421 \tValidation Loss: 2.478248\n",
      "Epoch: 24868 \tTraining Loss: 1.436743 \tValidation Loss: 2.477758\n",
      "Epoch: 24869 \tTraining Loss: 1.402164 \tValidation Loss: 2.477955\n",
      "Epoch: 24870 \tTraining Loss: 1.391271 \tValidation Loss: 2.478950\n",
      "Epoch: 24871 \tTraining Loss: 1.392291 \tValidation Loss: 2.479014\n",
      "Epoch: 24872 \tTraining Loss: 1.386845 \tValidation Loss: 2.478420\n",
      "Epoch: 24873 \tTraining Loss: 1.452556 \tValidation Loss: 2.477335\n",
      "Epoch: 24874 \tTraining Loss: 1.452241 \tValidation Loss: 2.477974\n",
      "Epoch: 24875 \tTraining Loss: 1.397668 \tValidation Loss: 2.478368\n",
      "Epoch: 24876 \tTraining Loss: 1.419340 \tValidation Loss: 2.478077\n",
      "Epoch: 24877 \tTraining Loss: 1.468127 \tValidation Loss: 2.478450\n",
      "Epoch: 24878 \tTraining Loss: 1.446252 \tValidation Loss: 2.478945\n",
      "Epoch: 24879 \tTraining Loss: 1.442824 \tValidation Loss: 2.479430\n",
      "Epoch: 24880 \tTraining Loss: 1.384394 \tValidation Loss: 2.479191\n",
      "Epoch: 24881 \tTraining Loss: 1.453893 \tValidation Loss: 2.478436\n",
      "Epoch: 24882 \tTraining Loss: 1.386191 \tValidation Loss: 2.478152\n",
      "Epoch: 24883 \tTraining Loss: 1.417933 \tValidation Loss: 2.479309\n",
      "Epoch: 24884 \tTraining Loss: 1.390391 \tValidation Loss: 2.478628\n",
      "Epoch: 24885 \tTraining Loss: 1.430791 \tValidation Loss: 2.478206\n",
      "Epoch: 24886 \tTraining Loss: 1.401335 \tValidation Loss: 2.478226\n",
      "Epoch: 24887 \tTraining Loss: 1.399483 \tValidation Loss: 2.477971\n",
      "Epoch: 24888 \tTraining Loss: 1.455379 \tValidation Loss: 2.478174\n",
      "Epoch: 24889 \tTraining Loss: 1.408427 \tValidation Loss: 2.478659\n",
      "Epoch: 24890 \tTraining Loss: 1.410348 \tValidation Loss: 2.478406\n",
      "Epoch: 24891 \tTraining Loss: 1.418589 \tValidation Loss: 2.478566\n",
      "Epoch: 24892 \tTraining Loss: 1.452192 \tValidation Loss: 2.479355\n",
      "Epoch: 24893 \tTraining Loss: 1.395756 \tValidation Loss: 2.479059\n",
      "Epoch: 24894 \tTraining Loss: 1.446446 \tValidation Loss: 2.479182\n",
      "Epoch: 24895 \tTraining Loss: 1.377548 \tValidation Loss: 2.479317\n",
      "Epoch: 24896 \tTraining Loss: 1.428182 \tValidation Loss: 2.478523\n",
      "Epoch: 24897 \tTraining Loss: 1.460697 \tValidation Loss: 2.477908\n",
      "Epoch: 24898 \tTraining Loss: 1.396807 \tValidation Loss: 2.479472\n",
      "Epoch: 24899 \tTraining Loss: 1.465014 \tValidation Loss: 2.477813\n",
      "Epoch: 24900 \tTraining Loss: 1.424402 \tValidation Loss: 2.479267\n",
      "Epoch: 24901 \tTraining Loss: 1.434552 \tValidation Loss: 2.477347\n",
      "Epoch: 24902 \tTraining Loss: 1.422893 \tValidation Loss: 2.478883\n",
      "Epoch: 24903 \tTraining Loss: 1.418373 \tValidation Loss: 2.478806\n",
      "Epoch: 24904 \tTraining Loss: 1.419114 \tValidation Loss: 2.479041\n",
      "Epoch: 24905 \tTraining Loss: 1.480240 \tValidation Loss: 2.479250\n",
      "Epoch: 24906 \tTraining Loss: 1.443984 \tValidation Loss: 2.478386\n",
      "Epoch: 24907 \tTraining Loss: 1.374741 \tValidation Loss: 2.479587\n",
      "Epoch: 24908 \tTraining Loss: 1.418616 \tValidation Loss: 2.479600\n",
      "Epoch: 24909 \tTraining Loss: 1.410094 \tValidation Loss: 2.480553\n",
      "Epoch: 24910 \tTraining Loss: 1.435940 \tValidation Loss: 2.478961\n",
      "Epoch: 24911 \tTraining Loss: 1.447343 \tValidation Loss: 2.480047\n",
      "Epoch: 24912 \tTraining Loss: 1.417765 \tValidation Loss: 2.478810\n",
      "Epoch: 24913 \tTraining Loss: 1.436375 \tValidation Loss: 2.478454\n",
      "Epoch: 24914 \tTraining Loss: 1.445652 \tValidation Loss: 2.478997\n",
      "Epoch: 24915 \tTraining Loss: 1.437263 \tValidation Loss: 2.479080\n",
      "Epoch: 24916 \tTraining Loss: 1.388039 \tValidation Loss: 2.479856\n",
      "Epoch: 24917 \tTraining Loss: 1.461128 \tValidation Loss: 2.477953\n",
      "Epoch: 24918 \tTraining Loss: 1.436908 \tValidation Loss: 2.477381\n",
      "Epoch: 24919 \tTraining Loss: 1.414035 \tValidation Loss: 2.478518\n",
      "Epoch: 24920 \tTraining Loss: 1.464886 \tValidation Loss: 2.479259\n",
      "Epoch: 24921 \tTraining Loss: 1.367840 \tValidation Loss: 2.480011\n",
      "Epoch: 24922 \tTraining Loss: 1.441920 \tValidation Loss: 2.480594\n",
      "Epoch: 24923 \tTraining Loss: 1.396485 \tValidation Loss: 2.480004\n",
      "Epoch: 24924 \tTraining Loss: 1.399819 \tValidation Loss: 2.480043\n",
      "Epoch: 24925 \tTraining Loss: 1.405070 \tValidation Loss: 2.479910\n",
      "Epoch: 24926 \tTraining Loss: 1.431223 \tValidation Loss: 2.479910\n",
      "Epoch: 24927 \tTraining Loss: 1.473576 \tValidation Loss: 2.478577\n",
      "Epoch: 24928 \tTraining Loss: 1.450920 \tValidation Loss: 2.478407\n",
      "Epoch: 24929 \tTraining Loss: 1.394245 \tValidation Loss: 2.478820\n",
      "Epoch: 24930 \tTraining Loss: 1.402113 \tValidation Loss: 2.479849\n",
      "Epoch: 24931 \tTraining Loss: 1.439538 \tValidation Loss: 2.479780\n",
      "Epoch: 24932 \tTraining Loss: 1.460091 \tValidation Loss: 2.479416\n",
      "Epoch: 24933 \tTraining Loss: 1.427324 \tValidation Loss: 2.478860\n",
      "Epoch: 24934 \tTraining Loss: 1.444295 \tValidation Loss: 2.479561\n",
      "Epoch: 24935 \tTraining Loss: 1.422487 \tValidation Loss: 2.480323\n",
      "Epoch: 24936 \tTraining Loss: 1.435244 \tValidation Loss: 2.479842\n",
      "Epoch: 24937 \tTraining Loss: 1.437863 \tValidation Loss: 2.479887\n",
      "Epoch: 24938 \tTraining Loss: 1.395066 \tValidation Loss: 2.480215\n",
      "Epoch: 24939 \tTraining Loss: 1.439610 \tValidation Loss: 2.479675\n",
      "Epoch: 24940 \tTraining Loss: 1.447414 \tValidation Loss: 2.479034\n",
      "Epoch: 24941 \tTraining Loss: 1.372139 \tValidation Loss: 2.479210\n",
      "Epoch: 24942 \tTraining Loss: 1.433150 \tValidation Loss: 2.479820\n",
      "Epoch: 24943 \tTraining Loss: 1.404843 \tValidation Loss: 2.479709\n",
      "Epoch: 24944 \tTraining Loss: 1.447811 \tValidation Loss: 2.478897\n",
      "Epoch: 24945 \tTraining Loss: 1.420310 \tValidation Loss: 2.479530\n",
      "Epoch: 24946 \tTraining Loss: 1.463654 \tValidation Loss: 2.479109\n",
      "Epoch: 24947 \tTraining Loss: 1.484669 \tValidation Loss: 2.478805\n",
      "Epoch: 24948 \tTraining Loss: 1.440843 \tValidation Loss: 2.479120\n",
      "Epoch: 24949 \tTraining Loss: 1.430150 \tValidation Loss: 2.478871\n",
      "Epoch: 24950 \tTraining Loss: 1.372019 \tValidation Loss: 2.480628\n",
      "Epoch: 24951 \tTraining Loss: 1.417849 \tValidation Loss: 2.479738\n",
      "Epoch: 24952 \tTraining Loss: 1.452899 \tValidation Loss: 2.478501\n",
      "Epoch: 24953 \tTraining Loss: 1.395068 \tValidation Loss: 2.478792\n",
      "Epoch: 24954 \tTraining Loss: 1.448967 \tValidation Loss: 2.479224\n",
      "Epoch: 24955 \tTraining Loss: 1.415664 \tValidation Loss: 2.479567\n",
      "Epoch: 24956 \tTraining Loss: 1.409093 \tValidation Loss: 2.481107\n",
      "Epoch: 24957 \tTraining Loss: 1.429834 \tValidation Loss: 2.479499\n",
      "Epoch: 24958 \tTraining Loss: 1.415837 \tValidation Loss: 2.479909\n",
      "Epoch: 24959 \tTraining Loss: 1.425880 \tValidation Loss: 2.480428\n",
      "Epoch: 24960 \tTraining Loss: 1.466040 \tValidation Loss: 2.479401\n",
      "Epoch: 24961 \tTraining Loss: 1.420673 \tValidation Loss: 2.479567\n",
      "Epoch: 24962 \tTraining Loss: 1.424340 \tValidation Loss: 2.480583\n",
      "Epoch: 24963 \tTraining Loss: 1.394574 \tValidation Loss: 2.481523\n",
      "Epoch: 24964 \tTraining Loss: 1.416988 \tValidation Loss: 2.480611\n",
      "Epoch: 24965 \tTraining Loss: 1.392604 \tValidation Loss: 2.480861\n",
      "Epoch: 24966 \tTraining Loss: 1.439091 \tValidation Loss: 2.480253\n",
      "Epoch: 24967 \tTraining Loss: 1.437293 \tValidation Loss: 2.480510\n",
      "Epoch: 24968 \tTraining Loss: 1.408669 \tValidation Loss: 2.480321\n",
      "Epoch: 24969 \tTraining Loss: 1.442292 \tValidation Loss: 2.481148\n",
      "Epoch: 24970 \tTraining Loss: 1.457068 \tValidation Loss: 2.480938\n",
      "Epoch: 24971 \tTraining Loss: 1.408598 \tValidation Loss: 2.480462\n",
      "Epoch: 24972 \tTraining Loss: 1.409566 \tValidation Loss: 2.480811\n",
      "Epoch: 24973 \tTraining Loss: 1.439206 \tValidation Loss: 2.480375\n",
      "Epoch: 24974 \tTraining Loss: 1.448678 \tValidation Loss: 2.479895\n",
      "Epoch: 24975 \tTraining Loss: 1.449711 \tValidation Loss: 2.480079\n",
      "Epoch: 24976 \tTraining Loss: 1.421267 \tValidation Loss: 2.481487\n",
      "Epoch: 24977 \tTraining Loss: 1.410534 \tValidation Loss: 2.481025\n",
      "Epoch: 24978 \tTraining Loss: 1.394696 \tValidation Loss: 2.481367\n",
      "Epoch: 24979 \tTraining Loss: 1.419894 \tValidation Loss: 2.481281\n",
      "Epoch: 24980 \tTraining Loss: 1.414010 \tValidation Loss: 2.480654\n",
      "Epoch: 24981 \tTraining Loss: 1.431714 \tValidation Loss: 2.479795\n",
      "Epoch: 24982 \tTraining Loss: 1.422599 \tValidation Loss: 2.480206\n",
      "Epoch: 24983 \tTraining Loss: 1.438969 \tValidation Loss: 2.481139\n",
      "Epoch: 24984 \tTraining Loss: 1.422616 \tValidation Loss: 2.480891\n",
      "Epoch: 24985 \tTraining Loss: 1.395846 \tValidation Loss: 2.481061\n",
      "Epoch: 24986 \tTraining Loss: 1.413689 \tValidation Loss: 2.480461\n",
      "Epoch: 24987 \tTraining Loss: 1.378972 \tValidation Loss: 2.480074\n",
      "Epoch: 24988 \tTraining Loss: 1.427577 \tValidation Loss: 2.481348\n",
      "Epoch: 24989 \tTraining Loss: 1.449281 \tValidation Loss: 2.481292\n",
      "Epoch: 24990 \tTraining Loss: 1.461343 \tValidation Loss: 2.480607\n",
      "Epoch: 24991 \tTraining Loss: 1.473727 \tValidation Loss: 2.480560\n",
      "Epoch: 24992 \tTraining Loss: 1.420008 \tValidation Loss: 2.479431\n",
      "Epoch: 24993 \tTraining Loss: 1.445193 \tValidation Loss: 2.480433\n",
      "Epoch: 24994 \tTraining Loss: 1.422535 \tValidation Loss: 2.481411\n",
      "Epoch: 24995 \tTraining Loss: 1.440907 \tValidation Loss: 2.480312\n",
      "Epoch: 24996 \tTraining Loss: 1.416610 \tValidation Loss: 2.480436\n",
      "Epoch: 24997 \tTraining Loss: 1.462022 \tValidation Loss: 2.480489\n",
      "Epoch: 24998 \tTraining Loss: 1.424365 \tValidation Loss: 2.481344\n",
      "Epoch: 24999 \tTraining Loss: 1.445824 \tValidation Loss: 2.479987\n",
      "Epoch: 25000 \tTraining Loss: 1.440059 \tValidation Loss: 2.480709\n",
      "Epoch: 25001 \tTraining Loss: 1.400364 \tValidation Loss: 2.481853\n",
      "Epoch: 25002 \tTraining Loss: 1.462351 \tValidation Loss: 2.481060\n",
      "Epoch: 25003 \tTraining Loss: 1.416767 \tValidation Loss: 2.481610\n",
      "Epoch: 25004 \tTraining Loss: 1.403336 \tValidation Loss: 2.481102\n",
      "Epoch: 25005 \tTraining Loss: 1.396781 \tValidation Loss: 2.480366\n",
      "Epoch: 25006 \tTraining Loss: 1.431750 \tValidation Loss: 2.481189\n",
      "Epoch: 25007 \tTraining Loss: 1.461124 \tValidation Loss: 2.481154\n",
      "Epoch: 25008 \tTraining Loss: 1.411270 \tValidation Loss: 2.481398\n",
      "Epoch: 25009 \tTraining Loss: 1.417129 \tValidation Loss: 2.481349\n",
      "Epoch: 25010 \tTraining Loss: 1.443815 \tValidation Loss: 2.480404\n",
      "Epoch: 25011 \tTraining Loss: 1.432319 \tValidation Loss: 2.480972\n",
      "Epoch: 25012 \tTraining Loss: 1.381519 \tValidation Loss: 2.482119\n",
      "Epoch: 25013 \tTraining Loss: 1.441185 \tValidation Loss: 2.480893\n",
      "Epoch: 25014 \tTraining Loss: 1.412592 \tValidation Loss: 2.481362\n",
      "Epoch: 25015 \tTraining Loss: 1.441512 \tValidation Loss: 2.481426\n",
      "Epoch: 25016 \tTraining Loss: 1.429869 \tValidation Loss: 2.481089\n",
      "Epoch: 25017 \tTraining Loss: 1.412338 \tValidation Loss: 2.482472\n",
      "Epoch: 25018 \tTraining Loss: 1.412057 \tValidation Loss: 2.481001\n",
      "Epoch: 25019 \tTraining Loss: 1.413833 \tValidation Loss: 2.481981\n",
      "Epoch: 25020 \tTraining Loss: 1.406372 \tValidation Loss: 2.481286\n",
      "Epoch: 25021 \tTraining Loss: 1.407273 \tValidation Loss: 2.482026\n",
      "Epoch: 25022 \tTraining Loss: 1.429104 \tValidation Loss: 2.481096\n",
      "Epoch: 25023 \tTraining Loss: 1.407240 \tValidation Loss: 2.481798\n",
      "Epoch: 25024 \tTraining Loss: 1.463660 \tValidation Loss: 2.482246\n",
      "Epoch: 25025 \tTraining Loss: 1.409004 \tValidation Loss: 2.481935\n",
      "Epoch: 25026 \tTraining Loss: 1.477028 \tValidation Loss: 2.481649\n",
      "Epoch: 25027 \tTraining Loss: 1.444750 \tValidation Loss: 2.480818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25028 \tTraining Loss: 1.400584 \tValidation Loss: 2.481598\n",
      "Epoch: 25029 \tTraining Loss: 1.449801 \tValidation Loss: 2.480793\n",
      "Epoch: 25030 \tTraining Loss: 1.398928 \tValidation Loss: 2.481243\n",
      "Epoch: 25031 \tTraining Loss: 1.428521 \tValidation Loss: 2.481886\n",
      "Epoch: 25032 \tTraining Loss: 1.432770 \tValidation Loss: 2.481746\n",
      "Epoch: 25033 \tTraining Loss: 1.425353 \tValidation Loss: 2.481302\n",
      "Epoch: 25034 \tTraining Loss: 1.385686 \tValidation Loss: 2.481338\n",
      "Epoch: 25035 \tTraining Loss: 1.408416 \tValidation Loss: 2.481531\n",
      "Epoch: 25036 \tTraining Loss: 1.443641 \tValidation Loss: 2.481194\n",
      "Epoch: 25037 \tTraining Loss: 1.453237 \tValidation Loss: 2.481420\n",
      "Epoch: 25038 \tTraining Loss: 1.432224 \tValidation Loss: 2.481498\n",
      "Epoch: 25039 \tTraining Loss: 1.436665 \tValidation Loss: 2.481055\n",
      "Epoch: 25040 \tTraining Loss: 1.414975 \tValidation Loss: 2.481791\n",
      "Epoch: 25041 \tTraining Loss: 1.472178 \tValidation Loss: 2.481421\n",
      "Epoch: 25042 \tTraining Loss: 1.415201 \tValidation Loss: 2.482724\n",
      "Epoch: 25043 \tTraining Loss: 1.470293 \tValidation Loss: 2.481440\n",
      "Epoch: 25044 \tTraining Loss: 1.415180 \tValidation Loss: 2.481631\n",
      "Epoch: 25045 \tTraining Loss: 1.456300 \tValidation Loss: 2.481409\n",
      "Epoch: 25046 \tTraining Loss: 1.438962 \tValidation Loss: 2.482485\n",
      "Epoch: 25047 \tTraining Loss: 1.448502 \tValidation Loss: 2.481486\n",
      "Epoch: 25048 \tTraining Loss: 1.402726 \tValidation Loss: 2.481833\n",
      "Epoch: 25049 \tTraining Loss: 1.407648 \tValidation Loss: 2.480873\n",
      "Epoch: 25050 \tTraining Loss: 1.440356 \tValidation Loss: 2.480640\n",
      "Epoch: 25051 \tTraining Loss: 1.452074 \tValidation Loss: 2.481669\n",
      "Epoch: 25052 \tTraining Loss: 1.435966 \tValidation Loss: 2.481290\n",
      "Epoch: 25053 \tTraining Loss: 1.409724 \tValidation Loss: 2.482132\n",
      "Epoch: 25054 \tTraining Loss: 1.444696 \tValidation Loss: 2.481468\n",
      "Epoch: 25055 \tTraining Loss: 1.423055 \tValidation Loss: 2.482068\n",
      "Epoch: 25056 \tTraining Loss: 1.390841 \tValidation Loss: 2.482539\n",
      "Epoch: 25057 \tTraining Loss: 1.420529 \tValidation Loss: 2.481679\n",
      "Epoch: 25058 \tTraining Loss: 1.393315 \tValidation Loss: 2.481877\n",
      "Epoch: 25059 \tTraining Loss: 1.457235 \tValidation Loss: 2.481959\n",
      "Epoch: 25060 \tTraining Loss: 1.454584 \tValidation Loss: 2.480823\n",
      "Epoch: 25061 \tTraining Loss: 1.414181 \tValidation Loss: 2.481827\n",
      "Epoch: 25062 \tTraining Loss: 1.388274 \tValidation Loss: 2.482122\n",
      "Epoch: 25063 \tTraining Loss: 1.412142 \tValidation Loss: 2.481570\n",
      "Epoch: 25064 \tTraining Loss: 1.395105 \tValidation Loss: 2.482133\n",
      "Epoch: 25065 \tTraining Loss: 1.387734 \tValidation Loss: 2.482232\n",
      "Epoch: 25066 \tTraining Loss: 1.382839 \tValidation Loss: 2.482640\n",
      "Epoch: 25067 \tTraining Loss: 1.372572 \tValidation Loss: 2.482865\n",
      "Epoch: 25068 \tTraining Loss: 1.432332 \tValidation Loss: 2.481908\n",
      "Epoch: 25069 \tTraining Loss: 1.435611 \tValidation Loss: 2.482831\n",
      "Epoch: 25070 \tTraining Loss: 1.442261 \tValidation Loss: 2.483191\n",
      "Epoch: 25071 \tTraining Loss: 1.423018 \tValidation Loss: 2.482113\n",
      "Epoch: 25072 \tTraining Loss: 1.396649 \tValidation Loss: 2.482469\n",
      "Epoch: 25073 \tTraining Loss: 1.436790 \tValidation Loss: 2.482765\n",
      "Epoch: 25074 \tTraining Loss: 1.389285 \tValidation Loss: 2.482395\n",
      "Epoch: 25075 \tTraining Loss: 1.405275 \tValidation Loss: 2.482609\n",
      "Epoch: 25076 \tTraining Loss: 1.390533 \tValidation Loss: 2.482502\n",
      "Epoch: 25077 \tTraining Loss: 1.401540 \tValidation Loss: 2.482521\n",
      "Epoch: 25078 \tTraining Loss: 1.422931 \tValidation Loss: 2.482973\n",
      "Epoch: 25079 \tTraining Loss: 1.442589 \tValidation Loss: 2.482290\n",
      "Epoch: 25080 \tTraining Loss: 1.405739 \tValidation Loss: 2.482755\n",
      "Epoch: 25081 \tTraining Loss: 1.467142 \tValidation Loss: 2.483204\n",
      "Epoch: 25082 \tTraining Loss: 1.421762 \tValidation Loss: 2.483331\n",
      "Epoch: 25083 \tTraining Loss: 1.436797 \tValidation Loss: 2.483216\n",
      "Epoch: 25084 \tTraining Loss: 1.429774 \tValidation Loss: 2.482714\n",
      "Epoch: 25085 \tTraining Loss: 1.445639 \tValidation Loss: 2.483496\n",
      "Epoch: 25086 \tTraining Loss: 1.443919 \tValidation Loss: 2.483125\n",
      "Epoch: 25087 \tTraining Loss: 1.410277 \tValidation Loss: 2.482420\n",
      "Epoch: 25088 \tTraining Loss: 1.394758 \tValidation Loss: 2.482865\n",
      "Epoch: 25089 \tTraining Loss: 1.395986 \tValidation Loss: 2.482059\n",
      "Epoch: 25090 \tTraining Loss: 1.463828 \tValidation Loss: 2.481728\n",
      "Epoch: 25091 \tTraining Loss: 1.438652 \tValidation Loss: 2.482407\n",
      "Epoch: 25092 \tTraining Loss: 1.417382 \tValidation Loss: 2.483299\n",
      "Epoch: 25093 \tTraining Loss: 1.421299 \tValidation Loss: 2.481702\n",
      "Epoch: 25094 \tTraining Loss: 1.424436 \tValidation Loss: 2.482399\n",
      "Epoch: 25095 \tTraining Loss: 1.409698 \tValidation Loss: 2.483205\n",
      "Epoch: 25096 \tTraining Loss: 1.406786 \tValidation Loss: 2.482751\n",
      "Epoch: 25097 \tTraining Loss: 1.417045 \tValidation Loss: 2.483305\n",
      "Epoch: 25098 \tTraining Loss: 1.407088 \tValidation Loss: 2.482359\n",
      "Epoch: 25099 \tTraining Loss: 1.434214 \tValidation Loss: 2.481945\n",
      "Epoch: 25100 \tTraining Loss: 1.419113 \tValidation Loss: 2.483371\n",
      "Epoch: 25101 \tTraining Loss: 1.410723 \tValidation Loss: 2.483445\n",
      "Epoch: 25102 \tTraining Loss: 1.428537 \tValidation Loss: 2.483521\n",
      "Epoch: 25103 \tTraining Loss: 1.389050 \tValidation Loss: 2.483822\n",
      "Epoch: 25104 \tTraining Loss: 1.380728 \tValidation Loss: 2.484387\n",
      "Epoch: 25105 \tTraining Loss: 1.409267 \tValidation Loss: 2.483380\n",
      "Epoch: 25106 \tTraining Loss: 1.453133 \tValidation Loss: 2.483119\n",
      "Epoch: 25107 \tTraining Loss: 1.441928 \tValidation Loss: 2.482103\n",
      "Epoch: 25108 \tTraining Loss: 1.458722 \tValidation Loss: 2.482345\n",
      "Epoch: 25109 \tTraining Loss: 1.425070 \tValidation Loss: 2.482631\n",
      "Epoch: 25110 \tTraining Loss: 1.466009 \tValidation Loss: 2.482640\n",
      "Epoch: 25111 \tTraining Loss: 1.465015 \tValidation Loss: 2.482543\n",
      "Epoch: 25112 \tTraining Loss: 1.397033 \tValidation Loss: 2.482758\n",
      "Epoch: 25113 \tTraining Loss: 1.391598 \tValidation Loss: 2.482926\n",
      "Epoch: 25114 \tTraining Loss: 1.472416 \tValidation Loss: 2.482961\n",
      "Epoch: 25115 \tTraining Loss: 1.382162 \tValidation Loss: 2.482521\n",
      "Epoch: 25116 \tTraining Loss: 1.459019 \tValidation Loss: 2.482331\n",
      "Epoch: 25117 \tTraining Loss: 1.414208 \tValidation Loss: 2.483639\n",
      "Epoch: 25118 \tTraining Loss: 1.430732 \tValidation Loss: 2.483579\n",
      "Epoch: 25119 \tTraining Loss: 1.431167 \tValidation Loss: 2.482659\n",
      "Epoch: 25120 \tTraining Loss: 1.417967 \tValidation Loss: 2.483095\n",
      "Epoch: 25121 \tTraining Loss: 1.406786 \tValidation Loss: 2.483665\n",
      "Epoch: 25122 \tTraining Loss: 1.425480 \tValidation Loss: 2.483023\n",
      "Epoch: 25123 \tTraining Loss: 1.412687 \tValidation Loss: 2.484110\n",
      "Epoch: 25124 \tTraining Loss: 1.385831 \tValidation Loss: 2.484560\n",
      "Epoch: 25125 \tTraining Loss: 1.428304 \tValidation Loss: 2.483817\n",
      "Epoch: 25126 \tTraining Loss: 1.383712 \tValidation Loss: 2.484623\n",
      "Epoch: 25127 \tTraining Loss: 1.436131 \tValidation Loss: 2.483500\n",
      "Epoch: 25128 \tTraining Loss: 1.454830 \tValidation Loss: 2.483573\n",
      "Epoch: 25129 \tTraining Loss: 1.426116 \tValidation Loss: 2.483576\n",
      "Epoch: 25130 \tTraining Loss: 1.427088 \tValidation Loss: 2.483262\n",
      "Epoch: 25131 \tTraining Loss: 1.439267 \tValidation Loss: 2.483578\n",
      "Epoch: 25132 \tTraining Loss: 1.419938 \tValidation Loss: 2.483518\n",
      "Epoch: 25133 \tTraining Loss: 1.406109 \tValidation Loss: 2.483640\n",
      "Epoch: 25134 \tTraining Loss: 1.393027 \tValidation Loss: 2.482873\n",
      "Epoch: 25135 \tTraining Loss: 1.400741 \tValidation Loss: 2.484318\n",
      "Epoch: 25136 \tTraining Loss: 1.478200 \tValidation Loss: 2.483325\n",
      "Epoch: 25137 \tTraining Loss: 1.422299 \tValidation Loss: 2.483676\n",
      "Epoch: 25138 \tTraining Loss: 1.405756 \tValidation Loss: 2.482965\n",
      "Epoch: 25139 \tTraining Loss: 1.440846 \tValidation Loss: 2.483304\n",
      "Epoch: 25140 \tTraining Loss: 1.460363 \tValidation Loss: 2.483655\n",
      "Epoch: 25141 \tTraining Loss: 1.420545 \tValidation Loss: 2.482849\n",
      "Epoch: 25142 \tTraining Loss: 1.394883 \tValidation Loss: 2.484369\n",
      "Epoch: 25143 \tTraining Loss: 1.387512 \tValidation Loss: 2.484228\n",
      "Epoch: 25144 \tTraining Loss: 1.377058 \tValidation Loss: 2.484008\n",
      "Epoch: 25145 \tTraining Loss: 1.420902 \tValidation Loss: 2.483363\n",
      "Epoch: 25146 \tTraining Loss: 1.365030 \tValidation Loss: 2.484377\n",
      "Epoch: 25147 \tTraining Loss: 1.403464 \tValidation Loss: 2.483552\n",
      "Epoch: 25148 \tTraining Loss: 1.457500 \tValidation Loss: 2.483657\n",
      "Epoch: 25149 \tTraining Loss: 1.404000 \tValidation Loss: 2.484169\n",
      "Epoch: 25150 \tTraining Loss: 1.412444 \tValidation Loss: 2.484351\n",
      "Epoch: 25151 \tTraining Loss: 1.423280 \tValidation Loss: 2.483755\n",
      "Epoch: 25152 \tTraining Loss: 1.440789 \tValidation Loss: 2.483334\n",
      "Epoch: 25153 \tTraining Loss: 1.434949 \tValidation Loss: 2.483690\n",
      "Epoch: 25154 \tTraining Loss: 1.419778 \tValidation Loss: 2.483273\n",
      "Epoch: 25155 \tTraining Loss: 1.428326 \tValidation Loss: 2.483903\n",
      "Epoch: 25156 \tTraining Loss: 1.421457 \tValidation Loss: 2.484008\n",
      "Epoch: 25157 \tTraining Loss: 1.434321 \tValidation Loss: 2.483607\n",
      "Epoch: 25158 \tTraining Loss: 1.433930 \tValidation Loss: 2.483797\n",
      "Epoch: 25159 \tTraining Loss: 1.425369 \tValidation Loss: 2.483109\n",
      "Epoch: 25160 \tTraining Loss: 1.403513 \tValidation Loss: 2.484385\n",
      "Epoch: 25161 \tTraining Loss: 1.399922 \tValidation Loss: 2.485037\n",
      "Epoch: 25162 \tTraining Loss: 1.399145 \tValidation Loss: 2.484475\n",
      "Epoch: 25163 \tTraining Loss: 1.407007 \tValidation Loss: 2.483476\n",
      "Epoch: 25164 \tTraining Loss: 1.403817 \tValidation Loss: 2.484112\n",
      "Epoch: 25165 \tTraining Loss: 1.413723 \tValidation Loss: 2.484353\n",
      "Epoch: 25166 \tTraining Loss: 1.422068 \tValidation Loss: 2.484091\n",
      "Epoch: 25167 \tTraining Loss: 1.451408 \tValidation Loss: 2.483463\n",
      "Epoch: 25168 \tTraining Loss: 1.408363 \tValidation Loss: 2.484318\n",
      "Epoch: 25169 \tTraining Loss: 1.381717 \tValidation Loss: 2.484693\n",
      "Epoch: 25170 \tTraining Loss: 1.391616 \tValidation Loss: 2.484595\n",
      "Epoch: 25171 \tTraining Loss: 1.435344 \tValidation Loss: 2.483700\n",
      "Epoch: 25172 \tTraining Loss: 1.469048 \tValidation Loss: 2.483092\n",
      "Epoch: 25173 \tTraining Loss: 1.442538 \tValidation Loss: 2.484238\n",
      "Epoch: 25174 \tTraining Loss: 1.401057 \tValidation Loss: 2.484845\n",
      "Epoch: 25175 \tTraining Loss: 1.461567 \tValidation Loss: 2.484357\n",
      "Epoch: 25176 \tTraining Loss: 1.422554 \tValidation Loss: 2.484164\n",
      "Epoch: 25177 \tTraining Loss: 1.433002 \tValidation Loss: 2.484415\n",
      "Epoch: 25178 \tTraining Loss: 1.441854 \tValidation Loss: 2.484608\n",
      "Epoch: 25179 \tTraining Loss: 1.450535 \tValidation Loss: 2.484319\n",
      "Epoch: 25180 \tTraining Loss: 1.437072 \tValidation Loss: 2.483654\n",
      "Epoch: 25181 \tTraining Loss: 1.460021 \tValidation Loss: 2.483659\n",
      "Epoch: 25182 \tTraining Loss: 1.420437 \tValidation Loss: 2.482982\n",
      "Epoch: 25183 \tTraining Loss: 1.417886 \tValidation Loss: 2.484344\n",
      "Epoch: 25184 \tTraining Loss: 1.460769 \tValidation Loss: 2.483788\n",
      "Epoch: 25185 \tTraining Loss: 1.435559 \tValidation Loss: 2.483349\n",
      "Epoch: 25186 \tTraining Loss: 1.423316 \tValidation Loss: 2.483624\n",
      "Epoch: 25187 \tTraining Loss: 1.395974 \tValidation Loss: 2.483973\n",
      "Epoch: 25188 \tTraining Loss: 1.415296 \tValidation Loss: 2.484014\n",
      "Epoch: 25189 \tTraining Loss: 1.392858 \tValidation Loss: 2.484201\n",
      "Epoch: 25190 \tTraining Loss: 1.389042 \tValidation Loss: 2.485044\n",
      "Epoch: 25191 \tTraining Loss: 1.432971 \tValidation Loss: 2.484157\n",
      "Epoch: 25192 \tTraining Loss: 1.448306 \tValidation Loss: 2.484787\n",
      "Epoch: 25193 \tTraining Loss: 1.408655 \tValidation Loss: 2.484920\n",
      "Epoch: 25194 \tTraining Loss: 1.382291 \tValidation Loss: 2.485610\n",
      "Epoch: 25195 \tTraining Loss: 1.461353 \tValidation Loss: 2.485045\n",
      "Epoch: 25196 \tTraining Loss: 1.437927 \tValidation Loss: 2.483820\n",
      "Epoch: 25197 \tTraining Loss: 1.409781 \tValidation Loss: 2.484921\n",
      "Epoch: 25198 \tTraining Loss: 1.425705 \tValidation Loss: 2.484596\n",
      "Epoch: 25199 \tTraining Loss: 1.399449 \tValidation Loss: 2.484570\n",
      "Epoch: 25200 \tTraining Loss: 1.423190 \tValidation Loss: 2.485361\n",
      "Epoch: 25201 \tTraining Loss: 1.379958 \tValidation Loss: 2.485915\n",
      "Epoch: 25202 \tTraining Loss: 1.402318 \tValidation Loss: 2.486511\n",
      "Epoch: 25203 \tTraining Loss: 1.435262 \tValidation Loss: 2.484971\n",
      "Epoch: 25204 \tTraining Loss: 1.412541 \tValidation Loss: 2.485610\n",
      "Epoch: 25205 \tTraining Loss: 1.433863 \tValidation Loss: 2.485322\n",
      "Epoch: 25206 \tTraining Loss: 1.383443 \tValidation Loss: 2.485457\n",
      "Epoch: 25207 \tTraining Loss: 1.428274 \tValidation Loss: 2.485078\n",
      "Epoch: 25208 \tTraining Loss: 1.419651 \tValidation Loss: 2.485389\n",
      "Epoch: 25209 \tTraining Loss: 1.408895 \tValidation Loss: 2.484874\n",
      "Epoch: 25210 \tTraining Loss: 1.407510 \tValidation Loss: 2.484994\n",
      "Epoch: 25211 \tTraining Loss: 1.424023 \tValidation Loss: 2.485008\n",
      "Epoch: 25212 \tTraining Loss: 1.439149 \tValidation Loss: 2.484483\n",
      "Epoch: 25213 \tTraining Loss: 1.418880 \tValidation Loss: 2.484303\n",
      "Epoch: 25214 \tTraining Loss: 1.419363 \tValidation Loss: 2.485365\n",
      "Epoch: 25215 \tTraining Loss: 1.422822 \tValidation Loss: 2.485569\n",
      "Epoch: 25216 \tTraining Loss: 1.421524 \tValidation Loss: 2.484849\n",
      "Epoch: 25217 \tTraining Loss: 1.418523 \tValidation Loss: 2.484899\n",
      "Epoch: 25218 \tTraining Loss: 1.410756 \tValidation Loss: 2.486591\n",
      "Epoch: 25219 \tTraining Loss: 1.402212 \tValidation Loss: 2.484566\n",
      "Epoch: 25220 \tTraining Loss: 1.392480 \tValidation Loss: 2.484886\n",
      "Epoch: 25221 \tTraining Loss: 1.416983 \tValidation Loss: 2.485052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25222 \tTraining Loss: 1.429554 \tValidation Loss: 2.485499\n",
      "Epoch: 25223 \tTraining Loss: 1.387962 \tValidation Loss: 2.486279\n",
      "Epoch: 25224 \tTraining Loss: 1.428940 \tValidation Loss: 2.486329\n",
      "Epoch: 25225 \tTraining Loss: 1.378280 \tValidation Loss: 2.485257\n",
      "Epoch: 25226 \tTraining Loss: 1.457602 \tValidation Loss: 2.485001\n",
      "Epoch: 25227 \tTraining Loss: 1.421983 \tValidation Loss: 2.484838\n",
      "Epoch: 25228 \tTraining Loss: 1.469832 \tValidation Loss: 2.484817\n",
      "Epoch: 25229 \tTraining Loss: 1.379070 \tValidation Loss: 2.486533\n",
      "Epoch: 25230 \tTraining Loss: 1.427758 \tValidation Loss: 2.485463\n",
      "Epoch: 25231 \tTraining Loss: 1.445904 \tValidation Loss: 2.486167\n",
      "Epoch: 25232 \tTraining Loss: 1.385194 \tValidation Loss: 2.485864\n",
      "Epoch: 25233 \tTraining Loss: 1.434170 \tValidation Loss: 2.485975\n",
      "Epoch: 25234 \tTraining Loss: 1.405246 \tValidation Loss: 2.485963\n",
      "Epoch: 25235 \tTraining Loss: 1.433169 \tValidation Loss: 2.486215\n",
      "Epoch: 25236 \tTraining Loss: 1.378815 \tValidation Loss: 2.486941\n",
      "Epoch: 25237 \tTraining Loss: 1.418601 \tValidation Loss: 2.484920\n",
      "Epoch: 25238 \tTraining Loss: 1.363149 \tValidation Loss: 2.486790\n",
      "Epoch: 25239 \tTraining Loss: 1.396652 \tValidation Loss: 2.485253\n",
      "Epoch: 25240 \tTraining Loss: 1.436216 \tValidation Loss: 2.484661\n",
      "Epoch: 25241 \tTraining Loss: 1.373064 \tValidation Loss: 2.486622\n",
      "Epoch: 25242 \tTraining Loss: 1.372967 \tValidation Loss: 2.486988\n",
      "Epoch: 25243 \tTraining Loss: 1.411350 \tValidation Loss: 2.485710\n",
      "Epoch: 25244 \tTraining Loss: 1.420602 \tValidation Loss: 2.485389\n",
      "Epoch: 25245 \tTraining Loss: 1.439615 \tValidation Loss: 2.485881\n",
      "Epoch: 25246 \tTraining Loss: 1.425740 \tValidation Loss: 2.485748\n",
      "Epoch: 25247 \tTraining Loss: 1.435932 \tValidation Loss: 2.484464\n",
      "Epoch: 25248 \tTraining Loss: 1.500134 \tValidation Loss: 2.484534\n",
      "Epoch: 25249 \tTraining Loss: 1.434776 \tValidation Loss: 2.484281\n",
      "Epoch: 25250 \tTraining Loss: 1.439593 \tValidation Loss: 2.484202\n",
      "Epoch: 25251 \tTraining Loss: 1.431891 \tValidation Loss: 2.484376\n",
      "Epoch: 25252 \tTraining Loss: 1.434502 \tValidation Loss: 2.484335\n",
      "Epoch: 25253 \tTraining Loss: 1.387815 \tValidation Loss: 2.485343\n",
      "Epoch: 25254 \tTraining Loss: 1.447521 \tValidation Loss: 2.485470\n",
      "Epoch: 25255 \tTraining Loss: 1.403785 \tValidation Loss: 2.486000\n",
      "Epoch: 25256 \tTraining Loss: 1.421781 \tValidation Loss: 2.485328\n",
      "Epoch: 25257 \tTraining Loss: 1.431948 \tValidation Loss: 2.484951\n",
      "Epoch: 25258 \tTraining Loss: 1.431261 \tValidation Loss: 2.485964\n",
      "Epoch: 25259 \tTraining Loss: 1.366062 \tValidation Loss: 2.486257\n",
      "Epoch: 25260 \tTraining Loss: 1.369427 \tValidation Loss: 2.486653\n",
      "Epoch: 25261 \tTraining Loss: 1.411095 \tValidation Loss: 2.486518\n",
      "Epoch: 25262 \tTraining Loss: 1.410527 \tValidation Loss: 2.486084\n",
      "Epoch: 25263 \tTraining Loss: 1.416403 \tValidation Loss: 2.485873\n",
      "Epoch: 25264 \tTraining Loss: 1.380504 \tValidation Loss: 2.486063\n",
      "Epoch: 25265 \tTraining Loss: 1.401644 \tValidation Loss: 2.486257\n",
      "Epoch: 25266 \tTraining Loss: 1.431642 \tValidation Loss: 2.485687\n",
      "Epoch: 25267 \tTraining Loss: 1.401857 \tValidation Loss: 2.485974\n",
      "Epoch: 25268 \tTraining Loss: 1.446121 \tValidation Loss: 2.486156\n",
      "Epoch: 25269 \tTraining Loss: 1.426968 \tValidation Loss: 2.485959\n",
      "Epoch: 25270 \tTraining Loss: 1.410585 \tValidation Loss: 2.486965\n",
      "Epoch: 25271 \tTraining Loss: 1.452934 \tValidation Loss: 2.486781\n",
      "Epoch: 25272 \tTraining Loss: 1.368348 \tValidation Loss: 2.486643\n",
      "Epoch: 25273 \tTraining Loss: 1.407277 \tValidation Loss: 2.486399\n",
      "Epoch: 25274 \tTraining Loss: 1.396463 \tValidation Loss: 2.486391\n",
      "Epoch: 25275 \tTraining Loss: 1.402486 \tValidation Loss: 2.486897\n",
      "Epoch: 25276 \tTraining Loss: 1.422634 \tValidation Loss: 2.487370\n",
      "Epoch: 25277 \tTraining Loss: 1.395235 \tValidation Loss: 2.488173\n",
      "Epoch: 25278 \tTraining Loss: 1.413139 \tValidation Loss: 2.486972\n",
      "Epoch: 25279 \tTraining Loss: 1.406670 \tValidation Loss: 2.486387\n",
      "Epoch: 25280 \tTraining Loss: 1.395782 \tValidation Loss: 2.486769\n",
      "Epoch: 25281 \tTraining Loss: 1.434984 \tValidation Loss: 2.487367\n",
      "Epoch: 25282 \tTraining Loss: 1.419453 \tValidation Loss: 2.485967\n",
      "Epoch: 25283 \tTraining Loss: 1.398117 \tValidation Loss: 2.486192\n",
      "Epoch: 25284 \tTraining Loss: 1.392668 \tValidation Loss: 2.487521\n",
      "Epoch: 25285 \tTraining Loss: 1.398918 \tValidation Loss: 2.487114\n",
      "Epoch: 25286 \tTraining Loss: 1.413292 \tValidation Loss: 2.485517\n",
      "Epoch: 25287 \tTraining Loss: 1.450776 \tValidation Loss: 2.486341\n",
      "Epoch: 25288 \tTraining Loss: 1.390372 \tValidation Loss: 2.485743\n",
      "Epoch: 25289 \tTraining Loss: 1.417043 \tValidation Loss: 2.486166\n",
      "Epoch: 25290 \tTraining Loss: 1.448714 \tValidation Loss: 2.485541\n",
      "Epoch: 25291 \tTraining Loss: 1.433857 \tValidation Loss: 2.485941\n",
      "Epoch: 25292 \tTraining Loss: 1.396449 \tValidation Loss: 2.487151\n",
      "Epoch: 25293 \tTraining Loss: 1.491260 \tValidation Loss: 2.485659\n",
      "Epoch: 25294 \tTraining Loss: 1.447231 \tValidation Loss: 2.486659\n",
      "Epoch: 25295 \tTraining Loss: 1.406353 \tValidation Loss: 2.487383\n",
      "Epoch: 25296 \tTraining Loss: 1.387203 \tValidation Loss: 2.487504\n",
      "Epoch: 25297 \tTraining Loss: 1.360320 \tValidation Loss: 2.487393\n",
      "Epoch: 25298 \tTraining Loss: 1.390800 \tValidation Loss: 2.486756\n",
      "Epoch: 25299 \tTraining Loss: 1.399931 \tValidation Loss: 2.486699\n",
      "Epoch: 25300 \tTraining Loss: 1.401771 \tValidation Loss: 2.486508\n",
      "Epoch: 25301 \tTraining Loss: 1.415768 \tValidation Loss: 2.488034\n",
      "Epoch: 25302 \tTraining Loss: 1.454379 \tValidation Loss: 2.486789\n",
      "Epoch: 25303 \tTraining Loss: 1.402874 \tValidation Loss: 2.488603\n",
      "Epoch: 25304 \tTraining Loss: 1.382553 \tValidation Loss: 2.487111\n",
      "Epoch: 25305 \tTraining Loss: 1.366669 \tValidation Loss: 2.487535\n",
      "Epoch: 25306 \tTraining Loss: 1.433966 \tValidation Loss: 2.487139\n",
      "Epoch: 25307 \tTraining Loss: 1.444692 \tValidation Loss: 2.486851\n",
      "Epoch: 25308 \tTraining Loss: 1.434684 \tValidation Loss: 2.486379\n",
      "Epoch: 25309 \tTraining Loss: 1.406999 \tValidation Loss: 2.486645\n",
      "Epoch: 25310 \tTraining Loss: 1.387738 \tValidation Loss: 2.486882\n",
      "Epoch: 25311 \tTraining Loss: 1.440838 \tValidation Loss: 2.487175\n",
      "Epoch: 25312 \tTraining Loss: 1.405382 \tValidation Loss: 2.486730\n",
      "Epoch: 25313 \tTraining Loss: 1.406506 \tValidation Loss: 2.487112\n",
      "Epoch: 25314 \tTraining Loss: 1.392626 \tValidation Loss: 2.488372\n",
      "Epoch: 25315 \tTraining Loss: 1.407874 \tValidation Loss: 2.487555\n",
      "Epoch: 25316 \tTraining Loss: 1.430569 \tValidation Loss: 2.487315\n",
      "Epoch: 25317 \tTraining Loss: 1.428715 \tValidation Loss: 2.487624\n",
      "Epoch: 25318 \tTraining Loss: 1.419546 \tValidation Loss: 2.488019\n",
      "Epoch: 25319 \tTraining Loss: 1.461025 \tValidation Loss: 2.486627\n",
      "Epoch: 25320 \tTraining Loss: 1.380808 \tValidation Loss: 2.487333\n",
      "Epoch: 25321 \tTraining Loss: 1.391458 \tValidation Loss: 2.487832\n",
      "Epoch: 25322 \tTraining Loss: 1.410979 \tValidation Loss: 2.487424\n",
      "Epoch: 25323 \tTraining Loss: 1.444974 \tValidation Loss: 2.486765\n",
      "Epoch: 25324 \tTraining Loss: 1.411741 \tValidation Loss: 2.487498\n",
      "Epoch: 25325 \tTraining Loss: 1.420806 \tValidation Loss: 2.488487\n",
      "Epoch: 25326 \tTraining Loss: 1.416946 \tValidation Loss: 2.486460\n",
      "Epoch: 25327 \tTraining Loss: 1.352394 \tValidation Loss: 2.488045\n",
      "Epoch: 25328 \tTraining Loss: 1.427830 \tValidation Loss: 2.487296\n",
      "Epoch: 25329 \tTraining Loss: 1.432335 \tValidation Loss: 2.487500\n",
      "Epoch: 25330 \tTraining Loss: 1.417222 \tValidation Loss: 2.487653\n",
      "Epoch: 25331 \tTraining Loss: 1.430530 \tValidation Loss: 2.488124\n",
      "Epoch: 25332 \tTraining Loss: 1.443996 \tValidation Loss: 2.487201\n",
      "Epoch: 25333 \tTraining Loss: 1.427697 \tValidation Loss: 2.487909\n",
      "Epoch: 25334 \tTraining Loss: 1.398090 \tValidation Loss: 2.487531\n",
      "Epoch: 25335 \tTraining Loss: 1.374725 \tValidation Loss: 2.489115\n",
      "Epoch: 25336 \tTraining Loss: 1.412895 \tValidation Loss: 2.487751\n",
      "Epoch: 25337 \tTraining Loss: 1.453080 \tValidation Loss: 2.487448\n",
      "Epoch: 25338 \tTraining Loss: 1.415475 \tValidation Loss: 2.487033\n",
      "Epoch: 25339 \tTraining Loss: 1.427732 \tValidation Loss: 2.486933\n",
      "Epoch: 25340 \tTraining Loss: 1.393826 \tValidation Loss: 2.487744\n",
      "Epoch: 25341 \tTraining Loss: 1.428627 \tValidation Loss: 2.487663\n",
      "Epoch: 25342 \tTraining Loss: 1.438794 \tValidation Loss: 2.487042\n",
      "Epoch: 25343 \tTraining Loss: 1.441618 \tValidation Loss: 2.487206\n",
      "Epoch: 25344 \tTraining Loss: 1.409846 \tValidation Loss: 2.487124\n",
      "Epoch: 25345 \tTraining Loss: 1.455250 \tValidation Loss: 2.487153\n",
      "Epoch: 25346 \tTraining Loss: 1.395070 \tValidation Loss: 2.487926\n",
      "Epoch: 25347 \tTraining Loss: 1.363287 \tValidation Loss: 2.487766\n",
      "Epoch: 25348 \tTraining Loss: 1.441268 \tValidation Loss: 2.488276\n",
      "Epoch: 25349 \tTraining Loss: 1.457081 \tValidation Loss: 2.487474\n",
      "Epoch: 25350 \tTraining Loss: 1.381440 \tValidation Loss: 2.488566\n",
      "Epoch: 25351 \tTraining Loss: 1.447424 \tValidation Loss: 2.487260\n",
      "Epoch: 25352 \tTraining Loss: 1.440304 \tValidation Loss: 2.487453\n",
      "Epoch: 25353 \tTraining Loss: 1.373610 \tValidation Loss: 2.488570\n",
      "Epoch: 25354 \tTraining Loss: 1.450667 \tValidation Loss: 2.487644\n",
      "Epoch: 25355 \tTraining Loss: 1.404139 \tValidation Loss: 2.487757\n",
      "Epoch: 25356 \tTraining Loss: 1.417650 \tValidation Loss: 2.487832\n",
      "Epoch: 25357 \tTraining Loss: 1.435971 \tValidation Loss: 2.487909\n",
      "Epoch: 25358 \tTraining Loss: 1.419692 \tValidation Loss: 2.488607\n",
      "Epoch: 25359 \tTraining Loss: 1.438209 \tValidation Loss: 2.489094\n",
      "Epoch: 25360 \tTraining Loss: 1.428543 \tValidation Loss: 2.487929\n",
      "Epoch: 25361 \tTraining Loss: 1.388026 \tValidation Loss: 2.489522\n",
      "Epoch: 25362 \tTraining Loss: 1.360031 \tValidation Loss: 2.489617\n",
      "Epoch: 25363 \tTraining Loss: 1.447946 \tValidation Loss: 2.487780\n",
      "Epoch: 25364 \tTraining Loss: 1.399646 \tValidation Loss: 2.488889\n",
      "Epoch: 25365 \tTraining Loss: 1.452638 \tValidation Loss: 2.488269\n",
      "Epoch: 25366 \tTraining Loss: 1.396039 \tValidation Loss: 2.487605\n",
      "Epoch: 25367 \tTraining Loss: 1.348065 \tValidation Loss: 2.489318\n",
      "Epoch: 25368 \tTraining Loss: 1.401978 \tValidation Loss: 2.489005\n",
      "Epoch: 25369 \tTraining Loss: 1.373629 \tValidation Loss: 2.489301\n",
      "Epoch: 25370 \tTraining Loss: 1.391919 \tValidation Loss: 2.488987\n",
      "Epoch: 25371 \tTraining Loss: 1.447580 \tValidation Loss: 2.487648\n",
      "Epoch: 25372 \tTraining Loss: 1.391566 \tValidation Loss: 2.488749\n",
      "Epoch: 25373 \tTraining Loss: 1.456673 \tValidation Loss: 2.488758\n",
      "Epoch: 25374 \tTraining Loss: 1.432335 \tValidation Loss: 2.489311\n",
      "Epoch: 25375 \tTraining Loss: 1.408870 \tValidation Loss: 2.488076\n",
      "Epoch: 25376 \tTraining Loss: 1.354079 \tValidation Loss: 2.489245\n",
      "Epoch: 25377 \tTraining Loss: 1.450556 \tValidation Loss: 2.488383\n",
      "Epoch: 25378 \tTraining Loss: 1.445213 \tValidation Loss: 2.488579\n",
      "Epoch: 25379 \tTraining Loss: 1.419051 \tValidation Loss: 2.488795\n",
      "Epoch: 25380 \tTraining Loss: 1.371330 \tValidation Loss: 2.488492\n",
      "Epoch: 25381 \tTraining Loss: 1.414040 \tValidation Loss: 2.488496\n",
      "Epoch: 25382 \tTraining Loss: 1.342389 \tValidation Loss: 2.489603\n",
      "Epoch: 25383 \tTraining Loss: 1.451946 \tValidation Loss: 2.488272\n",
      "Epoch: 25384 \tTraining Loss: 1.392730 \tValidation Loss: 2.489174\n",
      "Epoch: 25385 \tTraining Loss: 1.415019 \tValidation Loss: 2.489523\n",
      "Epoch: 25386 \tTraining Loss: 1.433887 \tValidation Loss: 2.489029\n",
      "Epoch: 25387 \tTraining Loss: 1.411312 \tValidation Loss: 2.490004\n",
      "Epoch: 25388 \tTraining Loss: 1.405447 \tValidation Loss: 2.490000\n",
      "Epoch: 25389 \tTraining Loss: 1.432512 \tValidation Loss: 2.488101\n",
      "Epoch: 25390 \tTraining Loss: 1.378755 \tValidation Loss: 2.489248\n",
      "Epoch: 25391 \tTraining Loss: 1.457781 \tValidation Loss: 2.488779\n",
      "Epoch: 25392 \tTraining Loss: 1.394235 \tValidation Loss: 2.489898\n",
      "Epoch: 25393 \tTraining Loss: 1.443743 \tValidation Loss: 2.488201\n",
      "Epoch: 25394 \tTraining Loss: 1.390609 \tValidation Loss: 2.488854\n",
      "Epoch: 25395 \tTraining Loss: 1.442367 \tValidation Loss: 2.488889\n",
      "Epoch: 25396 \tTraining Loss: 1.397048 \tValidation Loss: 2.489173\n",
      "Epoch: 25397 \tTraining Loss: 1.389140 \tValidation Loss: 2.489425\n",
      "Epoch: 25398 \tTraining Loss: 1.385337 \tValidation Loss: 2.490335\n",
      "Epoch: 25399 \tTraining Loss: 1.479520 \tValidation Loss: 2.489391\n",
      "Epoch: 25400 \tTraining Loss: 1.388320 \tValidation Loss: 2.489630\n",
      "Epoch: 25401 \tTraining Loss: 1.389588 \tValidation Loss: 2.489676\n",
      "Epoch: 25402 \tTraining Loss: 1.415027 \tValidation Loss: 2.489566\n",
      "Epoch: 25403 \tTraining Loss: 1.409881 \tValidation Loss: 2.490545\n",
      "Epoch: 25404 \tTraining Loss: 1.410838 \tValidation Loss: 2.489074\n",
      "Epoch: 25405 \tTraining Loss: 1.395276 \tValidation Loss: 2.488839\n",
      "Epoch: 25406 \tTraining Loss: 1.445668 \tValidation Loss: 2.489779\n",
      "Epoch: 25407 \tTraining Loss: 1.397014 \tValidation Loss: 2.488414\n",
      "Epoch: 25408 \tTraining Loss: 1.439507 \tValidation Loss: 2.488032\n",
      "Epoch: 25409 \tTraining Loss: 1.404623 \tValidation Loss: 2.488883\n",
      "Epoch: 25410 \tTraining Loss: 1.394513 \tValidation Loss: 2.489573\n",
      "Epoch: 25411 \tTraining Loss: 1.431141 \tValidation Loss: 2.488611\n",
      "Epoch: 25412 \tTraining Loss: 1.365588 \tValidation Loss: 2.489614\n",
      "Epoch: 25413 \tTraining Loss: 1.386373 \tValidation Loss: 2.488554\n",
      "Epoch: 25414 \tTraining Loss: 1.423903 \tValidation Loss: 2.488716\n",
      "Epoch: 25415 \tTraining Loss: 1.433275 \tValidation Loss: 2.489106\n",
      "Epoch: 25416 \tTraining Loss: 1.426971 \tValidation Loss: 2.488556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25417 \tTraining Loss: 1.423318 \tValidation Loss: 2.488654\n",
      "Epoch: 25418 \tTraining Loss: 1.375782 \tValidation Loss: 2.489778\n",
      "Epoch: 25419 \tTraining Loss: 1.422092 \tValidation Loss: 2.488895\n",
      "Epoch: 25420 \tTraining Loss: 1.409723 \tValidation Loss: 2.489755\n",
      "Epoch: 25421 \tTraining Loss: 1.450771 \tValidation Loss: 2.489596\n",
      "Epoch: 25422 \tTraining Loss: 1.416558 \tValidation Loss: 2.489254\n",
      "Epoch: 25423 \tTraining Loss: 1.415269 \tValidation Loss: 2.490196\n",
      "Epoch: 25424 \tTraining Loss: 1.430324 \tValidation Loss: 2.488989\n",
      "Epoch: 25425 \tTraining Loss: 1.444294 \tValidation Loss: 2.488895\n",
      "Epoch: 25426 \tTraining Loss: 1.389549 \tValidation Loss: 2.488893\n",
      "Epoch: 25427 \tTraining Loss: 1.428378 \tValidation Loss: 2.489208\n",
      "Epoch: 25428 \tTraining Loss: 1.408947 \tValidation Loss: 2.488041\n",
      "Epoch: 25429 \tTraining Loss: 1.420077 \tValidation Loss: 2.489768\n",
      "Epoch: 25430 \tTraining Loss: 1.393163 \tValidation Loss: 2.489497\n",
      "Epoch: 25431 \tTraining Loss: 1.420382 \tValidation Loss: 2.488930\n",
      "Epoch: 25432 \tTraining Loss: 1.386198 \tValidation Loss: 2.489704\n",
      "Epoch: 25433 \tTraining Loss: 1.388189 \tValidation Loss: 2.488827\n",
      "Epoch: 25434 \tTraining Loss: 1.413881 \tValidation Loss: 2.489755\n",
      "Epoch: 25435 \tTraining Loss: 1.440612 \tValidation Loss: 2.488833\n",
      "Epoch: 25436 \tTraining Loss: 1.430056 \tValidation Loss: 2.489515\n",
      "Epoch: 25437 \tTraining Loss: 1.391983 \tValidation Loss: 2.489892\n",
      "Epoch: 25438 \tTraining Loss: 1.444254 \tValidation Loss: 2.488994\n",
      "Epoch: 25439 \tTraining Loss: 1.451512 \tValidation Loss: 2.488643\n",
      "Epoch: 25440 \tTraining Loss: 1.398163 \tValidation Loss: 2.489403\n",
      "Epoch: 25441 \tTraining Loss: 1.434295 \tValidation Loss: 2.489772\n",
      "Epoch: 25442 \tTraining Loss: 1.398650 \tValidation Loss: 2.490615\n",
      "Epoch: 25443 \tTraining Loss: 1.450151 \tValidation Loss: 2.489020\n",
      "Epoch: 25444 \tTraining Loss: 1.381806 \tValidation Loss: 2.488950\n",
      "Epoch: 25445 \tTraining Loss: 1.409384 \tValidation Loss: 2.490265\n",
      "Epoch: 25446 \tTraining Loss: 1.384511 \tValidation Loss: 2.491174\n",
      "Epoch: 25447 \tTraining Loss: 1.399521 \tValidation Loss: 2.489841\n",
      "Epoch: 25448 \tTraining Loss: 1.404392 \tValidation Loss: 2.490233\n",
      "Epoch: 25449 \tTraining Loss: 1.391845 \tValidation Loss: 2.490587\n",
      "Epoch: 25450 \tTraining Loss: 1.414717 \tValidation Loss: 2.490441\n",
      "Epoch: 25451 \tTraining Loss: 1.408515 \tValidation Loss: 2.490470\n",
      "Epoch: 25452 \tTraining Loss: 1.416253 \tValidation Loss: 2.489157\n",
      "Epoch: 25453 \tTraining Loss: 1.428477 \tValidation Loss: 2.489060\n",
      "Epoch: 25454 \tTraining Loss: 1.399834 \tValidation Loss: 2.489813\n",
      "Epoch: 25455 \tTraining Loss: 1.416878 \tValidation Loss: 2.489309\n",
      "Epoch: 25456 \tTraining Loss: 1.404479 \tValidation Loss: 2.490087\n",
      "Epoch: 25457 \tTraining Loss: 1.419988 \tValidation Loss: 2.489640\n",
      "Epoch: 25458 \tTraining Loss: 1.400694 \tValidation Loss: 2.490306\n",
      "Epoch: 25459 \tTraining Loss: 1.438228 \tValidation Loss: 2.489085\n",
      "Epoch: 25460 \tTraining Loss: 1.375653 \tValidation Loss: 2.490562\n",
      "Epoch: 25461 \tTraining Loss: 1.400253 \tValidation Loss: 2.490330\n",
      "Epoch: 25462 \tTraining Loss: 1.422890 \tValidation Loss: 2.490088\n",
      "Epoch: 25463 \tTraining Loss: 1.406517 \tValidation Loss: 2.490102\n",
      "Epoch: 25464 \tTraining Loss: 1.338466 \tValidation Loss: 2.491446\n",
      "Epoch: 25465 \tTraining Loss: 1.400525 \tValidation Loss: 2.490821\n",
      "Epoch: 25466 \tTraining Loss: 1.420622 \tValidation Loss: 2.490165\n",
      "Epoch: 25467 \tTraining Loss: 1.426540 \tValidation Loss: 2.489710\n",
      "Epoch: 25468 \tTraining Loss: 1.441976 \tValidation Loss: 2.489928\n",
      "Epoch: 25469 \tTraining Loss: 1.419808 \tValidation Loss: 2.490293\n",
      "Epoch: 25470 \tTraining Loss: 1.453752 \tValidation Loss: 2.489852\n",
      "Epoch: 25471 \tTraining Loss: 1.403316 \tValidation Loss: 2.490665\n",
      "Epoch: 25472 \tTraining Loss: 1.451430 \tValidation Loss: 2.490877\n",
      "Epoch: 25473 \tTraining Loss: 1.438665 \tValidation Loss: 2.489663\n",
      "Epoch: 25474 \tTraining Loss: 1.432820 \tValidation Loss: 2.490878\n",
      "Epoch: 25475 \tTraining Loss: 1.401292 \tValidation Loss: 2.490628\n",
      "Epoch: 25476 \tTraining Loss: 1.396844 \tValidation Loss: 2.490606\n",
      "Epoch: 25477 \tTraining Loss: 1.429869 \tValidation Loss: 2.490997\n",
      "Epoch: 25478 \tTraining Loss: 1.409669 \tValidation Loss: 2.490153\n",
      "Epoch: 25479 \tTraining Loss: 1.442634 \tValidation Loss: 2.489614\n",
      "Epoch: 25480 \tTraining Loss: 1.384092 \tValidation Loss: 2.491133\n",
      "Epoch: 25481 \tTraining Loss: 1.397366 \tValidation Loss: 2.491313\n",
      "Epoch: 25482 \tTraining Loss: 1.385827 \tValidation Loss: 2.491399\n",
      "Epoch: 25483 \tTraining Loss: 1.420831 \tValidation Loss: 2.490149\n",
      "Epoch: 25484 \tTraining Loss: 1.442408 \tValidation Loss: 2.489785\n",
      "Epoch: 25485 \tTraining Loss: 1.416147 \tValidation Loss: 2.490685\n",
      "Epoch: 25486 \tTraining Loss: 1.425262 \tValidation Loss: 2.490292\n",
      "Epoch: 25487 \tTraining Loss: 1.411286 \tValidation Loss: 2.490808\n",
      "Epoch: 25488 \tTraining Loss: 1.420049 \tValidation Loss: 2.490750\n",
      "Epoch: 25489 \tTraining Loss: 1.438246 \tValidation Loss: 2.490543\n",
      "Epoch: 25490 \tTraining Loss: 1.431725 \tValidation Loss: 2.491235\n",
      "Epoch: 25491 \tTraining Loss: 1.405276 \tValidation Loss: 2.492469\n",
      "Epoch: 25492 \tTraining Loss: 1.407394 \tValidation Loss: 2.491281\n",
      "Epoch: 25493 \tTraining Loss: 1.452281 \tValidation Loss: 2.490371\n",
      "Epoch: 25494 \tTraining Loss: 1.426644 \tValidation Loss: 2.490893\n",
      "Epoch: 25495 \tTraining Loss: 1.460359 \tValidation Loss: 2.490985\n",
      "Epoch: 25496 \tTraining Loss: 1.385378 \tValidation Loss: 2.490650\n",
      "Epoch: 25497 \tTraining Loss: 1.427536 \tValidation Loss: 2.492294\n",
      "Epoch: 25498 \tTraining Loss: 1.423243 \tValidation Loss: 2.491271\n",
      "Epoch: 25499 \tTraining Loss: 1.425349 \tValidation Loss: 2.490835\n",
      "Epoch: 25500 \tTraining Loss: 1.434660 \tValidation Loss: 2.490985\n",
      "Epoch: 25501 \tTraining Loss: 1.412168 \tValidation Loss: 2.490383\n",
      "Epoch: 25502 \tTraining Loss: 1.430783 \tValidation Loss: 2.490279\n",
      "Epoch: 25503 \tTraining Loss: 1.419319 \tValidation Loss: 2.490470\n",
      "Epoch: 25504 \tTraining Loss: 1.435551 \tValidation Loss: 2.490195\n",
      "Epoch: 25505 \tTraining Loss: 1.461249 \tValidation Loss: 2.490294\n",
      "Epoch: 25506 \tTraining Loss: 1.411596 \tValidation Loss: 2.490895\n",
      "Epoch: 25507 \tTraining Loss: 1.416132 \tValidation Loss: 2.490948\n",
      "Epoch: 25508 \tTraining Loss: 1.422598 \tValidation Loss: 2.490907\n",
      "Epoch: 25509 \tTraining Loss: 1.422810 \tValidation Loss: 2.490555\n",
      "Epoch: 25510 \tTraining Loss: 1.376015 \tValidation Loss: 2.490700\n",
      "Epoch: 25511 \tTraining Loss: 1.419908 \tValidation Loss: 2.490411\n",
      "Epoch: 25512 \tTraining Loss: 1.431891 \tValidation Loss: 2.489290\n",
      "Epoch: 25513 \tTraining Loss: 1.383185 \tValidation Loss: 2.490601\n",
      "Epoch: 25514 \tTraining Loss: 1.419949 \tValidation Loss: 2.491582\n",
      "Epoch: 25515 \tTraining Loss: 1.418056 \tValidation Loss: 2.491769\n",
      "Epoch: 25516 \tTraining Loss: 1.404286 \tValidation Loss: 2.491944\n",
      "Epoch: 25517 \tTraining Loss: 1.385402 \tValidation Loss: 2.491057\n",
      "Epoch: 25518 \tTraining Loss: 1.447726 \tValidation Loss: 2.491291\n",
      "Epoch: 25519 \tTraining Loss: 1.391213 \tValidation Loss: 2.491343\n",
      "Epoch: 25520 \tTraining Loss: 1.455309 \tValidation Loss: 2.490543\n",
      "Epoch: 25521 \tTraining Loss: 1.420886 \tValidation Loss: 2.490914\n",
      "Epoch: 25522 \tTraining Loss: 1.415560 \tValidation Loss: 2.490879\n",
      "Epoch: 25523 \tTraining Loss: 1.455833 \tValidation Loss: 2.491310\n",
      "Epoch: 25524 \tTraining Loss: 1.400453 \tValidation Loss: 2.491127\n",
      "Epoch: 25525 \tTraining Loss: 1.413120 \tValidation Loss: 2.490838\n",
      "Epoch: 25526 \tTraining Loss: 1.435917 \tValidation Loss: 2.490588\n",
      "Epoch: 25527 \tTraining Loss: 1.423706 \tValidation Loss: 2.489997\n",
      "Epoch: 25528 \tTraining Loss: 1.402522 \tValidation Loss: 2.490019\n",
      "Epoch: 25529 \tTraining Loss: 1.435445 \tValidation Loss: 2.490587\n",
      "Epoch: 25530 \tTraining Loss: 1.402088 \tValidation Loss: 2.490592\n",
      "Epoch: 25531 \tTraining Loss: 1.421875 \tValidation Loss: 2.490503\n",
      "Epoch: 25532 \tTraining Loss: 1.378742 \tValidation Loss: 2.491559\n",
      "Epoch: 25533 \tTraining Loss: 1.409302 \tValidation Loss: 2.490266\n",
      "Epoch: 25534 \tTraining Loss: 1.401709 \tValidation Loss: 2.491758\n",
      "Epoch: 25535 \tTraining Loss: 1.445971 \tValidation Loss: 2.491395\n",
      "Epoch: 25536 \tTraining Loss: 1.427899 \tValidation Loss: 2.491603\n",
      "Epoch: 25537 \tTraining Loss: 1.411766 \tValidation Loss: 2.489977\n",
      "Epoch: 25538 \tTraining Loss: 1.391840 \tValidation Loss: 2.490336\n",
      "Epoch: 25539 \tTraining Loss: 1.404353 \tValidation Loss: 2.491126\n",
      "Epoch: 25540 \tTraining Loss: 1.394391 \tValidation Loss: 2.491621\n",
      "Epoch: 25541 \tTraining Loss: 1.439257 \tValidation Loss: 2.491314\n",
      "Epoch: 25542 \tTraining Loss: 1.395281 \tValidation Loss: 2.490777\n",
      "Epoch: 25543 \tTraining Loss: 1.417763 \tValidation Loss: 2.490839\n",
      "Epoch: 25544 \tTraining Loss: 1.413520 \tValidation Loss: 2.492440\n",
      "Epoch: 25545 \tTraining Loss: 1.399871 \tValidation Loss: 2.491877\n",
      "Epoch: 25546 \tTraining Loss: 1.406080 \tValidation Loss: 2.492447\n",
      "Epoch: 25547 \tTraining Loss: 1.401626 \tValidation Loss: 2.492554\n",
      "Epoch: 25548 \tTraining Loss: 1.469449 \tValidation Loss: 2.491884\n",
      "Epoch: 25549 \tTraining Loss: 1.375698 \tValidation Loss: 2.493061\n",
      "Epoch: 25550 \tTraining Loss: 1.398427 \tValidation Loss: 2.491866\n",
      "Epoch: 25551 \tTraining Loss: 1.437403 \tValidation Loss: 2.492513\n",
      "Epoch: 25552 \tTraining Loss: 1.431567 \tValidation Loss: 2.491664\n",
      "Epoch: 25553 \tTraining Loss: 1.356382 \tValidation Loss: 2.492972\n",
      "Epoch: 25554 \tTraining Loss: 1.409301 \tValidation Loss: 2.493035\n",
      "Epoch: 25555 \tTraining Loss: 1.424951 \tValidation Loss: 2.493432\n",
      "Epoch: 25556 \tTraining Loss: 1.428054 \tValidation Loss: 2.493112\n",
      "Epoch: 25557 \tTraining Loss: 1.365676 \tValidation Loss: 2.493252\n",
      "Epoch: 25558 \tTraining Loss: 1.418009 \tValidation Loss: 2.491983\n",
      "Epoch: 25559 \tTraining Loss: 1.385680 \tValidation Loss: 2.492321\n",
      "Epoch: 25560 \tTraining Loss: 1.430787 \tValidation Loss: 2.491695\n",
      "Epoch: 25561 \tTraining Loss: 1.399613 \tValidation Loss: 2.492543\n",
      "Epoch: 25562 \tTraining Loss: 1.423115 \tValidation Loss: 2.492162\n",
      "Epoch: 25563 \tTraining Loss: 1.388921 \tValidation Loss: 2.492323\n",
      "Epoch: 25564 \tTraining Loss: 1.405975 \tValidation Loss: 2.492446\n",
      "Epoch: 25565 \tTraining Loss: 1.402837 \tValidation Loss: 2.492190\n",
      "Epoch: 25566 \tTraining Loss: 1.383470 \tValidation Loss: 2.491805\n",
      "Epoch: 25567 \tTraining Loss: 1.389394 \tValidation Loss: 2.491685\n",
      "Epoch: 25568 \tTraining Loss: 1.409755 \tValidation Loss: 2.492637\n",
      "Epoch: 25569 \tTraining Loss: 1.418701 \tValidation Loss: 2.492431\n",
      "Epoch: 25570 \tTraining Loss: 1.417364 \tValidation Loss: 2.492628\n",
      "Epoch: 25571 \tTraining Loss: 1.390623 \tValidation Loss: 2.492193\n",
      "Epoch: 25572 \tTraining Loss: 1.390794 \tValidation Loss: 2.492472\n",
      "Epoch: 25573 \tTraining Loss: 1.462623 \tValidation Loss: 2.492041\n",
      "Epoch: 25574 \tTraining Loss: 1.406584 \tValidation Loss: 2.492813\n",
      "Epoch: 25575 \tTraining Loss: 1.442059 \tValidation Loss: 2.492204\n",
      "Epoch: 25576 \tTraining Loss: 1.390765 \tValidation Loss: 2.492437\n",
      "Epoch: 25577 \tTraining Loss: 1.399788 \tValidation Loss: 2.493473\n",
      "Epoch: 25578 \tTraining Loss: 1.406110 \tValidation Loss: 2.492574\n",
      "Epoch: 25579 \tTraining Loss: 1.405471 \tValidation Loss: 2.492409\n",
      "Epoch: 25580 \tTraining Loss: 1.451710 \tValidation Loss: 2.491576\n",
      "Epoch: 25581 \tTraining Loss: 1.440600 \tValidation Loss: 2.492166\n",
      "Epoch: 25582 \tTraining Loss: 1.403785 \tValidation Loss: 2.492054\n",
      "Epoch: 25583 \tTraining Loss: 1.405544 \tValidation Loss: 2.492954\n",
      "Epoch: 25584 \tTraining Loss: 1.399620 \tValidation Loss: 2.492331\n",
      "Epoch: 25585 \tTraining Loss: 1.453315 \tValidation Loss: 2.492244\n",
      "Epoch: 25586 \tTraining Loss: 1.397073 \tValidation Loss: 2.491839\n",
      "Epoch: 25587 \tTraining Loss: 1.388697 \tValidation Loss: 2.492549\n",
      "Epoch: 25588 \tTraining Loss: 1.415108 \tValidation Loss: 2.492950\n",
      "Epoch: 25589 \tTraining Loss: 1.454901 \tValidation Loss: 2.491578\n",
      "Epoch: 25590 \tTraining Loss: 1.437256 \tValidation Loss: 2.491736\n",
      "Epoch: 25591 \tTraining Loss: 1.444504 \tValidation Loss: 2.492214\n",
      "Epoch: 25592 \tTraining Loss: 1.408065 \tValidation Loss: 2.491283\n",
      "Epoch: 25593 \tTraining Loss: 1.429699 \tValidation Loss: 2.492532\n",
      "Epoch: 25594 \tTraining Loss: 1.413066 \tValidation Loss: 2.491934\n",
      "Epoch: 25595 \tTraining Loss: 1.387118 \tValidation Loss: 2.492772\n",
      "Epoch: 25596 \tTraining Loss: 1.426266 \tValidation Loss: 2.492525\n",
      "Epoch: 25597 \tTraining Loss: 1.405954 \tValidation Loss: 2.492918\n",
      "Epoch: 25598 \tTraining Loss: 1.374186 \tValidation Loss: 2.492694\n",
      "Epoch: 25599 \tTraining Loss: 1.416824 \tValidation Loss: 2.492343\n",
      "Epoch: 25600 \tTraining Loss: 1.420530 \tValidation Loss: 2.493122\n",
      "Epoch: 25601 \tTraining Loss: 1.391671 \tValidation Loss: 2.492826\n",
      "Epoch: 25602 \tTraining Loss: 1.388968 \tValidation Loss: 2.492613\n",
      "Epoch: 25603 \tTraining Loss: 1.441658 \tValidation Loss: 2.493347\n",
      "Epoch: 25604 \tTraining Loss: 1.402442 \tValidation Loss: 2.492710\n",
      "Epoch: 25605 \tTraining Loss: 1.445953 \tValidation Loss: 2.492630\n",
      "Epoch: 25606 \tTraining Loss: 1.425097 \tValidation Loss: 2.491728\n",
      "Epoch: 25607 \tTraining Loss: 1.438952 \tValidation Loss: 2.491851\n",
      "Epoch: 25608 \tTraining Loss: 1.407375 \tValidation Loss: 2.492829\n",
      "Epoch: 25609 \tTraining Loss: 1.418186 \tValidation Loss: 2.491388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25610 \tTraining Loss: 1.406217 \tValidation Loss: 2.492578\n",
      "Epoch: 25611 \tTraining Loss: 1.391385 \tValidation Loss: 2.492734\n",
      "Epoch: 25612 \tTraining Loss: 1.403566 \tValidation Loss: 2.492805\n",
      "Epoch: 25613 \tTraining Loss: 1.452905 \tValidation Loss: 2.492962\n",
      "Epoch: 25614 \tTraining Loss: 1.370174 \tValidation Loss: 2.493433\n",
      "Epoch: 25615 \tTraining Loss: 1.380058 \tValidation Loss: 2.493116\n",
      "Epoch: 25616 \tTraining Loss: 1.401464 \tValidation Loss: 2.492941\n",
      "Epoch: 25617 \tTraining Loss: 1.401828 \tValidation Loss: 2.493701\n",
      "Epoch: 25618 \tTraining Loss: 1.369677 \tValidation Loss: 2.493537\n",
      "Epoch: 25619 \tTraining Loss: 1.419710 \tValidation Loss: 2.493218\n",
      "Epoch: 25620 \tTraining Loss: 1.426560 \tValidation Loss: 2.493805\n",
      "Epoch: 25621 \tTraining Loss: 1.427927 \tValidation Loss: 2.493443\n",
      "Epoch: 25622 \tTraining Loss: 1.390960 \tValidation Loss: 2.493957\n",
      "Epoch: 25623 \tTraining Loss: 1.408138 \tValidation Loss: 2.493666\n",
      "Epoch: 25624 \tTraining Loss: 1.386134 \tValidation Loss: 2.493490\n",
      "Epoch: 25625 \tTraining Loss: 1.384039 \tValidation Loss: 2.493298\n",
      "Epoch: 25626 \tTraining Loss: 1.425069 \tValidation Loss: 2.493600\n",
      "Epoch: 25627 \tTraining Loss: 1.417085 \tValidation Loss: 2.494116\n",
      "Epoch: 25628 \tTraining Loss: 1.461170 \tValidation Loss: 2.493157\n",
      "Epoch: 25629 \tTraining Loss: 1.401728 \tValidation Loss: 2.493589\n",
      "Epoch: 25630 \tTraining Loss: 1.391060 \tValidation Loss: 2.493238\n",
      "Epoch: 25631 \tTraining Loss: 1.446393 \tValidation Loss: 2.492821\n",
      "Epoch: 25632 \tTraining Loss: 1.413916 \tValidation Loss: 2.493458\n",
      "Epoch: 25633 \tTraining Loss: 1.449483 \tValidation Loss: 2.493177\n",
      "Epoch: 25634 \tTraining Loss: 1.387398 \tValidation Loss: 2.493109\n",
      "Epoch: 25635 \tTraining Loss: 1.375982 \tValidation Loss: 2.493028\n",
      "Epoch: 25636 \tTraining Loss: 1.415828 \tValidation Loss: 2.493248\n",
      "Epoch: 25637 \tTraining Loss: 1.382712 \tValidation Loss: 2.492916\n",
      "Epoch: 25638 \tTraining Loss: 1.440841 \tValidation Loss: 2.492036\n",
      "Epoch: 25639 \tTraining Loss: 1.410304 \tValidation Loss: 2.492886\n",
      "Epoch: 25640 \tTraining Loss: 1.453937 \tValidation Loss: 2.493035\n",
      "Epoch: 25641 \tTraining Loss: 1.413013 \tValidation Loss: 2.493344\n",
      "Epoch: 25642 \tTraining Loss: 1.434393 \tValidation Loss: 2.492606\n",
      "Epoch: 25643 \tTraining Loss: 1.405712 \tValidation Loss: 2.493169\n",
      "Epoch: 25644 \tTraining Loss: 1.394886 \tValidation Loss: 2.492845\n",
      "Epoch: 25645 \tTraining Loss: 1.413104 \tValidation Loss: 2.493688\n",
      "Epoch: 25646 \tTraining Loss: 1.433148 \tValidation Loss: 2.494016\n",
      "Epoch: 25647 \tTraining Loss: 1.431676 \tValidation Loss: 2.493832\n",
      "Epoch: 25648 \tTraining Loss: 1.394040 \tValidation Loss: 2.494249\n",
      "Epoch: 25649 \tTraining Loss: 1.418177 \tValidation Loss: 2.495157\n",
      "Epoch: 25650 \tTraining Loss: 1.422045 \tValidation Loss: 2.493540\n",
      "Epoch: 25651 \tTraining Loss: 1.419217 \tValidation Loss: 2.492851\n",
      "Epoch: 25652 \tTraining Loss: 1.382535 \tValidation Loss: 2.494472\n",
      "Epoch: 25653 \tTraining Loss: 1.377693 \tValidation Loss: 2.494557\n",
      "Epoch: 25654 \tTraining Loss: 1.443931 \tValidation Loss: 2.493769\n",
      "Epoch: 25655 \tTraining Loss: 1.406872 \tValidation Loss: 2.493535\n",
      "Epoch: 25656 \tTraining Loss: 1.402790 \tValidation Loss: 2.493081\n",
      "Epoch: 25657 \tTraining Loss: 1.437160 \tValidation Loss: 2.492660\n",
      "Epoch: 25658 \tTraining Loss: 1.421727 \tValidation Loss: 2.493638\n",
      "Epoch: 25659 \tTraining Loss: 1.470761 \tValidation Loss: 2.493006\n",
      "Epoch: 25660 \tTraining Loss: 1.422594 \tValidation Loss: 2.493460\n",
      "Epoch: 25661 \tTraining Loss: 1.388612 \tValidation Loss: 2.493094\n",
      "Epoch: 25662 \tTraining Loss: 1.438651 \tValidation Loss: 2.493798\n",
      "Epoch: 25663 \tTraining Loss: 1.388533 \tValidation Loss: 2.493668\n",
      "Epoch: 25664 \tTraining Loss: 1.434451 \tValidation Loss: 2.493761\n",
      "Epoch: 25665 \tTraining Loss: 1.431066 \tValidation Loss: 2.494114\n",
      "Epoch: 25666 \tTraining Loss: 1.405185 \tValidation Loss: 2.493728\n",
      "Epoch: 25667 \tTraining Loss: 1.381950 \tValidation Loss: 2.494122\n",
      "Epoch: 25668 \tTraining Loss: 1.400971 \tValidation Loss: 2.494513\n",
      "Epoch: 25669 \tTraining Loss: 1.416161 \tValidation Loss: 2.493290\n",
      "Epoch: 25670 \tTraining Loss: 1.405052 \tValidation Loss: 2.494067\n",
      "Epoch: 25671 \tTraining Loss: 1.397025 \tValidation Loss: 2.493370\n",
      "Epoch: 25672 \tTraining Loss: 1.387681 \tValidation Loss: 2.494689\n",
      "Epoch: 25673 \tTraining Loss: 1.445812 \tValidation Loss: 2.494951\n",
      "Epoch: 25674 \tTraining Loss: 1.373079 \tValidation Loss: 2.494953\n",
      "Epoch: 25675 \tTraining Loss: 1.410828 \tValidation Loss: 2.493656\n",
      "Epoch: 25676 \tTraining Loss: 1.414066 \tValidation Loss: 2.493921\n",
      "Epoch: 25677 \tTraining Loss: 1.362802 \tValidation Loss: 2.494525\n",
      "Epoch: 25678 \tTraining Loss: 1.402692 \tValidation Loss: 2.494061\n",
      "Epoch: 25679 \tTraining Loss: 1.364225 \tValidation Loss: 2.494109\n",
      "Epoch: 25680 \tTraining Loss: 1.408147 \tValidation Loss: 2.493799\n",
      "Epoch: 25681 \tTraining Loss: 1.408713 \tValidation Loss: 2.493725\n",
      "Epoch: 25682 \tTraining Loss: 1.427797 \tValidation Loss: 2.493510\n",
      "Epoch: 25683 \tTraining Loss: 1.395695 \tValidation Loss: 2.493546\n",
      "Epoch: 25684 \tTraining Loss: 1.423279 \tValidation Loss: 2.493915\n",
      "Epoch: 25685 \tTraining Loss: 1.430780 \tValidation Loss: 2.494165\n",
      "Epoch: 25686 \tTraining Loss: 1.398975 \tValidation Loss: 2.493524\n",
      "Epoch: 25687 \tTraining Loss: 1.408865 \tValidation Loss: 2.493947\n",
      "Epoch: 25688 \tTraining Loss: 1.431403 \tValidation Loss: 2.493468\n",
      "Epoch: 25689 \tTraining Loss: 1.385852 \tValidation Loss: 2.495248\n",
      "Epoch: 25690 \tTraining Loss: 1.429286 \tValidation Loss: 2.494025\n",
      "Epoch: 25691 \tTraining Loss: 1.402101 \tValidation Loss: 2.494740\n",
      "Epoch: 25692 \tTraining Loss: 1.393330 \tValidation Loss: 2.494379\n",
      "Epoch: 25693 \tTraining Loss: 1.431213 \tValidation Loss: 2.495120\n",
      "Epoch: 25694 \tTraining Loss: 1.380869 \tValidation Loss: 2.494681\n",
      "Epoch: 25695 \tTraining Loss: 1.401460 \tValidation Loss: 2.494770\n",
      "Epoch: 25696 \tTraining Loss: 1.408249 \tValidation Loss: 2.495053\n",
      "Epoch: 25697 \tTraining Loss: 1.423961 \tValidation Loss: 2.494956\n",
      "Epoch: 25698 \tTraining Loss: 1.394265 \tValidation Loss: 2.494986\n",
      "Epoch: 25699 \tTraining Loss: 1.421937 \tValidation Loss: 2.495039\n",
      "Epoch: 25700 \tTraining Loss: 1.376225 \tValidation Loss: 2.495376\n",
      "Epoch: 25701 \tTraining Loss: 1.374274 \tValidation Loss: 2.494998\n",
      "Epoch: 25702 \tTraining Loss: 1.410769 \tValidation Loss: 2.494616\n",
      "Epoch: 25703 \tTraining Loss: 1.414775 \tValidation Loss: 2.495490\n",
      "Epoch: 25704 \tTraining Loss: 1.433418 \tValidation Loss: 2.495218\n",
      "Epoch: 25705 \tTraining Loss: 1.402482 \tValidation Loss: 2.494538\n",
      "Epoch: 25706 \tTraining Loss: 1.376302 \tValidation Loss: 2.494248\n",
      "Epoch: 25707 \tTraining Loss: 1.378873 \tValidation Loss: 2.495589\n",
      "Epoch: 25708 \tTraining Loss: 1.396551 \tValidation Loss: 2.494956\n",
      "Epoch: 25709 \tTraining Loss: 1.388923 \tValidation Loss: 2.495492\n",
      "Epoch: 25710 \tTraining Loss: 1.378844 \tValidation Loss: 2.495356\n",
      "Epoch: 25711 \tTraining Loss: 1.494799 \tValidation Loss: 2.493829\n",
      "Epoch: 25712 \tTraining Loss: 1.398427 \tValidation Loss: 2.494242\n",
      "Epoch: 25713 \tTraining Loss: 1.414838 \tValidation Loss: 2.495679\n",
      "Epoch: 25714 \tTraining Loss: 1.393560 \tValidation Loss: 2.494338\n",
      "Epoch: 25715 \tTraining Loss: 1.427424 \tValidation Loss: 2.493777\n",
      "Epoch: 25716 \tTraining Loss: 1.397835 \tValidation Loss: 2.495025\n",
      "Epoch: 25717 \tTraining Loss: 1.385946 \tValidation Loss: 2.495179\n",
      "Epoch: 25718 \tTraining Loss: 1.429653 \tValidation Loss: 2.495032\n",
      "Epoch: 25719 \tTraining Loss: 1.425592 \tValidation Loss: 2.494811\n",
      "Epoch: 25720 \tTraining Loss: 1.434362 \tValidation Loss: 2.495509\n",
      "Epoch: 25721 \tTraining Loss: 1.396466 \tValidation Loss: 2.493813\n",
      "Epoch: 25722 \tTraining Loss: 1.363329 \tValidation Loss: 2.494625\n",
      "Epoch: 25723 \tTraining Loss: 1.368402 \tValidation Loss: 2.495794\n",
      "Epoch: 25724 \tTraining Loss: 1.443545 \tValidation Loss: 2.494893\n",
      "Epoch: 25725 \tTraining Loss: 1.395951 \tValidation Loss: 2.495405\n",
      "Epoch: 25726 \tTraining Loss: 1.382097 \tValidation Loss: 2.495372\n",
      "Epoch: 25727 \tTraining Loss: 1.432875 \tValidation Loss: 2.494326\n",
      "Epoch: 25728 \tTraining Loss: 1.417988 \tValidation Loss: 2.494657\n",
      "Epoch: 25729 \tTraining Loss: 1.393861 \tValidation Loss: 2.494901\n",
      "Epoch: 25730 \tTraining Loss: 1.400991 \tValidation Loss: 2.495500\n",
      "Epoch: 25731 \tTraining Loss: 1.410391 \tValidation Loss: 2.496272\n",
      "Epoch: 25732 \tTraining Loss: 1.413934 \tValidation Loss: 2.495630\n",
      "Epoch: 25733 \tTraining Loss: 1.401383 \tValidation Loss: 2.495358\n",
      "Epoch: 25734 \tTraining Loss: 1.389359 \tValidation Loss: 2.495494\n",
      "Epoch: 25735 \tTraining Loss: 1.404781 \tValidation Loss: 2.496227\n",
      "Epoch: 25736 \tTraining Loss: 1.421740 \tValidation Loss: 2.495871\n",
      "Epoch: 25737 \tTraining Loss: 1.386199 \tValidation Loss: 2.494580\n",
      "Epoch: 25738 \tTraining Loss: 1.387772 \tValidation Loss: 2.495056\n",
      "Epoch: 25739 \tTraining Loss: 1.414399 \tValidation Loss: 2.495453\n",
      "Epoch: 25740 \tTraining Loss: 1.422468 \tValidation Loss: 2.494877\n",
      "Epoch: 25741 \tTraining Loss: 1.415393 \tValidation Loss: 2.495260\n",
      "Epoch: 25742 \tTraining Loss: 1.386245 \tValidation Loss: 2.495227\n",
      "Epoch: 25743 \tTraining Loss: 1.411448 \tValidation Loss: 2.496106\n",
      "Epoch: 25744 \tTraining Loss: 1.423301 \tValidation Loss: 2.495720\n",
      "Epoch: 25745 \tTraining Loss: 1.407113 \tValidation Loss: 2.495660\n",
      "Epoch: 25746 \tTraining Loss: 1.377593 \tValidation Loss: 2.495516\n",
      "Epoch: 25747 \tTraining Loss: 1.407823 \tValidation Loss: 2.496392\n",
      "Epoch: 25748 \tTraining Loss: 1.438491 \tValidation Loss: 2.496144\n",
      "Epoch: 25749 \tTraining Loss: 1.391934 \tValidation Loss: 2.496388\n",
      "Epoch: 25750 \tTraining Loss: 1.387194 \tValidation Loss: 2.496047\n",
      "Epoch: 25751 \tTraining Loss: 1.426588 \tValidation Loss: 2.496099\n",
      "Epoch: 25752 \tTraining Loss: 1.405635 \tValidation Loss: 2.495549\n",
      "Epoch: 25753 \tTraining Loss: 1.417327 \tValidation Loss: 2.495620\n",
      "Epoch: 25754 \tTraining Loss: 1.378702 \tValidation Loss: 2.495701\n",
      "Epoch: 25755 \tTraining Loss: 1.419946 \tValidation Loss: 2.494907\n",
      "Epoch: 25756 \tTraining Loss: 1.424224 \tValidation Loss: 2.496274\n",
      "Epoch: 25757 \tTraining Loss: 1.424883 \tValidation Loss: 2.496035\n",
      "Epoch: 25758 \tTraining Loss: 1.425341 \tValidation Loss: 2.494714\n",
      "Epoch: 25759 \tTraining Loss: 1.404693 \tValidation Loss: 2.495159\n",
      "Epoch: 25760 \tTraining Loss: 1.396632 \tValidation Loss: 2.495025\n",
      "Epoch: 25761 \tTraining Loss: 1.392150 \tValidation Loss: 2.496258\n",
      "Epoch: 25762 \tTraining Loss: 1.403067 \tValidation Loss: 2.496639\n",
      "Epoch: 25763 \tTraining Loss: 1.372546 \tValidation Loss: 2.497295\n",
      "Epoch: 25764 \tTraining Loss: 1.406093 \tValidation Loss: 2.497556\n",
      "Epoch: 25765 \tTraining Loss: 1.457129 \tValidation Loss: 2.496306\n",
      "Epoch: 25766 \tTraining Loss: 1.409581 \tValidation Loss: 2.496142\n",
      "Epoch: 25767 \tTraining Loss: 1.367809 \tValidation Loss: 2.496828\n",
      "Epoch: 25768 \tTraining Loss: 1.405997 \tValidation Loss: 2.496323\n",
      "Epoch: 25769 \tTraining Loss: 1.406684 \tValidation Loss: 2.496046\n",
      "Epoch: 25770 \tTraining Loss: 1.411133 \tValidation Loss: 2.496548\n",
      "Epoch: 25771 \tTraining Loss: 1.414451 \tValidation Loss: 2.495877\n",
      "Epoch: 25772 \tTraining Loss: 1.412023 \tValidation Loss: 2.496328\n",
      "Epoch: 25773 \tTraining Loss: 1.398693 \tValidation Loss: 2.496720\n",
      "Epoch: 25774 \tTraining Loss: 1.457602 \tValidation Loss: 2.496142\n",
      "Epoch: 25775 \tTraining Loss: 1.359885 \tValidation Loss: 2.496204\n",
      "Epoch: 25776 \tTraining Loss: 1.411008 \tValidation Loss: 2.496243\n",
      "Epoch: 25777 \tTraining Loss: 1.415569 \tValidation Loss: 2.496511\n",
      "Epoch: 25778 \tTraining Loss: 1.420725 \tValidation Loss: 2.495836\n",
      "Epoch: 25779 \tTraining Loss: 1.437933 \tValidation Loss: 2.495785\n",
      "Epoch: 25780 \tTraining Loss: 1.463076 \tValidation Loss: 2.495610\n",
      "Epoch: 25781 \tTraining Loss: 1.381494 \tValidation Loss: 2.497608\n",
      "Epoch: 25782 \tTraining Loss: 1.342280 \tValidation Loss: 2.497129\n",
      "Epoch: 25783 \tTraining Loss: 1.387464 \tValidation Loss: 2.497028\n",
      "Epoch: 25784 \tTraining Loss: 1.394810 \tValidation Loss: 2.496134\n",
      "Epoch: 25785 \tTraining Loss: 1.422101 \tValidation Loss: 2.496532\n",
      "Epoch: 25786 \tTraining Loss: 1.356402 \tValidation Loss: 2.496133\n",
      "Epoch: 25787 \tTraining Loss: 1.394996 \tValidation Loss: 2.497015\n",
      "Epoch: 25788 \tTraining Loss: 1.403117 \tValidation Loss: 2.497790\n",
      "Epoch: 25789 \tTraining Loss: 1.360717 \tValidation Loss: 2.497241\n",
      "Epoch: 25790 \tTraining Loss: 1.425014 \tValidation Loss: 2.496781\n",
      "Epoch: 25791 \tTraining Loss: 1.410103 \tValidation Loss: 2.497313\n",
      "Epoch: 25792 \tTraining Loss: 1.387425 \tValidation Loss: 2.497305\n",
      "Epoch: 25793 \tTraining Loss: 1.425205 \tValidation Loss: 2.496571\n",
      "Epoch: 25794 \tTraining Loss: 1.427268 \tValidation Loss: 2.496919\n",
      "Epoch: 25795 \tTraining Loss: 1.389263 \tValidation Loss: 2.496464\n",
      "Epoch: 25796 \tTraining Loss: 1.415186 \tValidation Loss: 2.496935\n",
      "Epoch: 25797 \tTraining Loss: 1.411155 \tValidation Loss: 2.497321\n",
      "Epoch: 25798 \tTraining Loss: 1.395644 \tValidation Loss: 2.496801\n",
      "Epoch: 25799 \tTraining Loss: 1.417383 \tValidation Loss: 2.496265\n",
      "Epoch: 25800 \tTraining Loss: 1.382624 \tValidation Loss: 2.497263\n",
      "Epoch: 25801 \tTraining Loss: 1.383582 \tValidation Loss: 2.497559\n",
      "Epoch: 25802 \tTraining Loss: 1.419103 \tValidation Loss: 2.496722\n",
      "Epoch: 25803 \tTraining Loss: 1.374583 \tValidation Loss: 2.496860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25804 \tTraining Loss: 1.419456 \tValidation Loss: 2.496704\n",
      "Epoch: 25805 \tTraining Loss: 1.382051 \tValidation Loss: 2.497154\n",
      "Epoch: 25806 \tTraining Loss: 1.337983 \tValidation Loss: 2.497440\n",
      "Epoch: 25807 \tTraining Loss: 1.390746 \tValidation Loss: 2.497620\n",
      "Epoch: 25808 \tTraining Loss: 1.405984 \tValidation Loss: 2.497574\n",
      "Epoch: 25809 \tTraining Loss: 1.408436 \tValidation Loss: 2.496073\n",
      "Epoch: 25810 \tTraining Loss: 1.387633 \tValidation Loss: 2.497514\n",
      "Epoch: 25811 \tTraining Loss: 1.384836 \tValidation Loss: 2.496567\n",
      "Epoch: 25812 \tTraining Loss: 1.389628 \tValidation Loss: 2.496993\n",
      "Epoch: 25813 \tTraining Loss: 1.400674 \tValidation Loss: 2.497740\n",
      "Epoch: 25814 \tTraining Loss: 1.380720 \tValidation Loss: 2.497570\n",
      "Epoch: 25815 \tTraining Loss: 1.437832 \tValidation Loss: 2.496714\n",
      "Epoch: 25816 \tTraining Loss: 1.363734 \tValidation Loss: 2.496874\n",
      "Epoch: 25817 \tTraining Loss: 1.383915 \tValidation Loss: 2.497244\n",
      "Epoch: 25818 \tTraining Loss: 1.346216 \tValidation Loss: 2.497197\n",
      "Epoch: 25819 \tTraining Loss: 1.379334 \tValidation Loss: 2.497804\n",
      "Epoch: 25820 \tTraining Loss: 1.366514 \tValidation Loss: 2.498241\n",
      "Epoch: 25821 \tTraining Loss: 1.397373 \tValidation Loss: 2.497647\n",
      "Epoch: 25822 \tTraining Loss: 1.393474 \tValidation Loss: 2.497188\n",
      "Epoch: 25823 \tTraining Loss: 1.385654 \tValidation Loss: 2.497000\n",
      "Epoch: 25824 \tTraining Loss: 1.408669 \tValidation Loss: 2.497157\n",
      "Epoch: 25825 \tTraining Loss: 1.380545 \tValidation Loss: 2.497370\n",
      "Epoch: 25826 \tTraining Loss: 1.433079 \tValidation Loss: 2.497039\n",
      "Epoch: 25827 \tTraining Loss: 1.409194 \tValidation Loss: 2.496866\n",
      "Epoch: 25828 \tTraining Loss: 1.394834 \tValidation Loss: 2.497328\n",
      "Epoch: 25829 \tTraining Loss: 1.403351 \tValidation Loss: 2.497549\n",
      "Epoch: 25830 \tTraining Loss: 1.407323 \tValidation Loss: 2.497156\n",
      "Epoch: 25831 \tTraining Loss: 1.417563 \tValidation Loss: 2.498008\n",
      "Epoch: 25832 \tTraining Loss: 1.411213 \tValidation Loss: 2.497201\n",
      "Epoch: 25833 \tTraining Loss: 1.399543 \tValidation Loss: 2.497721\n",
      "Epoch: 25834 \tTraining Loss: 1.409916 \tValidation Loss: 2.497670\n",
      "Epoch: 25835 \tTraining Loss: 1.399734 \tValidation Loss: 2.498703\n",
      "Epoch: 25836 \tTraining Loss: 1.402904 \tValidation Loss: 2.497504\n",
      "Epoch: 25837 \tTraining Loss: 1.370124 \tValidation Loss: 2.498339\n",
      "Epoch: 25838 \tTraining Loss: 1.419180 \tValidation Loss: 2.497348\n",
      "Epoch: 25839 \tTraining Loss: 1.377559 \tValidation Loss: 2.497493\n",
      "Epoch: 25840 \tTraining Loss: 1.439294 \tValidation Loss: 2.497587\n",
      "Epoch: 25841 \tTraining Loss: 1.401360 \tValidation Loss: 2.497444\n",
      "Epoch: 25842 \tTraining Loss: 1.400450 \tValidation Loss: 2.498163\n",
      "Epoch: 25843 \tTraining Loss: 1.399049 \tValidation Loss: 2.497801\n",
      "Epoch: 25844 \tTraining Loss: 1.427344 \tValidation Loss: 2.498075\n",
      "Epoch: 25845 \tTraining Loss: 1.423911 \tValidation Loss: 2.496916\n",
      "Epoch: 25846 \tTraining Loss: 1.415359 \tValidation Loss: 2.496779\n",
      "Epoch: 25847 \tTraining Loss: 1.370104 \tValidation Loss: 2.497559\n",
      "Epoch: 25848 \tTraining Loss: 1.389198 \tValidation Loss: 2.497467\n",
      "Epoch: 25849 \tTraining Loss: 1.406464 \tValidation Loss: 2.498414\n",
      "Epoch: 25850 \tTraining Loss: 1.391598 \tValidation Loss: 2.498066\n",
      "Epoch: 25851 \tTraining Loss: 1.366407 \tValidation Loss: 2.498040\n",
      "Epoch: 25852 \tTraining Loss: 1.425596 \tValidation Loss: 2.498155\n",
      "Epoch: 25853 \tTraining Loss: 1.454962 \tValidation Loss: 2.497572\n",
      "Epoch: 25854 \tTraining Loss: 1.426532 \tValidation Loss: 2.498263\n",
      "Epoch: 25855 \tTraining Loss: 1.393644 \tValidation Loss: 2.497852\n",
      "Epoch: 25856 \tTraining Loss: 1.377042 \tValidation Loss: 2.499076\n",
      "Epoch: 25857 \tTraining Loss: 1.395046 \tValidation Loss: 2.499093\n",
      "Epoch: 25858 \tTraining Loss: 1.382494 \tValidation Loss: 2.497855\n",
      "Epoch: 25859 \tTraining Loss: 1.399026 \tValidation Loss: 2.497880\n",
      "Epoch: 25860 \tTraining Loss: 1.404409 \tValidation Loss: 2.499221\n",
      "Epoch: 25861 \tTraining Loss: 1.411655 \tValidation Loss: 2.498232\n",
      "Epoch: 25862 \tTraining Loss: 1.453345 \tValidation Loss: 2.499110\n",
      "Epoch: 25863 \tTraining Loss: 1.409786 \tValidation Loss: 2.498754\n",
      "Epoch: 25864 \tTraining Loss: 1.404856 \tValidation Loss: 2.498320\n",
      "Epoch: 25865 \tTraining Loss: 1.415260 \tValidation Loss: 2.498330\n",
      "Epoch: 25866 \tTraining Loss: 1.406370 \tValidation Loss: 2.497515\n",
      "Epoch: 25867 \tTraining Loss: 1.409949 \tValidation Loss: 2.497819\n",
      "Epoch: 25868 \tTraining Loss: 1.404198 \tValidation Loss: 2.498653\n",
      "Epoch: 25869 \tTraining Loss: 1.387538 \tValidation Loss: 2.498482\n",
      "Epoch: 25870 \tTraining Loss: 1.402398 \tValidation Loss: 2.498418\n",
      "Epoch: 25871 \tTraining Loss: 1.388329 \tValidation Loss: 2.499812\n",
      "Epoch: 25872 \tTraining Loss: 1.386863 \tValidation Loss: 2.499187\n",
      "Epoch: 25873 \tTraining Loss: 1.409060 \tValidation Loss: 2.499197\n",
      "Epoch: 25874 \tTraining Loss: 1.359595 \tValidation Loss: 2.498569\n",
      "Epoch: 25875 \tTraining Loss: 1.383862 \tValidation Loss: 2.499992\n",
      "Epoch: 25876 \tTraining Loss: 1.430653 \tValidation Loss: 2.497322\n",
      "Epoch: 25877 \tTraining Loss: 1.380480 \tValidation Loss: 2.498554\n",
      "Epoch: 25878 \tTraining Loss: 1.397185 \tValidation Loss: 2.498992\n",
      "Epoch: 25879 \tTraining Loss: 1.386534 \tValidation Loss: 2.498227\n",
      "Epoch: 25880 \tTraining Loss: 1.418275 \tValidation Loss: 2.497182\n",
      "Epoch: 25881 \tTraining Loss: 1.415663 \tValidation Loss: 2.499099\n",
      "Epoch: 25882 \tTraining Loss: 1.406275 \tValidation Loss: 2.498202\n",
      "Epoch: 25883 \tTraining Loss: 1.417860 \tValidation Loss: 2.498384\n",
      "Epoch: 25884 \tTraining Loss: 1.373073 \tValidation Loss: 2.498627\n",
      "Epoch: 25885 \tTraining Loss: 1.403072 \tValidation Loss: 2.499798\n",
      "Epoch: 25886 \tTraining Loss: 1.399703 \tValidation Loss: 2.499928\n",
      "Epoch: 25887 \tTraining Loss: 1.448267 \tValidation Loss: 2.498327\n",
      "Epoch: 25888 \tTraining Loss: 1.372385 \tValidation Loss: 2.499187\n",
      "Epoch: 25889 \tTraining Loss: 1.353242 \tValidation Loss: 2.499743\n",
      "Epoch: 25890 \tTraining Loss: 1.419934 \tValidation Loss: 2.498976\n",
      "Epoch: 25891 \tTraining Loss: 1.347111 \tValidation Loss: 2.498624\n",
      "Epoch: 25892 \tTraining Loss: 1.405829 \tValidation Loss: 2.498346\n",
      "Epoch: 25893 \tTraining Loss: 1.423829 \tValidation Loss: 2.499011\n",
      "Epoch: 25894 \tTraining Loss: 1.394258 \tValidation Loss: 2.500109\n",
      "Epoch: 25895 \tTraining Loss: 1.388713 \tValidation Loss: 2.499477\n",
      "Epoch: 25896 \tTraining Loss: 1.377538 \tValidation Loss: 2.498869\n",
      "Epoch: 25897 \tTraining Loss: 1.379456 \tValidation Loss: 2.499177\n",
      "Epoch: 25898 \tTraining Loss: 1.373257 \tValidation Loss: 2.498341\n",
      "Epoch: 25899 \tTraining Loss: 1.408761 \tValidation Loss: 2.498785\n",
      "Epoch: 25900 \tTraining Loss: 1.387108 \tValidation Loss: 2.499485\n",
      "Epoch: 25901 \tTraining Loss: 1.361215 \tValidation Loss: 2.499442\n",
      "Epoch: 25902 \tTraining Loss: 1.344441 \tValidation Loss: 2.498975\n",
      "Epoch: 25903 \tTraining Loss: 1.415502 \tValidation Loss: 2.499183\n",
      "Epoch: 25904 \tTraining Loss: 1.444423 \tValidation Loss: 2.498684\n",
      "Epoch: 25905 \tTraining Loss: 1.395596 \tValidation Loss: 2.499362\n",
      "Epoch: 25906 \tTraining Loss: 1.440960 \tValidation Loss: 2.498808\n",
      "Epoch: 25907 \tTraining Loss: 1.377490 \tValidation Loss: 2.499783\n",
      "Epoch: 25908 \tTraining Loss: 1.421837 \tValidation Loss: 2.499681\n",
      "Epoch: 25909 \tTraining Loss: 1.426562 \tValidation Loss: 2.499500\n",
      "Epoch: 25910 \tTraining Loss: 1.401484 \tValidation Loss: 2.498862\n",
      "Epoch: 25911 \tTraining Loss: 1.447660 \tValidation Loss: 2.499654\n",
      "Epoch: 25912 \tTraining Loss: 1.438701 \tValidation Loss: 2.498338\n",
      "Epoch: 25913 \tTraining Loss: 1.409468 \tValidation Loss: 2.499143\n",
      "Epoch: 25914 \tTraining Loss: 1.390662 \tValidation Loss: 2.499333\n",
      "Epoch: 25915 \tTraining Loss: 1.393528 \tValidation Loss: 2.498453\n",
      "Epoch: 25916 \tTraining Loss: 1.424373 \tValidation Loss: 2.499897\n",
      "Epoch: 25917 \tTraining Loss: 1.405827 \tValidation Loss: 2.499497\n",
      "Epoch: 25918 \tTraining Loss: 1.403177 \tValidation Loss: 2.498747\n",
      "Epoch: 25919 \tTraining Loss: 1.428607 \tValidation Loss: 2.499101\n",
      "Epoch: 25920 \tTraining Loss: 1.419411 \tValidation Loss: 2.498911\n",
      "Epoch: 25921 \tTraining Loss: 1.378779 \tValidation Loss: 2.499450\n",
      "Epoch: 25922 \tTraining Loss: 1.390011 \tValidation Loss: 2.499180\n",
      "Epoch: 25923 \tTraining Loss: 1.385809 \tValidation Loss: 2.500191\n",
      "Epoch: 25924 \tTraining Loss: 1.399710 \tValidation Loss: 2.499265\n",
      "Epoch: 25925 \tTraining Loss: 1.368287 \tValidation Loss: 2.499197\n",
      "Epoch: 25926 \tTraining Loss: 1.379947 \tValidation Loss: 2.499180\n",
      "Epoch: 25927 \tTraining Loss: 1.386184 \tValidation Loss: 2.499322\n",
      "Epoch: 25928 \tTraining Loss: 1.382204 \tValidation Loss: 2.499803\n",
      "Epoch: 25929 \tTraining Loss: 1.399872 \tValidation Loss: 2.500240\n",
      "Epoch: 25930 \tTraining Loss: 1.396137 \tValidation Loss: 2.499018\n",
      "Epoch: 25931 \tTraining Loss: 1.387350 \tValidation Loss: 2.498949\n",
      "Epoch: 25932 \tTraining Loss: 1.329672 \tValidation Loss: 2.499539\n",
      "Epoch: 25933 \tTraining Loss: 1.395070 \tValidation Loss: 2.500506\n",
      "Epoch: 25934 \tTraining Loss: 1.403821 \tValidation Loss: 2.499959\n",
      "Epoch: 25935 \tTraining Loss: 1.433000 \tValidation Loss: 2.499863\n",
      "Epoch: 25936 \tTraining Loss: 1.438741 \tValidation Loss: 2.499802\n",
      "Epoch: 25937 \tTraining Loss: 1.373654 \tValidation Loss: 2.500077\n",
      "Epoch: 25938 \tTraining Loss: 1.386326 \tValidation Loss: 2.500454\n",
      "Epoch: 25939 \tTraining Loss: 1.397690 \tValidation Loss: 2.500154\n",
      "Epoch: 25940 \tTraining Loss: 1.431587 \tValidation Loss: 2.500149\n",
      "Epoch: 25941 \tTraining Loss: 1.381317 \tValidation Loss: 2.499974\n",
      "Epoch: 25942 \tTraining Loss: 1.392240 \tValidation Loss: 2.499375\n",
      "Epoch: 25943 \tTraining Loss: 1.364986 \tValidation Loss: 2.500218\n",
      "Epoch: 25944 \tTraining Loss: 1.459750 \tValidation Loss: 2.499808\n",
      "Epoch: 25945 \tTraining Loss: 1.394895 \tValidation Loss: 2.500203\n",
      "Epoch: 25946 \tTraining Loss: 1.390861 \tValidation Loss: 2.499705\n",
      "Epoch: 25947 \tTraining Loss: 1.342398 \tValidation Loss: 2.501283\n",
      "Epoch: 25948 \tTraining Loss: 1.383061 \tValidation Loss: 2.500138\n",
      "Epoch: 25949 \tTraining Loss: 1.391220 \tValidation Loss: 2.499526\n",
      "Epoch: 25950 \tTraining Loss: 1.408029 \tValidation Loss: 2.498822\n",
      "Epoch: 25951 \tTraining Loss: 1.409690 \tValidation Loss: 2.499012\n",
      "Epoch: 25952 \tTraining Loss: 1.361837 \tValidation Loss: 2.499596\n",
      "Epoch: 25953 \tTraining Loss: 1.413913 \tValidation Loss: 2.499056\n",
      "Epoch: 25954 \tTraining Loss: 1.438504 \tValidation Loss: 2.498538\n",
      "Epoch: 25955 \tTraining Loss: 1.379582 \tValidation Loss: 2.499797\n",
      "Epoch: 25956 \tTraining Loss: 1.386149 \tValidation Loss: 2.500251\n",
      "Epoch: 25957 \tTraining Loss: 1.393792 \tValidation Loss: 2.500444\n",
      "Epoch: 25958 \tTraining Loss: 1.411944 \tValidation Loss: 2.500614\n",
      "Epoch: 25959 \tTraining Loss: 1.425615 \tValidation Loss: 2.499945\n",
      "Epoch: 25960 \tTraining Loss: 1.371251 \tValidation Loss: 2.500532\n",
      "Epoch: 25961 \tTraining Loss: 1.378790 \tValidation Loss: 2.500054\n",
      "Epoch: 25962 \tTraining Loss: 1.373143 \tValidation Loss: 2.501152\n",
      "Epoch: 25963 \tTraining Loss: 1.421574 \tValidation Loss: 2.499757\n",
      "Epoch: 25964 \tTraining Loss: 1.408787 \tValidation Loss: 2.499981\n",
      "Epoch: 25965 \tTraining Loss: 1.460368 \tValidation Loss: 2.499032\n",
      "Epoch: 25966 \tTraining Loss: 1.399928 \tValidation Loss: 2.500376\n",
      "Epoch: 25967 \tTraining Loss: 1.390868 \tValidation Loss: 2.500057\n",
      "Epoch: 25968 \tTraining Loss: 1.433512 \tValidation Loss: 2.499048\n",
      "Epoch: 25969 \tTraining Loss: 1.403619 \tValidation Loss: 2.500377\n",
      "Epoch: 25970 \tTraining Loss: 1.455922 \tValidation Loss: 2.499419\n",
      "Epoch: 25971 \tTraining Loss: 1.406744 \tValidation Loss: 2.499873\n",
      "Epoch: 25972 \tTraining Loss: 1.439124 \tValidation Loss: 2.500283\n",
      "Epoch: 25973 \tTraining Loss: 1.373510 \tValidation Loss: 2.500628\n",
      "Epoch: 25974 \tTraining Loss: 1.369081 \tValidation Loss: 2.501737\n",
      "Epoch: 25975 \tTraining Loss: 1.376248 \tValidation Loss: 2.501225\n",
      "Epoch: 25976 \tTraining Loss: 1.424749 \tValidation Loss: 2.500618\n",
      "Epoch: 25977 \tTraining Loss: 1.363049 \tValidation Loss: 2.500462\n",
      "Epoch: 25978 \tTraining Loss: 1.421845 \tValidation Loss: 2.500405\n",
      "Epoch: 25979 \tTraining Loss: 1.401006 \tValidation Loss: 2.499274\n",
      "Epoch: 25980 \tTraining Loss: 1.381941 \tValidation Loss: 2.498736\n",
      "Epoch: 25981 \tTraining Loss: 1.367366 \tValidation Loss: 2.500722\n",
      "Epoch: 25982 \tTraining Loss: 1.416330 \tValidation Loss: 2.500250\n",
      "Epoch: 25983 \tTraining Loss: 1.418258 \tValidation Loss: 2.500655\n",
      "Epoch: 25984 \tTraining Loss: 1.379444 \tValidation Loss: 2.500365\n",
      "Epoch: 25985 \tTraining Loss: 1.430427 \tValidation Loss: 2.500109\n",
      "Epoch: 25986 \tTraining Loss: 1.396208 \tValidation Loss: 2.500689\n",
      "Epoch: 25987 \tTraining Loss: 1.423375 \tValidation Loss: 2.501327\n",
      "Epoch: 25988 \tTraining Loss: 1.417646 \tValidation Loss: 2.500537\n",
      "Epoch: 25989 \tTraining Loss: 1.406107 \tValidation Loss: 2.500085\n",
      "Epoch: 25990 \tTraining Loss: 1.398596 \tValidation Loss: 2.500833\n",
      "Epoch: 25991 \tTraining Loss: 1.378952 \tValidation Loss: 2.500781\n",
      "Epoch: 25992 \tTraining Loss: 1.412118 \tValidation Loss: 2.500454\n",
      "Epoch: 25993 \tTraining Loss: 1.396064 \tValidation Loss: 2.499944\n",
      "Epoch: 25994 \tTraining Loss: 1.358594 \tValidation Loss: 2.501454\n",
      "Epoch: 25995 \tTraining Loss: 1.384350 \tValidation Loss: 2.502443\n",
      "Epoch: 25996 \tTraining Loss: 1.433828 \tValidation Loss: 2.501733\n",
      "Epoch: 25997 \tTraining Loss: 1.386735 \tValidation Loss: 2.501694\n",
      "Epoch: 25998 \tTraining Loss: 1.389104 \tValidation Loss: 2.501632\n",
      "Epoch: 25999 \tTraining Loss: 1.375307 \tValidation Loss: 2.501494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26000 \tTraining Loss: 1.378443 \tValidation Loss: 2.501625\n",
      "Epoch: 26001 \tTraining Loss: 1.406096 \tValidation Loss: 2.500429\n",
      "Epoch: 26002 \tTraining Loss: 1.435997 \tValidation Loss: 2.500732\n",
      "Epoch: 26003 \tTraining Loss: 1.434581 \tValidation Loss: 2.500279\n",
      "Epoch: 26004 \tTraining Loss: 1.434996 \tValidation Loss: 2.500076\n",
      "Epoch: 26005 \tTraining Loss: 1.372842 \tValidation Loss: 2.501089\n",
      "Epoch: 26006 \tTraining Loss: 1.388796 \tValidation Loss: 2.501454\n",
      "Epoch: 26007 \tTraining Loss: 1.387399 \tValidation Loss: 2.500727\n",
      "Epoch: 26008 \tTraining Loss: 1.406848 \tValidation Loss: 2.500755\n",
      "Epoch: 26009 \tTraining Loss: 1.380210 \tValidation Loss: 2.501543\n",
      "Epoch: 26010 \tTraining Loss: 1.402017 \tValidation Loss: 2.501022\n",
      "Epoch: 26011 \tTraining Loss: 1.352546 \tValidation Loss: 2.501710\n",
      "Epoch: 26012 \tTraining Loss: 1.375894 \tValidation Loss: 2.501614\n",
      "Epoch: 26013 \tTraining Loss: 1.398084 \tValidation Loss: 2.500717\n",
      "Epoch: 26014 \tTraining Loss: 1.404279 \tValidation Loss: 2.500294\n",
      "Epoch: 26015 \tTraining Loss: 1.391441 \tValidation Loss: 2.500641\n",
      "Epoch: 26016 \tTraining Loss: 1.410737 \tValidation Loss: 2.501628\n",
      "Epoch: 26017 \tTraining Loss: 1.392864 \tValidation Loss: 2.501690\n",
      "Epoch: 26018 \tTraining Loss: 1.388816 \tValidation Loss: 2.501821\n",
      "Epoch: 26019 \tTraining Loss: 1.394673 \tValidation Loss: 2.501824\n",
      "Epoch: 26020 \tTraining Loss: 1.423081 \tValidation Loss: 2.500932\n",
      "Epoch: 26021 \tTraining Loss: 1.398053 \tValidation Loss: 2.501689\n",
      "Epoch: 26022 \tTraining Loss: 1.394992 \tValidation Loss: 2.500602\n",
      "Epoch: 26023 \tTraining Loss: 1.383387 \tValidation Loss: 2.501620\n",
      "Epoch: 26024 \tTraining Loss: 1.432655 \tValidation Loss: 2.500775\n",
      "Epoch: 26025 \tTraining Loss: 1.405191 \tValidation Loss: 2.501415\n",
      "Epoch: 26026 \tTraining Loss: 1.388885 \tValidation Loss: 2.500841\n",
      "Epoch: 26027 \tTraining Loss: 1.425709 \tValidation Loss: 2.501128\n",
      "Epoch: 26028 \tTraining Loss: 1.376511 \tValidation Loss: 2.501612\n",
      "Epoch: 26029 \tTraining Loss: 1.362260 \tValidation Loss: 2.501865\n",
      "Epoch: 26030 \tTraining Loss: 1.456798 \tValidation Loss: 2.502224\n",
      "Epoch: 26031 \tTraining Loss: 1.396931 \tValidation Loss: 2.501562\n",
      "Epoch: 26032 \tTraining Loss: 1.387309 \tValidation Loss: 2.501517\n",
      "Epoch: 26033 \tTraining Loss: 1.440434 \tValidation Loss: 2.500914\n",
      "Epoch: 26034 \tTraining Loss: 1.390049 \tValidation Loss: 2.501338\n",
      "Epoch: 26035 \tTraining Loss: 1.364987 \tValidation Loss: 2.501679\n",
      "Epoch: 26036 \tTraining Loss: 1.410557 \tValidation Loss: 2.501117\n",
      "Epoch: 26037 \tTraining Loss: 1.386036 \tValidation Loss: 2.502217\n",
      "Epoch: 26038 \tTraining Loss: 1.442072 \tValidation Loss: 2.501448\n",
      "Epoch: 26039 \tTraining Loss: 1.392020 \tValidation Loss: 2.502582\n",
      "Epoch: 26040 \tTraining Loss: 1.424244 \tValidation Loss: 2.500284\n",
      "Epoch: 26041 \tTraining Loss: 1.399033 \tValidation Loss: 2.501617\n",
      "Epoch: 26042 \tTraining Loss: 1.374960 \tValidation Loss: 2.501956\n",
      "Epoch: 26043 \tTraining Loss: 1.445217 \tValidation Loss: 2.501475\n",
      "Epoch: 26044 \tTraining Loss: 1.346721 \tValidation Loss: 2.502661\n",
      "Epoch: 26045 \tTraining Loss: 1.381022 \tValidation Loss: 2.502563\n",
      "Epoch: 26046 \tTraining Loss: 1.399082 \tValidation Loss: 2.501188\n",
      "Epoch: 26047 \tTraining Loss: 1.402376 \tValidation Loss: 2.501555\n",
      "Epoch: 26048 \tTraining Loss: 1.401278 \tValidation Loss: 2.501781\n",
      "Epoch: 26049 \tTraining Loss: 1.377662 \tValidation Loss: 2.502860\n",
      "Epoch: 26050 \tTraining Loss: 1.404983 \tValidation Loss: 2.502663\n",
      "Epoch: 26051 \tTraining Loss: 1.365495 \tValidation Loss: 2.502762\n",
      "Epoch: 26052 \tTraining Loss: 1.424214 \tValidation Loss: 2.502680\n",
      "Epoch: 26053 \tTraining Loss: 1.419893 \tValidation Loss: 2.501892\n",
      "Epoch: 26054 \tTraining Loss: 1.408949 \tValidation Loss: 2.502467\n",
      "Epoch: 26055 \tTraining Loss: 1.395722 \tValidation Loss: 2.502545\n",
      "Epoch: 26056 \tTraining Loss: 1.394515 \tValidation Loss: 2.502266\n",
      "Epoch: 26057 \tTraining Loss: 1.427194 \tValidation Loss: 2.502328\n",
      "Epoch: 26058 \tTraining Loss: 1.372497 \tValidation Loss: 2.502307\n",
      "Epoch: 26059 \tTraining Loss: 1.385837 \tValidation Loss: 2.502506\n",
      "Epoch: 26060 \tTraining Loss: 1.388945 \tValidation Loss: 2.501584\n",
      "Epoch: 26061 \tTraining Loss: 1.401564 \tValidation Loss: 2.502148\n",
      "Epoch: 26062 \tTraining Loss: 1.376568 \tValidation Loss: 2.502580\n",
      "Epoch: 26063 \tTraining Loss: 1.367434 \tValidation Loss: 2.502811\n",
      "Epoch: 26064 \tTraining Loss: 1.404511 \tValidation Loss: 2.502825\n",
      "Epoch: 26065 \tTraining Loss: 1.427702 \tValidation Loss: 2.501390\n",
      "Epoch: 26066 \tTraining Loss: 1.386337 \tValidation Loss: 2.501920\n",
      "Epoch: 26067 \tTraining Loss: 1.381177 \tValidation Loss: 2.503021\n",
      "Epoch: 26068 \tTraining Loss: 1.371260 \tValidation Loss: 2.502748\n",
      "Epoch: 26069 \tTraining Loss: 1.333273 \tValidation Loss: 2.503499\n",
      "Epoch: 26070 \tTraining Loss: 1.382135 \tValidation Loss: 2.503423\n",
      "Epoch: 26071 \tTraining Loss: 1.407606 \tValidation Loss: 2.503188\n",
      "Epoch: 26072 \tTraining Loss: 1.371086 \tValidation Loss: 2.501632\n",
      "Epoch: 26073 \tTraining Loss: 1.428821 \tValidation Loss: 2.501557\n",
      "Epoch: 26074 \tTraining Loss: 1.363864 \tValidation Loss: 2.502414\n",
      "Epoch: 26075 \tTraining Loss: 1.371873 \tValidation Loss: 2.501957\n",
      "Epoch: 26076 \tTraining Loss: 1.392612 \tValidation Loss: 2.502842\n",
      "Epoch: 26077 \tTraining Loss: 1.416567 \tValidation Loss: 2.502334\n",
      "Epoch: 26078 \tTraining Loss: 1.405438 \tValidation Loss: 2.503217\n",
      "Epoch: 26079 \tTraining Loss: 1.441726 \tValidation Loss: 2.502432\n",
      "Epoch: 26080 \tTraining Loss: 1.373955 \tValidation Loss: 2.502497\n",
      "Epoch: 26081 \tTraining Loss: 1.378852 \tValidation Loss: 2.503232\n",
      "Epoch: 26082 \tTraining Loss: 1.430573 \tValidation Loss: 2.502277\n",
      "Epoch: 26083 \tTraining Loss: 1.396258 \tValidation Loss: 2.502696\n",
      "Epoch: 26084 \tTraining Loss: 1.385860 \tValidation Loss: 2.504212\n",
      "Epoch: 26085 \tTraining Loss: 1.374091 \tValidation Loss: 2.504102\n",
      "Epoch: 26086 \tTraining Loss: 1.405883 \tValidation Loss: 2.502767\n",
      "Epoch: 26087 \tTraining Loss: 1.405448 \tValidation Loss: 2.502897\n",
      "Epoch: 26088 \tTraining Loss: 1.405507 \tValidation Loss: 2.502707\n",
      "Epoch: 26089 \tTraining Loss: 1.378825 \tValidation Loss: 2.502949\n",
      "Epoch: 26090 \tTraining Loss: 1.389838 \tValidation Loss: 2.503783\n",
      "Epoch: 26091 \tTraining Loss: 1.464676 \tValidation Loss: 2.501874\n",
      "Epoch: 26092 \tTraining Loss: 1.410133 \tValidation Loss: 2.502674\n",
      "Epoch: 26093 \tTraining Loss: 1.383243 \tValidation Loss: 2.504025\n",
      "Epoch: 26094 \tTraining Loss: 1.403597 \tValidation Loss: 2.502958\n",
      "Epoch: 26095 \tTraining Loss: 1.428347 \tValidation Loss: 2.503019\n",
      "Epoch: 26096 \tTraining Loss: 1.403602 \tValidation Loss: 2.502479\n",
      "Epoch: 26097 \tTraining Loss: 1.359186 \tValidation Loss: 2.501726\n",
      "Epoch: 26098 \tTraining Loss: 1.344612 \tValidation Loss: 2.503406\n",
      "Epoch: 26099 \tTraining Loss: 1.351448 \tValidation Loss: 2.504112\n",
      "Epoch: 26100 \tTraining Loss: 1.370231 \tValidation Loss: 2.503609\n",
      "Epoch: 26101 \tTraining Loss: 1.468189 \tValidation Loss: 2.501756\n",
      "Epoch: 26102 \tTraining Loss: 1.434105 \tValidation Loss: 2.502326\n",
      "Epoch: 26103 \tTraining Loss: 1.366289 \tValidation Loss: 2.501763\n",
      "Epoch: 26104 \tTraining Loss: 1.352192 \tValidation Loss: 2.502646\n",
      "Epoch: 26105 \tTraining Loss: 1.401069 \tValidation Loss: 2.502682\n",
      "Epoch: 26106 \tTraining Loss: 1.393784 \tValidation Loss: 2.503059\n",
      "Epoch: 26107 \tTraining Loss: 1.380512 \tValidation Loss: 2.502898\n",
      "Epoch: 26108 \tTraining Loss: 1.392418 \tValidation Loss: 2.502993\n",
      "Epoch: 26109 \tTraining Loss: 1.378801 \tValidation Loss: 2.503536\n",
      "Epoch: 26110 \tTraining Loss: 1.416509 \tValidation Loss: 2.503235\n",
      "Epoch: 26111 \tTraining Loss: 1.422111 \tValidation Loss: 2.503521\n",
      "Epoch: 26112 \tTraining Loss: 1.379964 \tValidation Loss: 2.503209\n",
      "Epoch: 26113 \tTraining Loss: 1.382293 \tValidation Loss: 2.503089\n",
      "Epoch: 26114 \tTraining Loss: 1.390202 \tValidation Loss: 2.503381\n",
      "Epoch: 26115 \tTraining Loss: 1.385224 \tValidation Loss: 2.502781\n",
      "Epoch: 26116 \tTraining Loss: 1.420772 \tValidation Loss: 2.503191\n",
      "Epoch: 26117 \tTraining Loss: 1.436726 \tValidation Loss: 2.503727\n",
      "Epoch: 26118 \tTraining Loss: 1.370097 \tValidation Loss: 2.503524\n",
      "Epoch: 26119 \tTraining Loss: 1.422985 \tValidation Loss: 2.503666\n",
      "Epoch: 26120 \tTraining Loss: 1.403718 \tValidation Loss: 2.503200\n",
      "Epoch: 26121 \tTraining Loss: 1.453220 \tValidation Loss: 2.502333\n",
      "Epoch: 26122 \tTraining Loss: 1.415679 \tValidation Loss: 2.503123\n",
      "Epoch: 26123 \tTraining Loss: 1.388927 \tValidation Loss: 2.503013\n",
      "Epoch: 26124 \tTraining Loss: 1.410473 \tValidation Loss: 2.503409\n",
      "Epoch: 26125 \tTraining Loss: 1.380711 \tValidation Loss: 2.504030\n",
      "Epoch: 26126 \tTraining Loss: 1.390488 \tValidation Loss: 2.503717\n",
      "Epoch: 26127 \tTraining Loss: 1.369558 \tValidation Loss: 2.504431\n",
      "Epoch: 26128 \tTraining Loss: 1.344177 \tValidation Loss: 2.502977\n",
      "Epoch: 26129 \tTraining Loss: 1.380462 \tValidation Loss: 2.504339\n",
      "Epoch: 26130 \tTraining Loss: 1.390208 \tValidation Loss: 2.504156\n",
      "Epoch: 26131 \tTraining Loss: 1.388063 \tValidation Loss: 2.503554\n",
      "Epoch: 26132 \tTraining Loss: 1.356881 \tValidation Loss: 2.503118\n",
      "Epoch: 26133 \tTraining Loss: 1.414666 \tValidation Loss: 2.504507\n",
      "Epoch: 26134 \tTraining Loss: 1.438300 \tValidation Loss: 2.502778\n",
      "Epoch: 26135 \tTraining Loss: 1.385048 \tValidation Loss: 2.503563\n",
      "Epoch: 26136 \tTraining Loss: 1.367595 \tValidation Loss: 2.503479\n",
      "Epoch: 26137 \tTraining Loss: 1.384034 \tValidation Loss: 2.503428\n",
      "Epoch: 26138 \tTraining Loss: 1.426908 \tValidation Loss: 2.503428\n",
      "Epoch: 26139 \tTraining Loss: 1.474714 \tValidation Loss: 2.502183\n",
      "Epoch: 26140 \tTraining Loss: 1.422377 \tValidation Loss: 2.503666\n",
      "Epoch: 26141 \tTraining Loss: 1.407459 \tValidation Loss: 2.504434\n",
      "Epoch: 26142 \tTraining Loss: 1.395830 \tValidation Loss: 2.504215\n",
      "Epoch: 26143 \tTraining Loss: 1.388293 \tValidation Loss: 2.503828\n",
      "Epoch: 26144 \tTraining Loss: 1.370893 \tValidation Loss: 2.503449\n",
      "Epoch: 26145 \tTraining Loss: 1.378401 \tValidation Loss: 2.504416\n",
      "Epoch: 26146 \tTraining Loss: 1.367621 \tValidation Loss: 2.505226\n",
      "Epoch: 26147 \tTraining Loss: 1.388009 \tValidation Loss: 2.504369\n",
      "Epoch: 26148 \tTraining Loss: 1.406921 \tValidation Loss: 2.504465\n",
      "Epoch: 26149 \tTraining Loss: 1.398425 \tValidation Loss: 2.502924\n",
      "Epoch: 26150 \tTraining Loss: 1.417336 \tValidation Loss: 2.505063\n",
      "Epoch: 26151 \tTraining Loss: 1.410006 \tValidation Loss: 2.503634\n",
      "Epoch: 26152 \tTraining Loss: 1.389330 \tValidation Loss: 2.503521\n",
      "Epoch: 26153 \tTraining Loss: 1.403945 \tValidation Loss: 2.504082\n",
      "Epoch: 26154 \tTraining Loss: 1.393613 \tValidation Loss: 2.502376\n",
      "Epoch: 26155 \tTraining Loss: 1.448837 \tValidation Loss: 2.503921\n",
      "Epoch: 26156 \tTraining Loss: 1.364815 \tValidation Loss: 2.504977\n",
      "Epoch: 26157 \tTraining Loss: 1.419582 \tValidation Loss: 2.503680\n",
      "Epoch: 26158 \tTraining Loss: 1.391931 \tValidation Loss: 2.503412\n",
      "Epoch: 26159 \tTraining Loss: 1.395084 \tValidation Loss: 2.503644\n",
      "Epoch: 26160 \tTraining Loss: 1.386199 \tValidation Loss: 2.504002\n",
      "Epoch: 26161 \tTraining Loss: 1.390600 \tValidation Loss: 2.504512\n",
      "Epoch: 26162 \tTraining Loss: 1.439166 \tValidation Loss: 2.503492\n",
      "Epoch: 26163 \tTraining Loss: 1.373752 \tValidation Loss: 2.504372\n",
      "Epoch: 26164 \tTraining Loss: 1.406985 \tValidation Loss: 2.503885\n",
      "Epoch: 26165 \tTraining Loss: 1.415189 \tValidation Loss: 2.503424\n",
      "Epoch: 26166 \tTraining Loss: 1.416818 \tValidation Loss: 2.503029\n",
      "Epoch: 26167 \tTraining Loss: 1.406575 \tValidation Loss: 2.503708\n",
      "Epoch: 26168 \tTraining Loss: 1.401395 \tValidation Loss: 2.503978\n",
      "Epoch: 26169 \tTraining Loss: 1.436319 \tValidation Loss: 2.503414\n",
      "Epoch: 26170 \tTraining Loss: 1.378752 \tValidation Loss: 2.504278\n",
      "Epoch: 26171 \tTraining Loss: 1.415524 \tValidation Loss: 2.504168\n",
      "Epoch: 26172 \tTraining Loss: 1.371753 \tValidation Loss: 2.504586\n",
      "Epoch: 26173 \tTraining Loss: 1.406881 \tValidation Loss: 2.504116\n",
      "Epoch: 26174 \tTraining Loss: 1.352156 \tValidation Loss: 2.503196\n",
      "Epoch: 26175 \tTraining Loss: 1.389296 \tValidation Loss: 2.504287\n",
      "Epoch: 26176 \tTraining Loss: 1.396842 \tValidation Loss: 2.504129\n",
      "Epoch: 26177 \tTraining Loss: 1.361051 \tValidation Loss: 2.504327\n",
      "Epoch: 26178 \tTraining Loss: 1.371520 \tValidation Loss: 2.503821\n",
      "Epoch: 26179 \tTraining Loss: 1.376092 \tValidation Loss: 2.504149\n",
      "Epoch: 26180 \tTraining Loss: 1.407038 \tValidation Loss: 2.503844\n",
      "Epoch: 26181 \tTraining Loss: 1.356134 \tValidation Loss: 2.503979\n",
      "Epoch: 26182 \tTraining Loss: 1.387398 \tValidation Loss: 2.503967\n",
      "Epoch: 26183 \tTraining Loss: 1.353889 \tValidation Loss: 2.504568\n",
      "Epoch: 26184 \tTraining Loss: 1.379634 \tValidation Loss: 2.504429\n",
      "Epoch: 26185 \tTraining Loss: 1.408438 \tValidation Loss: 2.504826\n",
      "Epoch: 26186 \tTraining Loss: 1.347589 \tValidation Loss: 2.505182\n",
      "Epoch: 26187 \tTraining Loss: 1.461652 \tValidation Loss: 2.504707\n",
      "Epoch: 26188 \tTraining Loss: 1.417190 \tValidation Loss: 2.504830\n",
      "Epoch: 26189 \tTraining Loss: 1.379630 \tValidation Loss: 2.504648\n",
      "Epoch: 26190 \tTraining Loss: 1.386122 \tValidation Loss: 2.504380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26191 \tTraining Loss: 1.398224 \tValidation Loss: 2.505142\n",
      "Epoch: 26192 \tTraining Loss: 1.407055 \tValidation Loss: 2.505105\n",
      "Epoch: 26193 \tTraining Loss: 1.361922 \tValidation Loss: 2.505435\n",
      "Epoch: 26194 \tTraining Loss: 1.415768 \tValidation Loss: 2.505028\n",
      "Epoch: 26195 \tTraining Loss: 1.392971 \tValidation Loss: 2.504864\n",
      "Epoch: 26196 \tTraining Loss: 1.395851 \tValidation Loss: 2.505878\n",
      "Epoch: 26197 \tTraining Loss: 1.410099 \tValidation Loss: 2.504966\n",
      "Epoch: 26198 \tTraining Loss: 1.363263 \tValidation Loss: 2.504895\n",
      "Epoch: 26199 \tTraining Loss: 1.409314 \tValidation Loss: 2.505563\n",
      "Epoch: 26200 \tTraining Loss: 1.420203 \tValidation Loss: 2.504621\n",
      "Epoch: 26201 \tTraining Loss: 1.396567 \tValidation Loss: 2.505268\n",
      "Epoch: 26202 \tTraining Loss: 1.390424 \tValidation Loss: 2.504859\n",
      "Epoch: 26203 \tTraining Loss: 1.404031 \tValidation Loss: 2.504854\n",
      "Epoch: 26204 \tTraining Loss: 1.375416 \tValidation Loss: 2.505064\n",
      "Epoch: 26205 \tTraining Loss: 1.401058 \tValidation Loss: 2.505262\n",
      "Epoch: 26206 \tTraining Loss: 1.406078 \tValidation Loss: 2.505118\n",
      "Epoch: 26207 \tTraining Loss: 1.399606 \tValidation Loss: 2.504819\n",
      "Epoch: 26208 \tTraining Loss: 1.348237 \tValidation Loss: 2.504932\n",
      "Epoch: 26209 \tTraining Loss: 1.376380 \tValidation Loss: 2.504875\n",
      "Epoch: 26210 \tTraining Loss: 1.372314 \tValidation Loss: 2.505512\n",
      "Epoch: 26211 \tTraining Loss: 1.361492 \tValidation Loss: 2.506851\n",
      "Epoch: 26212 \tTraining Loss: 1.382369 \tValidation Loss: 2.506212\n",
      "Epoch: 26213 \tTraining Loss: 1.413995 \tValidation Loss: 2.505078\n",
      "Epoch: 26214 \tTraining Loss: 1.443047 \tValidation Loss: 2.505073\n",
      "Epoch: 26215 \tTraining Loss: 1.404713 \tValidation Loss: 2.504508\n",
      "Epoch: 26216 \tTraining Loss: 1.403469 \tValidation Loss: 2.505012\n",
      "Epoch: 26217 \tTraining Loss: 1.372258 \tValidation Loss: 2.506479\n",
      "Epoch: 26218 \tTraining Loss: 1.394086 \tValidation Loss: 2.505716\n",
      "Epoch: 26219 \tTraining Loss: 1.401124 \tValidation Loss: 2.505330\n",
      "Epoch: 26220 \tTraining Loss: 1.410847 \tValidation Loss: 2.505251\n",
      "Epoch: 26221 \tTraining Loss: 1.394993 \tValidation Loss: 2.503874\n",
      "Epoch: 26222 \tTraining Loss: 1.390473 \tValidation Loss: 2.504734\n",
      "Epoch: 26223 \tTraining Loss: 1.420231 \tValidation Loss: 2.505266\n",
      "Epoch: 26224 \tTraining Loss: 1.395112 \tValidation Loss: 2.505504\n",
      "Epoch: 26225 \tTraining Loss: 1.376806 \tValidation Loss: 2.505931\n",
      "Epoch: 26226 \tTraining Loss: 1.395878 \tValidation Loss: 2.504361\n",
      "Epoch: 26227 \tTraining Loss: 1.368071 \tValidation Loss: 2.505478\n",
      "Epoch: 26228 \tTraining Loss: 1.392989 \tValidation Loss: 2.505934\n",
      "Epoch: 26229 \tTraining Loss: 1.364516 \tValidation Loss: 2.505608\n",
      "Epoch: 26230 \tTraining Loss: 1.420993 \tValidation Loss: 2.505332\n",
      "Epoch: 26231 \tTraining Loss: 1.437580 \tValidation Loss: 2.504488\n",
      "Epoch: 26232 \tTraining Loss: 1.390902 \tValidation Loss: 2.504680\n",
      "Epoch: 26233 \tTraining Loss: 1.397380 \tValidation Loss: 2.505144\n",
      "Epoch: 26234 \tTraining Loss: 1.375653 \tValidation Loss: 2.506643\n",
      "Epoch: 26235 \tTraining Loss: 1.392425 \tValidation Loss: 2.505186\n",
      "Epoch: 26236 \tTraining Loss: 1.384043 \tValidation Loss: 2.506012\n",
      "Epoch: 26237 \tTraining Loss: 1.403469 \tValidation Loss: 2.505282\n",
      "Epoch: 26238 \tTraining Loss: 1.439838 \tValidation Loss: 2.505913\n",
      "Epoch: 26239 \tTraining Loss: 1.397522 \tValidation Loss: 2.505483\n",
      "Epoch: 26240 \tTraining Loss: 1.392559 \tValidation Loss: 2.504977\n",
      "Epoch: 26241 \tTraining Loss: 1.409256 \tValidation Loss: 2.506002\n",
      "Epoch: 26242 \tTraining Loss: 1.426638 \tValidation Loss: 2.505053\n",
      "Epoch: 26243 \tTraining Loss: 1.366786 \tValidation Loss: 2.506322\n",
      "Epoch: 26244 \tTraining Loss: 1.387576 \tValidation Loss: 2.505648\n",
      "Epoch: 26245 \tTraining Loss: 1.400985 \tValidation Loss: 2.505934\n",
      "Epoch: 26246 \tTraining Loss: 1.363313 \tValidation Loss: 2.506260\n",
      "Epoch: 26247 \tTraining Loss: 1.380797 \tValidation Loss: 2.505077\n",
      "Epoch: 26248 \tTraining Loss: 1.411000 \tValidation Loss: 2.505321\n",
      "Epoch: 26249 \tTraining Loss: 1.383071 \tValidation Loss: 2.506047\n",
      "Epoch: 26250 \tTraining Loss: 1.417406 \tValidation Loss: 2.505019\n",
      "Epoch: 26251 \tTraining Loss: 1.359469 \tValidation Loss: 2.506456\n",
      "Epoch: 26252 \tTraining Loss: 1.395574 \tValidation Loss: 2.506846\n",
      "Epoch: 26253 \tTraining Loss: 1.417345 \tValidation Loss: 2.505296\n",
      "Epoch: 26254 \tTraining Loss: 1.472272 \tValidation Loss: 2.505253\n",
      "Epoch: 26255 \tTraining Loss: 1.433628 \tValidation Loss: 2.504915\n",
      "Epoch: 26256 \tTraining Loss: 1.401281 \tValidation Loss: 2.504843\n",
      "Epoch: 26257 \tTraining Loss: 1.336908 \tValidation Loss: 2.506689\n",
      "Epoch: 26258 \tTraining Loss: 1.394455 \tValidation Loss: 2.504970\n",
      "Epoch: 26259 \tTraining Loss: 1.419403 \tValidation Loss: 2.504796\n",
      "Epoch: 26260 \tTraining Loss: 1.376229 \tValidation Loss: 2.504824\n",
      "Epoch: 26261 \tTraining Loss: 1.361014 \tValidation Loss: 2.505692\n",
      "Epoch: 26262 \tTraining Loss: 1.436022 \tValidation Loss: 2.505000\n",
      "Epoch: 26263 \tTraining Loss: 1.382613 \tValidation Loss: 2.505581\n",
      "Epoch: 26264 \tTraining Loss: 1.388143 \tValidation Loss: 2.505243\n",
      "Epoch: 26265 \tTraining Loss: 1.415163 \tValidation Loss: 2.506025\n",
      "Epoch: 26266 \tTraining Loss: 1.367993 \tValidation Loss: 2.506941\n",
      "Epoch: 26267 \tTraining Loss: 1.389489 \tValidation Loss: 2.506818\n",
      "Epoch: 26268 \tTraining Loss: 1.374888 \tValidation Loss: 2.506057\n",
      "Epoch: 26269 \tTraining Loss: 1.374163 \tValidation Loss: 2.506977\n",
      "Epoch: 26270 \tTraining Loss: 1.369609 \tValidation Loss: 2.506130\n",
      "Epoch: 26271 \tTraining Loss: 1.439688 \tValidation Loss: 2.506061\n",
      "Epoch: 26272 \tTraining Loss: 1.382220 \tValidation Loss: 2.506535\n",
      "Epoch: 26273 \tTraining Loss: 1.401905 \tValidation Loss: 2.506914\n",
      "Epoch: 26274 \tTraining Loss: 1.424922 \tValidation Loss: 2.505864\n",
      "Epoch: 26275 \tTraining Loss: 1.434799 \tValidation Loss: 2.505717\n",
      "Epoch: 26276 \tTraining Loss: 1.391338 \tValidation Loss: 2.506526\n",
      "Epoch: 26277 \tTraining Loss: 1.387879 \tValidation Loss: 2.507546\n",
      "Epoch: 26278 \tTraining Loss: 1.425619 \tValidation Loss: 2.505997\n",
      "Epoch: 26279 \tTraining Loss: 1.395921 \tValidation Loss: 2.506594\n",
      "Epoch: 26280 \tTraining Loss: 1.387411 \tValidation Loss: 2.506727\n",
      "Epoch: 26281 \tTraining Loss: 1.383658 \tValidation Loss: 2.505978\n",
      "Epoch: 26282 \tTraining Loss: 1.385895 \tValidation Loss: 2.507475\n",
      "Epoch: 26283 \tTraining Loss: 1.375442 \tValidation Loss: 2.505728\n",
      "Epoch: 26284 \tTraining Loss: 1.401957 \tValidation Loss: 2.506576\n",
      "Epoch: 26285 \tTraining Loss: 1.414743 \tValidation Loss: 2.506385\n",
      "Epoch: 26286 \tTraining Loss: 1.401792 \tValidation Loss: 2.506285\n",
      "Epoch: 26287 \tTraining Loss: 1.356946 \tValidation Loss: 2.507092\n",
      "Epoch: 26288 \tTraining Loss: 1.449805 \tValidation Loss: 2.505664\n",
      "Epoch: 26289 \tTraining Loss: 1.426668 \tValidation Loss: 2.505202\n",
      "Epoch: 26290 \tTraining Loss: 1.399352 \tValidation Loss: 2.505466\n",
      "Epoch: 26291 \tTraining Loss: 1.347214 \tValidation Loss: 2.506073\n",
      "Epoch: 26292 \tTraining Loss: 1.381473 \tValidation Loss: 2.505635\n",
      "Epoch: 26293 \tTraining Loss: 1.390278 \tValidation Loss: 2.507246\n",
      "Epoch: 26294 \tTraining Loss: 1.386345 \tValidation Loss: 2.506831\n",
      "Epoch: 26295 \tTraining Loss: 1.393892 \tValidation Loss: 2.507198\n",
      "Epoch: 26296 \tTraining Loss: 1.443283 \tValidation Loss: 2.505600\n",
      "Epoch: 26297 \tTraining Loss: 1.411658 \tValidation Loss: 2.506450\n",
      "Epoch: 26298 \tTraining Loss: 1.328255 \tValidation Loss: 2.506647\n",
      "Epoch: 26299 \tTraining Loss: 1.383432 \tValidation Loss: 2.507364\n",
      "Epoch: 26300 \tTraining Loss: 1.362186 \tValidation Loss: 2.507557\n",
      "Epoch: 26301 \tTraining Loss: 1.413179 \tValidation Loss: 2.506395\n",
      "Epoch: 26302 \tTraining Loss: 1.412814 \tValidation Loss: 2.507584\n",
      "Epoch: 26303 \tTraining Loss: 1.375161 \tValidation Loss: 2.506323\n",
      "Epoch: 26304 \tTraining Loss: 1.385370 \tValidation Loss: 2.506048\n",
      "Epoch: 26305 \tTraining Loss: 1.403524 \tValidation Loss: 2.506996\n",
      "Epoch: 26306 \tTraining Loss: 1.404954 \tValidation Loss: 2.507536\n",
      "Epoch: 26307 \tTraining Loss: 1.398702 \tValidation Loss: 2.507756\n",
      "Epoch: 26308 \tTraining Loss: 1.388719 \tValidation Loss: 2.507661\n",
      "Epoch: 26309 \tTraining Loss: 1.396336 \tValidation Loss: 2.507437\n",
      "Epoch: 26310 \tTraining Loss: 1.388706 \tValidation Loss: 2.507080\n",
      "Epoch: 26311 \tTraining Loss: 1.409427 \tValidation Loss: 2.507788\n",
      "Epoch: 26312 \tTraining Loss: 1.370758 \tValidation Loss: 2.508043\n",
      "Epoch: 26313 \tTraining Loss: 1.395953 \tValidation Loss: 2.507475\n",
      "Epoch: 26314 \tTraining Loss: 1.396146 \tValidation Loss: 2.507237\n",
      "Epoch: 26315 \tTraining Loss: 1.390983 \tValidation Loss: 2.507953\n",
      "Epoch: 26316 \tTraining Loss: 1.377630 \tValidation Loss: 2.507271\n",
      "Epoch: 26317 \tTraining Loss: 1.361148 \tValidation Loss: 2.507344\n",
      "Epoch: 26318 \tTraining Loss: 1.395031 \tValidation Loss: 2.507577\n",
      "Epoch: 26319 \tTraining Loss: 1.400091 \tValidation Loss: 2.507130\n",
      "Epoch: 26320 \tTraining Loss: 1.409561 \tValidation Loss: 2.506991\n",
      "Epoch: 26321 \tTraining Loss: 1.391165 \tValidation Loss: 2.507930\n",
      "Epoch: 26322 \tTraining Loss: 1.354624 \tValidation Loss: 2.507789\n",
      "Epoch: 26323 \tTraining Loss: 1.413106 \tValidation Loss: 2.505752\n",
      "Epoch: 26324 \tTraining Loss: 1.372416 \tValidation Loss: 2.506700\n",
      "Epoch: 26325 \tTraining Loss: 1.406606 \tValidation Loss: 2.507123\n",
      "Epoch: 26326 \tTraining Loss: 1.440474 \tValidation Loss: 2.507370\n",
      "Epoch: 26327 \tTraining Loss: 1.349305 \tValidation Loss: 2.508765\n",
      "Epoch: 26328 \tTraining Loss: 1.432500 \tValidation Loss: 2.507237\n",
      "Epoch: 26329 \tTraining Loss: 1.413388 \tValidation Loss: 2.507994\n",
      "Epoch: 26330 \tTraining Loss: 1.402072 \tValidation Loss: 2.507599\n",
      "Epoch: 26331 \tTraining Loss: 1.365417 \tValidation Loss: 2.507541\n",
      "Epoch: 26332 \tTraining Loss: 1.379506 \tValidation Loss: 2.508058\n",
      "Epoch: 26333 \tTraining Loss: 1.407813 \tValidation Loss: 2.507149\n",
      "Epoch: 26334 \tTraining Loss: 1.376381 \tValidation Loss: 2.508154\n",
      "Epoch: 26335 \tTraining Loss: 1.400095 \tValidation Loss: 2.507710\n",
      "Epoch: 26336 \tTraining Loss: 1.356146 \tValidation Loss: 2.507235\n",
      "Epoch: 26337 \tTraining Loss: 1.419456 \tValidation Loss: 2.506737\n",
      "Epoch: 26338 \tTraining Loss: 1.427523 \tValidation Loss: 2.506699\n",
      "Epoch: 26339 \tTraining Loss: 1.377464 \tValidation Loss: 2.507343\n",
      "Epoch: 26340 \tTraining Loss: 1.406355 \tValidation Loss: 2.507491\n",
      "Epoch: 26341 \tTraining Loss: 1.383476 \tValidation Loss: 2.508219\n",
      "Epoch: 26342 \tTraining Loss: 1.424396 \tValidation Loss: 2.507339\n",
      "Epoch: 26343 \tTraining Loss: 1.419714 \tValidation Loss: 2.507656\n",
      "Epoch: 26344 \tTraining Loss: 1.400874 \tValidation Loss: 2.509052\n",
      "Epoch: 26345 \tTraining Loss: 1.359748 \tValidation Loss: 2.508484\n",
      "Epoch: 26346 \tTraining Loss: 1.417547 \tValidation Loss: 2.507906\n",
      "Epoch: 26347 \tTraining Loss: 1.439180 \tValidation Loss: 2.508913\n",
      "Epoch: 26348 \tTraining Loss: 1.375557 \tValidation Loss: 2.507980\n",
      "Epoch: 26349 \tTraining Loss: 1.365399 \tValidation Loss: 2.508708\n",
      "Epoch: 26350 \tTraining Loss: 1.396215 \tValidation Loss: 2.507394\n",
      "Epoch: 26351 \tTraining Loss: 1.385710 \tValidation Loss: 2.508654\n",
      "Epoch: 26352 \tTraining Loss: 1.437549 \tValidation Loss: 2.506778\n",
      "Epoch: 26353 \tTraining Loss: 1.404424 \tValidation Loss: 2.508268\n",
      "Epoch: 26354 \tTraining Loss: 1.372989 \tValidation Loss: 2.507942\n",
      "Epoch: 26355 \tTraining Loss: 1.365686 \tValidation Loss: 2.508212\n",
      "Epoch: 26356 \tTraining Loss: 1.372508 \tValidation Loss: 2.508811\n",
      "Epoch: 26357 \tTraining Loss: 1.401717 \tValidation Loss: 2.507730\n",
      "Epoch: 26358 \tTraining Loss: 1.402270 \tValidation Loss: 2.508143\n",
      "Epoch: 26359 \tTraining Loss: 1.367202 \tValidation Loss: 2.508096\n",
      "Epoch: 26360 \tTraining Loss: 1.371562 \tValidation Loss: 2.509021\n",
      "Epoch: 26361 \tTraining Loss: 1.381228 \tValidation Loss: 2.508127\n",
      "Epoch: 26362 \tTraining Loss: 1.409202 \tValidation Loss: 2.508216\n",
      "Epoch: 26363 \tTraining Loss: 1.385373 \tValidation Loss: 2.508963\n",
      "Epoch: 26364 \tTraining Loss: 1.378030 \tValidation Loss: 2.509067\n",
      "Epoch: 26365 \tTraining Loss: 1.355530 \tValidation Loss: 2.508773\n",
      "Epoch: 26366 \tTraining Loss: 1.361392 \tValidation Loss: 2.508702\n",
      "Epoch: 26367 \tTraining Loss: 1.417391 \tValidation Loss: 2.508146\n",
      "Epoch: 26368 \tTraining Loss: 1.426090 \tValidation Loss: 2.507721\n",
      "Epoch: 26369 \tTraining Loss: 1.386239 \tValidation Loss: 2.507928\n",
      "Epoch: 26370 \tTraining Loss: 1.425497 \tValidation Loss: 2.507907\n",
      "Epoch: 26371 \tTraining Loss: 1.366176 \tValidation Loss: 2.508102\n",
      "Epoch: 26372 \tTraining Loss: 1.391321 \tValidation Loss: 2.510274\n",
      "Epoch: 26373 \tTraining Loss: 1.373435 \tValidation Loss: 2.510170\n",
      "Epoch: 26374 \tTraining Loss: 1.390842 \tValidation Loss: 2.508919\n",
      "Epoch: 26375 \tTraining Loss: 1.403628 \tValidation Loss: 2.509547\n",
      "Epoch: 26376 \tTraining Loss: 1.381947 \tValidation Loss: 2.508643\n",
      "Epoch: 26377 \tTraining Loss: 1.393231 \tValidation Loss: 2.508910\n",
      "Epoch: 26378 \tTraining Loss: 1.366194 \tValidation Loss: 2.508531\n",
      "Epoch: 26379 \tTraining Loss: 1.416018 \tValidation Loss: 2.507495\n",
      "Epoch: 26380 \tTraining Loss: 1.421135 \tValidation Loss: 2.507751\n",
      "Epoch: 26381 \tTraining Loss: 1.396723 \tValidation Loss: 2.508440\n",
      "Epoch: 26382 \tTraining Loss: 1.417768 \tValidation Loss: 2.508433\n",
      "Epoch: 26383 \tTraining Loss: 1.419756 \tValidation Loss: 2.507101\n",
      "Epoch: 26384 \tTraining Loss: 1.384785 \tValidation Loss: 2.507616\n",
      "Epoch: 26385 \tTraining Loss: 1.450404 \tValidation Loss: 2.507073\n",
      "Epoch: 26386 \tTraining Loss: 1.386078 \tValidation Loss: 2.507660\n",
      "Epoch: 26387 \tTraining Loss: 1.441869 \tValidation Loss: 2.507834\n",
      "Epoch: 26388 \tTraining Loss: 1.347916 \tValidation Loss: 2.509282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26389 \tTraining Loss: 1.433601 \tValidation Loss: 2.507929\n",
      "Epoch: 26390 \tTraining Loss: 1.377091 \tValidation Loss: 2.508273\n",
      "Epoch: 26391 \tTraining Loss: 1.416461 \tValidation Loss: 2.507679\n",
      "Epoch: 26392 \tTraining Loss: 1.417310 \tValidation Loss: 2.509048\n",
      "Epoch: 26393 \tTraining Loss: 1.375178 \tValidation Loss: 2.509351\n",
      "Epoch: 26394 \tTraining Loss: 1.395616 \tValidation Loss: 2.509609\n",
      "Epoch: 26395 \tTraining Loss: 1.413007 \tValidation Loss: 2.508697\n",
      "Epoch: 26396 \tTraining Loss: 1.404220 \tValidation Loss: 2.508637\n",
      "Epoch: 26397 \tTraining Loss: 1.384992 \tValidation Loss: 2.508428\n",
      "Epoch: 26398 \tTraining Loss: 1.426362 \tValidation Loss: 2.508518\n",
      "Epoch: 26399 \tTraining Loss: 1.410736 \tValidation Loss: 2.509516\n",
      "Epoch: 26400 \tTraining Loss: 1.431838 \tValidation Loss: 2.508127\n",
      "Epoch: 26401 \tTraining Loss: 1.377564 \tValidation Loss: 2.508909\n",
      "Epoch: 26402 \tTraining Loss: 1.422786 \tValidation Loss: 2.508144\n",
      "Epoch: 26403 \tTraining Loss: 1.425950 \tValidation Loss: 2.508907\n",
      "Epoch: 26404 \tTraining Loss: 1.363144 \tValidation Loss: 2.509119\n",
      "Epoch: 26405 \tTraining Loss: 1.402120 \tValidation Loss: 2.509999\n",
      "Epoch: 26406 \tTraining Loss: 1.397077 \tValidation Loss: 2.508096\n",
      "Epoch: 26407 \tTraining Loss: 1.335543 \tValidation Loss: 2.509354\n",
      "Epoch: 26408 \tTraining Loss: 1.361095 \tValidation Loss: 2.509816\n",
      "Epoch: 26409 \tTraining Loss: 1.411350 \tValidation Loss: 2.508901\n",
      "Epoch: 26410 \tTraining Loss: 1.414954 \tValidation Loss: 2.508823\n",
      "Epoch: 26411 \tTraining Loss: 1.369878 \tValidation Loss: 2.509471\n",
      "Epoch: 26412 \tTraining Loss: 1.399058 \tValidation Loss: 2.509249\n",
      "Epoch: 26413 \tTraining Loss: 1.393951 \tValidation Loss: 2.508450\n",
      "Epoch: 26414 \tTraining Loss: 1.334956 \tValidation Loss: 2.509394\n",
      "Epoch: 26415 \tTraining Loss: 1.383504 \tValidation Loss: 2.508974\n",
      "Epoch: 26416 \tTraining Loss: 1.325981 \tValidation Loss: 2.510447\n",
      "Epoch: 26417 \tTraining Loss: 1.372368 \tValidation Loss: 2.509393\n",
      "Epoch: 26418 \tTraining Loss: 1.428793 \tValidation Loss: 2.509226\n",
      "Epoch: 26419 \tTraining Loss: 1.382061 \tValidation Loss: 2.508590\n",
      "Epoch: 26420 \tTraining Loss: 1.382837 \tValidation Loss: 2.508411\n",
      "Epoch: 26421 \tTraining Loss: 1.382000 \tValidation Loss: 2.509804\n",
      "Epoch: 26422 \tTraining Loss: 1.402224 \tValidation Loss: 2.509334\n",
      "Epoch: 26423 \tTraining Loss: 1.431874 \tValidation Loss: 2.508558\n",
      "Epoch: 26424 \tTraining Loss: 1.393386 \tValidation Loss: 2.509204\n",
      "Epoch: 26425 \tTraining Loss: 1.359006 \tValidation Loss: 2.508990\n",
      "Epoch: 26426 \tTraining Loss: 1.454146 \tValidation Loss: 2.509109\n",
      "Epoch: 26427 \tTraining Loss: 1.401808 \tValidation Loss: 2.508772\n",
      "Epoch: 26428 \tTraining Loss: 1.412390 \tValidation Loss: 2.509566\n",
      "Epoch: 26429 \tTraining Loss: 1.366033 \tValidation Loss: 2.508720\n",
      "Epoch: 26430 \tTraining Loss: 1.376869 \tValidation Loss: 2.508940\n",
      "Epoch: 26431 \tTraining Loss: 1.433420 \tValidation Loss: 2.508309\n",
      "Epoch: 26432 \tTraining Loss: 1.373754 \tValidation Loss: 2.509777\n",
      "Epoch: 26433 \tTraining Loss: 1.432473 \tValidation Loss: 2.508044\n",
      "Epoch: 26434 \tTraining Loss: 1.354690 \tValidation Loss: 2.509001\n",
      "Epoch: 26435 \tTraining Loss: 1.323861 \tValidation Loss: 2.510034\n",
      "Epoch: 26436 \tTraining Loss: 1.372529 \tValidation Loss: 2.508985\n",
      "Epoch: 26437 \tTraining Loss: 1.385736 \tValidation Loss: 2.508757\n",
      "Epoch: 26438 \tTraining Loss: 1.362359 \tValidation Loss: 2.509418\n",
      "Epoch: 26439 \tTraining Loss: 1.380269 \tValidation Loss: 2.509266\n",
      "Epoch: 26440 \tTraining Loss: 1.447391 \tValidation Loss: 2.508644\n",
      "Epoch: 26441 \tTraining Loss: 1.403708 \tValidation Loss: 2.509407\n",
      "Epoch: 26442 \tTraining Loss: 1.398145 \tValidation Loss: 2.508711\n",
      "Epoch: 26443 \tTraining Loss: 1.358777 \tValidation Loss: 2.509919\n",
      "Epoch: 26444 \tTraining Loss: 1.386284 \tValidation Loss: 2.509974\n",
      "Epoch: 26445 \tTraining Loss: 1.442335 \tValidation Loss: 2.509209\n",
      "Epoch: 26446 \tTraining Loss: 1.424087 \tValidation Loss: 2.509549\n",
      "Epoch: 26447 \tTraining Loss: 1.427910 \tValidation Loss: 2.509428\n",
      "Epoch: 26448 \tTraining Loss: 1.390602 \tValidation Loss: 2.509797\n",
      "Epoch: 26449 \tTraining Loss: 1.416011 \tValidation Loss: 2.509309\n",
      "Epoch: 26450 \tTraining Loss: 1.402452 \tValidation Loss: 2.508558\n",
      "Epoch: 26451 \tTraining Loss: 1.378563 \tValidation Loss: 2.509217\n",
      "Epoch: 26452 \tTraining Loss: 1.412541 \tValidation Loss: 2.509425\n",
      "Epoch: 26453 \tTraining Loss: 1.372333 \tValidation Loss: 2.509165\n",
      "Epoch: 26454 \tTraining Loss: 1.408701 \tValidation Loss: 2.508574\n",
      "Epoch: 26455 \tTraining Loss: 1.384149 \tValidation Loss: 2.509810\n",
      "Epoch: 26456 \tTraining Loss: 1.384532 \tValidation Loss: 2.509999\n",
      "Epoch: 26457 \tTraining Loss: 1.397828 \tValidation Loss: 2.510275\n",
      "Epoch: 26458 \tTraining Loss: 1.415367 \tValidation Loss: 2.509664\n",
      "Epoch: 26459 \tTraining Loss: 1.365490 \tValidation Loss: 2.510423\n",
      "Epoch: 26460 \tTraining Loss: 1.362569 \tValidation Loss: 2.509406\n",
      "Epoch: 26461 \tTraining Loss: 1.382426 \tValidation Loss: 2.510065\n",
      "Epoch: 26462 \tTraining Loss: 1.386599 \tValidation Loss: 2.510179\n",
      "Epoch: 26463 \tTraining Loss: 1.397016 \tValidation Loss: 2.510072\n",
      "Epoch: 26464 \tTraining Loss: 1.373572 \tValidation Loss: 2.509854\n",
      "Epoch: 26465 \tTraining Loss: 1.402233 \tValidation Loss: 2.509633\n",
      "Epoch: 26466 \tTraining Loss: 1.399491 \tValidation Loss: 2.509837\n",
      "Epoch: 26467 \tTraining Loss: 1.399475 \tValidation Loss: 2.509887\n",
      "Epoch: 26468 \tTraining Loss: 1.373656 \tValidation Loss: 2.510071\n",
      "Epoch: 26469 \tTraining Loss: 1.385211 \tValidation Loss: 2.510278\n",
      "Epoch: 26470 \tTraining Loss: 1.370319 \tValidation Loss: 2.510485\n",
      "Epoch: 26471 \tTraining Loss: 1.384973 \tValidation Loss: 2.510154\n",
      "Epoch: 26472 \tTraining Loss: 1.366718 \tValidation Loss: 2.510918\n",
      "Epoch: 26473 \tTraining Loss: 1.339228 \tValidation Loss: 2.510982\n",
      "Epoch: 26474 \tTraining Loss: 1.346970 \tValidation Loss: 2.510570\n",
      "Epoch: 26475 \tTraining Loss: 1.354907 \tValidation Loss: 2.510851\n",
      "Epoch: 26476 \tTraining Loss: 1.398102 \tValidation Loss: 2.509917\n",
      "Epoch: 26477 \tTraining Loss: 1.402743 \tValidation Loss: 2.509588\n",
      "Epoch: 26478 \tTraining Loss: 1.373364 \tValidation Loss: 2.509265\n",
      "Epoch: 26479 \tTraining Loss: 1.391535 \tValidation Loss: 2.510362\n",
      "Epoch: 26480 \tTraining Loss: 1.377611 \tValidation Loss: 2.510194\n",
      "Epoch: 26481 \tTraining Loss: 1.382361 \tValidation Loss: 2.510202\n",
      "Epoch: 26482 \tTraining Loss: 1.371182 \tValidation Loss: 2.511100\n",
      "Epoch: 26483 \tTraining Loss: 1.388335 \tValidation Loss: 2.509911\n",
      "Epoch: 26484 \tTraining Loss: 1.418872 \tValidation Loss: 2.509667\n",
      "Epoch: 26485 \tTraining Loss: 1.411035 \tValidation Loss: 2.509614\n",
      "Epoch: 26486 \tTraining Loss: 1.403303 \tValidation Loss: 2.509680\n",
      "Epoch: 26487 \tTraining Loss: 1.403041 \tValidation Loss: 2.509739\n",
      "Epoch: 26488 \tTraining Loss: 1.407364 \tValidation Loss: 2.509127\n",
      "Epoch: 26489 \tTraining Loss: 1.352424 \tValidation Loss: 2.510215\n",
      "Epoch: 26490 \tTraining Loss: 1.340771 \tValidation Loss: 2.511440\n",
      "Epoch: 26491 \tTraining Loss: 1.355826 \tValidation Loss: 2.510546\n",
      "Epoch: 26492 \tTraining Loss: 1.385948 \tValidation Loss: 2.510685\n",
      "Epoch: 26493 \tTraining Loss: 1.401914 \tValidation Loss: 2.510621\n",
      "Epoch: 26494 \tTraining Loss: 1.390687 \tValidation Loss: 2.510548\n",
      "Epoch: 26495 \tTraining Loss: 1.397929 \tValidation Loss: 2.510692\n",
      "Epoch: 26496 \tTraining Loss: 1.391643 \tValidation Loss: 2.510718\n",
      "Epoch: 26497 \tTraining Loss: 1.406476 \tValidation Loss: 2.510082\n",
      "Epoch: 26498 \tTraining Loss: 1.379592 \tValidation Loss: 2.510428\n",
      "Epoch: 26499 \tTraining Loss: 1.387836 \tValidation Loss: 2.511165\n",
      "Epoch: 26500 \tTraining Loss: 1.399129 \tValidation Loss: 2.510491\n",
      "Epoch: 26501 \tTraining Loss: 1.393212 \tValidation Loss: 2.510974\n",
      "Epoch: 26502 \tTraining Loss: 1.331755 \tValidation Loss: 2.510869\n",
      "Epoch: 26503 \tTraining Loss: 1.415991 \tValidation Loss: 2.510598\n",
      "Epoch: 26504 \tTraining Loss: 1.398165 \tValidation Loss: 2.510747\n",
      "Epoch: 26505 \tTraining Loss: 1.398377 \tValidation Loss: 2.510233\n",
      "Epoch: 26506 \tTraining Loss: 1.394291 \tValidation Loss: 2.511597\n",
      "Epoch: 26507 \tTraining Loss: 1.385793 \tValidation Loss: 2.511181\n",
      "Epoch: 26508 \tTraining Loss: 1.391899 \tValidation Loss: 2.510331\n",
      "Epoch: 26509 \tTraining Loss: 1.371974 \tValidation Loss: 2.510906\n",
      "Epoch: 26510 \tTraining Loss: 1.400598 \tValidation Loss: 2.510289\n",
      "Epoch: 26511 \tTraining Loss: 1.393536 \tValidation Loss: 2.510547\n",
      "Epoch: 26512 \tTraining Loss: 1.400061 \tValidation Loss: 2.510286\n",
      "Epoch: 26513 \tTraining Loss: 1.380348 \tValidation Loss: 2.510810\n",
      "Epoch: 26514 \tTraining Loss: 1.396843 \tValidation Loss: 2.509316\n",
      "Epoch: 26515 \tTraining Loss: 1.410074 \tValidation Loss: 2.509404\n",
      "Epoch: 26516 \tTraining Loss: 1.371670 \tValidation Loss: 2.510545\n",
      "Epoch: 26517 \tTraining Loss: 1.388638 \tValidation Loss: 2.509764\n",
      "Epoch: 26518 \tTraining Loss: 1.386360 \tValidation Loss: 2.510194\n",
      "Epoch: 26519 \tTraining Loss: 1.387366 \tValidation Loss: 2.510674\n",
      "Epoch: 26520 \tTraining Loss: 1.420253 \tValidation Loss: 2.509640\n",
      "Epoch: 26521 \tTraining Loss: 1.391536 \tValidation Loss: 2.510848\n",
      "Epoch: 26522 \tTraining Loss: 1.371880 \tValidation Loss: 2.510820\n",
      "Epoch: 26523 \tTraining Loss: 1.348928 \tValidation Loss: 2.511834\n",
      "Epoch: 26524 \tTraining Loss: 1.352704 \tValidation Loss: 2.510501\n",
      "Epoch: 26525 \tTraining Loss: 1.323446 \tValidation Loss: 2.511480\n",
      "Epoch: 26526 \tTraining Loss: 1.364686 \tValidation Loss: 2.510831\n",
      "Epoch: 26527 \tTraining Loss: 1.370796 \tValidation Loss: 2.510918\n",
      "Epoch: 26528 \tTraining Loss: 1.417082 \tValidation Loss: 2.511662\n",
      "Epoch: 26529 \tTraining Loss: 1.382006 \tValidation Loss: 2.510886\n",
      "Epoch: 26530 \tTraining Loss: 1.354047 \tValidation Loss: 2.511115\n",
      "Epoch: 26531 \tTraining Loss: 1.408757 \tValidation Loss: 2.512021\n",
      "Epoch: 26532 \tTraining Loss: 1.432445 \tValidation Loss: 2.511908\n",
      "Epoch: 26533 \tTraining Loss: 1.354528 \tValidation Loss: 2.511634\n",
      "Epoch: 26534 \tTraining Loss: 1.429452 \tValidation Loss: 2.511542\n",
      "Epoch: 26535 \tTraining Loss: 1.364263 \tValidation Loss: 2.512113\n",
      "Epoch: 26536 \tTraining Loss: 1.406401 \tValidation Loss: 2.511095\n",
      "Epoch: 26537 \tTraining Loss: 1.403194 \tValidation Loss: 2.510845\n",
      "Epoch: 26538 \tTraining Loss: 1.367983 \tValidation Loss: 2.511269\n",
      "Epoch: 26539 \tTraining Loss: 1.384813 \tValidation Loss: 2.512289\n",
      "Epoch: 26540 \tTraining Loss: 1.415171 \tValidation Loss: 2.511181\n",
      "Epoch: 26541 \tTraining Loss: 1.378710 \tValidation Loss: 2.512239\n",
      "Epoch: 26542 \tTraining Loss: 1.368391 \tValidation Loss: 2.511494\n",
      "Epoch: 26543 \tTraining Loss: 1.372186 \tValidation Loss: 2.510789\n",
      "Epoch: 26544 \tTraining Loss: 1.401451 \tValidation Loss: 2.511622\n",
      "Epoch: 26545 \tTraining Loss: 1.345594 \tValidation Loss: 2.513921\n",
      "Epoch: 26546 \tTraining Loss: 1.366010 \tValidation Loss: 2.513520\n",
      "Epoch: 26547 \tTraining Loss: 1.394550 \tValidation Loss: 2.512074\n",
      "Epoch: 26548 \tTraining Loss: 1.416342 \tValidation Loss: 2.511577\n",
      "Epoch: 26549 \tTraining Loss: 1.397816 \tValidation Loss: 2.511054\n",
      "Epoch: 26550 \tTraining Loss: 1.410661 \tValidation Loss: 2.511523\n",
      "Epoch: 26551 \tTraining Loss: 1.348505 \tValidation Loss: 2.513074\n",
      "Epoch: 26552 \tTraining Loss: 1.384901 \tValidation Loss: 2.511919\n",
      "Epoch: 26553 \tTraining Loss: 1.361777 \tValidation Loss: 2.512345\n",
      "Epoch: 26554 \tTraining Loss: 1.349312 \tValidation Loss: 2.512530\n",
      "Epoch: 26555 \tTraining Loss: 1.401741 \tValidation Loss: 2.511757\n",
      "Epoch: 26556 \tTraining Loss: 1.414725 \tValidation Loss: 2.511725\n",
      "Epoch: 26557 \tTraining Loss: 1.411112 \tValidation Loss: 2.511675\n",
      "Epoch: 26558 \tTraining Loss: 1.408584 \tValidation Loss: 2.511525\n",
      "Epoch: 26559 \tTraining Loss: 1.367316 \tValidation Loss: 2.512680\n",
      "Epoch: 26560 \tTraining Loss: 1.388475 \tValidation Loss: 2.512116\n",
      "Epoch: 26561 \tTraining Loss: 1.366434 \tValidation Loss: 2.511348\n",
      "Epoch: 26562 \tTraining Loss: 1.388705 \tValidation Loss: 2.511072\n",
      "Epoch: 26563 \tTraining Loss: 1.425865 \tValidation Loss: 2.511434\n",
      "Epoch: 26564 \tTraining Loss: 1.365582 \tValidation Loss: 2.512133\n",
      "Epoch: 26565 \tTraining Loss: 1.417814 \tValidation Loss: 2.512017\n",
      "Epoch: 26566 \tTraining Loss: 1.381614 \tValidation Loss: 2.512424\n",
      "Epoch: 26567 \tTraining Loss: 1.416000 \tValidation Loss: 2.510882\n",
      "Epoch: 26568 \tTraining Loss: 1.403093 \tValidation Loss: 2.512459\n",
      "Epoch: 26569 \tTraining Loss: 1.392867 \tValidation Loss: 2.512664\n",
      "Epoch: 26570 \tTraining Loss: 1.389650 \tValidation Loss: 2.511589\n",
      "Epoch: 26571 \tTraining Loss: 1.373111 \tValidation Loss: 2.512651\n",
      "Epoch: 26572 \tTraining Loss: 1.374486 \tValidation Loss: 2.512265\n",
      "Epoch: 26573 \tTraining Loss: 1.397216 \tValidation Loss: 2.512903\n",
      "Epoch: 26574 \tTraining Loss: 1.398432 \tValidation Loss: 2.512495\n",
      "Epoch: 26575 \tTraining Loss: 1.376181 \tValidation Loss: 2.513369\n",
      "Epoch: 26576 \tTraining Loss: 1.378146 \tValidation Loss: 2.512322\n",
      "Epoch: 26577 \tTraining Loss: 1.424911 \tValidation Loss: 2.511976\n",
      "Epoch: 26578 \tTraining Loss: 1.472929 \tValidation Loss: 2.510933\n",
      "Epoch: 26579 \tTraining Loss: 1.397992 \tValidation Loss: 2.510735\n",
      "Epoch: 26580 \tTraining Loss: 1.406703 \tValidation Loss: 2.511395\n",
      "Epoch: 26581 \tTraining Loss: 1.394617 \tValidation Loss: 2.512800\n",
      "Epoch: 26582 \tTraining Loss: 1.369201 \tValidation Loss: 2.511894\n",
      "Epoch: 26583 \tTraining Loss: 1.333362 \tValidation Loss: 2.512889\n",
      "Epoch: 26584 \tTraining Loss: 1.419558 \tValidation Loss: 2.512843\n",
      "Epoch: 26585 \tTraining Loss: 1.361589 \tValidation Loss: 2.513978\n",
      "Epoch: 26586 \tTraining Loss: 1.347804 \tValidation Loss: 2.512875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26587 \tTraining Loss: 1.396296 \tValidation Loss: 2.512886\n",
      "Epoch: 26588 \tTraining Loss: 1.423812 \tValidation Loss: 2.512420\n",
      "Epoch: 26589 \tTraining Loss: 1.383022 \tValidation Loss: 2.512972\n",
      "Epoch: 26590 \tTraining Loss: 1.385443 \tValidation Loss: 2.513062\n",
      "Epoch: 26591 \tTraining Loss: 1.418916 \tValidation Loss: 2.511206\n",
      "Epoch: 26592 \tTraining Loss: 1.402455 \tValidation Loss: 2.512583\n",
      "Epoch: 26593 \tTraining Loss: 1.371005 \tValidation Loss: 2.513433\n",
      "Epoch: 26594 \tTraining Loss: 1.400172 \tValidation Loss: 2.512470\n",
      "Epoch: 26595 \tTraining Loss: 1.356297 \tValidation Loss: 2.512667\n",
      "Epoch: 26596 \tTraining Loss: 1.399515 \tValidation Loss: 2.512981\n",
      "Epoch: 26597 \tTraining Loss: 1.381086 \tValidation Loss: 2.513809\n",
      "Epoch: 26598 \tTraining Loss: 1.378640 \tValidation Loss: 2.513872\n",
      "Epoch: 26599 \tTraining Loss: 1.405230 \tValidation Loss: 2.512688\n",
      "Epoch: 26600 \tTraining Loss: 1.352494 \tValidation Loss: 2.513321\n",
      "Epoch: 26601 \tTraining Loss: 1.369588 \tValidation Loss: 2.513420\n",
      "Epoch: 26602 \tTraining Loss: 1.461754 \tValidation Loss: 2.511544\n",
      "Epoch: 26603 \tTraining Loss: 1.380172 \tValidation Loss: 2.513175\n",
      "Epoch: 26604 \tTraining Loss: 1.363843 \tValidation Loss: 2.513591\n",
      "Epoch: 26605 \tTraining Loss: 1.363024 \tValidation Loss: 2.513024\n",
      "Epoch: 26606 \tTraining Loss: 1.362748 \tValidation Loss: 2.513438\n",
      "Epoch: 26607 \tTraining Loss: 1.392516 \tValidation Loss: 2.512451\n",
      "Epoch: 26608 \tTraining Loss: 1.366777 \tValidation Loss: 2.512615\n",
      "Epoch: 26609 \tTraining Loss: 1.351970 \tValidation Loss: 2.512646\n",
      "Epoch: 26610 \tTraining Loss: 1.393312 \tValidation Loss: 2.513271\n",
      "Epoch: 26611 \tTraining Loss: 1.418856 \tValidation Loss: 2.512572\n",
      "Epoch: 26612 \tTraining Loss: 1.416315 \tValidation Loss: 2.511982\n",
      "Epoch: 26613 \tTraining Loss: 1.356221 \tValidation Loss: 2.513350\n",
      "Epoch: 26614 \tTraining Loss: 1.406528 \tValidation Loss: 2.512459\n",
      "Epoch: 26615 \tTraining Loss: 1.357876 \tValidation Loss: 2.514436\n",
      "Epoch: 26616 \tTraining Loss: 1.363602 \tValidation Loss: 2.513063\n",
      "Epoch: 26617 \tTraining Loss: 1.356737 \tValidation Loss: 2.512944\n",
      "Epoch: 26618 \tTraining Loss: 1.412723 \tValidation Loss: 2.513122\n",
      "Epoch: 26619 \tTraining Loss: 1.413537 \tValidation Loss: 2.513380\n",
      "Epoch: 26620 \tTraining Loss: 1.343444 \tValidation Loss: 2.514736\n",
      "Epoch: 26621 \tTraining Loss: 1.400316 \tValidation Loss: 2.513834\n",
      "Epoch: 26622 \tTraining Loss: 1.375601 \tValidation Loss: 2.513557\n",
      "Epoch: 26623 \tTraining Loss: 1.400889 \tValidation Loss: 2.513103\n",
      "Epoch: 26624 \tTraining Loss: 1.378564 \tValidation Loss: 2.512736\n",
      "Epoch: 26625 \tTraining Loss: 1.412383 \tValidation Loss: 2.513006\n",
      "Epoch: 26626 \tTraining Loss: 1.401771 \tValidation Loss: 2.512728\n",
      "Epoch: 26627 \tTraining Loss: 1.408688 \tValidation Loss: 2.511926\n",
      "Epoch: 26628 \tTraining Loss: 1.397411 \tValidation Loss: 2.513480\n",
      "Epoch: 26629 \tTraining Loss: 1.406399 \tValidation Loss: 2.513646\n",
      "Epoch: 26630 \tTraining Loss: 1.355178 \tValidation Loss: 2.513188\n",
      "Epoch: 26631 \tTraining Loss: 1.380246 \tValidation Loss: 2.513514\n",
      "Epoch: 26632 \tTraining Loss: 1.407437 \tValidation Loss: 2.512843\n",
      "Epoch: 26633 \tTraining Loss: 1.394696 \tValidation Loss: 2.512310\n",
      "Epoch: 26634 \tTraining Loss: 1.358836 \tValidation Loss: 2.513162\n",
      "Epoch: 26635 \tTraining Loss: 1.352835 \tValidation Loss: 2.513418\n",
      "Epoch: 26636 \tTraining Loss: 1.395421 \tValidation Loss: 2.513797\n",
      "Epoch: 26637 \tTraining Loss: 1.407687 \tValidation Loss: 2.512715\n",
      "Epoch: 26638 \tTraining Loss: 1.387068 \tValidation Loss: 2.513525\n",
      "Epoch: 26639 \tTraining Loss: 1.390446 \tValidation Loss: 2.513035\n",
      "Epoch: 26640 \tTraining Loss: 1.391712 \tValidation Loss: 2.513907\n",
      "Epoch: 26641 \tTraining Loss: 1.355523 \tValidation Loss: 2.514132\n",
      "Epoch: 26642 \tTraining Loss: 1.378075 \tValidation Loss: 2.513743\n",
      "Epoch: 26643 \tTraining Loss: 1.432823 \tValidation Loss: 2.513184\n",
      "Epoch: 26644 \tTraining Loss: 1.394192 \tValidation Loss: 2.513821\n",
      "Epoch: 26645 \tTraining Loss: 1.392080 \tValidation Loss: 2.512883\n",
      "Epoch: 26646 \tTraining Loss: 1.410347 \tValidation Loss: 2.512871\n",
      "Epoch: 26647 \tTraining Loss: 1.348689 \tValidation Loss: 2.514385\n",
      "Epoch: 26648 \tTraining Loss: 1.339561 \tValidation Loss: 2.514347\n",
      "Epoch: 26649 \tTraining Loss: 1.337466 \tValidation Loss: 2.513530\n",
      "Epoch: 26650 \tTraining Loss: 1.397358 \tValidation Loss: 2.514503\n",
      "Epoch: 26651 \tTraining Loss: 1.389228 \tValidation Loss: 2.514123\n",
      "Epoch: 26652 \tTraining Loss: 1.461706 \tValidation Loss: 2.513645\n",
      "Epoch: 26653 \tTraining Loss: 1.320354 \tValidation Loss: 2.514583\n",
      "Epoch: 26654 \tTraining Loss: 1.392732 \tValidation Loss: 2.514031\n",
      "Epoch: 26655 \tTraining Loss: 1.395651 \tValidation Loss: 2.513701\n",
      "Epoch: 26656 \tTraining Loss: 1.371961 \tValidation Loss: 2.514262\n",
      "Epoch: 26657 \tTraining Loss: 1.356057 \tValidation Loss: 2.513384\n",
      "Epoch: 26658 \tTraining Loss: 1.341093 \tValidation Loss: 2.513795\n",
      "Epoch: 26659 \tTraining Loss: 1.354636 \tValidation Loss: 2.512971\n",
      "Epoch: 26660 \tTraining Loss: 1.362215 \tValidation Loss: 2.512706\n",
      "Epoch: 26661 \tTraining Loss: 1.396076 \tValidation Loss: 2.512829\n",
      "Epoch: 26662 \tTraining Loss: 1.368004 \tValidation Loss: 2.513783\n",
      "Epoch: 26663 \tTraining Loss: 1.367006 \tValidation Loss: 2.513643\n",
      "Epoch: 26664 \tTraining Loss: 1.408271 \tValidation Loss: 2.514659\n",
      "Epoch: 26665 \tTraining Loss: 1.412563 \tValidation Loss: 2.513609\n",
      "Epoch: 26666 \tTraining Loss: 1.389658 \tValidation Loss: 2.514937\n",
      "Epoch: 26667 \tTraining Loss: 1.392313 \tValidation Loss: 2.515218\n",
      "Epoch: 26668 \tTraining Loss: 1.334165 \tValidation Loss: 2.515354\n",
      "Epoch: 26669 \tTraining Loss: 1.358634 \tValidation Loss: 2.515331\n",
      "Epoch: 26670 \tTraining Loss: 1.384854 \tValidation Loss: 2.514235\n",
      "Epoch: 26671 \tTraining Loss: 1.403731 \tValidation Loss: 2.513408\n",
      "Epoch: 26672 \tTraining Loss: 1.330164 \tValidation Loss: 2.515507\n",
      "Epoch: 26673 \tTraining Loss: 1.381076 \tValidation Loss: 2.514300\n",
      "Epoch: 26674 \tTraining Loss: 1.359587 \tValidation Loss: 2.514603\n",
      "Epoch: 26675 \tTraining Loss: 1.344499 \tValidation Loss: 2.514951\n",
      "Epoch: 26676 \tTraining Loss: 1.372565 \tValidation Loss: 2.514800\n",
      "Epoch: 26677 \tTraining Loss: 1.429405 \tValidation Loss: 2.514142\n",
      "Epoch: 26678 \tTraining Loss: 1.405726 \tValidation Loss: 2.513382\n",
      "Epoch: 26679 \tTraining Loss: 1.402808 \tValidation Loss: 2.513774\n",
      "Epoch: 26680 \tTraining Loss: 1.419974 \tValidation Loss: 2.513150\n",
      "Epoch: 26681 \tTraining Loss: 1.419563 \tValidation Loss: 2.513346\n",
      "Epoch: 26682 \tTraining Loss: 1.390440 \tValidation Loss: 2.514017\n",
      "Epoch: 26683 \tTraining Loss: 1.389758 \tValidation Loss: 2.512900\n",
      "Epoch: 26684 \tTraining Loss: 1.394725 \tValidation Loss: 2.513156\n",
      "Epoch: 26685 \tTraining Loss: 1.397825 \tValidation Loss: 2.513994\n",
      "Epoch: 26686 \tTraining Loss: 1.382415 \tValidation Loss: 2.513911\n",
      "Epoch: 26687 \tTraining Loss: 1.370677 \tValidation Loss: 2.515346\n",
      "Epoch: 26688 \tTraining Loss: 1.384319 \tValidation Loss: 2.514688\n",
      "Epoch: 26689 \tTraining Loss: 1.359721 \tValidation Loss: 2.515488\n",
      "Epoch: 26690 \tTraining Loss: 1.423639 \tValidation Loss: 2.513806\n",
      "Epoch: 26691 \tTraining Loss: 1.402053 \tValidation Loss: 2.513740\n",
      "Epoch: 26692 \tTraining Loss: 1.378813 \tValidation Loss: 2.513737\n",
      "Epoch: 26693 \tTraining Loss: 1.375437 \tValidation Loss: 2.513572\n",
      "Epoch: 26694 \tTraining Loss: 1.366243 \tValidation Loss: 2.514284\n",
      "Epoch: 26695 \tTraining Loss: 1.395032 \tValidation Loss: 2.513921\n",
      "Epoch: 26696 \tTraining Loss: 1.361626 \tValidation Loss: 2.514447\n",
      "Epoch: 26697 \tTraining Loss: 1.353538 \tValidation Loss: 2.515227\n",
      "Epoch: 26698 \tTraining Loss: 1.350141 \tValidation Loss: 2.515247\n",
      "Epoch: 26699 \tTraining Loss: 1.410097 \tValidation Loss: 2.514124\n",
      "Epoch: 26700 \tTraining Loss: 1.413597 \tValidation Loss: 2.514786\n",
      "Epoch: 26701 \tTraining Loss: 1.394745 \tValidation Loss: 2.514471\n",
      "Epoch: 26702 \tTraining Loss: 1.385283 \tValidation Loss: 2.514893\n",
      "Epoch: 26703 \tTraining Loss: 1.398937 \tValidation Loss: 2.513418\n",
      "Epoch: 26704 \tTraining Loss: 1.409678 \tValidation Loss: 2.512817\n",
      "Epoch: 26705 \tTraining Loss: 1.385736 \tValidation Loss: 2.514374\n",
      "Epoch: 26706 \tTraining Loss: 1.386952 \tValidation Loss: 2.515036\n",
      "Epoch: 26707 \tTraining Loss: 1.440120 \tValidation Loss: 2.513969\n",
      "Epoch: 26708 \tTraining Loss: 1.402928 \tValidation Loss: 2.514403\n",
      "Epoch: 26709 \tTraining Loss: 1.414365 \tValidation Loss: 2.514421\n",
      "Epoch: 26710 \tTraining Loss: 1.406179 \tValidation Loss: 2.514506\n",
      "Epoch: 26711 \tTraining Loss: 1.373094 \tValidation Loss: 2.515053\n",
      "Epoch: 26712 \tTraining Loss: 1.353682 \tValidation Loss: 2.515264\n",
      "Epoch: 26713 \tTraining Loss: 1.415068 \tValidation Loss: 2.513751\n",
      "Epoch: 26714 \tTraining Loss: 1.408626 \tValidation Loss: 2.515249\n",
      "Epoch: 26715 \tTraining Loss: 1.426396 \tValidation Loss: 2.514117\n",
      "Epoch: 26716 \tTraining Loss: 1.373188 \tValidation Loss: 2.514825\n",
      "Epoch: 26717 \tTraining Loss: 1.386257 \tValidation Loss: 2.514566\n",
      "Epoch: 26718 \tTraining Loss: 1.350037 \tValidation Loss: 2.515573\n",
      "Epoch: 26719 \tTraining Loss: 1.365534 \tValidation Loss: 2.514192\n",
      "Epoch: 26720 \tTraining Loss: 1.348150 \tValidation Loss: 2.515510\n",
      "Epoch: 26721 \tTraining Loss: 1.406325 \tValidation Loss: 2.514716\n",
      "Epoch: 26722 \tTraining Loss: 1.404724 \tValidation Loss: 2.515463\n",
      "Epoch: 26723 \tTraining Loss: 1.362674 \tValidation Loss: 2.515494\n",
      "Epoch: 26724 \tTraining Loss: 1.370335 \tValidation Loss: 2.514682\n",
      "Epoch: 26725 \tTraining Loss: 1.429991 \tValidation Loss: 2.514672\n",
      "Epoch: 26726 \tTraining Loss: 1.355440 \tValidation Loss: 2.514975\n",
      "Epoch: 26727 \tTraining Loss: 1.386381 \tValidation Loss: 2.514641\n",
      "Epoch: 26728 \tTraining Loss: 1.354090 \tValidation Loss: 2.514660\n",
      "Epoch: 26729 \tTraining Loss: 1.405358 \tValidation Loss: 2.514861\n",
      "Epoch: 26730 \tTraining Loss: 1.396999 \tValidation Loss: 2.514139\n",
      "Epoch: 26731 \tTraining Loss: 1.352589 \tValidation Loss: 2.515599\n",
      "Epoch: 26732 \tTraining Loss: 1.396420 \tValidation Loss: 2.515708\n",
      "Epoch: 26733 \tTraining Loss: 1.387953 \tValidation Loss: 2.515847\n",
      "Epoch: 26734 \tTraining Loss: 1.409216 \tValidation Loss: 2.514473\n",
      "Epoch: 26735 \tTraining Loss: 1.421296 \tValidation Loss: 2.515233\n",
      "Epoch: 26736 \tTraining Loss: 1.380007 \tValidation Loss: 2.515167\n",
      "Epoch: 26737 \tTraining Loss: 1.406909 \tValidation Loss: 2.514963\n",
      "Epoch: 26738 \tTraining Loss: 1.349412 \tValidation Loss: 2.515626\n",
      "Epoch: 26739 \tTraining Loss: 1.424495 \tValidation Loss: 2.514580\n",
      "Epoch: 26740 \tTraining Loss: 1.352019 \tValidation Loss: 2.516198\n",
      "Epoch: 26741 \tTraining Loss: 1.386951 \tValidation Loss: 2.517139\n",
      "Epoch: 26742 \tTraining Loss: 1.404108 \tValidation Loss: 2.515820\n",
      "Epoch: 26743 \tTraining Loss: 1.393992 \tValidation Loss: 2.515462\n",
      "Epoch: 26744 \tTraining Loss: 1.376538 \tValidation Loss: 2.516366\n",
      "Epoch: 26745 \tTraining Loss: 1.413831 \tValidation Loss: 2.516024\n",
      "Epoch: 26746 \tTraining Loss: 1.399580 \tValidation Loss: 2.515637\n",
      "Epoch: 26747 \tTraining Loss: 1.387891 \tValidation Loss: 2.515271\n",
      "Epoch: 26748 \tTraining Loss: 1.405102 \tValidation Loss: 2.515238\n",
      "Epoch: 26749 \tTraining Loss: 1.410885 \tValidation Loss: 2.514393\n",
      "Epoch: 26750 \tTraining Loss: 1.381849 \tValidation Loss: 2.515361\n",
      "Epoch: 26751 \tTraining Loss: 1.345115 \tValidation Loss: 2.514887\n",
      "Epoch: 26752 \tTraining Loss: 1.395638 \tValidation Loss: 2.515468\n",
      "Epoch: 26753 \tTraining Loss: 1.361883 \tValidation Loss: 2.516002\n",
      "Epoch: 26754 \tTraining Loss: 1.360161 \tValidation Loss: 2.515768\n",
      "Epoch: 26755 \tTraining Loss: 1.393398 \tValidation Loss: 2.516049\n",
      "Epoch: 26756 \tTraining Loss: 1.424990 \tValidation Loss: 2.515570\n",
      "Epoch: 26757 \tTraining Loss: 1.398466 \tValidation Loss: 2.516875\n",
      "Epoch: 26758 \tTraining Loss: 1.363854 \tValidation Loss: 2.516236\n",
      "Epoch: 26759 \tTraining Loss: 1.383026 \tValidation Loss: 2.515731\n",
      "Epoch: 26760 \tTraining Loss: 1.387738 \tValidation Loss: 2.515007\n",
      "Epoch: 26761 \tTraining Loss: 1.370090 \tValidation Loss: 2.516078\n",
      "Epoch: 26762 \tTraining Loss: 1.410862 \tValidation Loss: 2.514943\n",
      "Epoch: 26763 \tTraining Loss: 1.435887 \tValidation Loss: 2.514770\n",
      "Epoch: 26764 \tTraining Loss: 1.438599 \tValidation Loss: 2.514692\n",
      "Epoch: 26765 \tTraining Loss: 1.382416 \tValidation Loss: 2.515429\n",
      "Epoch: 26766 \tTraining Loss: 1.368546 \tValidation Loss: 2.516424\n",
      "Epoch: 26767 \tTraining Loss: 1.383864 \tValidation Loss: 2.516337\n",
      "Epoch: 26768 \tTraining Loss: 1.361521 \tValidation Loss: 2.516677\n",
      "Epoch: 26769 \tTraining Loss: 1.376001 \tValidation Loss: 2.516441\n",
      "Epoch: 26770 \tTraining Loss: 1.359851 \tValidation Loss: 2.515985\n",
      "Epoch: 26771 \tTraining Loss: 1.396896 \tValidation Loss: 2.516862\n",
      "Epoch: 26772 \tTraining Loss: 1.395744 \tValidation Loss: 2.515183\n",
      "Epoch: 26773 \tTraining Loss: 1.360980 \tValidation Loss: 2.515826\n",
      "Epoch: 26774 \tTraining Loss: 1.394538 \tValidation Loss: 2.514919\n",
      "Epoch: 26775 \tTraining Loss: 1.404356 \tValidation Loss: 2.514582\n",
      "Epoch: 26776 \tTraining Loss: 1.396218 \tValidation Loss: 2.514764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26777 \tTraining Loss: 1.404135 \tValidation Loss: 2.514688\n",
      "Epoch: 26778 \tTraining Loss: 1.371960 \tValidation Loss: 2.515083\n",
      "Epoch: 26779 \tTraining Loss: 1.368278 \tValidation Loss: 2.515426\n",
      "Epoch: 26780 \tTraining Loss: 1.394211 \tValidation Loss: 2.515551\n",
      "Epoch: 26781 \tTraining Loss: 1.391377 \tValidation Loss: 2.516224\n",
      "Epoch: 26782 \tTraining Loss: 1.374190 \tValidation Loss: 2.515765\n",
      "Epoch: 26783 \tTraining Loss: 1.350119 \tValidation Loss: 2.516337\n",
      "Epoch: 26784 \tTraining Loss: 1.403556 \tValidation Loss: 2.514876\n",
      "Epoch: 26785 \tTraining Loss: 1.379893 \tValidation Loss: 2.516538\n",
      "Epoch: 26786 \tTraining Loss: 1.397551 \tValidation Loss: 2.516605\n",
      "Epoch: 26787 \tTraining Loss: 1.375919 \tValidation Loss: 2.517292\n",
      "Epoch: 26788 \tTraining Loss: 1.437641 \tValidation Loss: 2.515049\n",
      "Epoch: 26789 \tTraining Loss: 1.392218 \tValidation Loss: 2.515618\n",
      "Epoch: 26790 \tTraining Loss: 1.380686 \tValidation Loss: 2.515619\n",
      "Epoch: 26791 \tTraining Loss: 1.380619 \tValidation Loss: 2.516813\n",
      "Epoch: 26792 \tTraining Loss: 1.342066 \tValidation Loss: 2.517184\n",
      "Epoch: 26793 \tTraining Loss: 1.392886 \tValidation Loss: 2.516030\n",
      "Epoch: 26794 \tTraining Loss: 1.380010 \tValidation Loss: 2.515429\n",
      "Epoch: 26795 \tTraining Loss: 1.382346 \tValidation Loss: 2.515857\n",
      "Epoch: 26796 \tTraining Loss: 1.373563 \tValidation Loss: 2.516382\n",
      "Epoch: 26797 \tTraining Loss: 1.334226 \tValidation Loss: 2.516700\n",
      "Epoch: 26798 \tTraining Loss: 1.399391 \tValidation Loss: 2.515750\n",
      "Epoch: 26799 \tTraining Loss: 1.409876 \tValidation Loss: 2.516610\n",
      "Epoch: 26800 \tTraining Loss: 1.429103 \tValidation Loss: 2.515883\n",
      "Epoch: 26801 \tTraining Loss: 1.377329 \tValidation Loss: 2.516359\n",
      "Epoch: 26802 \tTraining Loss: 1.375888 \tValidation Loss: 2.517189\n",
      "Epoch: 26803 \tTraining Loss: 1.398109 \tValidation Loss: 2.516361\n",
      "Epoch: 26804 \tTraining Loss: 1.404926 \tValidation Loss: 2.516932\n",
      "Epoch: 26805 \tTraining Loss: 1.379551 \tValidation Loss: 2.517850\n",
      "Epoch: 26806 \tTraining Loss: 1.385631 \tValidation Loss: 2.517187\n",
      "Epoch: 26807 \tTraining Loss: 1.397991 \tValidation Loss: 2.516226\n",
      "Epoch: 26808 \tTraining Loss: 1.335807 \tValidation Loss: 2.517129\n",
      "Epoch: 26809 \tTraining Loss: 1.393027 \tValidation Loss: 2.515154\n",
      "Epoch: 26810 \tTraining Loss: 1.372570 \tValidation Loss: 2.515514\n",
      "Epoch: 26811 \tTraining Loss: 1.364924 \tValidation Loss: 2.516632\n",
      "Epoch: 26812 \tTraining Loss: 1.391293 \tValidation Loss: 2.516324\n",
      "Epoch: 26813 \tTraining Loss: 1.369467 \tValidation Loss: 2.515596\n",
      "Epoch: 26814 \tTraining Loss: 1.354256 \tValidation Loss: 2.516309\n",
      "Epoch: 26815 \tTraining Loss: 1.357805 \tValidation Loss: 2.516815\n",
      "Epoch: 26816 \tTraining Loss: 1.392969 \tValidation Loss: 2.516968\n",
      "Epoch: 26817 \tTraining Loss: 1.444109 \tValidation Loss: 2.515961\n",
      "Epoch: 26818 \tTraining Loss: 1.370731 \tValidation Loss: 2.516772\n",
      "Epoch: 26819 \tTraining Loss: 1.378687 \tValidation Loss: 2.517746\n",
      "Epoch: 26820 \tTraining Loss: 1.358032 \tValidation Loss: 2.517219\n",
      "Epoch: 26821 \tTraining Loss: 1.349658 \tValidation Loss: 2.517183\n",
      "Epoch: 26822 \tTraining Loss: 1.423077 \tValidation Loss: 2.516212\n",
      "Epoch: 26823 \tTraining Loss: 1.362513 \tValidation Loss: 2.516217\n",
      "Epoch: 26824 \tTraining Loss: 1.400142 \tValidation Loss: 2.517202\n",
      "Epoch: 26825 \tTraining Loss: 1.366979 \tValidation Loss: 2.515921\n",
      "Epoch: 26826 \tTraining Loss: 1.395080 \tValidation Loss: 2.516326\n",
      "Epoch: 26827 \tTraining Loss: 1.350020 \tValidation Loss: 2.516819\n",
      "Epoch: 26828 \tTraining Loss: 1.395107 \tValidation Loss: 2.516719\n",
      "Epoch: 26829 \tTraining Loss: 1.382449 \tValidation Loss: 2.517167\n",
      "Epoch: 26830 \tTraining Loss: 1.387920 \tValidation Loss: 2.517245\n",
      "Epoch: 26831 \tTraining Loss: 1.438449 \tValidation Loss: 2.516115\n",
      "Epoch: 26832 \tTraining Loss: 1.383673 \tValidation Loss: 2.516500\n",
      "Epoch: 26833 \tTraining Loss: 1.331982 \tValidation Loss: 2.516348\n",
      "Epoch: 26834 \tTraining Loss: 1.375537 \tValidation Loss: 2.516571\n",
      "Epoch: 26835 \tTraining Loss: 1.349308 \tValidation Loss: 2.516605\n",
      "Epoch: 26836 \tTraining Loss: 1.420089 \tValidation Loss: 2.516855\n",
      "Epoch: 26837 \tTraining Loss: 1.362345 \tValidation Loss: 2.517731\n",
      "Epoch: 26838 \tTraining Loss: 1.368542 \tValidation Loss: 2.516591\n",
      "Epoch: 26839 \tTraining Loss: 1.391883 \tValidation Loss: 2.517470\n",
      "Epoch: 26840 \tTraining Loss: 1.390306 \tValidation Loss: 2.517888\n",
      "Epoch: 26841 \tTraining Loss: 1.353335 \tValidation Loss: 2.517475\n",
      "Epoch: 26842 \tTraining Loss: 1.402423 \tValidation Loss: 2.516967\n",
      "Epoch: 26843 \tTraining Loss: 1.353213 \tValidation Loss: 2.516788\n",
      "Epoch: 26844 \tTraining Loss: 1.386184 \tValidation Loss: 2.516439\n",
      "Epoch: 26845 \tTraining Loss: 1.362546 \tValidation Loss: 2.516071\n",
      "Epoch: 26846 \tTraining Loss: 1.371155 \tValidation Loss: 2.517914\n",
      "Epoch: 26847 \tTraining Loss: 1.369745 \tValidation Loss: 2.518072\n",
      "Epoch: 26848 \tTraining Loss: 1.384845 \tValidation Loss: 2.517268\n",
      "Epoch: 26849 \tTraining Loss: 1.382807 \tValidation Loss: 2.517462\n",
      "Epoch: 26850 \tTraining Loss: 1.363727 \tValidation Loss: 2.519019\n",
      "Epoch: 26851 \tTraining Loss: 1.368209 \tValidation Loss: 2.517962\n",
      "Epoch: 26852 \tTraining Loss: 1.367775 \tValidation Loss: 2.518620\n",
      "Epoch: 26853 \tTraining Loss: 1.358850 \tValidation Loss: 2.517948\n",
      "Epoch: 26854 \tTraining Loss: 1.388016 \tValidation Loss: 2.517192\n",
      "Epoch: 26855 \tTraining Loss: 1.360204 \tValidation Loss: 2.516863\n",
      "Epoch: 26856 \tTraining Loss: 1.365596 \tValidation Loss: 2.517221\n",
      "Epoch: 26857 \tTraining Loss: 1.350737 \tValidation Loss: 2.517197\n",
      "Epoch: 26858 \tTraining Loss: 1.397314 \tValidation Loss: 2.517317\n",
      "Epoch: 26859 \tTraining Loss: 1.409746 \tValidation Loss: 2.516641\n",
      "Epoch: 26860 \tTraining Loss: 1.389448 \tValidation Loss: 2.517386\n",
      "Epoch: 26861 \tTraining Loss: 1.380929 \tValidation Loss: 2.517447\n",
      "Epoch: 26862 \tTraining Loss: 1.360533 \tValidation Loss: 2.518858\n",
      "Epoch: 26863 \tTraining Loss: 1.394451 \tValidation Loss: 2.518848\n",
      "Epoch: 26864 \tTraining Loss: 1.364996 \tValidation Loss: 2.517726\n",
      "Epoch: 26865 \tTraining Loss: 1.359406 \tValidation Loss: 2.517826\n",
      "Epoch: 26866 \tTraining Loss: 1.371178 \tValidation Loss: 2.516584\n",
      "Epoch: 26867 \tTraining Loss: 1.344017 \tValidation Loss: 2.517225\n",
      "Epoch: 26868 \tTraining Loss: 1.341570 \tValidation Loss: 2.517482\n",
      "Epoch: 26869 \tTraining Loss: 1.434122 \tValidation Loss: 2.516973\n",
      "Epoch: 26870 \tTraining Loss: 1.391574 \tValidation Loss: 2.516499\n",
      "Epoch: 26871 \tTraining Loss: 1.404119 \tValidation Loss: 2.516860\n",
      "Epoch: 26872 \tTraining Loss: 1.404315 \tValidation Loss: 2.516357\n",
      "Epoch: 26873 \tTraining Loss: 1.376934 \tValidation Loss: 2.516184\n",
      "Epoch: 26874 \tTraining Loss: 1.369095 \tValidation Loss: 2.517524\n",
      "Epoch: 26875 \tTraining Loss: 1.392678 \tValidation Loss: 2.515844\n",
      "Epoch: 26876 \tTraining Loss: 1.375069 \tValidation Loss: 2.518040\n",
      "Epoch: 26877 \tTraining Loss: 1.395806 \tValidation Loss: 2.517182\n",
      "Epoch: 26878 \tTraining Loss: 1.395681 \tValidation Loss: 2.517258\n",
      "Epoch: 26879 \tTraining Loss: 1.376081 \tValidation Loss: 2.516738\n",
      "Epoch: 26880 \tTraining Loss: 1.412768 \tValidation Loss: 2.517735\n",
      "Epoch: 26881 \tTraining Loss: 1.346895 \tValidation Loss: 2.518240\n",
      "Epoch: 26882 \tTraining Loss: 1.342404 \tValidation Loss: 2.518123\n",
      "Epoch: 26883 \tTraining Loss: 1.404609 \tValidation Loss: 2.517742\n",
      "Epoch: 26884 \tTraining Loss: 1.404441 \tValidation Loss: 2.518531\n",
      "Epoch: 26885 \tTraining Loss: 1.438020 \tValidation Loss: 2.517640\n",
      "Epoch: 26886 \tTraining Loss: 1.412708 \tValidation Loss: 2.516411\n",
      "Epoch: 26887 \tTraining Loss: 1.374656 \tValidation Loss: 2.517531\n",
      "Epoch: 26888 \tTraining Loss: 1.377389 \tValidation Loss: 2.518183\n",
      "Epoch: 26889 \tTraining Loss: 1.392370 \tValidation Loss: 2.516649\n",
      "Epoch: 26890 \tTraining Loss: 1.364722 \tValidation Loss: 2.518096\n",
      "Epoch: 26891 \tTraining Loss: 1.414511 \tValidation Loss: 2.518535\n",
      "Epoch: 26892 \tTraining Loss: 1.354641 \tValidation Loss: 2.518793\n",
      "Epoch: 26893 \tTraining Loss: 1.384649 \tValidation Loss: 2.518180\n",
      "Epoch: 26894 \tTraining Loss: 1.387679 \tValidation Loss: 2.517715\n",
      "Epoch: 26895 \tTraining Loss: 1.362197 \tValidation Loss: 2.518347\n",
      "Epoch: 26896 \tTraining Loss: 1.357661 \tValidation Loss: 2.518672\n",
      "Epoch: 26897 \tTraining Loss: 1.358504 \tValidation Loss: 2.518687\n",
      "Epoch: 26898 \tTraining Loss: 1.394174 \tValidation Loss: 2.518664\n",
      "Epoch: 26899 \tTraining Loss: 1.404796 \tValidation Loss: 2.518057\n",
      "Epoch: 26900 \tTraining Loss: 1.352285 \tValidation Loss: 2.519021\n",
      "Epoch: 26901 \tTraining Loss: 1.376669 \tValidation Loss: 2.518629\n",
      "Epoch: 26902 \tTraining Loss: 1.377040 \tValidation Loss: 2.518071\n",
      "Epoch: 26903 \tTraining Loss: 1.398742 \tValidation Loss: 2.518635\n",
      "Epoch: 26904 \tTraining Loss: 1.395693 \tValidation Loss: 2.517752\n",
      "Epoch: 26905 \tTraining Loss: 1.399390 \tValidation Loss: 2.518398\n",
      "Epoch: 26906 \tTraining Loss: 1.346489 \tValidation Loss: 2.518584\n",
      "Epoch: 26907 \tTraining Loss: 1.423630 \tValidation Loss: 2.518245\n",
      "Epoch: 26908 \tTraining Loss: 1.356206 \tValidation Loss: 2.518954\n",
      "Epoch: 26909 \tTraining Loss: 1.359915 \tValidation Loss: 2.518432\n",
      "Epoch: 26910 \tTraining Loss: 1.374835 \tValidation Loss: 2.519387\n",
      "Epoch: 26911 \tTraining Loss: 1.348033 \tValidation Loss: 2.518536\n",
      "Epoch: 26912 \tTraining Loss: 1.352743 \tValidation Loss: 2.519085\n",
      "Epoch: 26913 \tTraining Loss: 1.378560 \tValidation Loss: 2.519326\n",
      "Epoch: 26914 \tTraining Loss: 1.360822 \tValidation Loss: 2.518706\n",
      "Epoch: 26915 \tTraining Loss: 1.371258 \tValidation Loss: 2.518229\n",
      "Epoch: 26916 \tTraining Loss: 1.321709 \tValidation Loss: 2.518942\n",
      "Epoch: 26917 \tTraining Loss: 1.403925 \tValidation Loss: 2.518703\n",
      "Epoch: 26918 \tTraining Loss: 1.399199 \tValidation Loss: 2.517415\n",
      "Epoch: 26919 \tTraining Loss: 1.361357 \tValidation Loss: 2.518059\n",
      "Epoch: 26920 \tTraining Loss: 1.341890 \tValidation Loss: 2.518378\n",
      "Epoch: 26921 \tTraining Loss: 1.375265 \tValidation Loss: 2.518161\n",
      "Epoch: 26922 \tTraining Loss: 1.379256 \tValidation Loss: 2.518672\n",
      "Epoch: 26923 \tTraining Loss: 1.372114 \tValidation Loss: 2.518375\n",
      "Epoch: 26924 \tTraining Loss: 1.340565 \tValidation Loss: 2.518408\n",
      "Epoch: 26925 \tTraining Loss: 1.375960 \tValidation Loss: 2.518779\n",
      "Epoch: 26926 \tTraining Loss: 1.393700 \tValidation Loss: 2.519025\n",
      "Epoch: 26927 \tTraining Loss: 1.349307 \tValidation Loss: 2.519247\n",
      "Epoch: 26928 \tTraining Loss: 1.391450 \tValidation Loss: 2.518713\n",
      "Epoch: 26929 \tTraining Loss: 1.382895 \tValidation Loss: 2.519001\n",
      "Epoch: 26930 \tTraining Loss: 1.404703 \tValidation Loss: 2.518983\n",
      "Epoch: 26931 \tTraining Loss: 1.405148 \tValidation Loss: 2.519031\n",
      "Epoch: 26932 \tTraining Loss: 1.404347 \tValidation Loss: 2.518804\n",
      "Epoch: 26933 \tTraining Loss: 1.396459 \tValidation Loss: 2.519987\n",
      "Epoch: 26934 \tTraining Loss: 1.360488 \tValidation Loss: 2.518653\n",
      "Epoch: 26935 \tTraining Loss: 1.346396 \tValidation Loss: 2.519882\n",
      "Epoch: 26936 \tTraining Loss: 1.369597 \tValidation Loss: 2.518890\n",
      "Epoch: 26937 \tTraining Loss: 1.387717 \tValidation Loss: 2.519204\n",
      "Epoch: 26938 \tTraining Loss: 1.391513 \tValidation Loss: 2.518571\n",
      "Epoch: 26939 \tTraining Loss: 1.359182 \tValidation Loss: 2.519252\n",
      "Epoch: 26940 \tTraining Loss: 1.387715 \tValidation Loss: 2.518922\n",
      "Epoch: 26941 \tTraining Loss: 1.372784 \tValidation Loss: 2.519088\n",
      "Epoch: 26942 \tTraining Loss: 1.404107 \tValidation Loss: 2.518159\n",
      "Epoch: 26943 \tTraining Loss: 1.393638 \tValidation Loss: 2.518620\n",
      "Epoch: 26944 \tTraining Loss: 1.381742 \tValidation Loss: 2.519830\n",
      "Epoch: 26945 \tTraining Loss: 1.370715 \tValidation Loss: 2.519447\n",
      "Epoch: 26946 \tTraining Loss: 1.358715 \tValidation Loss: 2.520230\n",
      "Epoch: 26947 \tTraining Loss: 1.409165 \tValidation Loss: 2.519788\n",
      "Epoch: 26948 \tTraining Loss: 1.364735 \tValidation Loss: 2.519527\n",
      "Epoch: 26949 \tTraining Loss: 1.400900 \tValidation Loss: 2.519073\n",
      "Epoch: 26950 \tTraining Loss: 1.356170 \tValidation Loss: 2.518932\n",
      "Epoch: 26951 \tTraining Loss: 1.396897 \tValidation Loss: 2.518805\n",
      "Epoch: 26952 \tTraining Loss: 1.372505 \tValidation Loss: 2.518723\n",
      "Epoch: 26953 \tTraining Loss: 1.387040 \tValidation Loss: 2.519777\n",
      "Epoch: 26954 \tTraining Loss: 1.359079 \tValidation Loss: 2.521279\n",
      "Epoch: 26955 \tTraining Loss: 1.373250 \tValidation Loss: 2.520823\n",
      "Epoch: 26956 \tTraining Loss: 1.396806 \tValidation Loss: 2.519985\n",
      "Epoch: 26957 \tTraining Loss: 1.412752 \tValidation Loss: 2.519214\n",
      "Epoch: 26958 \tTraining Loss: 1.404014 \tValidation Loss: 2.519060\n",
      "Epoch: 26959 \tTraining Loss: 1.416870 \tValidation Loss: 2.519757\n",
      "Epoch: 26960 \tTraining Loss: 1.317898 \tValidation Loss: 2.520441\n",
      "Epoch: 26961 \tTraining Loss: 1.345969 \tValidation Loss: 2.520203\n",
      "Epoch: 26962 \tTraining Loss: 1.326416 \tValidation Loss: 2.520360\n",
      "Epoch: 26963 \tTraining Loss: 1.388052 \tValidation Loss: 2.519938\n",
      "Epoch: 26964 \tTraining Loss: 1.362302 \tValidation Loss: 2.519780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26965 \tTraining Loss: 1.388555 \tValidation Loss: 2.519638\n",
      "Epoch: 26966 \tTraining Loss: 1.371984 \tValidation Loss: 2.519853\n",
      "Epoch: 26967 \tTraining Loss: 1.408371 \tValidation Loss: 2.520199\n",
      "Epoch: 26968 \tTraining Loss: 1.372852 \tValidation Loss: 2.519583\n",
      "Epoch: 26969 \tTraining Loss: 1.387319 \tValidation Loss: 2.520600\n",
      "Epoch: 26970 \tTraining Loss: 1.377975 \tValidation Loss: 2.520063\n",
      "Epoch: 26971 \tTraining Loss: 1.347051 \tValidation Loss: 2.520254\n",
      "Epoch: 26972 \tTraining Loss: 1.389166 \tValidation Loss: 2.519999\n",
      "Epoch: 26973 \tTraining Loss: 1.412961 \tValidation Loss: 2.519240\n",
      "Epoch: 26974 \tTraining Loss: 1.392267 \tValidation Loss: 2.519839\n",
      "Epoch: 26975 \tTraining Loss: 1.371917 \tValidation Loss: 2.520255\n",
      "Epoch: 26976 \tTraining Loss: 1.375251 \tValidation Loss: 2.519978\n",
      "Epoch: 26977 \tTraining Loss: 1.364542 \tValidation Loss: 2.519392\n",
      "Epoch: 26978 \tTraining Loss: 1.356083 \tValidation Loss: 2.519104\n",
      "Epoch: 26979 \tTraining Loss: 1.402675 \tValidation Loss: 2.520310\n",
      "Epoch: 26980 \tTraining Loss: 1.394953 \tValidation Loss: 2.518916\n",
      "Epoch: 26981 \tTraining Loss: 1.367466 \tValidation Loss: 2.519377\n",
      "Epoch: 26982 \tTraining Loss: 1.391660 \tValidation Loss: 2.519623\n",
      "Epoch: 26983 \tTraining Loss: 1.371487 \tValidation Loss: 2.519260\n",
      "Epoch: 26984 \tTraining Loss: 1.350996 \tValidation Loss: 2.519892\n",
      "Epoch: 26985 \tTraining Loss: 1.398231 \tValidation Loss: 2.519638\n",
      "Epoch: 26986 \tTraining Loss: 1.366068 \tValidation Loss: 2.519861\n",
      "Epoch: 26987 \tTraining Loss: 1.404305 \tValidation Loss: 2.520246\n",
      "Epoch: 26988 \tTraining Loss: 1.366499 \tValidation Loss: 2.520538\n",
      "Epoch: 26989 \tTraining Loss: 1.359078 \tValidation Loss: 2.519403\n",
      "Epoch: 26990 \tTraining Loss: 1.416797 \tValidation Loss: 2.517856\n",
      "Epoch: 26991 \tTraining Loss: 1.358398 \tValidation Loss: 2.519279\n",
      "Epoch: 26992 \tTraining Loss: 1.376378 \tValidation Loss: 2.520095\n",
      "Epoch: 26993 \tTraining Loss: 1.354781 \tValidation Loss: 2.520136\n",
      "Epoch: 26994 \tTraining Loss: 1.366532 \tValidation Loss: 2.520112\n",
      "Epoch: 26995 \tTraining Loss: 1.365104 \tValidation Loss: 2.520981\n",
      "Epoch: 26996 \tTraining Loss: 1.403044 \tValidation Loss: 2.520717\n",
      "Epoch: 26997 \tTraining Loss: 1.370561 \tValidation Loss: 2.520040\n",
      "Epoch: 26998 \tTraining Loss: 1.380701 \tValidation Loss: 2.519437\n",
      "Epoch: 26999 \tTraining Loss: 1.386909 \tValidation Loss: 2.519679\n",
      "Epoch: 27000 \tTraining Loss: 1.394446 \tValidation Loss: 2.518885\n",
      "Epoch: 27001 \tTraining Loss: 1.340513 \tValidation Loss: 2.520720\n",
      "Epoch: 27002 \tTraining Loss: 1.425213 \tValidation Loss: 2.518937\n",
      "Epoch: 27003 \tTraining Loss: 1.383741 \tValidation Loss: 2.519883\n",
      "Epoch: 27004 \tTraining Loss: 1.388129 \tValidation Loss: 2.519883\n",
      "Epoch: 27005 \tTraining Loss: 1.366637 \tValidation Loss: 2.521300\n",
      "Epoch: 27006 \tTraining Loss: 1.378949 \tValidation Loss: 2.519912\n",
      "Epoch: 27007 \tTraining Loss: 1.386038 \tValidation Loss: 2.519309\n",
      "Epoch: 27008 \tTraining Loss: 1.387436 \tValidation Loss: 2.520034\n",
      "Epoch: 27009 \tTraining Loss: 1.360866 \tValidation Loss: 2.521239\n",
      "Epoch: 27010 \tTraining Loss: 1.376891 \tValidation Loss: 2.520366\n",
      "Epoch: 27011 \tTraining Loss: 1.358451 \tValidation Loss: 2.520794\n",
      "Epoch: 27012 \tTraining Loss: 1.378798 \tValidation Loss: 2.520588\n",
      "Epoch: 27013 \tTraining Loss: 1.393329 \tValidation Loss: 2.519532\n",
      "Epoch: 27014 \tTraining Loss: 1.374345 \tValidation Loss: 2.519887\n",
      "Epoch: 27015 \tTraining Loss: 1.338219 \tValidation Loss: 2.520536\n",
      "Epoch: 27016 \tTraining Loss: 1.358997 \tValidation Loss: 2.519677\n",
      "Epoch: 27017 \tTraining Loss: 1.392474 \tValidation Loss: 2.518711\n",
      "Epoch: 27018 \tTraining Loss: 1.367246 \tValidation Loss: 2.520032\n",
      "Epoch: 27019 \tTraining Loss: 1.387958 \tValidation Loss: 2.520716\n",
      "Epoch: 27020 \tTraining Loss: 1.369440 \tValidation Loss: 2.520110\n",
      "Epoch: 27021 \tTraining Loss: 1.381999 \tValidation Loss: 2.520047\n",
      "Epoch: 27022 \tTraining Loss: 1.357238 \tValidation Loss: 2.521280\n",
      "Epoch: 27023 \tTraining Loss: 1.340099 \tValidation Loss: 2.520553\n",
      "Epoch: 27024 \tTraining Loss: 1.364835 \tValidation Loss: 2.521111\n",
      "Epoch: 27025 \tTraining Loss: 1.322040 \tValidation Loss: 2.521374\n",
      "Epoch: 27026 \tTraining Loss: 1.398530 \tValidation Loss: 2.521080\n",
      "Epoch: 27027 \tTraining Loss: 1.378178 \tValidation Loss: 2.521155\n",
      "Epoch: 27028 \tTraining Loss: 1.366358 \tValidation Loss: 2.519629\n",
      "Epoch: 27029 \tTraining Loss: 1.357646 \tValidation Loss: 2.520329\n",
      "Epoch: 27030 \tTraining Loss: 1.369202 \tValidation Loss: 2.520546\n",
      "Epoch: 27031 \tTraining Loss: 1.420899 \tValidation Loss: 2.520285\n",
      "Epoch: 27032 \tTraining Loss: 1.357998 \tValidation Loss: 2.520716\n",
      "Epoch: 27033 \tTraining Loss: 1.366652 \tValidation Loss: 2.521371\n",
      "Epoch: 27034 \tTraining Loss: 1.376628 \tValidation Loss: 2.521338\n",
      "Epoch: 27035 \tTraining Loss: 1.428379 \tValidation Loss: 2.520100\n",
      "Epoch: 27036 \tTraining Loss: 1.401492 \tValidation Loss: 2.520711\n",
      "Epoch: 27037 \tTraining Loss: 1.368570 \tValidation Loss: 2.521087\n",
      "Epoch: 27038 \tTraining Loss: 1.345376 \tValidation Loss: 2.520353\n",
      "Epoch: 27039 \tTraining Loss: 1.389764 \tValidation Loss: 2.521480\n",
      "Epoch: 27040 \tTraining Loss: 1.391944 \tValidation Loss: 2.521240\n",
      "Epoch: 27041 \tTraining Loss: 1.394859 \tValidation Loss: 2.520681\n",
      "Epoch: 27042 \tTraining Loss: 1.390057 \tValidation Loss: 2.521315\n",
      "Epoch: 27043 \tTraining Loss: 1.401043 \tValidation Loss: 2.521281\n",
      "Epoch: 27044 \tTraining Loss: 1.384392 \tValidation Loss: 2.521052\n",
      "Epoch: 27045 \tTraining Loss: 1.379919 \tValidation Loss: 2.520764\n",
      "Epoch: 27046 \tTraining Loss: 1.332260 \tValidation Loss: 2.521467\n",
      "Epoch: 27047 \tTraining Loss: 1.342095 \tValidation Loss: 2.520985\n",
      "Epoch: 27048 \tTraining Loss: 1.350364 \tValidation Loss: 2.521951\n",
      "Epoch: 27049 \tTraining Loss: 1.357068 \tValidation Loss: 2.521769\n",
      "Epoch: 27050 \tTraining Loss: 1.351748 \tValidation Loss: 2.521081\n",
      "Epoch: 27051 \tTraining Loss: 1.367797 \tValidation Loss: 2.520671\n",
      "Epoch: 27052 \tTraining Loss: 1.365722 \tValidation Loss: 2.522247\n",
      "Epoch: 27053 \tTraining Loss: 1.370256 \tValidation Loss: 2.521711\n",
      "Epoch: 27054 \tTraining Loss: 1.393229 \tValidation Loss: 2.521960\n",
      "Epoch: 27055 \tTraining Loss: 1.369259 \tValidation Loss: 2.523110\n",
      "Epoch: 27056 \tTraining Loss: 1.355411 \tValidation Loss: 2.521416\n",
      "Epoch: 27057 \tTraining Loss: 1.395620 \tValidation Loss: 2.522028\n",
      "Epoch: 27058 \tTraining Loss: 1.406235 \tValidation Loss: 2.521122\n",
      "Epoch: 27059 \tTraining Loss: 1.419183 \tValidation Loss: 2.522041\n",
      "Epoch: 27060 \tTraining Loss: 1.375250 \tValidation Loss: 2.521130\n",
      "Epoch: 27061 \tTraining Loss: 1.380560 \tValidation Loss: 2.521524\n",
      "Epoch: 27062 \tTraining Loss: 1.388353 \tValidation Loss: 2.521186\n",
      "Epoch: 27063 \tTraining Loss: 1.370245 \tValidation Loss: 2.520518\n",
      "Epoch: 27064 \tTraining Loss: 1.398799 \tValidation Loss: 2.521270\n",
      "Epoch: 27065 \tTraining Loss: 1.365442 \tValidation Loss: 2.522130\n",
      "Epoch: 27066 \tTraining Loss: 1.365414 \tValidation Loss: 2.522236\n",
      "Epoch: 27067 \tTraining Loss: 1.375940 \tValidation Loss: 2.521126\n",
      "Epoch: 27068 \tTraining Loss: 1.379806 \tValidation Loss: 2.521910\n",
      "Epoch: 27069 \tTraining Loss: 1.335145 \tValidation Loss: 2.521703\n",
      "Epoch: 27070 \tTraining Loss: 1.384689 \tValidation Loss: 2.521808\n",
      "Epoch: 27071 \tTraining Loss: 1.362088 \tValidation Loss: 2.521864\n",
      "Epoch: 27072 \tTraining Loss: 1.360860 \tValidation Loss: 2.522932\n",
      "Epoch: 27073 \tTraining Loss: 1.384462 \tValidation Loss: 2.522525\n",
      "Epoch: 27074 \tTraining Loss: 1.370075 \tValidation Loss: 2.522713\n",
      "Epoch: 27075 \tTraining Loss: 1.405249 \tValidation Loss: 2.521864\n",
      "Epoch: 27076 \tTraining Loss: 1.364461 \tValidation Loss: 2.521913\n",
      "Epoch: 27077 \tTraining Loss: 1.399745 \tValidation Loss: 2.521776\n",
      "Epoch: 27078 \tTraining Loss: 1.390445 \tValidation Loss: 2.521857\n",
      "Epoch: 27079 \tTraining Loss: 1.409813 \tValidation Loss: 2.521580\n",
      "Epoch: 27080 \tTraining Loss: 1.340059 \tValidation Loss: 2.521569\n",
      "Epoch: 27081 \tTraining Loss: 1.377544 \tValidation Loss: 2.521773\n",
      "Epoch: 27082 \tTraining Loss: 1.362018 \tValidation Loss: 2.521725\n",
      "Epoch: 27083 \tTraining Loss: 1.363633 \tValidation Loss: 2.522365\n",
      "Epoch: 27084 \tTraining Loss: 1.393835 \tValidation Loss: 2.522423\n",
      "Epoch: 27085 \tTraining Loss: 1.363753 \tValidation Loss: 2.522268\n",
      "Epoch: 27086 \tTraining Loss: 1.325231 \tValidation Loss: 2.522455\n",
      "Epoch: 27087 \tTraining Loss: 1.372194 \tValidation Loss: 2.522774\n",
      "Epoch: 27088 \tTraining Loss: 1.365517 \tValidation Loss: 2.521583\n",
      "Epoch: 27089 \tTraining Loss: 1.385389 \tValidation Loss: 2.521749\n",
      "Epoch: 27090 \tTraining Loss: 1.374762 \tValidation Loss: 2.521506\n",
      "Epoch: 27091 \tTraining Loss: 1.377162 \tValidation Loss: 2.521814\n",
      "Epoch: 27092 \tTraining Loss: 1.393665 \tValidation Loss: 2.522124\n",
      "Epoch: 27093 \tTraining Loss: 1.371929 \tValidation Loss: 2.521342\n",
      "Epoch: 27094 \tTraining Loss: 1.415435 \tValidation Loss: 2.520859\n",
      "Epoch: 27095 \tTraining Loss: 1.330747 \tValidation Loss: 2.521429\n",
      "Epoch: 27096 \tTraining Loss: 1.336882 \tValidation Loss: 2.522074\n",
      "Epoch: 27097 \tTraining Loss: 1.333028 \tValidation Loss: 2.521926\n",
      "Epoch: 27098 \tTraining Loss: 1.347047 \tValidation Loss: 2.523356\n",
      "Epoch: 27099 \tTraining Loss: 1.394558 \tValidation Loss: 2.522858\n",
      "Epoch: 27100 \tTraining Loss: 1.352701 \tValidation Loss: 2.522133\n",
      "Epoch: 27101 \tTraining Loss: 1.330675 \tValidation Loss: 2.522460\n",
      "Epoch: 27102 \tTraining Loss: 1.410546 \tValidation Loss: 2.523591\n",
      "Epoch: 27103 \tTraining Loss: 1.351509 \tValidation Loss: 2.521517\n",
      "Epoch: 27104 \tTraining Loss: 1.354560 \tValidation Loss: 2.522101\n",
      "Epoch: 27105 \tTraining Loss: 1.324364 \tValidation Loss: 2.524831\n",
      "Epoch: 27106 \tTraining Loss: 1.383820 \tValidation Loss: 2.523474\n",
      "Epoch: 27107 \tTraining Loss: 1.389717 \tValidation Loss: 2.521939\n",
      "Epoch: 27108 \tTraining Loss: 1.379906 \tValidation Loss: 2.522239\n",
      "Epoch: 27109 \tTraining Loss: 1.335358 \tValidation Loss: 2.523434\n",
      "Epoch: 27110 \tTraining Loss: 1.363033 \tValidation Loss: 2.524004\n",
      "Epoch: 27111 \tTraining Loss: 1.360260 \tValidation Loss: 2.521802\n",
      "Epoch: 27112 \tTraining Loss: 1.337820 \tValidation Loss: 2.523105\n",
      "Epoch: 27113 \tTraining Loss: 1.346008 \tValidation Loss: 2.522799\n",
      "Epoch: 27114 \tTraining Loss: 1.347593 \tValidation Loss: 2.522981\n",
      "Epoch: 27115 \tTraining Loss: 1.335003 \tValidation Loss: 2.522668\n",
      "Epoch: 27116 \tTraining Loss: 1.394762 \tValidation Loss: 2.523495\n",
      "Epoch: 27117 \tTraining Loss: 1.388086 \tValidation Loss: 2.522743\n",
      "Epoch: 27118 \tTraining Loss: 1.379197 \tValidation Loss: 2.522947\n",
      "Epoch: 27119 \tTraining Loss: 1.395309 \tValidation Loss: 2.523151\n",
      "Epoch: 27120 \tTraining Loss: 1.362201 \tValidation Loss: 2.523496\n",
      "Epoch: 27121 \tTraining Loss: 1.345442 \tValidation Loss: 2.523684\n",
      "Epoch: 27122 \tTraining Loss: 1.393003 \tValidation Loss: 2.523502\n",
      "Epoch: 27123 \tTraining Loss: 1.374604 \tValidation Loss: 2.523708\n",
      "Epoch: 27124 \tTraining Loss: 1.366916 \tValidation Loss: 2.523898\n",
      "Epoch: 27125 \tTraining Loss: 1.370841 \tValidation Loss: 2.522185\n",
      "Epoch: 27126 \tTraining Loss: 1.387955 \tValidation Loss: 2.523083\n",
      "Epoch: 27127 \tTraining Loss: 1.412044 \tValidation Loss: 2.521554\n",
      "Epoch: 27128 \tTraining Loss: 1.396947 \tValidation Loss: 2.523081\n",
      "Epoch: 27129 \tTraining Loss: 1.389542 \tValidation Loss: 2.522728\n",
      "Epoch: 27130 \tTraining Loss: 1.339047 \tValidation Loss: 2.524582\n",
      "Epoch: 27131 \tTraining Loss: 1.384771 \tValidation Loss: 2.523613\n",
      "Epoch: 27132 \tTraining Loss: 1.388819 \tValidation Loss: 2.523380\n",
      "Epoch: 27133 \tTraining Loss: 1.370337 \tValidation Loss: 2.523377\n",
      "Epoch: 27134 \tTraining Loss: 1.407318 \tValidation Loss: 2.522868\n",
      "Epoch: 27135 \tTraining Loss: 1.372357 \tValidation Loss: 2.523306\n",
      "Epoch: 27136 \tTraining Loss: 1.415766 \tValidation Loss: 2.522597\n",
      "Epoch: 27137 \tTraining Loss: 1.400604 \tValidation Loss: 2.523533\n",
      "Epoch: 27138 \tTraining Loss: 1.423898 \tValidation Loss: 2.522087\n",
      "Epoch: 27139 \tTraining Loss: 1.343288 \tValidation Loss: 2.523228\n",
      "Epoch: 27140 \tTraining Loss: 1.335032 \tValidation Loss: 2.521894\n",
      "Epoch: 27141 \tTraining Loss: 1.380740 \tValidation Loss: 2.523708\n",
      "Epoch: 27142 \tTraining Loss: 1.366788 \tValidation Loss: 2.522789\n",
      "Epoch: 27143 \tTraining Loss: 1.347830 \tValidation Loss: 2.523630\n",
      "Epoch: 27144 \tTraining Loss: 1.360581 \tValidation Loss: 2.523199\n",
      "Epoch: 27145 \tTraining Loss: 1.358033 \tValidation Loss: 2.523148\n",
      "Epoch: 27146 \tTraining Loss: 1.381677 \tValidation Loss: 2.522590\n",
      "Epoch: 27147 \tTraining Loss: 1.352761 \tValidation Loss: 2.524309\n",
      "Epoch: 27148 \tTraining Loss: 1.381132 \tValidation Loss: 2.523550\n",
      "Epoch: 27149 \tTraining Loss: 1.340591 \tValidation Loss: 2.524695\n",
      "Epoch: 27150 \tTraining Loss: 1.340064 \tValidation Loss: 2.523289\n",
      "Epoch: 27151 \tTraining Loss: 1.363173 \tValidation Loss: 2.524401\n",
      "Epoch: 27152 \tTraining Loss: 1.369051 \tValidation Loss: 2.523583\n",
      "Epoch: 27153 \tTraining Loss: 1.389706 \tValidation Loss: 2.523663\n",
      "Epoch: 27154 \tTraining Loss: 1.359021 \tValidation Loss: 2.524058\n",
      "Epoch: 27155 \tTraining Loss: 1.373295 \tValidation Loss: 2.523615\n",
      "Epoch: 27156 \tTraining Loss: 1.414114 \tValidation Loss: 2.522871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27157 \tTraining Loss: 1.359251 \tValidation Loss: 2.524367\n",
      "Epoch: 27158 \tTraining Loss: 1.373336 \tValidation Loss: 2.523937\n",
      "Epoch: 27159 \tTraining Loss: 1.410897 \tValidation Loss: 2.523063\n",
      "Epoch: 27160 \tTraining Loss: 1.370222 \tValidation Loss: 2.523884\n",
      "Epoch: 27161 \tTraining Loss: 1.371979 \tValidation Loss: 2.523443\n",
      "Epoch: 27162 \tTraining Loss: 1.348880 \tValidation Loss: 2.524075\n",
      "Epoch: 27163 \tTraining Loss: 1.336394 \tValidation Loss: 2.525431\n",
      "Epoch: 27164 \tTraining Loss: 1.349344 \tValidation Loss: 2.523414\n",
      "Epoch: 27165 \tTraining Loss: 1.348048 \tValidation Loss: 2.522951\n",
      "Epoch: 27166 \tTraining Loss: 1.386448 \tValidation Loss: 2.523756\n",
      "Epoch: 27167 \tTraining Loss: 1.380668 \tValidation Loss: 2.524238\n",
      "Epoch: 27168 \tTraining Loss: 1.405223 \tValidation Loss: 2.523314\n",
      "Epoch: 27169 \tTraining Loss: 1.355778 \tValidation Loss: 2.523952\n",
      "Epoch: 27170 \tTraining Loss: 1.358260 \tValidation Loss: 2.524905\n",
      "Epoch: 27171 \tTraining Loss: 1.368096 \tValidation Loss: 2.524806\n",
      "Epoch: 27172 \tTraining Loss: 1.355666 \tValidation Loss: 2.523329\n",
      "Epoch: 27173 \tTraining Loss: 1.374122 \tValidation Loss: 2.523600\n",
      "Epoch: 27174 \tTraining Loss: 1.402126 \tValidation Loss: 2.524481\n",
      "Epoch: 27175 \tTraining Loss: 1.377442 \tValidation Loss: 2.522954\n",
      "Epoch: 27176 \tTraining Loss: 1.372588 \tValidation Loss: 2.523704\n",
      "Epoch: 27177 \tTraining Loss: 1.374709 \tValidation Loss: 2.523773\n",
      "Epoch: 27178 \tTraining Loss: 1.351298 \tValidation Loss: 2.522986\n",
      "Epoch: 27179 \tTraining Loss: 1.368481 \tValidation Loss: 2.523497\n",
      "Epoch: 27180 \tTraining Loss: 1.340724 \tValidation Loss: 2.523730\n",
      "Epoch: 27181 \tTraining Loss: 1.384596 \tValidation Loss: 2.523447\n",
      "Epoch: 27182 \tTraining Loss: 1.399361 \tValidation Loss: 2.523519\n",
      "Epoch: 27183 \tTraining Loss: 1.344146 \tValidation Loss: 2.524455\n",
      "Epoch: 27184 \tTraining Loss: 1.438340 \tValidation Loss: 2.524172\n",
      "Epoch: 27185 \tTraining Loss: 1.387741 \tValidation Loss: 2.524799\n",
      "Epoch: 27186 \tTraining Loss: 1.387629 \tValidation Loss: 2.525115\n",
      "Epoch: 27187 \tTraining Loss: 1.365830 \tValidation Loss: 2.524079\n",
      "Epoch: 27188 \tTraining Loss: 1.359408 \tValidation Loss: 2.524144\n",
      "Epoch: 27189 \tTraining Loss: 1.349201 \tValidation Loss: 2.525273\n",
      "Epoch: 27190 \tTraining Loss: 1.347659 \tValidation Loss: 2.523551\n",
      "Epoch: 27191 \tTraining Loss: 1.379271 \tValidation Loss: 2.524354\n",
      "Epoch: 27192 \tTraining Loss: 1.395854 \tValidation Loss: 2.523229\n",
      "Epoch: 27193 \tTraining Loss: 1.304253 \tValidation Loss: 2.524819\n",
      "Epoch: 27194 \tTraining Loss: 1.376822 \tValidation Loss: 2.523993\n",
      "Epoch: 27195 \tTraining Loss: 1.355682 \tValidation Loss: 2.524168\n",
      "Epoch: 27196 \tTraining Loss: 1.385822 \tValidation Loss: 2.525509\n",
      "Epoch: 27197 \tTraining Loss: 1.366978 \tValidation Loss: 2.524964\n",
      "Epoch: 27198 \tTraining Loss: 1.385539 \tValidation Loss: 2.525268\n",
      "Epoch: 27199 \tTraining Loss: 1.395690 \tValidation Loss: 2.523321\n",
      "Epoch: 27200 \tTraining Loss: 1.370013 \tValidation Loss: 2.524593\n",
      "Epoch: 27201 \tTraining Loss: 1.415219 \tValidation Loss: 2.523954\n",
      "Epoch: 27202 \tTraining Loss: 1.340604 \tValidation Loss: 2.523915\n",
      "Epoch: 27203 \tTraining Loss: 1.353984 \tValidation Loss: 2.523532\n",
      "Epoch: 27204 \tTraining Loss: 1.346498 \tValidation Loss: 2.525119\n",
      "Epoch: 27205 \tTraining Loss: 1.331365 \tValidation Loss: 2.524128\n",
      "Epoch: 27206 \tTraining Loss: 1.381088 \tValidation Loss: 2.524219\n",
      "Epoch: 27207 \tTraining Loss: 1.379075 \tValidation Loss: 2.524492\n",
      "Epoch: 27208 \tTraining Loss: 1.339538 \tValidation Loss: 2.524716\n",
      "Epoch: 27209 \tTraining Loss: 1.377325 \tValidation Loss: 2.524123\n",
      "Epoch: 27210 \tTraining Loss: 1.344582 \tValidation Loss: 2.524835\n",
      "Epoch: 27211 \tTraining Loss: 1.396121 \tValidation Loss: 2.524071\n",
      "Epoch: 27212 \tTraining Loss: 1.404496 \tValidation Loss: 2.524120\n",
      "Epoch: 27213 \tTraining Loss: 1.400451 \tValidation Loss: 2.523861\n",
      "Epoch: 27214 \tTraining Loss: 1.326248 \tValidation Loss: 2.524810\n",
      "Epoch: 27215 \tTraining Loss: 1.377559 \tValidation Loss: 2.523897\n",
      "Epoch: 27216 \tTraining Loss: 1.368758 \tValidation Loss: 2.524114\n",
      "Epoch: 27217 \tTraining Loss: 1.379026 \tValidation Loss: 2.524199\n",
      "Epoch: 27218 \tTraining Loss: 1.363006 \tValidation Loss: 2.524003\n",
      "Epoch: 27219 \tTraining Loss: 1.361509 \tValidation Loss: 2.524759\n",
      "Epoch: 27220 \tTraining Loss: 1.427850 \tValidation Loss: 2.523752\n",
      "Epoch: 27221 \tTraining Loss: 1.343766 \tValidation Loss: 2.525025\n",
      "Epoch: 27222 \tTraining Loss: 1.376316 \tValidation Loss: 2.524746\n",
      "Epoch: 27223 \tTraining Loss: 1.356276 \tValidation Loss: 2.524749\n",
      "Epoch: 27224 \tTraining Loss: 1.398310 \tValidation Loss: 2.523500\n",
      "Epoch: 27225 \tTraining Loss: 1.367342 \tValidation Loss: 2.525091\n",
      "Epoch: 27226 \tTraining Loss: 1.388316 \tValidation Loss: 2.524306\n",
      "Epoch: 27227 \tTraining Loss: 1.396913 \tValidation Loss: 2.524825\n",
      "Epoch: 27228 \tTraining Loss: 1.362855 \tValidation Loss: 2.524858\n",
      "Epoch: 27229 \tTraining Loss: 1.440707 \tValidation Loss: 2.522824\n",
      "Epoch: 27230 \tTraining Loss: 1.374582 \tValidation Loss: 2.524168\n",
      "Epoch: 27231 \tTraining Loss: 1.401981 \tValidation Loss: 2.524533\n",
      "Epoch: 27232 \tTraining Loss: 1.395366 \tValidation Loss: 2.525312\n",
      "Epoch: 27233 \tTraining Loss: 1.414843 \tValidation Loss: 2.525718\n",
      "Epoch: 27234 \tTraining Loss: 1.382253 \tValidation Loss: 2.525297\n",
      "Epoch: 27235 \tTraining Loss: 1.401935 \tValidation Loss: 2.524360\n",
      "Epoch: 27236 \tTraining Loss: 1.367866 \tValidation Loss: 2.523975\n",
      "Epoch: 27237 \tTraining Loss: 1.380213 \tValidation Loss: 2.524603\n",
      "Epoch: 27238 \tTraining Loss: 1.368495 \tValidation Loss: 2.525613\n",
      "Epoch: 27239 \tTraining Loss: 1.351886 \tValidation Loss: 2.525132\n",
      "Epoch: 27240 \tTraining Loss: 1.374944 \tValidation Loss: 2.525525\n",
      "Epoch: 27241 \tTraining Loss: 1.342771 \tValidation Loss: 2.525478\n",
      "Epoch: 27242 \tTraining Loss: 1.341627 \tValidation Loss: 2.524556\n",
      "Epoch: 27243 \tTraining Loss: 1.356977 \tValidation Loss: 2.524961\n",
      "Epoch: 27244 \tTraining Loss: 1.379541 \tValidation Loss: 2.524950\n",
      "Epoch: 27245 \tTraining Loss: 1.389643 \tValidation Loss: 2.524158\n",
      "Epoch: 27246 \tTraining Loss: 1.380145 \tValidation Loss: 2.522996\n",
      "Epoch: 27247 \tTraining Loss: 1.376485 \tValidation Loss: 2.523696\n",
      "Epoch: 27248 \tTraining Loss: 1.400535 \tValidation Loss: 2.524064\n",
      "Epoch: 27249 \tTraining Loss: 1.385340 \tValidation Loss: 2.524070\n",
      "Epoch: 27250 \tTraining Loss: 1.393658 \tValidation Loss: 2.524734\n",
      "Epoch: 27251 \tTraining Loss: 1.385126 \tValidation Loss: 2.524768\n",
      "Epoch: 27252 \tTraining Loss: 1.368170 \tValidation Loss: 2.525401\n",
      "Epoch: 27253 \tTraining Loss: 1.333221 \tValidation Loss: 2.524876\n",
      "Epoch: 27254 \tTraining Loss: 1.398145 \tValidation Loss: 2.525199\n",
      "Epoch: 27255 \tTraining Loss: 1.355839 \tValidation Loss: 2.524015\n",
      "Epoch: 27256 \tTraining Loss: 1.401677 \tValidation Loss: 2.525003\n",
      "Epoch: 27257 \tTraining Loss: 1.436404 \tValidation Loss: 2.524052\n",
      "Epoch: 27258 \tTraining Loss: 1.371312 \tValidation Loss: 2.525825\n",
      "Epoch: 27259 \tTraining Loss: 1.390414 \tValidation Loss: 2.524877\n",
      "Epoch: 27260 \tTraining Loss: 1.358145 \tValidation Loss: 2.525524\n",
      "Epoch: 27261 \tTraining Loss: 1.357044 \tValidation Loss: 2.525272\n",
      "Epoch: 27262 \tTraining Loss: 1.382081 \tValidation Loss: 2.524822\n",
      "Epoch: 27263 \tTraining Loss: 1.387190 \tValidation Loss: 2.525066\n",
      "Epoch: 27264 \tTraining Loss: 1.380675 \tValidation Loss: 2.525446\n",
      "Epoch: 27265 \tTraining Loss: 1.344680 \tValidation Loss: 2.527005\n",
      "Epoch: 27266 \tTraining Loss: 1.379275 \tValidation Loss: 2.524573\n",
      "Epoch: 27267 \tTraining Loss: 1.343780 \tValidation Loss: 2.526108\n",
      "Epoch: 27268 \tTraining Loss: 1.309331 \tValidation Loss: 2.526155\n",
      "Epoch: 27269 \tTraining Loss: 1.382379 \tValidation Loss: 2.525515\n",
      "Epoch: 27270 \tTraining Loss: 1.366113 \tValidation Loss: 2.525284\n",
      "Epoch: 27271 \tTraining Loss: 1.335881 \tValidation Loss: 2.524927\n",
      "Epoch: 27272 \tTraining Loss: 1.391509 \tValidation Loss: 2.524958\n",
      "Epoch: 27273 \tTraining Loss: 1.389176 \tValidation Loss: 2.525096\n",
      "Epoch: 27274 \tTraining Loss: 1.383100 \tValidation Loss: 2.525872\n",
      "Epoch: 27275 \tTraining Loss: 1.353684 \tValidation Loss: 2.526478\n",
      "Epoch: 27276 \tTraining Loss: 1.396726 \tValidation Loss: 2.525824\n",
      "Epoch: 27277 \tTraining Loss: 1.318290 \tValidation Loss: 2.525914\n",
      "Epoch: 27278 \tTraining Loss: 1.337808 \tValidation Loss: 2.525110\n",
      "Epoch: 27279 \tTraining Loss: 1.381007 \tValidation Loss: 2.524970\n",
      "Epoch: 27280 \tTraining Loss: 1.368411 \tValidation Loss: 2.525237\n",
      "Epoch: 27281 \tTraining Loss: 1.416753 \tValidation Loss: 2.524630\n",
      "Epoch: 27282 \tTraining Loss: 1.405801 \tValidation Loss: 2.525629\n",
      "Epoch: 27283 \tTraining Loss: 1.359051 \tValidation Loss: 2.526127\n",
      "Epoch: 27284 \tTraining Loss: 1.332923 \tValidation Loss: 2.524491\n",
      "Epoch: 27285 \tTraining Loss: 1.304572 \tValidation Loss: 2.525836\n",
      "Epoch: 27286 \tTraining Loss: 1.328557 \tValidation Loss: 2.526214\n",
      "Epoch: 27287 \tTraining Loss: 1.399012 \tValidation Loss: 2.525091\n",
      "Epoch: 27288 \tTraining Loss: 1.363936 \tValidation Loss: 2.526203\n",
      "Epoch: 27289 \tTraining Loss: 1.362207 \tValidation Loss: 2.525935\n",
      "Epoch: 27290 \tTraining Loss: 1.353591 \tValidation Loss: 2.525641\n",
      "Epoch: 27291 \tTraining Loss: 1.369019 \tValidation Loss: 2.527219\n",
      "Epoch: 27292 \tTraining Loss: 1.413034 \tValidation Loss: 2.525690\n",
      "Epoch: 27293 \tTraining Loss: 1.380116 \tValidation Loss: 2.525919\n",
      "Epoch: 27294 \tTraining Loss: 1.379582 \tValidation Loss: 2.525710\n",
      "Epoch: 27295 \tTraining Loss: 1.408450 \tValidation Loss: 2.526300\n",
      "Epoch: 27296 \tTraining Loss: 1.368732 \tValidation Loss: 2.525977\n",
      "Epoch: 27297 \tTraining Loss: 1.396810 \tValidation Loss: 2.525846\n",
      "Epoch: 27298 \tTraining Loss: 1.372124 \tValidation Loss: 2.525547\n",
      "Epoch: 27299 \tTraining Loss: 1.372308 \tValidation Loss: 2.525236\n",
      "Epoch: 27300 \tTraining Loss: 1.354394 \tValidation Loss: 2.525874\n",
      "Epoch: 27301 \tTraining Loss: 1.425891 \tValidation Loss: 2.525971\n",
      "Epoch: 27302 \tTraining Loss: 1.363466 \tValidation Loss: 2.525780\n",
      "Epoch: 27303 \tTraining Loss: 1.361757 \tValidation Loss: 2.526856\n",
      "Epoch: 27304 \tTraining Loss: 1.372954 \tValidation Loss: 2.525392\n",
      "Epoch: 27305 \tTraining Loss: 1.343403 \tValidation Loss: 2.526595\n",
      "Epoch: 27306 \tTraining Loss: 1.377571 \tValidation Loss: 2.525918\n",
      "Epoch: 27307 \tTraining Loss: 1.390533 \tValidation Loss: 2.525853\n",
      "Epoch: 27308 \tTraining Loss: 1.373614 \tValidation Loss: 2.526260\n",
      "Epoch: 27309 \tTraining Loss: 1.372857 \tValidation Loss: 2.525256\n",
      "Epoch: 27310 \tTraining Loss: 1.370530 \tValidation Loss: 2.526054\n",
      "Epoch: 27311 \tTraining Loss: 1.364380 \tValidation Loss: 2.526187\n",
      "Epoch: 27312 \tTraining Loss: 1.331041 \tValidation Loss: 2.525617\n",
      "Epoch: 27313 \tTraining Loss: 1.390805 \tValidation Loss: 2.525656\n",
      "Epoch: 27314 \tTraining Loss: 1.377307 \tValidation Loss: 2.525393\n",
      "Epoch: 27315 \tTraining Loss: 1.385564 \tValidation Loss: 2.526896\n",
      "Epoch: 27316 \tTraining Loss: 1.355902 \tValidation Loss: 2.526695\n",
      "Epoch: 27317 \tTraining Loss: 1.343713 \tValidation Loss: 2.527052\n",
      "Epoch: 27318 \tTraining Loss: 1.359149 \tValidation Loss: 2.526862\n",
      "Epoch: 27319 \tTraining Loss: 1.396037 \tValidation Loss: 2.526948\n",
      "Epoch: 27320 \tTraining Loss: 1.373381 \tValidation Loss: 2.526713\n",
      "Epoch: 27321 \tTraining Loss: 1.333907 \tValidation Loss: 2.527190\n",
      "Epoch: 27322 \tTraining Loss: 1.379394 \tValidation Loss: 2.526777\n",
      "Epoch: 27323 \tTraining Loss: 1.363720 \tValidation Loss: 2.527162\n",
      "Epoch: 27324 \tTraining Loss: 1.367553 \tValidation Loss: 2.526243\n",
      "Epoch: 27325 \tTraining Loss: 1.424536 \tValidation Loss: 2.526612\n",
      "Epoch: 27326 \tTraining Loss: 1.352848 \tValidation Loss: 2.526687\n",
      "Epoch: 27327 \tTraining Loss: 1.367422 \tValidation Loss: 2.526785\n",
      "Epoch: 27328 \tTraining Loss: 1.334822 \tValidation Loss: 2.526831\n",
      "Epoch: 27329 \tTraining Loss: 1.411344 \tValidation Loss: 2.525778\n",
      "Epoch: 27330 \tTraining Loss: 1.385395 \tValidation Loss: 2.525989\n",
      "Epoch: 27331 \tTraining Loss: 1.374934 \tValidation Loss: 2.526667\n",
      "Epoch: 27332 \tTraining Loss: 1.364232 \tValidation Loss: 2.525690\n",
      "Epoch: 27333 \tTraining Loss: 1.367630 \tValidation Loss: 2.526499\n",
      "Epoch: 27334 \tTraining Loss: 1.371524 \tValidation Loss: 2.526755\n",
      "Epoch: 27335 \tTraining Loss: 1.364996 \tValidation Loss: 2.525624\n",
      "Epoch: 27336 \tTraining Loss: 1.377801 \tValidation Loss: 2.526476\n",
      "Epoch: 27337 \tTraining Loss: 1.361649 \tValidation Loss: 2.526537\n",
      "Epoch: 27338 \tTraining Loss: 1.348769 \tValidation Loss: 2.526020\n",
      "Epoch: 27339 \tTraining Loss: 1.358890 \tValidation Loss: 2.526447\n",
      "Epoch: 27340 \tTraining Loss: 1.368886 \tValidation Loss: 2.525182\n",
      "Epoch: 27341 \tTraining Loss: 1.350507 \tValidation Loss: 2.526659\n",
      "Epoch: 27342 \tTraining Loss: 1.387921 \tValidation Loss: 2.526720\n",
      "Epoch: 27343 \tTraining Loss: 1.374094 \tValidation Loss: 2.527196\n",
      "Epoch: 27344 \tTraining Loss: 1.426040 \tValidation Loss: 2.527208\n",
      "Epoch: 27345 \tTraining Loss: 1.355986 \tValidation Loss: 2.526372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27346 \tTraining Loss: 1.382187 \tValidation Loss: 2.527194\n",
      "Epoch: 27347 \tTraining Loss: 1.366367 \tValidation Loss: 2.526778\n",
      "Epoch: 27348 \tTraining Loss: 1.403640 \tValidation Loss: 2.526058\n",
      "Epoch: 27349 \tTraining Loss: 1.416149 \tValidation Loss: 2.526441\n",
      "Epoch: 27350 \tTraining Loss: 1.343074 \tValidation Loss: 2.526343\n",
      "Epoch: 27351 \tTraining Loss: 1.376121 \tValidation Loss: 2.526432\n",
      "Epoch: 27352 \tTraining Loss: 1.352690 \tValidation Loss: 2.526784\n",
      "Epoch: 27353 \tTraining Loss: 1.350831 \tValidation Loss: 2.527558\n",
      "Epoch: 27354 \tTraining Loss: 1.364638 \tValidation Loss: 2.527169\n",
      "Epoch: 27355 \tTraining Loss: 1.356592 \tValidation Loss: 2.527156\n",
      "Epoch: 27356 \tTraining Loss: 1.384325 \tValidation Loss: 2.526455\n",
      "Epoch: 27357 \tTraining Loss: 1.357329 \tValidation Loss: 2.527824\n",
      "Epoch: 27358 \tTraining Loss: 1.390647 \tValidation Loss: 2.528570\n",
      "Epoch: 27359 \tTraining Loss: 1.395458 \tValidation Loss: 2.527306\n",
      "Epoch: 27360 \tTraining Loss: 1.346860 \tValidation Loss: 2.528772\n",
      "Epoch: 27361 \tTraining Loss: 1.382795 \tValidation Loss: 2.528074\n",
      "Epoch: 27362 \tTraining Loss: 1.362844 \tValidation Loss: 2.528179\n",
      "Epoch: 27363 \tTraining Loss: 1.324630 \tValidation Loss: 2.527812\n",
      "Epoch: 27364 \tTraining Loss: 1.356894 \tValidation Loss: 2.528179\n",
      "Epoch: 27365 \tTraining Loss: 1.376093 \tValidation Loss: 2.526138\n",
      "Epoch: 27366 \tTraining Loss: 1.372018 \tValidation Loss: 2.526630\n",
      "Epoch: 27367 \tTraining Loss: 1.355693 \tValidation Loss: 2.527460\n",
      "Epoch: 27368 \tTraining Loss: 1.367025 \tValidation Loss: 2.528768\n",
      "Epoch: 27369 \tTraining Loss: 1.362681 \tValidation Loss: 2.526885\n",
      "Epoch: 27370 \tTraining Loss: 1.391349 \tValidation Loss: 2.527334\n",
      "Epoch: 27371 \tTraining Loss: 1.392170 \tValidation Loss: 2.527061\n",
      "Epoch: 27372 \tTraining Loss: 1.378391 \tValidation Loss: 2.527721\n",
      "Epoch: 27373 \tTraining Loss: 1.419309 \tValidation Loss: 2.526070\n",
      "Epoch: 27374 \tTraining Loss: 1.368589 \tValidation Loss: 2.526847\n",
      "Epoch: 27375 \tTraining Loss: 1.393652 \tValidation Loss: 2.526565\n",
      "Epoch: 27376 \tTraining Loss: 1.339671 \tValidation Loss: 2.527706\n",
      "Epoch: 27377 \tTraining Loss: 1.384862 \tValidation Loss: 2.526374\n",
      "Epoch: 27378 \tTraining Loss: 1.346135 \tValidation Loss: 2.528062\n",
      "Epoch: 27379 \tTraining Loss: 1.359703 \tValidation Loss: 2.527420\n",
      "Epoch: 27380 \tTraining Loss: 1.340494 \tValidation Loss: 2.528054\n",
      "Epoch: 27381 \tTraining Loss: 1.367001 \tValidation Loss: 2.527143\n",
      "Epoch: 27382 \tTraining Loss: 1.353233 \tValidation Loss: 2.527981\n",
      "Epoch: 27383 \tTraining Loss: 1.349769 \tValidation Loss: 2.528064\n",
      "Epoch: 27384 \tTraining Loss: 1.345924 \tValidation Loss: 2.527205\n",
      "Epoch: 27385 \tTraining Loss: 1.353397 \tValidation Loss: 2.527532\n",
      "Epoch: 27386 \tTraining Loss: 1.366248 \tValidation Loss: 2.528464\n",
      "Epoch: 27387 \tTraining Loss: 1.414444 \tValidation Loss: 2.527597\n",
      "Epoch: 27388 \tTraining Loss: 1.351876 \tValidation Loss: 2.526608\n",
      "Epoch: 27389 \tTraining Loss: 1.393148 \tValidation Loss: 2.526884\n",
      "Epoch: 27390 \tTraining Loss: 1.347647 \tValidation Loss: 2.526959\n",
      "Epoch: 27391 \tTraining Loss: 1.369900 \tValidation Loss: 2.528245\n",
      "Epoch: 27392 \tTraining Loss: 1.377589 \tValidation Loss: 2.527731\n",
      "Epoch: 27393 \tTraining Loss: 1.374966 \tValidation Loss: 2.526845\n",
      "Epoch: 27394 \tTraining Loss: 1.390650 \tValidation Loss: 2.527081\n",
      "Epoch: 27395 \tTraining Loss: 1.389395 \tValidation Loss: 2.526761\n",
      "Epoch: 27396 \tTraining Loss: 1.370419 \tValidation Loss: 2.527881\n",
      "Epoch: 27397 \tTraining Loss: 1.338452 \tValidation Loss: 2.528491\n",
      "Epoch: 27398 \tTraining Loss: 1.397339 \tValidation Loss: 2.527503\n",
      "Epoch: 27399 \tTraining Loss: 1.332633 \tValidation Loss: 2.527843\n",
      "Epoch: 27400 \tTraining Loss: 1.342517 \tValidation Loss: 2.527913\n",
      "Epoch: 27401 \tTraining Loss: 1.347006 \tValidation Loss: 2.529217\n",
      "Epoch: 27402 \tTraining Loss: 1.350075 \tValidation Loss: 2.527834\n",
      "Epoch: 27403 \tTraining Loss: 1.337280 \tValidation Loss: 2.529423\n",
      "Epoch: 27404 \tTraining Loss: 1.345728 \tValidation Loss: 2.529012\n",
      "Epoch: 27405 \tTraining Loss: 1.386590 \tValidation Loss: 2.528621\n",
      "Epoch: 27406 \tTraining Loss: 1.347654 \tValidation Loss: 2.528640\n",
      "Epoch: 27407 \tTraining Loss: 1.384939 \tValidation Loss: 2.528614\n",
      "Epoch: 27408 \tTraining Loss: 1.389045 \tValidation Loss: 2.527574\n",
      "Epoch: 27409 \tTraining Loss: 1.327095 \tValidation Loss: 2.528996\n",
      "Epoch: 27410 \tTraining Loss: 1.385849 \tValidation Loss: 2.527817\n",
      "Epoch: 27411 \tTraining Loss: 1.344337 \tValidation Loss: 2.529462\n",
      "Epoch: 27412 \tTraining Loss: 1.380379 \tValidation Loss: 2.529094\n",
      "Epoch: 27413 \tTraining Loss: 1.397751 \tValidation Loss: 2.527693\n",
      "Epoch: 27414 \tTraining Loss: 1.367396 \tValidation Loss: 2.529054\n",
      "Epoch: 27415 \tTraining Loss: 1.389362 \tValidation Loss: 2.528165\n",
      "Epoch: 27416 \tTraining Loss: 1.376162 \tValidation Loss: 2.528677\n",
      "Epoch: 27417 \tTraining Loss: 1.324225 \tValidation Loss: 2.529233\n",
      "Epoch: 27418 \tTraining Loss: 1.380946 \tValidation Loss: 2.527745\n",
      "Epoch: 27419 \tTraining Loss: 1.339833 \tValidation Loss: 2.528321\n",
      "Epoch: 27420 \tTraining Loss: 1.343542 \tValidation Loss: 2.527970\n",
      "Epoch: 27421 \tTraining Loss: 1.394309 \tValidation Loss: 2.527877\n",
      "Epoch: 27422 \tTraining Loss: 1.359204 \tValidation Loss: 2.528369\n",
      "Epoch: 27423 \tTraining Loss: 1.361413 \tValidation Loss: 2.527976\n",
      "Epoch: 27424 \tTraining Loss: 1.392658 \tValidation Loss: 2.528693\n",
      "Epoch: 27425 \tTraining Loss: 1.370999 \tValidation Loss: 2.528418\n",
      "Epoch: 27426 \tTraining Loss: 1.380911 \tValidation Loss: 2.528347\n",
      "Epoch: 27427 \tTraining Loss: 1.328942 \tValidation Loss: 2.528979\n",
      "Epoch: 27428 \tTraining Loss: 1.374704 \tValidation Loss: 2.527697\n",
      "Epoch: 27429 \tTraining Loss: 1.376006 \tValidation Loss: 2.528872\n",
      "Epoch: 27430 \tTraining Loss: 1.374639 \tValidation Loss: 2.528717\n",
      "Epoch: 27431 \tTraining Loss: 1.377536 \tValidation Loss: 2.528708\n",
      "Epoch: 27432 \tTraining Loss: 1.317100 \tValidation Loss: 2.529035\n",
      "Epoch: 27433 \tTraining Loss: 1.397266 \tValidation Loss: 2.527973\n",
      "Epoch: 27434 \tTraining Loss: 1.341455 \tValidation Loss: 2.527855\n",
      "Epoch: 27435 \tTraining Loss: 1.293310 \tValidation Loss: 2.529581\n",
      "Epoch: 27436 \tTraining Loss: 1.308864 \tValidation Loss: 2.529370\n",
      "Epoch: 27437 \tTraining Loss: 1.386594 \tValidation Loss: 2.528935\n",
      "Epoch: 27438 \tTraining Loss: 1.411365 \tValidation Loss: 2.528129\n",
      "Epoch: 27439 \tTraining Loss: 1.380543 \tValidation Loss: 2.528342\n",
      "Epoch: 27440 \tTraining Loss: 1.356202 \tValidation Loss: 2.530074\n",
      "Epoch: 27441 \tTraining Loss: 1.344906 \tValidation Loss: 2.528768\n",
      "Epoch: 27442 \tTraining Loss: 1.335119 \tValidation Loss: 2.529679\n",
      "Epoch: 27443 \tTraining Loss: 1.391300 \tValidation Loss: 2.530416\n",
      "Epoch: 27444 \tTraining Loss: 1.348807 \tValidation Loss: 2.530125\n",
      "Epoch: 27445 \tTraining Loss: 1.379327 \tValidation Loss: 2.528347\n",
      "Epoch: 27446 \tTraining Loss: 1.367011 \tValidation Loss: 2.527028\n",
      "Epoch: 27447 \tTraining Loss: 1.376095 \tValidation Loss: 2.528100\n",
      "Epoch: 27448 \tTraining Loss: 1.346439 \tValidation Loss: 2.528446\n",
      "Epoch: 27449 \tTraining Loss: 1.347011 \tValidation Loss: 2.529139\n",
      "Epoch: 27450 \tTraining Loss: 1.355683 \tValidation Loss: 2.528776\n",
      "Epoch: 27451 \tTraining Loss: 1.398325 \tValidation Loss: 2.528520\n",
      "Epoch: 27452 \tTraining Loss: 1.382908 \tValidation Loss: 2.528824\n",
      "Epoch: 27453 \tTraining Loss: 1.363639 \tValidation Loss: 2.528786\n",
      "Epoch: 27454 \tTraining Loss: 1.414986 \tValidation Loss: 2.528962\n",
      "Epoch: 27455 \tTraining Loss: 1.367082 \tValidation Loss: 2.529262\n",
      "Epoch: 27456 \tTraining Loss: 1.381993 \tValidation Loss: 2.529685\n",
      "Epoch: 27457 \tTraining Loss: 1.371680 \tValidation Loss: 2.529890\n",
      "Epoch: 27458 \tTraining Loss: 1.387776 \tValidation Loss: 2.528948\n",
      "Epoch: 27459 \tTraining Loss: 1.352837 \tValidation Loss: 2.529571\n",
      "Epoch: 27460 \tTraining Loss: 1.397176 \tValidation Loss: 2.529622\n",
      "Epoch: 27461 \tTraining Loss: 1.359562 \tValidation Loss: 2.530284\n",
      "Epoch: 27462 \tTraining Loss: 1.359066 \tValidation Loss: 2.529431\n",
      "Epoch: 27463 \tTraining Loss: 1.408732 \tValidation Loss: 2.529934\n",
      "Epoch: 27464 \tTraining Loss: 1.353660 \tValidation Loss: 2.530334\n",
      "Epoch: 27465 \tTraining Loss: 1.389184 \tValidation Loss: 2.528888\n",
      "Epoch: 27466 \tTraining Loss: 1.362431 \tValidation Loss: 2.529452\n",
      "Epoch: 27467 \tTraining Loss: 1.417178 \tValidation Loss: 2.529153\n",
      "Epoch: 27468 \tTraining Loss: 1.363488 \tValidation Loss: 2.529864\n",
      "Epoch: 27469 \tTraining Loss: 1.390814 \tValidation Loss: 2.530016\n",
      "Epoch: 27470 \tTraining Loss: 1.371261 \tValidation Loss: 2.529605\n",
      "Epoch: 27471 \tTraining Loss: 1.336850 \tValidation Loss: 2.530388\n",
      "Epoch: 27472 \tTraining Loss: 1.348974 \tValidation Loss: 2.529969\n",
      "Epoch: 27473 \tTraining Loss: 1.401722 \tValidation Loss: 2.528931\n",
      "Epoch: 27474 \tTraining Loss: 1.345221 \tValidation Loss: 2.529930\n",
      "Epoch: 27475 \tTraining Loss: 1.314596 \tValidation Loss: 2.529203\n",
      "Epoch: 27476 \tTraining Loss: 1.362953 \tValidation Loss: 2.530586\n",
      "Epoch: 27477 \tTraining Loss: 1.352014 \tValidation Loss: 2.529088\n",
      "Epoch: 27478 \tTraining Loss: 1.358141 \tValidation Loss: 2.528265\n",
      "Epoch: 27479 \tTraining Loss: 1.360830 \tValidation Loss: 2.528984\n",
      "Epoch: 27480 \tTraining Loss: 1.309402 \tValidation Loss: 2.529852\n",
      "Epoch: 27481 \tTraining Loss: 1.357948 \tValidation Loss: 2.529931\n",
      "Epoch: 27482 \tTraining Loss: 1.398225 \tValidation Loss: 2.528990\n",
      "Epoch: 27483 \tTraining Loss: 1.368716 \tValidation Loss: 2.529850\n",
      "Epoch: 27484 \tTraining Loss: 1.408667 \tValidation Loss: 2.529994\n",
      "Epoch: 27485 \tTraining Loss: 1.411731 \tValidation Loss: 2.529338\n",
      "Epoch: 27486 \tTraining Loss: 1.354281 \tValidation Loss: 2.529999\n",
      "Epoch: 27487 \tTraining Loss: 1.347081 \tValidation Loss: 2.529073\n",
      "Epoch: 27488 \tTraining Loss: 1.356057 \tValidation Loss: 2.529353\n",
      "Epoch: 27489 \tTraining Loss: 1.313202 \tValidation Loss: 2.531354\n",
      "Epoch: 27490 \tTraining Loss: 1.381159 \tValidation Loss: 2.530077\n",
      "Epoch: 27491 \tTraining Loss: 1.347435 \tValidation Loss: 2.530156\n",
      "Epoch: 27492 \tTraining Loss: 1.342152 \tValidation Loss: 2.529068\n",
      "Epoch: 27493 \tTraining Loss: 1.382100 \tValidation Loss: 2.529722\n",
      "Epoch: 27494 \tTraining Loss: 1.355671 \tValidation Loss: 2.528933\n",
      "Epoch: 27495 \tTraining Loss: 1.379483 \tValidation Loss: 2.529200\n",
      "Epoch: 27496 \tTraining Loss: 1.369836 \tValidation Loss: 2.528932\n",
      "Epoch: 27497 \tTraining Loss: 1.333919 \tValidation Loss: 2.530045\n",
      "Epoch: 27498 \tTraining Loss: 1.369511 \tValidation Loss: 2.529205\n",
      "Epoch: 27499 \tTraining Loss: 1.404533 \tValidation Loss: 2.529909\n",
      "Epoch: 27500 \tTraining Loss: 1.384509 \tValidation Loss: 2.529204\n",
      "Epoch: 27501 \tTraining Loss: 1.385519 \tValidation Loss: 2.530028\n",
      "Epoch: 27502 \tTraining Loss: 1.355850 \tValidation Loss: 2.529826\n",
      "Epoch: 27503 \tTraining Loss: 1.338265 \tValidation Loss: 2.529669\n",
      "Epoch: 27504 \tTraining Loss: 1.357865 \tValidation Loss: 2.529771\n",
      "Epoch: 27505 \tTraining Loss: 1.340887 \tValidation Loss: 2.531117\n",
      "Epoch: 27506 \tTraining Loss: 1.322327 \tValidation Loss: 2.530671\n",
      "Epoch: 27507 \tTraining Loss: 1.376376 \tValidation Loss: 2.530966\n",
      "Epoch: 27508 \tTraining Loss: 1.359139 \tValidation Loss: 2.530540\n",
      "Epoch: 27509 \tTraining Loss: 1.381244 \tValidation Loss: 2.530491\n",
      "Epoch: 27510 \tTraining Loss: 1.395806 \tValidation Loss: 2.527962\n",
      "Epoch: 27511 \tTraining Loss: 1.356588 \tValidation Loss: 2.529809\n",
      "Epoch: 27512 \tTraining Loss: 1.352049 \tValidation Loss: 2.530091\n",
      "Epoch: 27513 \tTraining Loss: 1.325822 \tValidation Loss: 2.531317\n",
      "Epoch: 27514 \tTraining Loss: 1.371766 \tValidation Loss: 2.530035\n",
      "Epoch: 27515 \tTraining Loss: 1.396080 \tValidation Loss: 2.530698\n",
      "Epoch: 27516 \tTraining Loss: 1.355050 \tValidation Loss: 2.530499\n",
      "Epoch: 27517 \tTraining Loss: 1.334705 \tValidation Loss: 2.531783\n",
      "Epoch: 27518 \tTraining Loss: 1.363974 \tValidation Loss: 2.531213\n",
      "Epoch: 27519 \tTraining Loss: 1.389348 \tValidation Loss: 2.530904\n",
      "Epoch: 27520 \tTraining Loss: 1.376215 \tValidation Loss: 2.531033\n",
      "Epoch: 27521 \tTraining Loss: 1.330014 \tValidation Loss: 2.531011\n",
      "Epoch: 27522 \tTraining Loss: 1.306621 \tValidation Loss: 2.531708\n",
      "Epoch: 27523 \tTraining Loss: 1.378065 \tValidation Loss: 2.530793\n",
      "Epoch: 27524 \tTraining Loss: 1.310397 \tValidation Loss: 2.532032\n",
      "Epoch: 27525 \tTraining Loss: 1.352202 \tValidation Loss: 2.530935\n",
      "Epoch: 27526 \tTraining Loss: 1.399528 \tValidation Loss: 2.530079\n",
      "Epoch: 27527 \tTraining Loss: 1.350914 \tValidation Loss: 2.530849\n",
      "Epoch: 27528 \tTraining Loss: 1.371730 \tValidation Loss: 2.531566\n",
      "Epoch: 27529 \tTraining Loss: 1.373708 \tValidation Loss: 2.531395\n",
      "Epoch: 27530 \tTraining Loss: 1.374706 \tValidation Loss: 2.531147\n",
      "Epoch: 27531 \tTraining Loss: 1.392386 \tValidation Loss: 2.530930\n",
      "Epoch: 27532 \tTraining Loss: 1.368768 \tValidation Loss: 2.531274\n",
      "Epoch: 27533 \tTraining Loss: 1.390816 \tValidation Loss: 2.530569\n",
      "Epoch: 27534 \tTraining Loss: 1.355577 \tValidation Loss: 2.530277\n",
      "Epoch: 27535 \tTraining Loss: 1.349884 \tValidation Loss: 2.531276\n",
      "Epoch: 27536 \tTraining Loss: 1.343736 \tValidation Loss: 2.531246\n",
      "Epoch: 27537 \tTraining Loss: 1.385327 \tValidation Loss: 2.530874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27538 \tTraining Loss: 1.352053 \tValidation Loss: 2.531574\n",
      "Epoch: 27539 \tTraining Loss: 1.350084 \tValidation Loss: 2.531408\n",
      "Epoch: 27540 \tTraining Loss: 1.364826 \tValidation Loss: 2.532735\n",
      "Epoch: 27541 \tTraining Loss: 1.353088 \tValidation Loss: 2.531168\n",
      "Epoch: 27542 \tTraining Loss: 1.385943 \tValidation Loss: 2.531769\n",
      "Epoch: 27543 \tTraining Loss: 1.320495 \tValidation Loss: 2.532208\n",
      "Epoch: 27544 \tTraining Loss: 1.386997 \tValidation Loss: 2.530668\n",
      "Epoch: 27545 \tTraining Loss: 1.367917 \tValidation Loss: 2.530852\n",
      "Epoch: 27546 \tTraining Loss: 1.375357 \tValidation Loss: 2.531353\n",
      "Epoch: 27547 \tTraining Loss: 1.370787 \tValidation Loss: 2.531199\n",
      "Epoch: 27548 \tTraining Loss: 1.373834 \tValidation Loss: 2.531444\n",
      "Epoch: 27549 \tTraining Loss: 1.339507 \tValidation Loss: 2.531461\n",
      "Epoch: 27550 \tTraining Loss: 1.368847 \tValidation Loss: 2.530847\n",
      "Epoch: 27551 \tTraining Loss: 1.355973 \tValidation Loss: 2.532202\n",
      "Epoch: 27552 \tTraining Loss: 1.372295 \tValidation Loss: 2.532161\n",
      "Epoch: 27553 \tTraining Loss: 1.361314 \tValidation Loss: 2.531084\n",
      "Epoch: 27554 \tTraining Loss: 1.379958 \tValidation Loss: 2.530991\n",
      "Epoch: 27555 \tTraining Loss: 1.367334 \tValidation Loss: 2.531643\n",
      "Epoch: 27556 \tTraining Loss: 1.364302 \tValidation Loss: 2.530936\n",
      "Epoch: 27557 \tTraining Loss: 1.391837 \tValidation Loss: 2.530913\n",
      "Epoch: 27558 \tTraining Loss: 1.347010 \tValidation Loss: 2.531263\n",
      "Epoch: 27559 \tTraining Loss: 1.385054 \tValidation Loss: 2.530581\n",
      "Epoch: 27560 \tTraining Loss: 1.329206 \tValidation Loss: 2.531791\n",
      "Epoch: 27561 \tTraining Loss: 1.358357 \tValidation Loss: 2.530916\n",
      "Epoch: 27562 \tTraining Loss: 1.361949 \tValidation Loss: 2.531239\n",
      "Epoch: 27563 \tTraining Loss: 1.355551 \tValidation Loss: 2.531151\n",
      "Epoch: 27564 \tTraining Loss: 1.375765 \tValidation Loss: 2.531157\n",
      "Epoch: 27565 \tTraining Loss: 1.333077 \tValidation Loss: 2.532055\n",
      "Epoch: 27566 \tTraining Loss: 1.373634 \tValidation Loss: 2.530893\n",
      "Epoch: 27567 \tTraining Loss: 1.335376 \tValidation Loss: 2.531668\n",
      "Epoch: 27568 \tTraining Loss: 1.330987 \tValidation Loss: 2.532388\n",
      "Epoch: 27569 \tTraining Loss: 1.363337 \tValidation Loss: 2.531231\n",
      "Epoch: 27570 \tTraining Loss: 1.371127 \tValidation Loss: 2.531101\n",
      "Epoch: 27571 \tTraining Loss: 1.344594 \tValidation Loss: 2.531924\n",
      "Epoch: 27572 \tTraining Loss: 1.373794 \tValidation Loss: 2.531919\n",
      "Epoch: 27573 \tTraining Loss: 1.348074 \tValidation Loss: 2.530558\n",
      "Epoch: 27574 \tTraining Loss: 1.402844 \tValidation Loss: 2.531247\n",
      "Epoch: 27575 \tTraining Loss: 1.356026 \tValidation Loss: 2.530829\n",
      "Epoch: 27576 \tTraining Loss: 1.359803 \tValidation Loss: 2.532412\n",
      "Epoch: 27577 \tTraining Loss: 1.396924 \tValidation Loss: 2.532916\n",
      "Epoch: 27578 \tTraining Loss: 1.339289 \tValidation Loss: 2.531963\n",
      "Epoch: 27579 \tTraining Loss: 1.396347 \tValidation Loss: 2.530916\n",
      "Epoch: 27580 \tTraining Loss: 1.376420 \tValidation Loss: 2.531931\n",
      "Epoch: 27581 \tTraining Loss: 1.423198 \tValidation Loss: 2.531168\n",
      "Epoch: 27582 \tTraining Loss: 1.364462 \tValidation Loss: 2.532203\n",
      "Epoch: 27583 \tTraining Loss: 1.347599 \tValidation Loss: 2.531309\n",
      "Epoch: 27584 \tTraining Loss: 1.378960 \tValidation Loss: 2.532129\n",
      "Epoch: 27585 \tTraining Loss: 1.358718 \tValidation Loss: 2.532468\n",
      "Epoch: 27586 \tTraining Loss: 1.338783 \tValidation Loss: 2.531624\n",
      "Epoch: 27587 \tTraining Loss: 1.364973 \tValidation Loss: 2.532148\n",
      "Epoch: 27588 \tTraining Loss: 1.379813 \tValidation Loss: 2.532307\n",
      "Epoch: 27589 \tTraining Loss: 1.370057 \tValidation Loss: 2.531944\n",
      "Epoch: 27590 \tTraining Loss: 1.382412 \tValidation Loss: 2.532796\n",
      "Epoch: 27591 \tTraining Loss: 1.349658 \tValidation Loss: 2.533205\n",
      "Epoch: 27592 \tTraining Loss: 1.369541 \tValidation Loss: 2.532110\n",
      "Epoch: 27593 \tTraining Loss: 1.372909 \tValidation Loss: 2.532327\n",
      "Epoch: 27594 \tTraining Loss: 1.358007 \tValidation Loss: 2.532230\n",
      "Epoch: 27595 \tTraining Loss: 1.335795 \tValidation Loss: 2.532431\n",
      "Epoch: 27596 \tTraining Loss: 1.355933 \tValidation Loss: 2.532118\n",
      "Epoch: 27597 \tTraining Loss: 1.333558 \tValidation Loss: 2.532374\n",
      "Epoch: 27598 \tTraining Loss: 1.357878 \tValidation Loss: 2.532376\n",
      "Epoch: 27599 \tTraining Loss: 1.364428 \tValidation Loss: 2.532103\n",
      "Epoch: 27600 \tTraining Loss: 1.316338 \tValidation Loss: 2.533445\n",
      "Epoch: 27601 \tTraining Loss: 1.316548 \tValidation Loss: 2.532296\n",
      "Epoch: 27602 \tTraining Loss: 1.354584 \tValidation Loss: 2.532411\n",
      "Epoch: 27603 \tTraining Loss: 1.378504 \tValidation Loss: 2.531552\n",
      "Epoch: 27604 \tTraining Loss: 1.372103 \tValidation Loss: 2.532080\n",
      "Epoch: 27605 \tTraining Loss: 1.373673 \tValidation Loss: 2.531358\n",
      "Epoch: 27606 \tTraining Loss: 1.338167 \tValidation Loss: 2.531624\n",
      "Epoch: 27607 \tTraining Loss: 1.332990 \tValidation Loss: 2.532938\n",
      "Epoch: 27608 \tTraining Loss: 1.291849 \tValidation Loss: 2.532772\n",
      "Epoch: 27609 \tTraining Loss: 1.371957 \tValidation Loss: 2.531497\n",
      "Epoch: 27610 \tTraining Loss: 1.333464 \tValidation Loss: 2.532281\n",
      "Epoch: 27611 \tTraining Loss: 1.351778 \tValidation Loss: 2.532486\n",
      "Epoch: 27612 \tTraining Loss: 1.365927 \tValidation Loss: 2.532761\n",
      "Epoch: 27613 \tTraining Loss: 1.316953 \tValidation Loss: 2.533864\n",
      "Epoch: 27614 \tTraining Loss: 1.382790 \tValidation Loss: 2.532033\n",
      "Epoch: 27615 \tTraining Loss: 1.370119 \tValidation Loss: 2.532614\n",
      "Epoch: 27616 \tTraining Loss: 1.373780 \tValidation Loss: 2.532836\n",
      "Epoch: 27617 \tTraining Loss: 1.389513 \tValidation Loss: 2.531779\n",
      "Epoch: 27618 \tTraining Loss: 1.353685 \tValidation Loss: 2.531899\n",
      "Epoch: 27619 \tTraining Loss: 1.360396 \tValidation Loss: 2.531832\n",
      "Epoch: 27620 \tTraining Loss: 1.386700 \tValidation Loss: 2.532394\n",
      "Epoch: 27621 \tTraining Loss: 1.382565 \tValidation Loss: 2.533423\n",
      "Epoch: 27622 \tTraining Loss: 1.363932 \tValidation Loss: 2.532884\n",
      "Epoch: 27623 \tTraining Loss: 1.368155 \tValidation Loss: 2.532641\n",
      "Epoch: 27624 \tTraining Loss: 1.366760 \tValidation Loss: 2.533772\n",
      "Epoch: 27625 \tTraining Loss: 1.389056 \tValidation Loss: 2.532608\n",
      "Epoch: 27626 \tTraining Loss: 1.385823 \tValidation Loss: 2.533222\n",
      "Epoch: 27627 \tTraining Loss: 1.388250 \tValidation Loss: 2.532853\n",
      "Epoch: 27628 \tTraining Loss: 1.389593 \tValidation Loss: 2.533673\n",
      "Epoch: 27629 \tTraining Loss: 1.361965 \tValidation Loss: 2.534200\n",
      "Epoch: 27630 \tTraining Loss: 1.339719 \tValidation Loss: 2.533621\n",
      "Epoch: 27631 \tTraining Loss: 1.334115 \tValidation Loss: 2.534138\n",
      "Epoch: 27632 \tTraining Loss: 1.325912 \tValidation Loss: 2.532578\n",
      "Epoch: 27633 \tTraining Loss: 1.339409 \tValidation Loss: 2.533806\n",
      "Epoch: 27634 \tTraining Loss: 1.399513 \tValidation Loss: 2.532186\n",
      "Epoch: 27635 \tTraining Loss: 1.360697 \tValidation Loss: 2.533665\n",
      "Epoch: 27636 \tTraining Loss: 1.377736 \tValidation Loss: 2.534149\n",
      "Epoch: 27637 \tTraining Loss: 1.332112 \tValidation Loss: 2.534839\n",
      "Epoch: 27638 \tTraining Loss: 1.354312 \tValidation Loss: 2.533941\n",
      "Epoch: 27639 \tTraining Loss: 1.382909 \tValidation Loss: 2.533190\n",
      "Epoch: 27640 \tTraining Loss: 1.356487 \tValidation Loss: 2.533338\n",
      "Epoch: 27641 \tTraining Loss: 1.338954 \tValidation Loss: 2.533916\n",
      "Epoch: 27642 \tTraining Loss: 1.391197 \tValidation Loss: 2.533211\n",
      "Epoch: 27643 \tTraining Loss: 1.365454 \tValidation Loss: 2.534053\n",
      "Epoch: 27644 \tTraining Loss: 1.378938 \tValidation Loss: 2.533646\n",
      "Epoch: 27645 \tTraining Loss: 1.355067 \tValidation Loss: 2.533036\n",
      "Epoch: 27646 \tTraining Loss: 1.380046 \tValidation Loss: 2.534244\n",
      "Epoch: 27647 \tTraining Loss: 1.424361 \tValidation Loss: 2.533920\n",
      "Epoch: 27648 \tTraining Loss: 1.339448 \tValidation Loss: 2.533819\n",
      "Epoch: 27649 \tTraining Loss: 1.331012 \tValidation Loss: 2.533734\n",
      "Epoch: 27650 \tTraining Loss: 1.333843 \tValidation Loss: 2.535278\n",
      "Epoch: 27651 \tTraining Loss: 1.368621 \tValidation Loss: 2.534290\n",
      "Epoch: 27652 \tTraining Loss: 1.363191 \tValidation Loss: 2.533731\n",
      "Epoch: 27653 \tTraining Loss: 1.389555 \tValidation Loss: 2.532734\n",
      "Epoch: 27654 \tTraining Loss: 1.404948 \tValidation Loss: 2.532637\n",
      "Epoch: 27655 \tTraining Loss: 1.331365 \tValidation Loss: 2.533967\n",
      "Epoch: 27656 \tTraining Loss: 1.310760 \tValidation Loss: 2.534867\n",
      "Epoch: 27657 \tTraining Loss: 1.343691 \tValidation Loss: 2.533346\n",
      "Epoch: 27658 \tTraining Loss: 1.356209 \tValidation Loss: 2.532861\n",
      "Epoch: 27659 \tTraining Loss: 1.377588 \tValidation Loss: 2.533232\n",
      "Epoch: 27660 \tTraining Loss: 1.381892 \tValidation Loss: 2.532834\n",
      "Epoch: 27661 \tTraining Loss: 1.371247 \tValidation Loss: 2.532785\n",
      "Epoch: 27662 \tTraining Loss: 1.347215 \tValidation Loss: 2.533652\n",
      "Epoch: 27663 \tTraining Loss: 1.412611 \tValidation Loss: 2.532633\n",
      "Epoch: 27664 \tTraining Loss: 1.416118 \tValidation Loss: 2.533236\n",
      "Epoch: 27665 \tTraining Loss: 1.356423 \tValidation Loss: 2.535053\n",
      "Epoch: 27666 \tTraining Loss: 1.353752 \tValidation Loss: 2.534214\n",
      "Epoch: 27667 \tTraining Loss: 1.347983 \tValidation Loss: 2.533860\n",
      "Epoch: 27668 \tTraining Loss: 1.402008 \tValidation Loss: 2.533843\n",
      "Epoch: 27669 \tTraining Loss: 1.342028 \tValidation Loss: 2.534481\n",
      "Epoch: 27670 \tTraining Loss: 1.360236 \tValidation Loss: 2.535208\n",
      "Epoch: 27671 \tTraining Loss: 1.377692 \tValidation Loss: 2.534617\n",
      "Epoch: 27672 \tTraining Loss: 1.365874 \tValidation Loss: 2.534829\n",
      "Epoch: 27673 \tTraining Loss: 1.349396 \tValidation Loss: 2.534322\n",
      "Epoch: 27674 \tTraining Loss: 1.352242 \tValidation Loss: 2.535352\n",
      "Epoch: 27675 \tTraining Loss: 1.367499 \tValidation Loss: 2.533844\n",
      "Epoch: 27676 \tTraining Loss: 1.343082 \tValidation Loss: 2.533308\n",
      "Epoch: 27677 \tTraining Loss: 1.332468 \tValidation Loss: 2.533801\n",
      "Epoch: 27678 \tTraining Loss: 1.356447 \tValidation Loss: 2.534376\n",
      "Epoch: 27679 \tTraining Loss: 1.344636 \tValidation Loss: 2.534801\n",
      "Epoch: 27680 \tTraining Loss: 1.336113 \tValidation Loss: 2.533876\n",
      "Epoch: 27681 \tTraining Loss: 1.312444 \tValidation Loss: 2.534758\n",
      "Epoch: 27682 \tTraining Loss: 1.374570 \tValidation Loss: 2.533591\n",
      "Epoch: 27683 \tTraining Loss: 1.340507 \tValidation Loss: 2.534102\n",
      "Epoch: 27684 \tTraining Loss: 1.318052 \tValidation Loss: 2.535366\n",
      "Epoch: 27685 \tTraining Loss: 1.371196 \tValidation Loss: 2.534114\n",
      "Epoch: 27686 \tTraining Loss: 1.331112 \tValidation Loss: 2.535826\n",
      "Epoch: 27687 \tTraining Loss: 1.331039 \tValidation Loss: 2.534359\n",
      "Epoch: 27688 \tTraining Loss: 1.368667 \tValidation Loss: 2.534328\n",
      "Epoch: 27689 \tTraining Loss: 1.362635 \tValidation Loss: 2.534422\n",
      "Epoch: 27690 \tTraining Loss: 1.365853 \tValidation Loss: 2.533339\n",
      "Epoch: 27691 \tTraining Loss: 1.356983 \tValidation Loss: 2.534535\n",
      "Epoch: 27692 \tTraining Loss: 1.346521 \tValidation Loss: 2.534733\n",
      "Epoch: 27693 \tTraining Loss: 1.330224 \tValidation Loss: 2.534086\n",
      "Epoch: 27694 \tTraining Loss: 1.359113 \tValidation Loss: 2.534806\n",
      "Epoch: 27695 \tTraining Loss: 1.373163 \tValidation Loss: 2.534262\n",
      "Epoch: 27696 \tTraining Loss: 1.374436 \tValidation Loss: 2.534014\n",
      "Epoch: 27697 \tTraining Loss: 1.357432 \tValidation Loss: 2.534030\n",
      "Epoch: 27698 \tTraining Loss: 1.322514 \tValidation Loss: 2.535165\n",
      "Epoch: 27699 \tTraining Loss: 1.373685 \tValidation Loss: 2.534836\n",
      "Epoch: 27700 \tTraining Loss: 1.401624 \tValidation Loss: 2.533637\n",
      "Epoch: 27701 \tTraining Loss: 1.380086 \tValidation Loss: 2.534158\n",
      "Epoch: 27702 \tTraining Loss: 1.323515 \tValidation Loss: 2.535764\n",
      "Epoch: 27703 \tTraining Loss: 1.342781 \tValidation Loss: 2.535078\n",
      "Epoch: 27704 \tTraining Loss: 1.396107 \tValidation Loss: 2.533601\n",
      "Epoch: 27705 \tTraining Loss: 1.353760 \tValidation Loss: 2.534599\n",
      "Epoch: 27706 \tTraining Loss: 1.330333 \tValidation Loss: 2.534860\n",
      "Epoch: 27707 \tTraining Loss: 1.351292 \tValidation Loss: 2.534845\n",
      "Epoch: 27708 \tTraining Loss: 1.359094 \tValidation Loss: 2.535848\n",
      "Epoch: 27709 \tTraining Loss: 1.339642 \tValidation Loss: 2.535283\n",
      "Epoch: 27710 \tTraining Loss: 1.329373 \tValidation Loss: 2.535472\n",
      "Epoch: 27711 \tTraining Loss: 1.363955 \tValidation Loss: 2.535511\n",
      "Epoch: 27712 \tTraining Loss: 1.324287 \tValidation Loss: 2.535143\n",
      "Epoch: 27713 \tTraining Loss: 1.325015 \tValidation Loss: 2.535219\n",
      "Epoch: 27714 \tTraining Loss: 1.362703 \tValidation Loss: 2.534533\n",
      "Epoch: 27715 \tTraining Loss: 1.379221 \tValidation Loss: 2.533911\n",
      "Epoch: 27716 \tTraining Loss: 1.359055 \tValidation Loss: 2.534134\n",
      "Epoch: 27717 \tTraining Loss: 1.345714 \tValidation Loss: 2.535136\n",
      "Epoch: 27718 \tTraining Loss: 1.347790 \tValidation Loss: 2.534378\n",
      "Epoch: 27719 \tTraining Loss: 1.362621 \tValidation Loss: 2.535154\n",
      "Epoch: 27720 \tTraining Loss: 1.389189 \tValidation Loss: 2.534433\n",
      "Epoch: 27721 \tTraining Loss: 1.347792 \tValidation Loss: 2.535506\n",
      "Epoch: 27722 \tTraining Loss: 1.344392 \tValidation Loss: 2.535240\n",
      "Epoch: 27723 \tTraining Loss: 1.359075 \tValidation Loss: 2.534166\n",
      "Epoch: 27724 \tTraining Loss: 1.355159 \tValidation Loss: 2.536020\n",
      "Epoch: 27725 \tTraining Loss: 1.336486 \tValidation Loss: 2.535738\n",
      "Epoch: 27726 \tTraining Loss: 1.352128 \tValidation Loss: 2.534718\n",
      "Epoch: 27727 \tTraining Loss: 1.360365 \tValidation Loss: 2.535547\n",
      "Epoch: 27728 \tTraining Loss: 1.390855 \tValidation Loss: 2.534018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27729 \tTraining Loss: 1.334592 \tValidation Loss: 2.535504\n",
      "Epoch: 27730 \tTraining Loss: 1.340505 \tValidation Loss: 2.535830\n",
      "Epoch: 27731 \tTraining Loss: 1.333091 \tValidation Loss: 2.535350\n",
      "Epoch: 27732 \tTraining Loss: 1.382226 \tValidation Loss: 2.535796\n",
      "Epoch: 27733 \tTraining Loss: 1.358979 \tValidation Loss: 2.534999\n",
      "Epoch: 27734 \tTraining Loss: 1.324678 \tValidation Loss: 2.535987\n",
      "Epoch: 27735 \tTraining Loss: 1.316973 \tValidation Loss: 2.535289\n",
      "Epoch: 27736 \tTraining Loss: 1.356588 \tValidation Loss: 2.535425\n",
      "Epoch: 27737 \tTraining Loss: 1.359987 \tValidation Loss: 2.534840\n",
      "Epoch: 27738 \tTraining Loss: 1.355379 \tValidation Loss: 2.534883\n",
      "Epoch: 27739 \tTraining Loss: 1.379548 \tValidation Loss: 2.533513\n",
      "Epoch: 27740 \tTraining Loss: 1.358089 \tValidation Loss: 2.534172\n",
      "Epoch: 27741 \tTraining Loss: 1.361233 \tValidation Loss: 2.534801\n",
      "Epoch: 27742 \tTraining Loss: 1.393992 \tValidation Loss: 2.534625\n",
      "Epoch: 27743 \tTraining Loss: 1.343441 \tValidation Loss: 2.535291\n",
      "Epoch: 27744 \tTraining Loss: 1.316539 \tValidation Loss: 2.534372\n",
      "Epoch: 27745 \tTraining Loss: 1.366815 \tValidation Loss: 2.534328\n",
      "Epoch: 27746 \tTraining Loss: 1.345437 \tValidation Loss: 2.535235\n",
      "Epoch: 27747 \tTraining Loss: 1.365368 \tValidation Loss: 2.534680\n",
      "Epoch: 27748 \tTraining Loss: 1.350852 \tValidation Loss: 2.535078\n",
      "Epoch: 27749 \tTraining Loss: 1.372912 \tValidation Loss: 2.534759\n",
      "Epoch: 27750 \tTraining Loss: 1.336470 \tValidation Loss: 2.536079\n",
      "Epoch: 27751 \tTraining Loss: 1.382748 \tValidation Loss: 2.536280\n",
      "Epoch: 27752 \tTraining Loss: 1.343208 \tValidation Loss: 2.536276\n",
      "Epoch: 27753 \tTraining Loss: 1.344305 \tValidation Loss: 2.535213\n",
      "Epoch: 27754 \tTraining Loss: 1.405590 \tValidation Loss: 2.536305\n",
      "Epoch: 27755 \tTraining Loss: 1.308101 \tValidation Loss: 2.537337\n",
      "Epoch: 27756 \tTraining Loss: 1.378695 \tValidation Loss: 2.535776\n",
      "Epoch: 27757 \tTraining Loss: 1.345255 \tValidation Loss: 2.535408\n",
      "Epoch: 27758 \tTraining Loss: 1.395102 \tValidation Loss: 2.535812\n",
      "Epoch: 27759 \tTraining Loss: 1.372741 \tValidation Loss: 2.535951\n",
      "Epoch: 27760 \tTraining Loss: 1.341402 \tValidation Loss: 2.535226\n",
      "Epoch: 27761 \tTraining Loss: 1.368348 \tValidation Loss: 2.535519\n",
      "Epoch: 27762 \tTraining Loss: 1.345201 \tValidation Loss: 2.536916\n",
      "Epoch: 27763 \tTraining Loss: 1.364626 \tValidation Loss: 2.535291\n",
      "Epoch: 27764 \tTraining Loss: 1.322298 \tValidation Loss: 2.536386\n",
      "Epoch: 27765 \tTraining Loss: 1.362019 \tValidation Loss: 2.535089\n",
      "Epoch: 27766 \tTraining Loss: 1.402694 \tValidation Loss: 2.534799\n",
      "Epoch: 27767 \tTraining Loss: 1.389509 \tValidation Loss: 2.533850\n",
      "Epoch: 27768 \tTraining Loss: 1.359670 \tValidation Loss: 2.534703\n",
      "Epoch: 27769 \tTraining Loss: 1.356394 \tValidation Loss: 2.534979\n",
      "Epoch: 27770 \tTraining Loss: 1.352492 \tValidation Loss: 2.535041\n",
      "Epoch: 27771 \tTraining Loss: 1.360754 \tValidation Loss: 2.536335\n",
      "Epoch: 27772 \tTraining Loss: 1.334027 \tValidation Loss: 2.536337\n",
      "Epoch: 27773 \tTraining Loss: 1.352684 \tValidation Loss: 2.535615\n",
      "Epoch: 27774 \tTraining Loss: 1.372956 \tValidation Loss: 2.535043\n",
      "Epoch: 27775 \tTraining Loss: 1.371164 \tValidation Loss: 2.535866\n",
      "Epoch: 27776 \tTraining Loss: 1.417493 \tValidation Loss: 2.535131\n",
      "Epoch: 27777 \tTraining Loss: 1.385157 \tValidation Loss: 2.535583\n",
      "Epoch: 27778 \tTraining Loss: 1.366685 \tValidation Loss: 2.535411\n",
      "Epoch: 27779 \tTraining Loss: 1.338980 \tValidation Loss: 2.535816\n",
      "Epoch: 27780 \tTraining Loss: 1.391127 \tValidation Loss: 2.535025\n",
      "Epoch: 27781 \tTraining Loss: 1.359421 \tValidation Loss: 2.535232\n",
      "Epoch: 27782 \tTraining Loss: 1.380653 \tValidation Loss: 2.535968\n",
      "Epoch: 27783 \tTraining Loss: 1.419949 \tValidation Loss: 2.535479\n",
      "Epoch: 27784 \tTraining Loss: 1.398425 \tValidation Loss: 2.534689\n",
      "Epoch: 27785 \tTraining Loss: 1.328786 \tValidation Loss: 2.536005\n",
      "Epoch: 27786 \tTraining Loss: 1.362160 \tValidation Loss: 2.535709\n",
      "Epoch: 27787 \tTraining Loss: 1.366516 \tValidation Loss: 2.536498\n",
      "Epoch: 27788 \tTraining Loss: 1.376667 \tValidation Loss: 2.535856\n",
      "Epoch: 27789 \tTraining Loss: 1.339186 \tValidation Loss: 2.536818\n",
      "Epoch: 27790 \tTraining Loss: 1.373887 \tValidation Loss: 2.537254\n",
      "Epoch: 27791 \tTraining Loss: 1.357858 \tValidation Loss: 2.536982\n",
      "Epoch: 27792 \tTraining Loss: 1.389218 \tValidation Loss: 2.536555\n",
      "Epoch: 27793 \tTraining Loss: 1.351450 \tValidation Loss: 2.537443\n",
      "Epoch: 27794 \tTraining Loss: 1.391757 \tValidation Loss: 2.536625\n",
      "Epoch: 27795 \tTraining Loss: 1.333076 \tValidation Loss: 2.536447\n",
      "Epoch: 27796 \tTraining Loss: 1.323831 \tValidation Loss: 2.537443\n",
      "Epoch: 27797 \tTraining Loss: 1.334838 \tValidation Loss: 2.537750\n",
      "Epoch: 27798 \tTraining Loss: 1.395834 \tValidation Loss: 2.536224\n",
      "Epoch: 27799 \tTraining Loss: 1.389399 \tValidation Loss: 2.536245\n",
      "Epoch: 27800 \tTraining Loss: 1.382613 \tValidation Loss: 2.536556\n",
      "Epoch: 27801 \tTraining Loss: 1.346035 \tValidation Loss: 2.537091\n",
      "Epoch: 27802 \tTraining Loss: 1.379610 \tValidation Loss: 2.536757\n",
      "Epoch: 27803 \tTraining Loss: 1.298105 \tValidation Loss: 2.536170\n",
      "Epoch: 27804 \tTraining Loss: 1.318503 \tValidation Loss: 2.536621\n",
      "Epoch: 27805 \tTraining Loss: 1.336845 \tValidation Loss: 2.536881\n",
      "Epoch: 27806 \tTraining Loss: 1.381989 \tValidation Loss: 2.537578\n",
      "Epoch: 27807 \tTraining Loss: 1.334897 \tValidation Loss: 2.536226\n",
      "Epoch: 27808 \tTraining Loss: 1.358121 \tValidation Loss: 2.536939\n",
      "Epoch: 27809 \tTraining Loss: 1.417982 \tValidation Loss: 2.536054\n",
      "Epoch: 27810 \tTraining Loss: 1.375389 \tValidation Loss: 2.536769\n",
      "Epoch: 27811 \tTraining Loss: 1.359894 \tValidation Loss: 2.537414\n",
      "Epoch: 27812 \tTraining Loss: 1.370579 \tValidation Loss: 2.536910\n",
      "Epoch: 27813 \tTraining Loss: 1.330781 \tValidation Loss: 2.536958\n",
      "Epoch: 27814 \tTraining Loss: 1.339710 \tValidation Loss: 2.537092\n",
      "Epoch: 27815 \tTraining Loss: 1.389932 \tValidation Loss: 2.535255\n",
      "Epoch: 27816 \tTraining Loss: 1.365402 \tValidation Loss: 2.535056\n",
      "Epoch: 27817 \tTraining Loss: 1.386546 \tValidation Loss: 2.536729\n",
      "Epoch: 27818 \tTraining Loss: 1.353891 \tValidation Loss: 2.536508\n",
      "Epoch: 27819 \tTraining Loss: 1.359469 \tValidation Loss: 2.537292\n",
      "Epoch: 27820 \tTraining Loss: 1.363599 \tValidation Loss: 2.536953\n",
      "Epoch: 27821 \tTraining Loss: 1.336849 \tValidation Loss: 2.536685\n",
      "Epoch: 27822 \tTraining Loss: 1.325418 \tValidation Loss: 2.537349\n",
      "Epoch: 27823 \tTraining Loss: 1.355562 \tValidation Loss: 2.537664\n",
      "Epoch: 27824 \tTraining Loss: 1.382776 \tValidation Loss: 2.536759\n",
      "Epoch: 27825 \tTraining Loss: 1.355644 \tValidation Loss: 2.536531\n",
      "Epoch: 27826 \tTraining Loss: 1.343128 \tValidation Loss: 2.536846\n",
      "Epoch: 27827 \tTraining Loss: 1.349148 \tValidation Loss: 2.537425\n",
      "Epoch: 27828 \tTraining Loss: 1.354302 \tValidation Loss: 2.537402\n",
      "Epoch: 27829 \tTraining Loss: 1.336352 \tValidation Loss: 2.537057\n",
      "Epoch: 27830 \tTraining Loss: 1.358878 \tValidation Loss: 2.536431\n",
      "Epoch: 27831 \tTraining Loss: 1.349745 \tValidation Loss: 2.538221\n",
      "Epoch: 27832 \tTraining Loss: 1.316988 \tValidation Loss: 2.537432\n",
      "Epoch: 27833 \tTraining Loss: 1.354386 \tValidation Loss: 2.537388\n",
      "Epoch: 27834 \tTraining Loss: 1.408916 \tValidation Loss: 2.536808\n",
      "Epoch: 27835 \tTraining Loss: 1.365764 \tValidation Loss: 2.536681\n",
      "Epoch: 27836 \tTraining Loss: 1.338490 \tValidation Loss: 2.536036\n",
      "Epoch: 27837 \tTraining Loss: 1.349672 \tValidation Loss: 2.536746\n",
      "Epoch: 27838 \tTraining Loss: 1.384736 \tValidation Loss: 2.536670\n",
      "Epoch: 27839 \tTraining Loss: 1.324180 \tValidation Loss: 2.536548\n",
      "Epoch: 27840 \tTraining Loss: 1.320780 \tValidation Loss: 2.538085\n",
      "Epoch: 27841 \tTraining Loss: 1.358383 \tValidation Loss: 2.536791\n",
      "Epoch: 27842 \tTraining Loss: 1.367348 \tValidation Loss: 2.536279\n",
      "Epoch: 27843 \tTraining Loss: 1.335201 \tValidation Loss: 2.536811\n",
      "Epoch: 27844 \tTraining Loss: 1.381307 \tValidation Loss: 2.537444\n",
      "Epoch: 27845 \tTraining Loss: 1.380734 \tValidation Loss: 2.537840\n",
      "Epoch: 27846 \tTraining Loss: 1.372237 \tValidation Loss: 2.537524\n",
      "Epoch: 27847 \tTraining Loss: 1.383694 \tValidation Loss: 2.537713\n",
      "Epoch: 27848 \tTraining Loss: 1.359022 \tValidation Loss: 2.537806\n",
      "Epoch: 27849 \tTraining Loss: 1.339487 \tValidation Loss: 2.538745\n",
      "Epoch: 27850 \tTraining Loss: 1.369722 \tValidation Loss: 2.537852\n",
      "Epoch: 27851 \tTraining Loss: 1.399396 \tValidation Loss: 2.535348\n",
      "Epoch: 27852 \tTraining Loss: 1.367050 \tValidation Loss: 2.536765\n",
      "Epoch: 27853 \tTraining Loss: 1.278668 \tValidation Loss: 2.537550\n",
      "Epoch: 27854 \tTraining Loss: 1.350040 \tValidation Loss: 2.537515\n",
      "Epoch: 27855 \tTraining Loss: 1.352369 \tValidation Loss: 2.536811\n",
      "Epoch: 27856 \tTraining Loss: 1.365121 \tValidation Loss: 2.535800\n",
      "Epoch: 27857 \tTraining Loss: 1.266270 \tValidation Loss: 2.538194\n",
      "Epoch: 27858 \tTraining Loss: 1.358306 \tValidation Loss: 2.537152\n",
      "Epoch: 27859 \tTraining Loss: 1.370236 \tValidation Loss: 2.538433\n",
      "Epoch: 27860 \tTraining Loss: 1.325432 \tValidation Loss: 2.538075\n",
      "Epoch: 27861 \tTraining Loss: 1.383388 \tValidation Loss: 2.538366\n",
      "Epoch: 27862 \tTraining Loss: 1.332693 \tValidation Loss: 2.538018\n",
      "Epoch: 27863 \tTraining Loss: 1.365186 \tValidation Loss: 2.537675\n",
      "Epoch: 27864 \tTraining Loss: 1.361580 \tValidation Loss: 2.537484\n",
      "Epoch: 27865 \tTraining Loss: 1.324922 \tValidation Loss: 2.538047\n",
      "Epoch: 27866 \tTraining Loss: 1.365875 \tValidation Loss: 2.538606\n",
      "Epoch: 27867 \tTraining Loss: 1.364340 \tValidation Loss: 2.537946\n",
      "Epoch: 27868 \tTraining Loss: 1.436883 \tValidation Loss: 2.537733\n",
      "Epoch: 27869 \tTraining Loss: 1.368723 \tValidation Loss: 2.537542\n",
      "Epoch: 27870 \tTraining Loss: 1.403914 \tValidation Loss: 2.537050\n",
      "Epoch: 27871 \tTraining Loss: 1.357602 \tValidation Loss: 2.536815\n",
      "Epoch: 27872 \tTraining Loss: 1.363428 \tValidation Loss: 2.537658\n",
      "Epoch: 27873 \tTraining Loss: 1.396214 \tValidation Loss: 2.536294\n",
      "Epoch: 27874 \tTraining Loss: 1.311967 \tValidation Loss: 2.537538\n",
      "Epoch: 27875 \tTraining Loss: 1.356151 \tValidation Loss: 2.538890\n",
      "Epoch: 27876 \tTraining Loss: 1.367490 \tValidation Loss: 2.537621\n",
      "Epoch: 27877 \tTraining Loss: 1.351363 \tValidation Loss: 2.538214\n",
      "Epoch: 27878 \tTraining Loss: 1.368629 \tValidation Loss: 2.537810\n",
      "Epoch: 27879 \tTraining Loss: 1.379242 \tValidation Loss: 2.538514\n",
      "Epoch: 27880 \tTraining Loss: 1.347254 \tValidation Loss: 2.538002\n",
      "Epoch: 27881 \tTraining Loss: 1.358575 \tValidation Loss: 2.538152\n",
      "Epoch: 27882 \tTraining Loss: 1.337791 \tValidation Loss: 2.538329\n",
      "Epoch: 27883 \tTraining Loss: 1.392805 \tValidation Loss: 2.537449\n",
      "Epoch: 27884 \tTraining Loss: 1.372017 \tValidation Loss: 2.538194\n",
      "Epoch: 27885 \tTraining Loss: 1.347151 \tValidation Loss: 2.537236\n",
      "Epoch: 27886 \tTraining Loss: 1.360336 \tValidation Loss: 2.537901\n",
      "Epoch: 27887 \tTraining Loss: 1.396808 \tValidation Loss: 2.537770\n",
      "Epoch: 27888 \tTraining Loss: 1.366402 \tValidation Loss: 2.538023\n",
      "Epoch: 27889 \tTraining Loss: 1.376501 \tValidation Loss: 2.537176\n",
      "Epoch: 27890 \tTraining Loss: 1.330212 \tValidation Loss: 2.538186\n",
      "Epoch: 27891 \tTraining Loss: 1.392401 \tValidation Loss: 2.537222\n",
      "Epoch: 27892 \tTraining Loss: 1.364080 \tValidation Loss: 2.536999\n",
      "Epoch: 27893 \tTraining Loss: 1.322255 \tValidation Loss: 2.537851\n",
      "Epoch: 27894 \tTraining Loss: 1.321605 \tValidation Loss: 2.537604\n",
      "Epoch: 27895 \tTraining Loss: 1.392943 \tValidation Loss: 2.538630\n",
      "Epoch: 27896 \tTraining Loss: 1.361403 \tValidation Loss: 2.537990\n",
      "Epoch: 27897 \tTraining Loss: 1.372712 \tValidation Loss: 2.537646\n",
      "Epoch: 27898 \tTraining Loss: 1.306556 \tValidation Loss: 2.538817\n",
      "Epoch: 27899 \tTraining Loss: 1.386522 \tValidation Loss: 2.538505\n",
      "Epoch: 27900 \tTraining Loss: 1.371265 \tValidation Loss: 2.538101\n",
      "Epoch: 27901 \tTraining Loss: 1.330401 \tValidation Loss: 2.538648\n",
      "Epoch: 27902 \tTraining Loss: 1.359093 \tValidation Loss: 2.539859\n",
      "Epoch: 27903 \tTraining Loss: 1.324255 \tValidation Loss: 2.539448\n",
      "Epoch: 27904 \tTraining Loss: 1.352248 \tValidation Loss: 2.538148\n",
      "Epoch: 27905 \tTraining Loss: 1.330487 \tValidation Loss: 2.538443\n",
      "Epoch: 27906 \tTraining Loss: 1.325836 \tValidation Loss: 2.538725\n",
      "Epoch: 27907 \tTraining Loss: 1.325278 \tValidation Loss: 2.539160\n",
      "Epoch: 27908 \tTraining Loss: 1.301325 \tValidation Loss: 2.538418\n",
      "Epoch: 27909 \tTraining Loss: 1.366828 \tValidation Loss: 2.537614\n",
      "Epoch: 27910 \tTraining Loss: 1.362168 \tValidation Loss: 2.537085\n",
      "Epoch: 27911 \tTraining Loss: 1.350920 \tValidation Loss: 2.537914\n",
      "Epoch: 27912 \tTraining Loss: 1.369070 \tValidation Loss: 2.538487\n",
      "Epoch: 27913 \tTraining Loss: 1.347477 \tValidation Loss: 2.538761\n",
      "Epoch: 27914 \tTraining Loss: 1.365017 \tValidation Loss: 2.538911\n",
      "Epoch: 27915 \tTraining Loss: 1.326869 \tValidation Loss: 2.540187\n",
      "Epoch: 27916 \tTraining Loss: 1.347246 \tValidation Loss: 2.539246\n",
      "Epoch: 27917 \tTraining Loss: 1.350198 \tValidation Loss: 2.537696\n",
      "Epoch: 27918 \tTraining Loss: 1.368990 \tValidation Loss: 2.538146\n",
      "Epoch: 27919 \tTraining Loss: 1.323301 \tValidation Loss: 2.538623\n",
      "Epoch: 27920 \tTraining Loss: 1.381212 \tValidation Loss: 2.539131\n",
      "Epoch: 27921 \tTraining Loss: 1.344364 \tValidation Loss: 2.538743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27922 \tTraining Loss: 1.352014 \tValidation Loss: 2.538604\n",
      "Epoch: 27923 \tTraining Loss: 1.342303 \tValidation Loss: 2.538806\n",
      "Epoch: 27924 \tTraining Loss: 1.353064 \tValidation Loss: 2.538905\n",
      "Epoch: 27925 \tTraining Loss: 1.370416 \tValidation Loss: 2.540027\n",
      "Epoch: 27926 \tTraining Loss: 1.386290 \tValidation Loss: 2.538760\n",
      "Epoch: 27927 \tTraining Loss: 1.363795 \tValidation Loss: 2.538503\n",
      "Epoch: 27928 \tTraining Loss: 1.371123 \tValidation Loss: 2.537597\n",
      "Epoch: 27929 \tTraining Loss: 1.349429 \tValidation Loss: 2.539070\n",
      "Epoch: 27930 \tTraining Loss: 1.365838 \tValidation Loss: 2.538950\n",
      "Epoch: 27931 \tTraining Loss: 1.326383 \tValidation Loss: 2.538746\n",
      "Epoch: 27932 \tTraining Loss: 1.350132 \tValidation Loss: 2.539231\n",
      "Epoch: 27933 \tTraining Loss: 1.346373 \tValidation Loss: 2.538268\n",
      "Epoch: 27934 \tTraining Loss: 1.344969 \tValidation Loss: 2.538583\n",
      "Epoch: 27935 \tTraining Loss: 1.366339 \tValidation Loss: 2.538586\n",
      "Epoch: 27936 \tTraining Loss: 1.391004 \tValidation Loss: 2.538748\n",
      "Epoch: 27937 \tTraining Loss: 1.342363 \tValidation Loss: 2.538824\n",
      "Epoch: 27938 \tTraining Loss: 1.324765 \tValidation Loss: 2.538562\n",
      "Epoch: 27939 \tTraining Loss: 1.379346 \tValidation Loss: 2.539327\n",
      "Epoch: 27940 \tTraining Loss: 1.394436 \tValidation Loss: 2.538513\n",
      "Epoch: 27941 \tTraining Loss: 1.389552 \tValidation Loss: 2.538807\n",
      "Epoch: 27942 \tTraining Loss: 1.400738 \tValidation Loss: 2.538999\n",
      "Epoch: 27943 \tTraining Loss: 1.394711 \tValidation Loss: 2.538696\n",
      "Epoch: 27944 \tTraining Loss: 1.350537 \tValidation Loss: 2.538908\n",
      "Epoch: 27945 \tTraining Loss: 1.339921 \tValidation Loss: 2.538095\n",
      "Epoch: 27946 \tTraining Loss: 1.346789 \tValidation Loss: 2.538861\n",
      "Epoch: 27947 \tTraining Loss: 1.358495 \tValidation Loss: 2.537980\n",
      "Epoch: 27948 \tTraining Loss: 1.383314 \tValidation Loss: 2.538068\n",
      "Epoch: 27949 \tTraining Loss: 1.365422 \tValidation Loss: 2.538495\n",
      "Epoch: 27950 \tTraining Loss: 1.348822 \tValidation Loss: 2.538836\n",
      "Epoch: 27951 \tTraining Loss: 1.385872 \tValidation Loss: 2.538227\n",
      "Epoch: 27952 \tTraining Loss: 1.356866 \tValidation Loss: 2.538538\n",
      "Epoch: 27953 \tTraining Loss: 1.382386 \tValidation Loss: 2.537520\n",
      "Epoch: 27954 \tTraining Loss: 1.374261 \tValidation Loss: 2.538207\n",
      "Epoch: 27955 \tTraining Loss: 1.347106 \tValidation Loss: 2.538515\n",
      "Epoch: 27956 \tTraining Loss: 1.328233 \tValidation Loss: 2.538812\n",
      "Epoch: 27957 \tTraining Loss: 1.381608 \tValidation Loss: 2.537959\n",
      "Epoch: 27958 \tTraining Loss: 1.291567 \tValidation Loss: 2.539568\n",
      "Epoch: 27959 \tTraining Loss: 1.318770 \tValidation Loss: 2.540471\n",
      "Epoch: 27960 \tTraining Loss: 1.339144 \tValidation Loss: 2.539811\n",
      "Epoch: 27961 \tTraining Loss: 1.375442 \tValidation Loss: 2.539437\n",
      "Epoch: 27962 \tTraining Loss: 1.325327 \tValidation Loss: 2.539236\n",
      "Epoch: 27963 \tTraining Loss: 1.320266 \tValidation Loss: 2.539841\n",
      "Epoch: 27964 \tTraining Loss: 1.403700 \tValidation Loss: 2.538935\n",
      "Epoch: 27965 \tTraining Loss: 1.382162 \tValidation Loss: 2.539991\n",
      "Epoch: 27966 \tTraining Loss: 1.356144 \tValidation Loss: 2.539630\n",
      "Epoch: 27967 \tTraining Loss: 1.344032 \tValidation Loss: 2.539540\n",
      "Epoch: 27968 \tTraining Loss: 1.428414 \tValidation Loss: 2.538616\n",
      "Epoch: 27969 \tTraining Loss: 1.395661 \tValidation Loss: 2.538628\n",
      "Epoch: 27970 \tTraining Loss: 1.363447 \tValidation Loss: 2.540240\n",
      "Epoch: 27971 \tTraining Loss: 1.367197 \tValidation Loss: 2.539153\n",
      "Epoch: 27972 \tTraining Loss: 1.392382 \tValidation Loss: 2.539617\n",
      "Epoch: 27973 \tTraining Loss: 1.350019 \tValidation Loss: 2.539898\n",
      "Epoch: 27974 \tTraining Loss: 1.312390 \tValidation Loss: 2.540134\n",
      "Epoch: 27975 \tTraining Loss: 1.360024 \tValidation Loss: 2.540020\n",
      "Epoch: 27976 \tTraining Loss: 1.420397 \tValidation Loss: 2.539548\n",
      "Epoch: 27977 \tTraining Loss: 1.357046 \tValidation Loss: 2.539855\n",
      "Epoch: 27978 \tTraining Loss: 1.346981 \tValidation Loss: 2.539437\n",
      "Epoch: 27979 \tTraining Loss: 1.336299 \tValidation Loss: 2.539863\n",
      "Epoch: 27980 \tTraining Loss: 1.352533 \tValidation Loss: 2.539547\n",
      "Epoch: 27981 \tTraining Loss: 1.340263 \tValidation Loss: 2.539587\n",
      "Epoch: 27982 \tTraining Loss: 1.372523 \tValidation Loss: 2.540484\n",
      "Epoch: 27983 \tTraining Loss: 1.341291 \tValidation Loss: 2.540927\n",
      "Epoch: 27984 \tTraining Loss: 1.330500 \tValidation Loss: 2.542240\n",
      "Epoch: 27985 \tTraining Loss: 1.390350 \tValidation Loss: 2.539913\n",
      "Epoch: 27986 \tTraining Loss: 1.336813 \tValidation Loss: 2.540297\n",
      "Epoch: 27987 \tTraining Loss: 1.331088 \tValidation Loss: 2.539905\n",
      "Epoch: 27988 \tTraining Loss: 1.359452 \tValidation Loss: 2.540037\n",
      "Epoch: 27989 \tTraining Loss: 1.378391 \tValidation Loss: 2.539859\n",
      "Epoch: 27990 \tTraining Loss: 1.344476 \tValidation Loss: 2.539988\n",
      "Epoch: 27991 \tTraining Loss: 1.349293 \tValidation Loss: 2.540368\n",
      "Epoch: 27992 \tTraining Loss: 1.320972 \tValidation Loss: 2.540462\n",
      "Epoch: 27993 \tTraining Loss: 1.421054 \tValidation Loss: 2.539674\n",
      "Epoch: 27994 \tTraining Loss: 1.345862 \tValidation Loss: 2.540146\n",
      "Epoch: 27995 \tTraining Loss: 1.327919 \tValidation Loss: 2.540359\n",
      "Epoch: 27996 \tTraining Loss: 1.304841 \tValidation Loss: 2.540766\n",
      "Epoch: 27997 \tTraining Loss: 1.345460 \tValidation Loss: 2.540121\n",
      "Epoch: 27998 \tTraining Loss: 1.359261 \tValidation Loss: 2.540964\n",
      "Epoch: 27999 \tTraining Loss: 1.353131 \tValidation Loss: 2.541363\n",
      "Epoch: 28000 \tTraining Loss: 1.366592 \tValidation Loss: 2.539372\n",
      "Epoch: 28001 \tTraining Loss: 1.331183 \tValidation Loss: 2.541028\n",
      "Epoch: 28002 \tTraining Loss: 1.373584 \tValidation Loss: 2.541138\n",
      "Epoch: 28003 \tTraining Loss: 1.297767 \tValidation Loss: 2.541306\n",
      "Epoch: 28004 \tTraining Loss: 1.360475 \tValidation Loss: 2.540928\n",
      "Epoch: 28005 \tTraining Loss: 1.353939 \tValidation Loss: 2.540506\n",
      "Epoch: 28006 \tTraining Loss: 1.352149 \tValidation Loss: 2.541350\n",
      "Epoch: 28007 \tTraining Loss: 1.334633 \tValidation Loss: 2.541983\n",
      "Epoch: 28008 \tTraining Loss: 1.341899 \tValidation Loss: 2.540894\n",
      "Epoch: 28009 \tTraining Loss: 1.385615 \tValidation Loss: 2.541095\n",
      "Epoch: 28010 \tTraining Loss: 1.347518 \tValidation Loss: 2.540484\n",
      "Epoch: 28011 \tTraining Loss: 1.375077 \tValidation Loss: 2.540322\n",
      "Epoch: 28012 \tTraining Loss: 1.347332 \tValidation Loss: 2.541427\n",
      "Epoch: 28013 \tTraining Loss: 1.354855 \tValidation Loss: 2.541438\n",
      "Epoch: 28014 \tTraining Loss: 1.328588 \tValidation Loss: 2.541606\n",
      "Epoch: 28015 \tTraining Loss: 1.330512 \tValidation Loss: 2.541271\n",
      "Epoch: 28016 \tTraining Loss: 1.341552 \tValidation Loss: 2.540839\n",
      "Epoch: 28017 \tTraining Loss: 1.321959 \tValidation Loss: 2.541785\n",
      "Epoch: 28018 \tTraining Loss: 1.351480 \tValidation Loss: 2.541945\n",
      "Epoch: 28019 \tTraining Loss: 1.382502 \tValidation Loss: 2.540921\n",
      "Epoch: 28020 \tTraining Loss: 1.300136 \tValidation Loss: 2.541774\n",
      "Epoch: 28021 \tTraining Loss: 1.330761 \tValidation Loss: 2.541161\n",
      "Epoch: 28022 \tTraining Loss: 1.339349 \tValidation Loss: 2.541274\n",
      "Epoch: 28023 \tTraining Loss: 1.339325 \tValidation Loss: 2.541915\n",
      "Epoch: 28024 \tTraining Loss: 1.329357 \tValidation Loss: 2.542431\n",
      "Epoch: 28025 \tTraining Loss: 1.346234 \tValidation Loss: 2.541121\n",
      "Epoch: 28026 \tTraining Loss: 1.359844 \tValidation Loss: 2.541087\n",
      "Epoch: 28027 \tTraining Loss: 1.361685 \tValidation Loss: 2.541246\n",
      "Epoch: 28028 \tTraining Loss: 1.361798 \tValidation Loss: 2.540567\n",
      "Epoch: 28029 \tTraining Loss: 1.345243 \tValidation Loss: 2.542058\n",
      "Epoch: 28030 \tTraining Loss: 1.356159 \tValidation Loss: 2.541122\n",
      "Epoch: 28031 \tTraining Loss: 1.362224 \tValidation Loss: 2.541231\n",
      "Epoch: 28032 \tTraining Loss: 1.343035 \tValidation Loss: 2.541444\n",
      "Epoch: 28033 \tTraining Loss: 1.299627 \tValidation Loss: 2.541663\n",
      "Epoch: 28034 \tTraining Loss: 1.366755 \tValidation Loss: 2.540843\n",
      "Epoch: 28035 \tTraining Loss: 1.332391 \tValidation Loss: 2.541756\n",
      "Epoch: 28036 \tTraining Loss: 1.368242 \tValidation Loss: 2.541130\n",
      "Epoch: 28037 \tTraining Loss: 1.372353 \tValidation Loss: 2.540595\n",
      "Epoch: 28038 \tTraining Loss: 1.383476 \tValidation Loss: 2.541279\n",
      "Epoch: 28039 \tTraining Loss: 1.345725 \tValidation Loss: 2.541161\n",
      "Epoch: 28040 \tTraining Loss: 1.366607 \tValidation Loss: 2.540243\n",
      "Epoch: 28041 \tTraining Loss: 1.345221 \tValidation Loss: 2.541996\n",
      "Epoch: 28042 \tTraining Loss: 1.330052 \tValidation Loss: 2.541633\n",
      "Epoch: 28043 \tTraining Loss: 1.353626 \tValidation Loss: 2.541651\n",
      "Epoch: 28044 \tTraining Loss: 1.378630 \tValidation Loss: 2.541256\n",
      "Epoch: 28045 \tTraining Loss: 1.352340 \tValidation Loss: 2.540327\n",
      "Epoch: 28046 \tTraining Loss: 1.345683 \tValidation Loss: 2.541183\n",
      "Epoch: 28047 \tTraining Loss: 1.357720 \tValidation Loss: 2.541332\n",
      "Epoch: 28048 \tTraining Loss: 1.345139 \tValidation Loss: 2.541343\n",
      "Epoch: 28049 \tTraining Loss: 1.366244 \tValidation Loss: 2.541863\n",
      "Epoch: 28050 \tTraining Loss: 1.350216 \tValidation Loss: 2.542755\n",
      "Epoch: 28051 \tTraining Loss: 1.329520 \tValidation Loss: 2.541308\n",
      "Epoch: 28052 \tTraining Loss: 1.382194 \tValidation Loss: 2.541838\n",
      "Epoch: 28053 \tTraining Loss: 1.357886 \tValidation Loss: 2.542069\n",
      "Epoch: 28054 \tTraining Loss: 1.337103 \tValidation Loss: 2.542370\n",
      "Epoch: 28055 \tTraining Loss: 1.390779 \tValidation Loss: 2.541825\n",
      "Epoch: 28056 \tTraining Loss: 1.341071 \tValidation Loss: 2.541285\n",
      "Epoch: 28057 \tTraining Loss: 1.354274 \tValidation Loss: 2.542108\n",
      "Epoch: 28058 \tTraining Loss: 1.344795 \tValidation Loss: 2.542261\n",
      "Epoch: 28059 \tTraining Loss: 1.354828 \tValidation Loss: 2.542407\n",
      "Epoch: 28060 \tTraining Loss: 1.337495 \tValidation Loss: 2.543491\n",
      "Epoch: 28061 \tTraining Loss: 1.329588 \tValidation Loss: 2.543478\n",
      "Epoch: 28062 \tTraining Loss: 1.347970 \tValidation Loss: 2.541773\n",
      "Epoch: 28063 \tTraining Loss: 1.331311 \tValidation Loss: 2.543454\n",
      "Epoch: 28064 \tTraining Loss: 1.385293 \tValidation Loss: 2.542883\n",
      "Epoch: 28065 \tTraining Loss: 1.337772 \tValidation Loss: 2.540774\n",
      "Epoch: 28066 \tTraining Loss: 1.367816 \tValidation Loss: 2.541689\n",
      "Epoch: 28067 \tTraining Loss: 1.365288 \tValidation Loss: 2.542136\n",
      "Epoch: 28068 \tTraining Loss: 1.327341 \tValidation Loss: 2.543031\n",
      "Epoch: 28069 \tTraining Loss: 1.343992 \tValidation Loss: 2.541864\n",
      "Epoch: 28070 \tTraining Loss: 1.327680 \tValidation Loss: 2.543893\n",
      "Epoch: 28071 \tTraining Loss: 1.365065 \tValidation Loss: 2.542045\n",
      "Epoch: 28072 \tTraining Loss: 1.351498 \tValidation Loss: 2.543382\n",
      "Epoch: 28073 \tTraining Loss: 1.372762 \tValidation Loss: 2.541800\n",
      "Epoch: 28074 \tTraining Loss: 1.358840 \tValidation Loss: 2.542200\n",
      "Epoch: 28075 \tTraining Loss: 1.351590 \tValidation Loss: 2.542135\n",
      "Epoch: 28076 \tTraining Loss: 1.361617 \tValidation Loss: 2.542179\n",
      "Epoch: 28077 \tTraining Loss: 1.388265 \tValidation Loss: 2.541638\n",
      "Epoch: 28078 \tTraining Loss: 1.377129 \tValidation Loss: 2.542422\n",
      "Epoch: 28079 \tTraining Loss: 1.348459 \tValidation Loss: 2.542131\n",
      "Epoch: 28080 \tTraining Loss: 1.309639 \tValidation Loss: 2.544049\n",
      "Epoch: 28081 \tTraining Loss: 1.366377 \tValidation Loss: 2.541921\n",
      "Epoch: 28082 \tTraining Loss: 1.358048 \tValidation Loss: 2.543224\n",
      "Epoch: 28083 \tTraining Loss: 1.317819 \tValidation Loss: 2.542401\n",
      "Epoch: 28084 \tTraining Loss: 1.348132 \tValidation Loss: 2.542124\n",
      "Epoch: 28085 \tTraining Loss: 1.318370 \tValidation Loss: 2.542351\n",
      "Epoch: 28086 \tTraining Loss: 1.343611 \tValidation Loss: 2.542192\n",
      "Epoch: 28087 \tTraining Loss: 1.340430 \tValidation Loss: 2.542561\n",
      "Epoch: 28088 \tTraining Loss: 1.361213 \tValidation Loss: 2.542403\n",
      "Epoch: 28089 \tTraining Loss: 1.378444 \tValidation Loss: 2.542150\n",
      "Epoch: 28090 \tTraining Loss: 1.355097 \tValidation Loss: 2.542198\n",
      "Epoch: 28091 \tTraining Loss: 1.345453 \tValidation Loss: 2.542704\n",
      "Epoch: 28092 \tTraining Loss: 1.352272 \tValidation Loss: 2.542677\n",
      "Epoch: 28093 \tTraining Loss: 1.377415 \tValidation Loss: 2.542383\n",
      "Epoch: 28094 \tTraining Loss: 1.384413 \tValidation Loss: 2.542170\n",
      "Epoch: 28095 \tTraining Loss: 1.299711 \tValidation Loss: 2.543248\n",
      "Epoch: 28096 \tTraining Loss: 1.349158 \tValidation Loss: 2.542112\n",
      "Epoch: 28097 \tTraining Loss: 1.363098 \tValidation Loss: 2.542385\n",
      "Epoch: 28098 \tTraining Loss: 1.294611 \tValidation Loss: 2.543365\n",
      "Epoch: 28099 \tTraining Loss: 1.330436 \tValidation Loss: 2.543356\n",
      "Epoch: 28100 \tTraining Loss: 1.392829 \tValidation Loss: 2.542529\n",
      "Epoch: 28101 \tTraining Loss: 1.367374 \tValidation Loss: 2.542836\n",
      "Epoch: 28102 \tTraining Loss: 1.358272 \tValidation Loss: 2.542643\n",
      "Epoch: 28103 \tTraining Loss: 1.311884 \tValidation Loss: 2.542274\n",
      "Epoch: 28104 \tTraining Loss: 1.347371 \tValidation Loss: 2.542263\n",
      "Epoch: 28105 \tTraining Loss: 1.347720 \tValidation Loss: 2.543172\n",
      "Epoch: 28106 \tTraining Loss: 1.333485 \tValidation Loss: 2.544234\n",
      "Epoch: 28107 \tTraining Loss: 1.272331 \tValidation Loss: 2.543573\n",
      "Epoch: 28108 \tTraining Loss: 1.341731 \tValidation Loss: 2.545158\n",
      "Epoch: 28109 \tTraining Loss: 1.333000 \tValidation Loss: 2.543642\n",
      "Epoch: 28110 \tTraining Loss: 1.383832 \tValidation Loss: 2.541781\n",
      "Epoch: 28111 \tTraining Loss: 1.388229 \tValidation Loss: 2.542401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28112 \tTraining Loss: 1.332886 \tValidation Loss: 2.543037\n",
      "Epoch: 28113 \tTraining Loss: 1.346142 \tValidation Loss: 2.542802\n",
      "Epoch: 28114 \tTraining Loss: 1.345244 \tValidation Loss: 2.542392\n",
      "Epoch: 28115 \tTraining Loss: 1.347794 \tValidation Loss: 2.543029\n",
      "Epoch: 28116 \tTraining Loss: 1.341594 \tValidation Loss: 2.542552\n",
      "Epoch: 28117 \tTraining Loss: 1.352156 \tValidation Loss: 2.542781\n",
      "Epoch: 28118 \tTraining Loss: 1.371123 \tValidation Loss: 2.542594\n",
      "Epoch: 28119 \tTraining Loss: 1.345512 \tValidation Loss: 2.543494\n",
      "Epoch: 28120 \tTraining Loss: 1.369813 \tValidation Loss: 2.543009\n",
      "Epoch: 28121 \tTraining Loss: 1.343897 \tValidation Loss: 2.542566\n",
      "Epoch: 28122 \tTraining Loss: 1.310103 \tValidation Loss: 2.543399\n",
      "Epoch: 28123 \tTraining Loss: 1.360411 \tValidation Loss: 2.542073\n",
      "Epoch: 28124 \tTraining Loss: 1.330834 \tValidation Loss: 2.543152\n",
      "Epoch: 28125 \tTraining Loss: 1.312615 \tValidation Loss: 2.542915\n",
      "Epoch: 28126 \tTraining Loss: 1.351164 \tValidation Loss: 2.543125\n",
      "Epoch: 28127 \tTraining Loss: 1.374350 \tValidation Loss: 2.541869\n",
      "Epoch: 28128 \tTraining Loss: 1.325376 \tValidation Loss: 2.543452\n",
      "Epoch: 28129 \tTraining Loss: 1.353517 \tValidation Loss: 2.543686\n",
      "Epoch: 28130 \tTraining Loss: 1.325487 \tValidation Loss: 2.544693\n",
      "Epoch: 28131 \tTraining Loss: 1.345511 \tValidation Loss: 2.543432\n",
      "Epoch: 28132 \tTraining Loss: 1.379324 \tValidation Loss: 2.543157\n",
      "Epoch: 28133 \tTraining Loss: 1.339740 \tValidation Loss: 2.544509\n",
      "Epoch: 28134 \tTraining Loss: 1.352256 \tValidation Loss: 2.543479\n",
      "Epoch: 28135 \tTraining Loss: 1.361576 \tValidation Loss: 2.543730\n",
      "Epoch: 28136 \tTraining Loss: 1.333611 \tValidation Loss: 2.543349\n",
      "Epoch: 28137 \tTraining Loss: 1.344940 \tValidation Loss: 2.543130\n",
      "Epoch: 28138 \tTraining Loss: 1.279159 \tValidation Loss: 2.544161\n",
      "Epoch: 28139 \tTraining Loss: 1.396700 \tValidation Loss: 2.543647\n",
      "Epoch: 28140 \tTraining Loss: 1.382692 \tValidation Loss: 2.542676\n",
      "Epoch: 28141 \tTraining Loss: 1.336377 \tValidation Loss: 2.543701\n",
      "Epoch: 28142 \tTraining Loss: 1.377742 \tValidation Loss: 2.543888\n",
      "Epoch: 28143 \tTraining Loss: 1.355585 \tValidation Loss: 2.542778\n",
      "Epoch: 28144 \tTraining Loss: 1.359405 \tValidation Loss: 2.543223\n",
      "Epoch: 28145 \tTraining Loss: 1.398178 \tValidation Loss: 2.541712\n",
      "Epoch: 28146 \tTraining Loss: 1.323734 \tValidation Loss: 2.543061\n",
      "Epoch: 28147 \tTraining Loss: 1.362820 \tValidation Loss: 2.541671\n",
      "Epoch: 28148 \tTraining Loss: 1.344058 \tValidation Loss: 2.544614\n",
      "Epoch: 28149 \tTraining Loss: 1.392408 \tValidation Loss: 2.543073\n",
      "Epoch: 28150 \tTraining Loss: 1.342957 \tValidation Loss: 2.543256\n",
      "Epoch: 28151 \tTraining Loss: 1.342104 \tValidation Loss: 2.544185\n",
      "Epoch: 28152 \tTraining Loss: 1.359266 \tValidation Loss: 2.544140\n",
      "Epoch: 28153 \tTraining Loss: 1.294070 \tValidation Loss: 2.545101\n",
      "Epoch: 28154 \tTraining Loss: 1.367289 \tValidation Loss: 2.544238\n",
      "Epoch: 28155 \tTraining Loss: 1.327349 \tValidation Loss: 2.543894\n",
      "Epoch: 28156 \tTraining Loss: 1.335526 \tValidation Loss: 2.543656\n",
      "Epoch: 28157 \tTraining Loss: 1.295944 \tValidation Loss: 2.543648\n",
      "Epoch: 28158 \tTraining Loss: 1.376954 \tValidation Loss: 2.542578\n",
      "Epoch: 28159 \tTraining Loss: 1.349339 \tValidation Loss: 2.543762\n",
      "Epoch: 28160 \tTraining Loss: 1.330471 \tValidation Loss: 2.544394\n",
      "Epoch: 28161 \tTraining Loss: 1.343365 \tValidation Loss: 2.544160\n",
      "Epoch: 28162 \tTraining Loss: 1.341020 \tValidation Loss: 2.543929\n",
      "Epoch: 28163 \tTraining Loss: 1.394098 \tValidation Loss: 2.542605\n",
      "Epoch: 28164 \tTraining Loss: 1.356052 \tValidation Loss: 2.543809\n",
      "Epoch: 28165 \tTraining Loss: 1.336969 \tValidation Loss: 2.543247\n",
      "Epoch: 28166 \tTraining Loss: 1.318602 \tValidation Loss: 2.543522\n",
      "Epoch: 28167 \tTraining Loss: 1.393581 \tValidation Loss: 2.544506\n",
      "Epoch: 28168 \tTraining Loss: 1.378513 \tValidation Loss: 2.544902\n",
      "Epoch: 28169 \tTraining Loss: 1.400458 \tValidation Loss: 2.543813\n",
      "Epoch: 28170 \tTraining Loss: 1.396421 \tValidation Loss: 2.543370\n",
      "Epoch: 28171 \tTraining Loss: 1.377593 \tValidation Loss: 2.543511\n",
      "Epoch: 28172 \tTraining Loss: 1.387912 \tValidation Loss: 2.543927\n",
      "Epoch: 28173 \tTraining Loss: 1.388906 \tValidation Loss: 2.543694\n",
      "Epoch: 28174 \tTraining Loss: 1.357290 \tValidation Loss: 2.544059\n",
      "Epoch: 28175 \tTraining Loss: 1.348416 \tValidation Loss: 2.544181\n",
      "Epoch: 28176 \tTraining Loss: 1.388900 \tValidation Loss: 2.544559\n",
      "Epoch: 28177 \tTraining Loss: 1.351706 \tValidation Loss: 2.544801\n",
      "Epoch: 28178 \tTraining Loss: 1.328209 \tValidation Loss: 2.544393\n",
      "Epoch: 28179 \tTraining Loss: 1.320778 \tValidation Loss: 2.545308\n",
      "Epoch: 28180 \tTraining Loss: 1.335113 \tValidation Loss: 2.545054\n",
      "Epoch: 28181 \tTraining Loss: 1.299170 \tValidation Loss: 2.545355\n",
      "Epoch: 28182 \tTraining Loss: 1.348722 \tValidation Loss: 2.544723\n",
      "Epoch: 28183 \tTraining Loss: 1.363524 \tValidation Loss: 2.545208\n",
      "Epoch: 28184 \tTraining Loss: 1.358822 \tValidation Loss: 2.543772\n",
      "Epoch: 28185 \tTraining Loss: 1.346460 \tValidation Loss: 2.545096\n",
      "Epoch: 28186 \tTraining Loss: 1.369175 \tValidation Loss: 2.543592\n",
      "Epoch: 28187 \tTraining Loss: 1.397369 \tValidation Loss: 2.543593\n",
      "Epoch: 28188 \tTraining Loss: 1.407855 \tValidation Loss: 2.545026\n",
      "Epoch: 28189 \tTraining Loss: 1.384902 \tValidation Loss: 2.544110\n",
      "Epoch: 28190 \tTraining Loss: 1.384048 \tValidation Loss: 2.544772\n",
      "Epoch: 28191 \tTraining Loss: 1.362848 \tValidation Loss: 2.544312\n",
      "Epoch: 28192 \tTraining Loss: 1.347786 \tValidation Loss: 2.545190\n",
      "Epoch: 28193 \tTraining Loss: 1.334300 \tValidation Loss: 2.543895\n",
      "Epoch: 28194 \tTraining Loss: 1.319164 \tValidation Loss: 2.545004\n",
      "Epoch: 28195 \tTraining Loss: 1.348262 \tValidation Loss: 2.544773\n",
      "Epoch: 28196 \tTraining Loss: 1.361769 \tValidation Loss: 2.545418\n",
      "Epoch: 28197 \tTraining Loss: 1.354767 \tValidation Loss: 2.546296\n",
      "Epoch: 28198 \tTraining Loss: 1.367877 \tValidation Loss: 2.546535\n",
      "Epoch: 28199 \tTraining Loss: 1.353492 \tValidation Loss: 2.543942\n",
      "Epoch: 28200 \tTraining Loss: 1.330945 \tValidation Loss: 2.545511\n",
      "Epoch: 28201 \tTraining Loss: 1.347908 \tValidation Loss: 2.545403\n",
      "Epoch: 28202 \tTraining Loss: 1.368647 \tValidation Loss: 2.544150\n",
      "Epoch: 28203 \tTraining Loss: 1.359047 \tValidation Loss: 2.544871\n",
      "Epoch: 28204 \tTraining Loss: 1.293247 \tValidation Loss: 2.547244\n",
      "Epoch: 28205 \tTraining Loss: 1.386838 \tValidation Loss: 2.544988\n",
      "Epoch: 28206 \tTraining Loss: 1.358204 \tValidation Loss: 2.544796\n",
      "Epoch: 28207 \tTraining Loss: 1.395867 \tValidation Loss: 2.544595\n",
      "Epoch: 28208 \tTraining Loss: 1.331354 \tValidation Loss: 2.545748\n",
      "Epoch: 28209 \tTraining Loss: 1.335911 \tValidation Loss: 2.544869\n",
      "Epoch: 28210 \tTraining Loss: 1.371124 \tValidation Loss: 2.546996\n",
      "Epoch: 28211 \tTraining Loss: 1.379443 \tValidation Loss: 2.544130\n",
      "Epoch: 28212 \tTraining Loss: 1.344886 \tValidation Loss: 2.544921\n",
      "Epoch: 28213 \tTraining Loss: 1.362849 \tValidation Loss: 2.544482\n",
      "Epoch: 28214 \tTraining Loss: 1.361073 \tValidation Loss: 2.544769\n",
      "Epoch: 28215 \tTraining Loss: 1.354836 \tValidation Loss: 2.545413\n",
      "Epoch: 28216 \tTraining Loss: 1.346658 \tValidation Loss: 2.546192\n",
      "Epoch: 28217 \tTraining Loss: 1.360228 \tValidation Loss: 2.546224\n",
      "Epoch: 28218 \tTraining Loss: 1.288471 \tValidation Loss: 2.546746\n",
      "Epoch: 28219 \tTraining Loss: 1.342834 \tValidation Loss: 2.546100\n",
      "Epoch: 28220 \tTraining Loss: 1.349344 \tValidation Loss: 2.546242\n",
      "Epoch: 28221 \tTraining Loss: 1.376644 \tValidation Loss: 2.545289\n",
      "Epoch: 28222 \tTraining Loss: 1.339245 \tValidation Loss: 2.544640\n",
      "Epoch: 28223 \tTraining Loss: 1.316569 \tValidation Loss: 2.546899\n",
      "Epoch: 28224 \tTraining Loss: 1.326368 \tValidation Loss: 2.545326\n",
      "Epoch: 28225 \tTraining Loss: 1.371754 \tValidation Loss: 2.545630\n",
      "Epoch: 28226 \tTraining Loss: 1.426664 \tValidation Loss: 2.544471\n",
      "Epoch: 28227 \tTraining Loss: 1.351706 \tValidation Loss: 2.545110\n",
      "Epoch: 28228 \tTraining Loss: 1.356716 \tValidation Loss: 2.545335\n",
      "Epoch: 28229 \tTraining Loss: 1.329075 \tValidation Loss: 2.545866\n",
      "Epoch: 28230 \tTraining Loss: 1.407414 \tValidation Loss: 2.546185\n",
      "Epoch: 28231 \tTraining Loss: 1.346877 \tValidation Loss: 2.545208\n",
      "Epoch: 28232 \tTraining Loss: 1.334394 \tValidation Loss: 2.545935\n",
      "Epoch: 28233 \tTraining Loss: 1.311412 \tValidation Loss: 2.545869\n",
      "Epoch: 28234 \tTraining Loss: 1.385271 \tValidation Loss: 2.546089\n",
      "Epoch: 28235 \tTraining Loss: 1.353784 \tValidation Loss: 2.545940\n",
      "Epoch: 28236 \tTraining Loss: 1.336683 \tValidation Loss: 2.545123\n",
      "Epoch: 28237 \tTraining Loss: 1.360510 \tValidation Loss: 2.544497\n",
      "Epoch: 28238 \tTraining Loss: 1.326777 \tValidation Loss: 2.544984\n",
      "Epoch: 28239 \tTraining Loss: 1.322003 \tValidation Loss: 2.546257\n",
      "Epoch: 28240 \tTraining Loss: 1.357729 \tValidation Loss: 2.546103\n",
      "Epoch: 28241 \tTraining Loss: 1.337112 \tValidation Loss: 2.545638\n",
      "Epoch: 28242 \tTraining Loss: 1.341842 \tValidation Loss: 2.546292\n",
      "Epoch: 28243 \tTraining Loss: 1.354093 \tValidation Loss: 2.544578\n",
      "Epoch: 28244 \tTraining Loss: 1.382430 \tValidation Loss: 2.545639\n",
      "Epoch: 28245 \tTraining Loss: 1.369401 \tValidation Loss: 2.545036\n",
      "Epoch: 28246 \tTraining Loss: 1.334777 \tValidation Loss: 2.545236\n",
      "Epoch: 28247 \tTraining Loss: 1.377861 \tValidation Loss: 2.545618\n",
      "Epoch: 28248 \tTraining Loss: 1.351113 \tValidation Loss: 2.546172\n",
      "Epoch: 28249 \tTraining Loss: 1.366194 \tValidation Loss: 2.545312\n",
      "Epoch: 28250 \tTraining Loss: 1.331598 \tValidation Loss: 2.546097\n",
      "Epoch: 28251 \tTraining Loss: 1.351539 \tValidation Loss: 2.547249\n",
      "Epoch: 28252 \tTraining Loss: 1.327578 \tValidation Loss: 2.545194\n",
      "Epoch: 28253 \tTraining Loss: 1.376893 \tValidation Loss: 2.545992\n",
      "Epoch: 28254 \tTraining Loss: 1.369888 \tValidation Loss: 2.546266\n",
      "Epoch: 28255 \tTraining Loss: 1.313559 \tValidation Loss: 2.546324\n",
      "Epoch: 28256 \tTraining Loss: 1.336123 \tValidation Loss: 2.547018\n",
      "Epoch: 28257 \tTraining Loss: 1.327842 \tValidation Loss: 2.545682\n",
      "Epoch: 28258 \tTraining Loss: 1.265132 \tValidation Loss: 2.547256\n",
      "Epoch: 28259 \tTraining Loss: 1.358773 \tValidation Loss: 2.546359\n",
      "Epoch: 28260 \tTraining Loss: 1.361789 \tValidation Loss: 2.546222\n",
      "Epoch: 28261 \tTraining Loss: 1.380479 \tValidation Loss: 2.546654\n",
      "Epoch: 28262 \tTraining Loss: 1.307942 \tValidation Loss: 2.546609\n",
      "Epoch: 28263 \tTraining Loss: 1.351291 \tValidation Loss: 2.545661\n",
      "Epoch: 28264 \tTraining Loss: 1.359870 \tValidation Loss: 2.547352\n",
      "Epoch: 28265 \tTraining Loss: 1.344229 \tValidation Loss: 2.546511\n",
      "Epoch: 28266 \tTraining Loss: 1.364915 \tValidation Loss: 2.546160\n",
      "Epoch: 28267 \tTraining Loss: 1.339820 \tValidation Loss: 2.547269\n",
      "Epoch: 28268 \tTraining Loss: 1.320044 \tValidation Loss: 2.548804\n",
      "Epoch: 28269 \tTraining Loss: 1.320546 \tValidation Loss: 2.546831\n",
      "Epoch: 28270 \tTraining Loss: 1.359337 \tValidation Loss: 2.547536\n",
      "Epoch: 28271 \tTraining Loss: 1.380486 \tValidation Loss: 2.547070\n",
      "Epoch: 28272 \tTraining Loss: 1.336249 \tValidation Loss: 2.546443\n",
      "Epoch: 28273 \tTraining Loss: 1.353999 \tValidation Loss: 2.546484\n",
      "Epoch: 28274 \tTraining Loss: 1.306992 \tValidation Loss: 2.547067\n",
      "Epoch: 28275 \tTraining Loss: 1.355962 \tValidation Loss: 2.545962\n",
      "Epoch: 28276 \tTraining Loss: 1.376433 \tValidation Loss: 2.546425\n",
      "Epoch: 28277 \tTraining Loss: 1.303973 \tValidation Loss: 2.546453\n",
      "Epoch: 28278 \tTraining Loss: 1.323851 \tValidation Loss: 2.547598\n",
      "Epoch: 28279 \tTraining Loss: 1.383348 \tValidation Loss: 2.546064\n",
      "Epoch: 28280 \tTraining Loss: 1.328770 \tValidation Loss: 2.545856\n",
      "Epoch: 28281 \tTraining Loss: 1.378169 \tValidation Loss: 2.547144\n",
      "Epoch: 28282 \tTraining Loss: 1.324375 \tValidation Loss: 2.545610\n",
      "Epoch: 28283 \tTraining Loss: 1.318051 \tValidation Loss: 2.546016\n",
      "Epoch: 28284 \tTraining Loss: 1.320138 \tValidation Loss: 2.547196\n",
      "Epoch: 28285 \tTraining Loss: 1.304442 \tValidation Loss: 2.547020\n",
      "Epoch: 28286 \tTraining Loss: 1.313171 \tValidation Loss: 2.547390\n",
      "Epoch: 28287 \tTraining Loss: 1.354100 \tValidation Loss: 2.546413\n",
      "Epoch: 28288 \tTraining Loss: 1.371621 \tValidation Loss: 2.545773\n",
      "Epoch: 28289 \tTraining Loss: 1.389419 \tValidation Loss: 2.546098\n",
      "Epoch: 28290 \tTraining Loss: 1.379600 \tValidation Loss: 2.546540\n",
      "Epoch: 28291 \tTraining Loss: 1.342136 \tValidation Loss: 2.547328\n",
      "Epoch: 28292 \tTraining Loss: 1.351814 \tValidation Loss: 2.547260\n",
      "Epoch: 28293 \tTraining Loss: 1.368953 \tValidation Loss: 2.546566\n",
      "Epoch: 28294 \tTraining Loss: 1.335578 \tValidation Loss: 2.547526\n",
      "Epoch: 28295 \tTraining Loss: 1.378828 \tValidation Loss: 2.546959\n",
      "Epoch: 28296 \tTraining Loss: 1.354699 \tValidation Loss: 2.546911\n",
      "Epoch: 28297 \tTraining Loss: 1.417065 \tValidation Loss: 2.546054\n",
      "Epoch: 28298 \tTraining Loss: 1.321708 \tValidation Loss: 2.546904\n",
      "Epoch: 28299 \tTraining Loss: 1.388188 \tValidation Loss: 2.546632\n",
      "Epoch: 28300 \tTraining Loss: 1.326525 \tValidation Loss: 2.547929\n",
      "Epoch: 28301 \tTraining Loss: 1.325665 \tValidation Loss: 2.546866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28302 \tTraining Loss: 1.329533 \tValidation Loss: 2.548475\n",
      "Epoch: 28303 \tTraining Loss: 1.387937 \tValidation Loss: 2.547660\n",
      "Epoch: 28304 \tTraining Loss: 1.304888 \tValidation Loss: 2.548425\n",
      "Epoch: 28305 \tTraining Loss: 1.341064 \tValidation Loss: 2.548161\n",
      "Epoch: 28306 \tTraining Loss: 1.370332 \tValidation Loss: 2.546928\n",
      "Epoch: 28307 \tTraining Loss: 1.334064 \tValidation Loss: 2.548230\n",
      "Epoch: 28308 \tTraining Loss: 1.321830 \tValidation Loss: 2.547671\n",
      "Epoch: 28309 \tTraining Loss: 1.359351 \tValidation Loss: 2.547879\n",
      "Epoch: 28310 \tTraining Loss: 1.316008 \tValidation Loss: 2.548167\n",
      "Epoch: 28311 \tTraining Loss: 1.348683 \tValidation Loss: 2.547627\n",
      "Epoch: 28312 \tTraining Loss: 1.364583 \tValidation Loss: 2.547174\n",
      "Epoch: 28313 \tTraining Loss: 1.301133 \tValidation Loss: 2.547920\n",
      "Epoch: 28314 \tTraining Loss: 1.327194 \tValidation Loss: 2.547976\n",
      "Epoch: 28315 \tTraining Loss: 1.361253 \tValidation Loss: 2.546803\n",
      "Epoch: 28316 \tTraining Loss: 1.336264 \tValidation Loss: 2.546971\n",
      "Epoch: 28317 \tTraining Loss: 1.355441 \tValidation Loss: 2.547578\n",
      "Epoch: 28318 \tTraining Loss: 1.324380 \tValidation Loss: 2.548305\n",
      "Epoch: 28319 \tTraining Loss: 1.358927 \tValidation Loss: 2.547933\n",
      "Epoch: 28320 \tTraining Loss: 1.356757 \tValidation Loss: 2.548042\n",
      "Epoch: 28321 \tTraining Loss: 1.360939 \tValidation Loss: 2.548480\n",
      "Epoch: 28322 \tTraining Loss: 1.336603 \tValidation Loss: 2.547688\n",
      "Epoch: 28323 \tTraining Loss: 1.326030 \tValidation Loss: 2.548530\n",
      "Epoch: 28324 \tTraining Loss: 1.335052 \tValidation Loss: 2.547102\n",
      "Epoch: 28325 \tTraining Loss: 1.313879 \tValidation Loss: 2.547399\n",
      "Epoch: 28326 \tTraining Loss: 1.361572 \tValidation Loss: 2.547379\n",
      "Epoch: 28327 \tTraining Loss: 1.379936 \tValidation Loss: 2.546386\n",
      "Epoch: 28328 \tTraining Loss: 1.337392 \tValidation Loss: 2.547739\n",
      "Epoch: 28329 \tTraining Loss: 1.364403 \tValidation Loss: 2.547333\n",
      "Epoch: 28330 \tTraining Loss: 1.380693 \tValidation Loss: 2.547071\n",
      "Epoch: 28331 \tTraining Loss: 1.333291 \tValidation Loss: 2.548167\n",
      "Epoch: 28332 \tTraining Loss: 1.343953 \tValidation Loss: 2.548542\n",
      "Epoch: 28333 \tTraining Loss: 1.312938 \tValidation Loss: 2.548049\n",
      "Epoch: 28334 \tTraining Loss: 1.345343 \tValidation Loss: 2.547368\n",
      "Epoch: 28335 \tTraining Loss: 1.350705 \tValidation Loss: 2.547676\n",
      "Epoch: 28336 \tTraining Loss: 1.321850 \tValidation Loss: 2.548502\n",
      "Epoch: 28337 \tTraining Loss: 1.313802 \tValidation Loss: 2.548629\n",
      "Epoch: 28338 \tTraining Loss: 1.394171 \tValidation Loss: 2.548815\n",
      "Epoch: 28339 \tTraining Loss: 1.357702 \tValidation Loss: 2.548702\n",
      "Epoch: 28340 \tTraining Loss: 1.378192 \tValidation Loss: 2.547111\n",
      "Epoch: 28341 \tTraining Loss: 1.330746 \tValidation Loss: 2.549026\n",
      "Epoch: 28342 \tTraining Loss: 1.333301 \tValidation Loss: 2.548252\n",
      "Epoch: 28343 \tTraining Loss: 1.399652 \tValidation Loss: 2.547588\n",
      "Epoch: 28344 \tTraining Loss: 1.372345 \tValidation Loss: 2.547246\n",
      "Epoch: 28345 \tTraining Loss: 1.375092 \tValidation Loss: 2.548186\n",
      "Epoch: 28346 \tTraining Loss: 1.342146 \tValidation Loss: 2.548810\n",
      "Epoch: 28347 \tTraining Loss: 1.354219 \tValidation Loss: 2.548477\n",
      "Epoch: 28348 \tTraining Loss: 1.285926 \tValidation Loss: 2.549828\n",
      "Epoch: 28349 \tTraining Loss: 1.381486 \tValidation Loss: 2.548181\n",
      "Epoch: 28350 \tTraining Loss: 1.348723 \tValidation Loss: 2.549199\n",
      "Epoch: 28351 \tTraining Loss: 1.317853 \tValidation Loss: 2.548806\n",
      "Epoch: 28352 \tTraining Loss: 1.307964 \tValidation Loss: 2.548883\n",
      "Epoch: 28353 \tTraining Loss: 1.366219 \tValidation Loss: 2.548539\n",
      "Epoch: 28354 \tTraining Loss: 1.348846 \tValidation Loss: 2.548959\n",
      "Epoch: 28355 \tTraining Loss: 1.351754 \tValidation Loss: 2.547928\n",
      "Epoch: 28356 \tTraining Loss: 1.351945 \tValidation Loss: 2.549069\n",
      "Epoch: 28357 \tTraining Loss: 1.338205 \tValidation Loss: 2.548524\n",
      "Epoch: 28358 \tTraining Loss: 1.340818 \tValidation Loss: 2.548579\n",
      "Epoch: 28359 \tTraining Loss: 1.339931 \tValidation Loss: 2.550223\n",
      "Epoch: 28360 \tTraining Loss: 1.361814 \tValidation Loss: 2.548836\n",
      "Epoch: 28361 \tTraining Loss: 1.359599 \tValidation Loss: 2.548857\n",
      "Epoch: 28362 \tTraining Loss: 1.386050 \tValidation Loss: 2.548053\n",
      "Epoch: 28363 \tTraining Loss: 1.307701 \tValidation Loss: 2.549338\n",
      "Epoch: 28364 \tTraining Loss: 1.390662 \tValidation Loss: 2.548810\n",
      "Epoch: 28365 \tTraining Loss: 1.340554 \tValidation Loss: 2.549244\n",
      "Epoch: 28366 \tTraining Loss: 1.296354 \tValidation Loss: 2.550339\n",
      "Epoch: 28367 \tTraining Loss: 1.325105 \tValidation Loss: 2.548661\n",
      "Epoch: 28368 \tTraining Loss: 1.390068 \tValidation Loss: 2.548400\n",
      "Epoch: 28369 \tTraining Loss: 1.349393 \tValidation Loss: 2.548700\n",
      "Epoch: 28370 \tTraining Loss: 1.332402 \tValidation Loss: 2.549893\n",
      "Epoch: 28371 \tTraining Loss: 1.342139 \tValidation Loss: 2.549284\n",
      "Epoch: 28372 \tTraining Loss: 1.305288 \tValidation Loss: 2.548885\n",
      "Epoch: 28373 \tTraining Loss: 1.300879 \tValidation Loss: 2.550913\n",
      "Epoch: 28374 \tTraining Loss: 1.320722 \tValidation Loss: 2.549563\n",
      "Epoch: 28375 \tTraining Loss: 1.358486 \tValidation Loss: 2.548411\n",
      "Epoch: 28376 \tTraining Loss: 1.324707 \tValidation Loss: 2.549949\n",
      "Epoch: 28377 \tTraining Loss: 1.324343 \tValidation Loss: 2.550034\n",
      "Epoch: 28378 \tTraining Loss: 1.356912 \tValidation Loss: 2.550549\n",
      "Epoch: 28379 \tTraining Loss: 1.350442 \tValidation Loss: 2.550170\n",
      "Epoch: 28380 \tTraining Loss: 1.309542 \tValidation Loss: 2.548927\n",
      "Epoch: 28381 \tTraining Loss: 1.338226 \tValidation Loss: 2.548709\n",
      "Epoch: 28382 \tTraining Loss: 1.359954 \tValidation Loss: 2.548803\n",
      "Epoch: 28383 \tTraining Loss: 1.346831 \tValidation Loss: 2.549909\n",
      "Epoch: 28384 \tTraining Loss: 1.361209 \tValidation Loss: 2.549229\n",
      "Epoch: 28385 \tTraining Loss: 1.297402 \tValidation Loss: 2.550777\n",
      "Epoch: 28386 \tTraining Loss: 1.318731 \tValidation Loss: 2.548797\n",
      "Epoch: 28387 \tTraining Loss: 1.317406 \tValidation Loss: 2.549205\n",
      "Epoch: 28388 \tTraining Loss: 1.332139 \tValidation Loss: 2.549207\n",
      "Epoch: 28389 \tTraining Loss: 1.338097 \tValidation Loss: 2.549246\n",
      "Epoch: 28390 \tTraining Loss: 1.372733 \tValidation Loss: 2.548311\n",
      "Epoch: 28391 \tTraining Loss: 1.364995 \tValidation Loss: 2.548972\n",
      "Epoch: 28392 \tTraining Loss: 1.337559 \tValidation Loss: 2.549914\n",
      "Epoch: 28393 \tTraining Loss: 1.357125 \tValidation Loss: 2.548192\n",
      "Epoch: 28394 \tTraining Loss: 1.367617 \tValidation Loss: 2.549526\n",
      "Epoch: 28395 \tTraining Loss: 1.383613 \tValidation Loss: 2.549567\n",
      "Epoch: 28396 \tTraining Loss: 1.303860 \tValidation Loss: 2.548955\n",
      "Epoch: 28397 \tTraining Loss: 1.377402 \tValidation Loss: 2.548298\n",
      "Epoch: 28398 \tTraining Loss: 1.321420 \tValidation Loss: 2.549682\n",
      "Epoch: 28399 \tTraining Loss: 1.335133 \tValidation Loss: 2.550323\n",
      "Epoch: 28400 \tTraining Loss: 1.348213 \tValidation Loss: 2.549479\n",
      "Epoch: 28401 \tTraining Loss: 1.370295 \tValidation Loss: 2.549446\n",
      "Epoch: 28402 \tTraining Loss: 1.339213 \tValidation Loss: 2.549637\n",
      "Epoch: 28403 \tTraining Loss: 1.346710 \tValidation Loss: 2.550370\n",
      "Epoch: 28404 \tTraining Loss: 1.352536 \tValidation Loss: 2.548994\n",
      "Epoch: 28405 \tTraining Loss: 1.398608 \tValidation Loss: 2.548970\n",
      "Epoch: 28406 \tTraining Loss: 1.385489 \tValidation Loss: 2.549774\n",
      "Epoch: 28407 \tTraining Loss: 1.362772 \tValidation Loss: 2.548794\n",
      "Epoch: 28408 \tTraining Loss: 1.383598 \tValidation Loss: 2.550216\n",
      "Epoch: 28409 \tTraining Loss: 1.375715 \tValidation Loss: 2.548728\n",
      "Epoch: 28410 \tTraining Loss: 1.351157 \tValidation Loss: 2.549379\n",
      "Epoch: 28411 \tTraining Loss: 1.380689 \tValidation Loss: 2.549550\n",
      "Epoch: 28412 \tTraining Loss: 1.358695 \tValidation Loss: 2.550232\n",
      "Epoch: 28413 \tTraining Loss: 1.345666 \tValidation Loss: 2.548010\n",
      "Epoch: 28414 \tTraining Loss: 1.313864 \tValidation Loss: 2.548502\n",
      "Epoch: 28415 \tTraining Loss: 1.351972 \tValidation Loss: 2.549378\n",
      "Epoch: 28416 \tTraining Loss: 1.377578 \tValidation Loss: 2.548732\n",
      "Epoch: 28417 \tTraining Loss: 1.333692 \tValidation Loss: 2.550555\n",
      "Epoch: 28418 \tTraining Loss: 1.337047 \tValidation Loss: 2.550987\n",
      "Epoch: 28419 \tTraining Loss: 1.323002 \tValidation Loss: 2.550278\n",
      "Epoch: 28420 \tTraining Loss: 1.321499 \tValidation Loss: 2.550625\n",
      "Epoch: 28421 \tTraining Loss: 1.293207 \tValidation Loss: 2.551923\n",
      "Epoch: 28422 \tTraining Loss: 1.380175 \tValidation Loss: 2.549284\n",
      "Epoch: 28423 \tTraining Loss: 1.365585 \tValidation Loss: 2.549884\n",
      "Epoch: 28424 \tTraining Loss: 1.300525 \tValidation Loss: 2.550088\n",
      "Epoch: 28425 \tTraining Loss: 1.369229 \tValidation Loss: 2.550252\n",
      "Epoch: 28426 \tTraining Loss: 1.370659 \tValidation Loss: 2.551081\n",
      "Epoch: 28427 \tTraining Loss: 1.390216 \tValidation Loss: 2.549574\n",
      "Epoch: 28428 \tTraining Loss: 1.307604 \tValidation Loss: 2.549972\n",
      "Epoch: 28429 \tTraining Loss: 1.336987 \tValidation Loss: 2.550102\n",
      "Epoch: 28430 \tTraining Loss: 1.384196 \tValidation Loss: 2.550352\n",
      "Epoch: 28431 \tTraining Loss: 1.308684 \tValidation Loss: 2.550575\n",
      "Epoch: 28432 \tTraining Loss: 1.349093 \tValidation Loss: 2.549413\n",
      "Epoch: 28433 \tTraining Loss: 1.286405 \tValidation Loss: 2.549867\n",
      "Epoch: 28434 \tTraining Loss: 1.335090 \tValidation Loss: 2.549626\n",
      "Epoch: 28435 \tTraining Loss: 1.307903 \tValidation Loss: 2.550651\n",
      "Epoch: 28436 \tTraining Loss: 1.374135 \tValidation Loss: 2.548383\n",
      "Epoch: 28437 \tTraining Loss: 1.339651 \tValidation Loss: 2.549169\n",
      "Epoch: 28438 \tTraining Loss: 1.328034 \tValidation Loss: 2.549274\n",
      "Epoch: 28439 \tTraining Loss: 1.382649 \tValidation Loss: 2.550387\n",
      "Epoch: 28440 \tTraining Loss: 1.369270 \tValidation Loss: 2.548262\n",
      "Epoch: 28441 \tTraining Loss: 1.333647 \tValidation Loss: 2.549450\n",
      "Epoch: 28442 \tTraining Loss: 1.352292 \tValidation Loss: 2.550111\n",
      "Epoch: 28443 \tTraining Loss: 1.341925 \tValidation Loss: 2.549579\n",
      "Epoch: 28444 \tTraining Loss: 1.345594 \tValidation Loss: 2.550256\n",
      "Epoch: 28445 \tTraining Loss: 1.331258 \tValidation Loss: 2.549984\n",
      "Epoch: 28446 \tTraining Loss: 1.327549 \tValidation Loss: 2.550311\n",
      "Epoch: 28447 \tTraining Loss: 1.329331 \tValidation Loss: 2.550280\n",
      "Epoch: 28448 \tTraining Loss: 1.338889 \tValidation Loss: 2.550118\n",
      "Epoch: 28449 \tTraining Loss: 1.333813 \tValidation Loss: 2.549908\n",
      "Epoch: 28450 \tTraining Loss: 1.321954 \tValidation Loss: 2.550570\n",
      "Epoch: 28451 \tTraining Loss: 1.320156 \tValidation Loss: 2.550324\n",
      "Epoch: 28452 \tTraining Loss: 1.352375 \tValidation Loss: 2.550684\n",
      "Epoch: 28453 \tTraining Loss: 1.377277 \tValidation Loss: 2.549699\n",
      "Epoch: 28454 \tTraining Loss: 1.317386 \tValidation Loss: 2.550043\n",
      "Epoch: 28455 \tTraining Loss: 1.331552 \tValidation Loss: 2.552023\n",
      "Epoch: 28456 \tTraining Loss: 1.360715 \tValidation Loss: 2.551152\n",
      "Epoch: 28457 \tTraining Loss: 1.362377 \tValidation Loss: 2.550920\n",
      "Epoch: 28458 \tTraining Loss: 1.362501 \tValidation Loss: 2.549341\n",
      "Epoch: 28459 \tTraining Loss: 1.325123 \tValidation Loss: 2.550487\n",
      "Epoch: 28460 \tTraining Loss: 1.335630 \tValidation Loss: 2.549977\n",
      "Epoch: 28461 \tTraining Loss: 1.331054 \tValidation Loss: 2.550910\n",
      "Epoch: 28462 \tTraining Loss: 1.342845 \tValidation Loss: 2.551190\n",
      "Epoch: 28463 \tTraining Loss: 1.332572 \tValidation Loss: 2.551678\n",
      "Epoch: 28464 \tTraining Loss: 1.322770 \tValidation Loss: 2.549447\n",
      "Epoch: 28465 \tTraining Loss: 1.332600 \tValidation Loss: 2.550289\n",
      "Epoch: 28466 \tTraining Loss: 1.347841 \tValidation Loss: 2.549852\n",
      "Epoch: 28467 \tTraining Loss: 1.359801 \tValidation Loss: 2.549413\n",
      "Epoch: 28468 \tTraining Loss: 1.380191 \tValidation Loss: 2.551009\n",
      "Epoch: 28469 \tTraining Loss: 1.341562 \tValidation Loss: 2.550924\n",
      "Epoch: 28470 \tTraining Loss: 1.311939 \tValidation Loss: 2.551943\n",
      "Epoch: 28471 \tTraining Loss: 1.331023 \tValidation Loss: 2.551534\n",
      "Epoch: 28472 \tTraining Loss: 1.359704 \tValidation Loss: 2.550468\n",
      "Epoch: 28473 \tTraining Loss: 1.382474 \tValidation Loss: 2.550186\n",
      "Epoch: 28474 \tTraining Loss: 1.330491 \tValidation Loss: 2.551655\n",
      "Epoch: 28475 \tTraining Loss: 1.351525 \tValidation Loss: 2.551172\n",
      "Epoch: 28476 \tTraining Loss: 1.323758 \tValidation Loss: 2.551798\n",
      "Epoch: 28477 \tTraining Loss: 1.325433 \tValidation Loss: 2.549877\n",
      "Epoch: 28478 \tTraining Loss: 1.319972 \tValidation Loss: 2.552335\n",
      "Epoch: 28479 \tTraining Loss: 1.328099 \tValidation Loss: 2.551444\n",
      "Epoch: 28480 \tTraining Loss: 1.376050 \tValidation Loss: 2.549944\n",
      "Epoch: 28481 \tTraining Loss: 1.324089 \tValidation Loss: 2.550801\n",
      "Epoch: 28482 \tTraining Loss: 1.356133 \tValidation Loss: 2.550946\n",
      "Epoch: 28483 \tTraining Loss: 1.314827 \tValidation Loss: 2.550314\n",
      "Epoch: 28484 \tTraining Loss: 1.293647 \tValidation Loss: 2.551178\n",
      "Epoch: 28485 \tTraining Loss: 1.353361 \tValidation Loss: 2.551051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28486 \tTraining Loss: 1.347263 \tValidation Loss: 2.551684\n",
      "Epoch: 28487 \tTraining Loss: 1.368188 \tValidation Loss: 2.550170\n",
      "Epoch: 28488 \tTraining Loss: 1.338974 \tValidation Loss: 2.550598\n",
      "Epoch: 28489 \tTraining Loss: 1.346016 \tValidation Loss: 2.551020\n",
      "Epoch: 28490 \tTraining Loss: 1.333727 \tValidation Loss: 2.550701\n",
      "Epoch: 28491 \tTraining Loss: 1.309734 \tValidation Loss: 2.552127\n",
      "Epoch: 28492 \tTraining Loss: 1.368123 \tValidation Loss: 2.551276\n",
      "Epoch: 28493 \tTraining Loss: 1.346609 \tValidation Loss: 2.551717\n",
      "Epoch: 28494 \tTraining Loss: 1.337756 \tValidation Loss: 2.552183\n",
      "Epoch: 28495 \tTraining Loss: 1.364013 \tValidation Loss: 2.551728\n",
      "Epoch: 28496 \tTraining Loss: 1.334079 \tValidation Loss: 2.551766\n",
      "Epoch: 28497 \tTraining Loss: 1.322951 \tValidation Loss: 2.551618\n",
      "Epoch: 28498 \tTraining Loss: 1.347880 \tValidation Loss: 2.551716\n",
      "Epoch: 28499 \tTraining Loss: 1.314251 \tValidation Loss: 2.552240\n",
      "Epoch: 28500 \tTraining Loss: 1.307112 \tValidation Loss: 2.551314\n",
      "Epoch: 28501 \tTraining Loss: 1.278621 \tValidation Loss: 2.551466\n",
      "Epoch: 28502 \tTraining Loss: 1.333168 \tValidation Loss: 2.551399\n",
      "Epoch: 28503 \tTraining Loss: 1.325157 \tValidation Loss: 2.552404\n",
      "Epoch: 28504 \tTraining Loss: 1.314206 \tValidation Loss: 2.552140\n",
      "Epoch: 28505 \tTraining Loss: 1.348596 \tValidation Loss: 2.552767\n",
      "Epoch: 28506 \tTraining Loss: 1.325158 \tValidation Loss: 2.553472\n",
      "Epoch: 28507 \tTraining Loss: 1.336951 \tValidation Loss: 2.552141\n",
      "Epoch: 28508 \tTraining Loss: 1.402825 \tValidation Loss: 2.551901\n",
      "Epoch: 28509 \tTraining Loss: 1.351821 \tValidation Loss: 2.552155\n",
      "Epoch: 28510 \tTraining Loss: 1.377238 \tValidation Loss: 2.551738\n",
      "Epoch: 28511 \tTraining Loss: 1.326101 \tValidation Loss: 2.552018\n",
      "Epoch: 28512 \tTraining Loss: 1.372270 \tValidation Loss: 2.551501\n",
      "Epoch: 28513 \tTraining Loss: 1.323677 \tValidation Loss: 2.552465\n",
      "Epoch: 28514 \tTraining Loss: 1.314689 \tValidation Loss: 2.552622\n",
      "Epoch: 28515 \tTraining Loss: 1.347290 \tValidation Loss: 2.551023\n",
      "Epoch: 28516 \tTraining Loss: 1.299707 \tValidation Loss: 2.551345\n",
      "Epoch: 28517 \tTraining Loss: 1.337286 \tValidation Loss: 2.552884\n",
      "Epoch: 28518 \tTraining Loss: 1.364991 \tValidation Loss: 2.552098\n",
      "Epoch: 28519 \tTraining Loss: 1.355235 \tValidation Loss: 2.551356\n",
      "Epoch: 28520 \tTraining Loss: 1.317948 \tValidation Loss: 2.553649\n",
      "Epoch: 28521 \tTraining Loss: 1.324459 \tValidation Loss: 2.551565\n",
      "Epoch: 28522 \tTraining Loss: 1.309302 \tValidation Loss: 2.551702\n",
      "Epoch: 28523 \tTraining Loss: 1.374819 \tValidation Loss: 2.551320\n",
      "Epoch: 28524 \tTraining Loss: 1.320790 \tValidation Loss: 2.552467\n",
      "Epoch: 28525 \tTraining Loss: 1.336668 \tValidation Loss: 2.552418\n",
      "Epoch: 28526 \tTraining Loss: 1.388111 \tValidation Loss: 2.552223\n",
      "Epoch: 28527 \tTraining Loss: 1.344104 \tValidation Loss: 2.552318\n",
      "Epoch: 28528 \tTraining Loss: 1.343543 \tValidation Loss: 2.552501\n",
      "Epoch: 28529 \tTraining Loss: 1.342224 \tValidation Loss: 2.552213\n",
      "Epoch: 28530 \tTraining Loss: 1.335426 \tValidation Loss: 2.552119\n",
      "Epoch: 28531 \tTraining Loss: 1.359832 \tValidation Loss: 2.551081\n",
      "Epoch: 28532 \tTraining Loss: 1.308004 \tValidation Loss: 2.552686\n",
      "Epoch: 28533 \tTraining Loss: 1.310204 \tValidation Loss: 2.552326\n",
      "Epoch: 28534 \tTraining Loss: 1.324590 \tValidation Loss: 2.552391\n",
      "Epoch: 28535 \tTraining Loss: 1.345878 \tValidation Loss: 2.550976\n",
      "Epoch: 28536 \tTraining Loss: 1.357561 \tValidation Loss: 2.552113\n",
      "Epoch: 28537 \tTraining Loss: 1.345723 \tValidation Loss: 2.550769\n",
      "Epoch: 28538 \tTraining Loss: 1.343114 \tValidation Loss: 2.552470\n",
      "Epoch: 28539 \tTraining Loss: 1.366002 \tValidation Loss: 2.551870\n",
      "Epoch: 28540 \tTraining Loss: 1.335111 \tValidation Loss: 2.552999\n",
      "Epoch: 28541 \tTraining Loss: 1.342348 \tValidation Loss: 2.552367\n",
      "Epoch: 28542 \tTraining Loss: 1.333717 \tValidation Loss: 2.552889\n",
      "Epoch: 28543 \tTraining Loss: 1.305481 \tValidation Loss: 2.552032\n",
      "Epoch: 28544 \tTraining Loss: 1.326487 \tValidation Loss: 2.553465\n",
      "Epoch: 28545 \tTraining Loss: 1.345603 \tValidation Loss: 2.553768\n",
      "Epoch: 28546 \tTraining Loss: 1.315791 \tValidation Loss: 2.553464\n",
      "Epoch: 28547 \tTraining Loss: 1.336447 \tValidation Loss: 2.553951\n",
      "Epoch: 28548 \tTraining Loss: 1.319938 \tValidation Loss: 2.552907\n",
      "Epoch: 28549 \tTraining Loss: 1.335143 \tValidation Loss: 2.552583\n",
      "Epoch: 28550 \tTraining Loss: 1.330246 \tValidation Loss: 2.553567\n",
      "Epoch: 28551 \tTraining Loss: 1.375118 \tValidation Loss: 2.552556\n",
      "Epoch: 28552 \tTraining Loss: 1.373251 \tValidation Loss: 2.553500\n",
      "Epoch: 28553 \tTraining Loss: 1.399783 \tValidation Loss: 2.552504\n",
      "Epoch: 28554 \tTraining Loss: 1.374589 \tValidation Loss: 2.552264\n",
      "Epoch: 28555 \tTraining Loss: 1.336415 \tValidation Loss: 2.552000\n",
      "Epoch: 28556 \tTraining Loss: 1.368039 \tValidation Loss: 2.552931\n",
      "Epoch: 28557 \tTraining Loss: 1.321921 \tValidation Loss: 2.553742\n",
      "Epoch: 28558 \tTraining Loss: 1.341708 \tValidation Loss: 2.553203\n",
      "Epoch: 28559 \tTraining Loss: 1.349318 \tValidation Loss: 2.553566\n",
      "Epoch: 28560 \tTraining Loss: 1.353889 \tValidation Loss: 2.554535\n",
      "Epoch: 28561 \tTraining Loss: 1.338554 \tValidation Loss: 2.554012\n",
      "Epoch: 28562 \tTraining Loss: 1.333046 \tValidation Loss: 2.553688\n",
      "Epoch: 28563 \tTraining Loss: 1.333593 \tValidation Loss: 2.553414\n",
      "Epoch: 28564 \tTraining Loss: 1.349393 \tValidation Loss: 2.554092\n",
      "Epoch: 28565 \tTraining Loss: 1.311632 \tValidation Loss: 2.553649\n",
      "Epoch: 28566 \tTraining Loss: 1.319720 \tValidation Loss: 2.554604\n",
      "Epoch: 28567 \tTraining Loss: 1.364265 \tValidation Loss: 2.553387\n",
      "Epoch: 28568 \tTraining Loss: 1.345815 \tValidation Loss: 2.553274\n",
      "Epoch: 28569 \tTraining Loss: 1.350026 \tValidation Loss: 2.553294\n",
      "Epoch: 28570 \tTraining Loss: 1.341110 \tValidation Loss: 2.552639\n",
      "Epoch: 28571 \tTraining Loss: 1.324130 \tValidation Loss: 2.552341\n",
      "Epoch: 28572 \tTraining Loss: 1.354721 \tValidation Loss: 2.553075\n",
      "Epoch: 28573 \tTraining Loss: 1.291072 \tValidation Loss: 2.553853\n",
      "Epoch: 28574 \tTraining Loss: 1.319321 \tValidation Loss: 2.553546\n",
      "Epoch: 28575 \tTraining Loss: 1.349588 \tValidation Loss: 2.552839\n",
      "Epoch: 28576 \tTraining Loss: 1.310274 \tValidation Loss: 2.553305\n",
      "Epoch: 28577 \tTraining Loss: 1.330796 \tValidation Loss: 2.554352\n",
      "Epoch: 28578 \tTraining Loss: 1.349821 \tValidation Loss: 2.553592\n",
      "Epoch: 28579 \tTraining Loss: 1.345252 \tValidation Loss: 2.553245\n",
      "Epoch: 28580 \tTraining Loss: 1.347382 \tValidation Loss: 2.553026\n",
      "Epoch: 28581 \tTraining Loss: 1.377934 \tValidation Loss: 2.552767\n",
      "Epoch: 28582 \tTraining Loss: 1.317928 \tValidation Loss: 2.554422\n",
      "Epoch: 28583 \tTraining Loss: 1.383726 \tValidation Loss: 2.552385\n",
      "Epoch: 28584 \tTraining Loss: 1.360800 \tValidation Loss: 2.554004\n",
      "Epoch: 28585 \tTraining Loss: 1.358400 \tValidation Loss: 2.553591\n",
      "Epoch: 28586 \tTraining Loss: 1.347782 \tValidation Loss: 2.553715\n",
      "Epoch: 28587 \tTraining Loss: 1.336738 \tValidation Loss: 2.553262\n",
      "Epoch: 28588 \tTraining Loss: 1.365088 \tValidation Loss: 2.553066\n",
      "Epoch: 28589 \tTraining Loss: 1.371858 \tValidation Loss: 2.552749\n",
      "Epoch: 28590 \tTraining Loss: 1.337020 \tValidation Loss: 2.554233\n",
      "Epoch: 28591 \tTraining Loss: 1.316791 \tValidation Loss: 2.553592\n",
      "Epoch: 28592 \tTraining Loss: 1.349532 \tValidation Loss: 2.552596\n",
      "Epoch: 28593 \tTraining Loss: 1.361816 \tValidation Loss: 2.553647\n",
      "Epoch: 28594 \tTraining Loss: 1.356280 \tValidation Loss: 2.553735\n",
      "Epoch: 28595 \tTraining Loss: 1.335084 \tValidation Loss: 2.553550\n",
      "Epoch: 28596 \tTraining Loss: 1.315486 \tValidation Loss: 2.553812\n",
      "Epoch: 28597 \tTraining Loss: 1.346401 \tValidation Loss: 2.553118\n",
      "Epoch: 28598 \tTraining Loss: 1.368792 \tValidation Loss: 2.553358\n",
      "Epoch: 28599 \tTraining Loss: 1.366055 \tValidation Loss: 2.552547\n",
      "Epoch: 28600 \tTraining Loss: 1.305654 \tValidation Loss: 2.553447\n",
      "Epoch: 28601 \tTraining Loss: 1.343142 \tValidation Loss: 2.554142\n",
      "Epoch: 28602 \tTraining Loss: 1.325045 \tValidation Loss: 2.553830\n",
      "Epoch: 28603 \tTraining Loss: 1.315580 \tValidation Loss: 2.553983\n",
      "Epoch: 28604 \tTraining Loss: 1.362978 \tValidation Loss: 2.554712\n",
      "Epoch: 28605 \tTraining Loss: 1.331625 \tValidation Loss: 2.554552\n",
      "Epoch: 28606 \tTraining Loss: 1.351233 \tValidation Loss: 2.553579\n",
      "Epoch: 28607 \tTraining Loss: 1.328093 \tValidation Loss: 2.553339\n",
      "Epoch: 28608 \tTraining Loss: 1.314596 \tValidation Loss: 2.552805\n",
      "Epoch: 28609 \tTraining Loss: 1.350204 \tValidation Loss: 2.552123\n",
      "Epoch: 28610 \tTraining Loss: 1.288223 \tValidation Loss: 2.553059\n",
      "Epoch: 28611 \tTraining Loss: 1.328012 \tValidation Loss: 2.553898\n",
      "Epoch: 28612 \tTraining Loss: 1.369982 \tValidation Loss: 2.554326\n",
      "Epoch: 28613 \tTraining Loss: 1.336104 \tValidation Loss: 2.553744\n",
      "Epoch: 28614 \tTraining Loss: 1.335321 \tValidation Loss: 2.554732\n",
      "Epoch: 28615 \tTraining Loss: 1.335028 \tValidation Loss: 2.554052\n",
      "Epoch: 28616 \tTraining Loss: 1.364738 \tValidation Loss: 2.554237\n",
      "Epoch: 28617 \tTraining Loss: 1.346689 \tValidation Loss: 2.554514\n",
      "Epoch: 28618 \tTraining Loss: 1.346262 \tValidation Loss: 2.554185\n",
      "Epoch: 28619 \tTraining Loss: 1.387305 \tValidation Loss: 2.553037\n",
      "Epoch: 28620 \tTraining Loss: 1.304265 \tValidation Loss: 2.553946\n",
      "Epoch: 28621 \tTraining Loss: 1.366666 \tValidation Loss: 2.553003\n",
      "Epoch: 28622 \tTraining Loss: 1.346651 \tValidation Loss: 2.554154\n",
      "Epoch: 28623 \tTraining Loss: 1.391820 \tValidation Loss: 2.553456\n",
      "Epoch: 28624 \tTraining Loss: 1.312288 \tValidation Loss: 2.553506\n",
      "Epoch: 28625 \tTraining Loss: 1.305714 \tValidation Loss: 2.554686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28626 \tTraining Loss: 1.361081 \tValidation Loss: 2.552968\n",
      "Epoch: 28627 \tTraining Loss: 1.337343 \tValidation Loss: 2.553628\n",
      "Epoch: 28628 \tTraining Loss: 1.352692 \tValidation Loss: 2.553957\n",
      "Epoch: 28629 \tTraining Loss: 1.296747 \tValidation Loss: 2.554524\n",
      "Epoch: 28630 \tTraining Loss: 1.354752 \tValidation Loss: 2.553940\n",
      "Epoch: 28631 \tTraining Loss: 1.380378 \tValidation Loss: 2.553668\n",
      "Epoch: 28632 \tTraining Loss: 1.344585 \tValidation Loss: 2.554641\n",
      "Epoch: 28633 \tTraining Loss: 1.348572 \tValidation Loss: 2.555343\n",
      "Epoch: 28634 \tTraining Loss: 1.330449 \tValidation Loss: 2.554701\n",
      "Epoch: 28635 \tTraining Loss: 1.332306 \tValidation Loss: 2.554458\n",
      "Epoch: 28636 \tTraining Loss: 1.361215 \tValidation Loss: 2.555366\n",
      "Epoch: 28637 \tTraining Loss: 1.282746 \tValidation Loss: 2.553563\n",
      "Epoch: 28638 \tTraining Loss: 1.336668 \tValidation Loss: 2.554788\n",
      "Epoch: 28639 \tTraining Loss: 1.357178 \tValidation Loss: 2.554613\n",
      "Epoch: 28640 \tTraining Loss: 1.330966 \tValidation Loss: 2.555397\n",
      "Epoch: 28641 \tTraining Loss: 1.372222 \tValidation Loss: 2.554505\n",
      "Epoch: 28642 \tTraining Loss: 1.303311 \tValidation Loss: 2.553656\n",
      "Epoch: 28643 \tTraining Loss: 1.303117 \tValidation Loss: 2.555000\n",
      "Epoch: 28644 \tTraining Loss: 1.355765 \tValidation Loss: 2.554402\n",
      "Epoch: 28645 \tTraining Loss: 1.358386 \tValidation Loss: 2.554734\n",
      "Epoch: 28646 \tTraining Loss: 1.358438 \tValidation Loss: 2.553674\n",
      "Epoch: 28647 \tTraining Loss: 1.341505 \tValidation Loss: 2.554173\n",
      "Epoch: 28648 \tTraining Loss: 1.396808 \tValidation Loss: 2.554039\n",
      "Epoch: 28649 \tTraining Loss: 1.335846 \tValidation Loss: 2.555366\n",
      "Epoch: 28650 \tTraining Loss: 1.360300 \tValidation Loss: 2.555465\n",
      "Epoch: 28651 \tTraining Loss: 1.326808 \tValidation Loss: 2.554456\n",
      "Epoch: 28652 \tTraining Loss: 1.324966 \tValidation Loss: 2.555732\n",
      "Epoch: 28653 \tTraining Loss: 1.349696 \tValidation Loss: 2.555925\n",
      "Epoch: 28654 \tTraining Loss: 1.338094 \tValidation Loss: 2.555361\n",
      "Epoch: 28655 \tTraining Loss: 1.322395 \tValidation Loss: 2.554920\n",
      "Epoch: 28656 \tTraining Loss: 1.370146 \tValidation Loss: 2.554424\n",
      "Epoch: 28657 \tTraining Loss: 1.369487 \tValidation Loss: 2.554774\n",
      "Epoch: 28658 \tTraining Loss: 1.344674 \tValidation Loss: 2.554711\n",
      "Epoch: 28659 \tTraining Loss: 1.303251 \tValidation Loss: 2.555225\n",
      "Epoch: 28660 \tTraining Loss: 1.355800 \tValidation Loss: 2.555823\n",
      "Epoch: 28661 \tTraining Loss: 1.346334 \tValidation Loss: 2.555696\n",
      "Epoch: 28662 \tTraining Loss: 1.326928 \tValidation Loss: 2.555361\n",
      "Epoch: 28663 \tTraining Loss: 1.300401 \tValidation Loss: 2.555449\n",
      "Epoch: 28664 \tTraining Loss: 1.327646 \tValidation Loss: 2.554840\n",
      "Epoch: 28665 \tTraining Loss: 1.330184 \tValidation Loss: 2.555260\n",
      "Epoch: 28666 \tTraining Loss: 1.343290 \tValidation Loss: 2.555277\n",
      "Epoch: 28667 \tTraining Loss: 1.319117 \tValidation Loss: 2.556340\n",
      "Epoch: 28668 \tTraining Loss: 1.289178 \tValidation Loss: 2.556553\n",
      "Epoch: 28669 \tTraining Loss: 1.306870 \tValidation Loss: 2.554837\n",
      "Epoch: 28670 \tTraining Loss: 1.357914 \tValidation Loss: 2.554385\n",
      "Epoch: 28671 \tTraining Loss: 1.365944 \tValidation Loss: 2.555424\n",
      "Epoch: 28672 \tTraining Loss: 1.342289 \tValidation Loss: 2.555843\n",
      "Epoch: 28673 \tTraining Loss: 1.377973 \tValidation Loss: 2.555115\n",
      "Epoch: 28674 \tTraining Loss: 1.350047 \tValidation Loss: 2.555344\n",
      "Epoch: 28675 \tTraining Loss: 1.300624 \tValidation Loss: 2.555969\n",
      "Epoch: 28676 \tTraining Loss: 1.341745 \tValidation Loss: 2.555511\n",
      "Epoch: 28677 \tTraining Loss: 1.364057 \tValidation Loss: 2.556184\n",
      "Epoch: 28678 \tTraining Loss: 1.312008 \tValidation Loss: 2.555917\n",
      "Epoch: 28679 \tTraining Loss: 1.344697 \tValidation Loss: 2.555429\n",
      "Epoch: 28680 \tTraining Loss: 1.346674 \tValidation Loss: 2.555728\n",
      "Epoch: 28681 \tTraining Loss: 1.353743 \tValidation Loss: 2.556509\n",
      "Epoch: 28682 \tTraining Loss: 1.289903 \tValidation Loss: 2.556141\n",
      "Epoch: 28683 \tTraining Loss: 1.340449 \tValidation Loss: 2.556351\n",
      "Epoch: 28684 \tTraining Loss: 1.305743 \tValidation Loss: 2.556656\n",
      "Epoch: 28685 \tTraining Loss: 1.336039 \tValidation Loss: 2.554133\n",
      "Epoch: 28686 \tTraining Loss: 1.313945 \tValidation Loss: 2.555774\n",
      "Epoch: 28687 \tTraining Loss: 1.368341 \tValidation Loss: 2.555913\n",
      "Epoch: 28688 \tTraining Loss: 1.335570 \tValidation Loss: 2.554732\n",
      "Epoch: 28689 \tTraining Loss: 1.349307 \tValidation Loss: 2.555103\n",
      "Epoch: 28690 \tTraining Loss: 1.322116 \tValidation Loss: 2.556916\n",
      "Epoch: 28691 \tTraining Loss: 1.348248 \tValidation Loss: 2.555567\n",
      "Epoch: 28692 \tTraining Loss: 1.293857 \tValidation Loss: 2.556875\n",
      "Epoch: 28693 \tTraining Loss: 1.320035 \tValidation Loss: 2.555754\n",
      "Epoch: 28694 \tTraining Loss: 1.422860 \tValidation Loss: 2.555123\n",
      "Epoch: 28695 \tTraining Loss: 1.297174 \tValidation Loss: 2.556744\n",
      "Epoch: 28696 \tTraining Loss: 1.316056 \tValidation Loss: 2.555965\n",
      "Epoch: 28697 \tTraining Loss: 1.318590 \tValidation Loss: 2.556144\n",
      "Epoch: 28698 \tTraining Loss: 1.359069 \tValidation Loss: 2.555776\n",
      "Epoch: 28699 \tTraining Loss: 1.318081 \tValidation Loss: 2.556466\n",
      "Epoch: 28700 \tTraining Loss: 1.352057 \tValidation Loss: 2.555696\n",
      "Epoch: 28701 \tTraining Loss: 1.348547 \tValidation Loss: 2.554887\n",
      "Epoch: 28702 \tTraining Loss: 1.315060 \tValidation Loss: 2.556885\n",
      "Epoch: 28703 \tTraining Loss: 1.342812 \tValidation Loss: 2.556170\n",
      "Epoch: 28704 \tTraining Loss: 1.350935 \tValidation Loss: 2.555609\n",
      "Epoch: 28705 \tTraining Loss: 1.364918 \tValidation Loss: 2.555760\n",
      "Epoch: 28706 \tTraining Loss: 1.406198 \tValidation Loss: 2.554880\n",
      "Epoch: 28707 \tTraining Loss: 1.333320 \tValidation Loss: 2.555246\n",
      "Epoch: 28708 \tTraining Loss: 1.372308 \tValidation Loss: 2.555614\n",
      "Epoch: 28709 \tTraining Loss: 1.389676 \tValidation Loss: 2.555426\n",
      "Epoch: 28710 \tTraining Loss: 1.350116 \tValidation Loss: 2.554628\n",
      "Epoch: 28711 \tTraining Loss: 1.414094 \tValidation Loss: 2.555448\n",
      "Epoch: 28712 \tTraining Loss: 1.352347 \tValidation Loss: 2.555843\n",
      "Epoch: 28713 \tTraining Loss: 1.330731 \tValidation Loss: 2.556093\n",
      "Epoch: 28714 \tTraining Loss: 1.356714 \tValidation Loss: 2.556307\n",
      "Epoch: 28715 \tTraining Loss: 1.330196 \tValidation Loss: 2.555787\n",
      "Epoch: 28716 \tTraining Loss: 1.357636 \tValidation Loss: 2.556729\n",
      "Epoch: 28717 \tTraining Loss: 1.330031 \tValidation Loss: 2.556022\n",
      "Epoch: 28718 \tTraining Loss: 1.345988 \tValidation Loss: 2.556669\n",
      "Epoch: 28719 \tTraining Loss: 1.355314 \tValidation Loss: 2.555753\n",
      "Epoch: 28720 \tTraining Loss: 1.366194 \tValidation Loss: 2.554536\n",
      "Epoch: 28721 \tTraining Loss: 1.358864 \tValidation Loss: 2.555076\n",
      "Epoch: 28722 \tTraining Loss: 1.336047 \tValidation Loss: 2.555043\n",
      "Epoch: 28723 \tTraining Loss: 1.360945 \tValidation Loss: 2.556404\n",
      "Epoch: 28724 \tTraining Loss: 1.359692 \tValidation Loss: 2.556443\n",
      "Epoch: 28725 \tTraining Loss: 1.342993 \tValidation Loss: 2.556856\n",
      "Epoch: 28726 \tTraining Loss: 1.314476 \tValidation Loss: 2.557214\n",
      "Epoch: 28727 \tTraining Loss: 1.359608 \tValidation Loss: 2.556279\n",
      "Epoch: 28728 \tTraining Loss: 1.329928 \tValidation Loss: 2.556517\n",
      "Epoch: 28729 \tTraining Loss: 1.348057 \tValidation Loss: 2.556164\n",
      "Epoch: 28730 \tTraining Loss: 1.295328 \tValidation Loss: 2.556840\n",
      "Epoch: 28731 \tTraining Loss: 1.353715 \tValidation Loss: 2.556810\n",
      "Epoch: 28732 \tTraining Loss: 1.334186 \tValidation Loss: 2.556681\n",
      "Epoch: 28733 \tTraining Loss: 1.322824 \tValidation Loss: 2.557809\n",
      "Epoch: 28734 \tTraining Loss: 1.347330 \tValidation Loss: 2.557340\n",
      "Epoch: 28735 \tTraining Loss: 1.348962 \tValidation Loss: 2.556418\n",
      "Epoch: 28736 \tTraining Loss: 1.316082 \tValidation Loss: 2.556684\n",
      "Epoch: 28737 \tTraining Loss: 1.344799 \tValidation Loss: 2.557521\n",
      "Epoch: 28738 \tTraining Loss: 1.324727 \tValidation Loss: 2.555869\n",
      "Epoch: 28739 \tTraining Loss: 1.345715 \tValidation Loss: 2.556641\n",
      "Epoch: 28740 \tTraining Loss: 1.349665 \tValidation Loss: 2.556770\n",
      "Epoch: 28741 \tTraining Loss: 1.293673 \tValidation Loss: 2.556660\n",
      "Epoch: 28742 \tTraining Loss: 1.373761 \tValidation Loss: 2.555877\n",
      "Epoch: 28743 \tTraining Loss: 1.290789 \tValidation Loss: 2.557443\n",
      "Epoch: 28744 \tTraining Loss: 1.369203 \tValidation Loss: 2.555648\n",
      "Epoch: 28745 \tTraining Loss: 1.334279 \tValidation Loss: 2.556643\n",
      "Epoch: 28746 \tTraining Loss: 1.352631 \tValidation Loss: 2.557304\n",
      "Epoch: 28747 \tTraining Loss: 1.354256 \tValidation Loss: 2.556707\n",
      "Epoch: 28748 \tTraining Loss: 1.354357 \tValidation Loss: 2.556878\n",
      "Epoch: 28749 \tTraining Loss: 1.411923 \tValidation Loss: 2.556248\n",
      "Epoch: 28750 \tTraining Loss: 1.374094 \tValidation Loss: 2.557172\n",
      "Epoch: 28751 \tTraining Loss: 1.339746 \tValidation Loss: 2.556448\n",
      "Epoch: 28752 \tTraining Loss: 1.349469 \tValidation Loss: 2.557437\n",
      "Epoch: 28753 \tTraining Loss: 1.334000 \tValidation Loss: 2.557078\n",
      "Epoch: 28754 \tTraining Loss: 1.341182 \tValidation Loss: 2.556982\n",
      "Epoch: 28755 \tTraining Loss: 1.308715 \tValidation Loss: 2.557512\n",
      "Epoch: 28756 \tTraining Loss: 1.332663 \tValidation Loss: 2.557557\n",
      "Epoch: 28757 \tTraining Loss: 1.370575 \tValidation Loss: 2.557324\n",
      "Epoch: 28758 \tTraining Loss: 1.368031 \tValidation Loss: 2.556171\n",
      "Epoch: 28759 \tTraining Loss: 1.355029 \tValidation Loss: 2.556628\n",
      "Epoch: 28760 \tTraining Loss: 1.351928 \tValidation Loss: 2.555782\n",
      "Epoch: 28761 \tTraining Loss: 1.315212 \tValidation Loss: 2.557872\n",
      "Epoch: 28762 \tTraining Loss: 1.375333 \tValidation Loss: 2.556638\n",
      "Epoch: 28763 \tTraining Loss: 1.349960 \tValidation Loss: 2.556445\n",
      "Epoch: 28764 \tTraining Loss: 1.365347 \tValidation Loss: 2.557256\n",
      "Epoch: 28765 \tTraining Loss: 1.351470 \tValidation Loss: 2.556541\n",
      "Epoch: 28766 \tTraining Loss: 1.379078 \tValidation Loss: 2.556445\n",
      "Epoch: 28767 \tTraining Loss: 1.372980 \tValidation Loss: 2.556656\n",
      "Epoch: 28768 \tTraining Loss: 1.325202 \tValidation Loss: 2.556470\n",
      "Epoch: 28769 \tTraining Loss: 1.336243 \tValidation Loss: 2.557348\n",
      "Epoch: 28770 \tTraining Loss: 1.355669 \tValidation Loss: 2.557165\n",
      "Epoch: 28771 \tTraining Loss: 1.300100 \tValidation Loss: 2.558065\n",
      "Epoch: 28772 \tTraining Loss: 1.327880 \tValidation Loss: 2.557734\n",
      "Epoch: 28773 \tTraining Loss: 1.360345 \tValidation Loss: 2.555870\n",
      "Epoch: 28774 \tTraining Loss: 1.335348 \tValidation Loss: 2.556478\n",
      "Epoch: 28775 \tTraining Loss: 1.298497 \tValidation Loss: 2.557223\n",
      "Epoch: 28776 \tTraining Loss: 1.300403 \tValidation Loss: 2.557348\n",
      "Epoch: 28777 \tTraining Loss: 1.337425 \tValidation Loss: 2.556366\n",
      "Epoch: 28778 \tTraining Loss: 1.311092 \tValidation Loss: 2.559014\n",
      "Epoch: 28779 \tTraining Loss: 1.347805 \tValidation Loss: 2.558201\n",
      "Epoch: 28780 \tTraining Loss: 1.370525 \tValidation Loss: 2.557530\n",
      "Epoch: 28781 \tTraining Loss: 1.313425 \tValidation Loss: 2.557767\n",
      "Epoch: 28782 \tTraining Loss: 1.370037 \tValidation Loss: 2.558296\n",
      "Epoch: 28783 \tTraining Loss: 1.334159 \tValidation Loss: 2.557819\n",
      "Epoch: 28784 \tTraining Loss: 1.355979 \tValidation Loss: 2.557931\n",
      "Epoch: 28785 \tTraining Loss: 1.368451 \tValidation Loss: 2.556954\n",
      "Epoch: 28786 \tTraining Loss: 1.309536 \tValidation Loss: 2.558214\n",
      "Epoch: 28787 \tTraining Loss: 1.342319 \tValidation Loss: 2.557257\n",
      "Epoch: 28788 \tTraining Loss: 1.327710 \tValidation Loss: 2.559010\n",
      "Epoch: 28789 \tTraining Loss: 1.324477 \tValidation Loss: 2.559251\n",
      "Epoch: 28790 \tTraining Loss: 1.334806 \tValidation Loss: 2.558475\n",
      "Epoch: 28791 \tTraining Loss: 1.334061 \tValidation Loss: 2.559271\n",
      "Epoch: 28792 \tTraining Loss: 1.317428 \tValidation Loss: 2.558658\n",
      "Epoch: 28793 \tTraining Loss: 1.349899 \tValidation Loss: 2.558388\n",
      "Epoch: 28794 \tTraining Loss: 1.323326 \tValidation Loss: 2.557171\n",
      "Epoch: 28795 \tTraining Loss: 1.367680 \tValidation Loss: 2.557572\n",
      "Epoch: 28796 \tTraining Loss: 1.346255 \tValidation Loss: 2.557150\n",
      "Epoch: 28797 \tTraining Loss: 1.320128 \tValidation Loss: 2.557838\n",
      "Epoch: 28798 \tTraining Loss: 1.338980 \tValidation Loss: 2.558319\n",
      "Epoch: 28799 \tTraining Loss: 1.332626 \tValidation Loss: 2.558241\n",
      "Epoch: 28800 \tTraining Loss: 1.324224 \tValidation Loss: 2.558429\n",
      "Epoch: 28801 \tTraining Loss: 1.305766 \tValidation Loss: 2.559586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28802 \tTraining Loss: 1.325294 \tValidation Loss: 2.558761\n",
      "Epoch: 28803 \tTraining Loss: 1.357604 \tValidation Loss: 2.558131\n",
      "Epoch: 28804 \tTraining Loss: 1.316982 \tValidation Loss: 2.558581\n",
      "Epoch: 28805 \tTraining Loss: 1.357146 \tValidation Loss: 2.557354\n",
      "Epoch: 28806 \tTraining Loss: 1.364086 \tValidation Loss: 2.558085\n",
      "Epoch: 28807 \tTraining Loss: 1.359698 \tValidation Loss: 2.557263\n",
      "Epoch: 28808 \tTraining Loss: 1.299331 \tValidation Loss: 2.557061\n",
      "Epoch: 28809 \tTraining Loss: 1.289200 \tValidation Loss: 2.558931\n",
      "Epoch: 28810 \tTraining Loss: 1.366980 \tValidation Loss: 2.557759\n",
      "Epoch: 28811 \tTraining Loss: 1.314311 \tValidation Loss: 2.557045\n",
      "Epoch: 28812 \tTraining Loss: 1.400174 \tValidation Loss: 2.557767\n",
      "Epoch: 28813 \tTraining Loss: 1.292157 \tValidation Loss: 2.558052\n",
      "Epoch: 28814 \tTraining Loss: 1.286290 \tValidation Loss: 2.558585\n",
      "Epoch: 28815 \tTraining Loss: 1.401822 \tValidation Loss: 2.557175\n",
      "Epoch: 28816 \tTraining Loss: 1.362730 \tValidation Loss: 2.557219\n",
      "Epoch: 28817 \tTraining Loss: 1.333210 \tValidation Loss: 2.558156\n",
      "Epoch: 28818 \tTraining Loss: 1.330703 \tValidation Loss: 2.558235\n",
      "Epoch: 28819 \tTraining Loss: 1.295638 \tValidation Loss: 2.558525\n",
      "Epoch: 28820 \tTraining Loss: 1.368009 \tValidation Loss: 2.557317\n",
      "Epoch: 28821 \tTraining Loss: 1.381972 \tValidation Loss: 2.557645\n",
      "Epoch: 28822 \tTraining Loss: 1.313397 \tValidation Loss: 2.558695\n",
      "Epoch: 28823 \tTraining Loss: 1.340069 \tValidation Loss: 2.558436\n",
      "Epoch: 28824 \tTraining Loss: 1.358984 \tValidation Loss: 2.558271\n",
      "Epoch: 28825 \tTraining Loss: 1.331619 \tValidation Loss: 2.559185\n",
      "Epoch: 28826 \tTraining Loss: 1.364565 \tValidation Loss: 2.557167\n",
      "Epoch: 28827 \tTraining Loss: 1.370696 \tValidation Loss: 2.556562\n",
      "Epoch: 28828 \tTraining Loss: 1.342213 \tValidation Loss: 2.557646\n",
      "Epoch: 28829 \tTraining Loss: 1.347842 \tValidation Loss: 2.558458\n",
      "Epoch: 28830 \tTraining Loss: 1.317415 \tValidation Loss: 2.560090\n",
      "Epoch: 28831 \tTraining Loss: 1.312963 \tValidation Loss: 2.559108\n",
      "Epoch: 28832 \tTraining Loss: 1.314846 \tValidation Loss: 2.559576\n",
      "Epoch: 28833 \tTraining Loss: 1.345606 \tValidation Loss: 2.558933\n",
      "Epoch: 28834 \tTraining Loss: 1.311007 \tValidation Loss: 2.559189\n",
      "Epoch: 28835 \tTraining Loss: 1.267464 \tValidation Loss: 2.558613\n",
      "Epoch: 28836 \tTraining Loss: 1.311894 \tValidation Loss: 2.559532\n",
      "Epoch: 28837 \tTraining Loss: 1.319480 \tValidation Loss: 2.558130\n",
      "Epoch: 28838 \tTraining Loss: 1.323811 \tValidation Loss: 2.558475\n",
      "Epoch: 28839 \tTraining Loss: 1.321665 \tValidation Loss: 2.557532\n",
      "Epoch: 28840 \tTraining Loss: 1.309080 \tValidation Loss: 2.558648\n",
      "Epoch: 28841 \tTraining Loss: 1.312489 \tValidation Loss: 2.558765\n",
      "Epoch: 28842 \tTraining Loss: 1.300135 \tValidation Loss: 2.558066\n",
      "Epoch: 28843 \tTraining Loss: 1.284565 \tValidation Loss: 2.559005\n",
      "Epoch: 28844 \tTraining Loss: 1.296653 \tValidation Loss: 2.560810\n",
      "Epoch: 28845 \tTraining Loss: 1.354379 \tValidation Loss: 2.558851\n",
      "Epoch: 28846 \tTraining Loss: 1.350845 \tValidation Loss: 2.559459\n",
      "Epoch: 28847 \tTraining Loss: 1.338854 \tValidation Loss: 2.558351\n",
      "Epoch: 28848 \tTraining Loss: 1.336144 \tValidation Loss: 2.558479\n",
      "Epoch: 28849 \tTraining Loss: 1.346681 \tValidation Loss: 2.559063\n",
      "Epoch: 28850 \tTraining Loss: 1.355789 \tValidation Loss: 2.557473\n",
      "Epoch: 28851 \tTraining Loss: 1.336640 \tValidation Loss: 2.560376\n",
      "Epoch: 28852 \tTraining Loss: 1.308494 \tValidation Loss: 2.559457\n",
      "Epoch: 28853 \tTraining Loss: 1.362442 \tValidation Loss: 2.559097\n",
      "Epoch: 28854 \tTraining Loss: 1.272943 \tValidation Loss: 2.559834\n",
      "Epoch: 28855 \tTraining Loss: 1.372546 \tValidation Loss: 2.558345\n",
      "Epoch: 28856 \tTraining Loss: 1.326277 \tValidation Loss: 2.558790\n",
      "Epoch: 28857 \tTraining Loss: 1.346832 \tValidation Loss: 2.558284\n",
      "Epoch: 28858 \tTraining Loss: 1.327000 \tValidation Loss: 2.558593\n",
      "Epoch: 28859 \tTraining Loss: 1.362861 \tValidation Loss: 2.557222\n",
      "Epoch: 28860 \tTraining Loss: 1.345086 \tValidation Loss: 2.558570\n",
      "Epoch: 28861 \tTraining Loss: 1.323166 \tValidation Loss: 2.558926\n",
      "Epoch: 28862 \tTraining Loss: 1.323771 \tValidation Loss: 2.558948\n",
      "Epoch: 28863 \tTraining Loss: 1.359837 \tValidation Loss: 2.558435\n",
      "Epoch: 28864 \tTraining Loss: 1.312279 \tValidation Loss: 2.559801\n",
      "Epoch: 28865 \tTraining Loss: 1.278819 \tValidation Loss: 2.560072\n",
      "Epoch: 28866 \tTraining Loss: 1.348672 \tValidation Loss: 2.558982\n",
      "Epoch: 28867 \tTraining Loss: 1.365132 \tValidation Loss: 2.558744\n",
      "Epoch: 28868 \tTraining Loss: 1.330549 \tValidation Loss: 2.559906\n",
      "Epoch: 28869 \tTraining Loss: 1.325990 \tValidation Loss: 2.559190\n",
      "Epoch: 28870 \tTraining Loss: 1.334217 \tValidation Loss: 2.559766\n",
      "Epoch: 28871 \tTraining Loss: 1.281869 \tValidation Loss: 2.559854\n",
      "Epoch: 28872 \tTraining Loss: 1.371784 \tValidation Loss: 2.559284\n",
      "Epoch: 28873 \tTraining Loss: 1.347683 \tValidation Loss: 2.559978\n",
      "Epoch: 28874 \tTraining Loss: 1.363436 \tValidation Loss: 2.558848\n",
      "Epoch: 28875 \tTraining Loss: 1.266853 \tValidation Loss: 2.560921\n",
      "Epoch: 28876 \tTraining Loss: 1.347343 \tValidation Loss: 2.559777\n",
      "Epoch: 28877 \tTraining Loss: 1.339528 \tValidation Loss: 2.559875\n",
      "Epoch: 28878 \tTraining Loss: 1.273010 \tValidation Loss: 2.559580\n",
      "Epoch: 28879 \tTraining Loss: 1.349439 \tValidation Loss: 2.560053\n",
      "Epoch: 28880 \tTraining Loss: 1.339872 \tValidation Loss: 2.558660\n",
      "Epoch: 28881 \tTraining Loss: 1.322677 \tValidation Loss: 2.559194\n",
      "Epoch: 28882 \tTraining Loss: 1.316539 \tValidation Loss: 2.560711\n",
      "Epoch: 28883 \tTraining Loss: 1.356078 \tValidation Loss: 2.559550\n",
      "Epoch: 28884 \tTraining Loss: 1.311460 \tValidation Loss: 2.559152\n",
      "Epoch: 28885 \tTraining Loss: 1.314433 \tValidation Loss: 2.561193\n",
      "Epoch: 28886 \tTraining Loss: 1.331720 \tValidation Loss: 2.560491\n",
      "Epoch: 28887 \tTraining Loss: 1.277921 \tValidation Loss: 2.560068\n",
      "Epoch: 28888 \tTraining Loss: 1.345762 \tValidation Loss: 2.559805\n",
      "Epoch: 28889 \tTraining Loss: 1.345143 \tValidation Loss: 2.561057\n",
      "Epoch: 28890 \tTraining Loss: 1.355926 \tValidation Loss: 2.559765\n",
      "Epoch: 28891 \tTraining Loss: 1.352702 \tValidation Loss: 2.560763\n",
      "Epoch: 28892 \tTraining Loss: 1.365523 \tValidation Loss: 2.559507\n",
      "Epoch: 28893 \tTraining Loss: 1.320899 \tValidation Loss: 2.560807\n",
      "Epoch: 28894 \tTraining Loss: 1.349010 \tValidation Loss: 2.558900\n",
      "Epoch: 28895 \tTraining Loss: 1.326321 \tValidation Loss: 2.559926\n",
      "Epoch: 28896 \tTraining Loss: 1.316426 \tValidation Loss: 2.560727\n",
      "Epoch: 28897 \tTraining Loss: 1.296400 \tValidation Loss: 2.560325\n",
      "Epoch: 28898 \tTraining Loss: 1.330602 \tValidation Loss: 2.560456\n",
      "Epoch: 28899 \tTraining Loss: 1.315789 \tValidation Loss: 2.560379\n",
      "Epoch: 28900 \tTraining Loss: 1.315361 \tValidation Loss: 2.561568\n",
      "Epoch: 28901 \tTraining Loss: 1.348845 \tValidation Loss: 2.559230\n",
      "Epoch: 28902 \tTraining Loss: 1.314946 \tValidation Loss: 2.560469\n",
      "Epoch: 28903 \tTraining Loss: 1.329464 \tValidation Loss: 2.561240\n",
      "Epoch: 28904 \tTraining Loss: 1.354198 \tValidation Loss: 2.559170\n",
      "Epoch: 28905 \tTraining Loss: 1.333592 \tValidation Loss: 2.560733\n",
      "Epoch: 28906 \tTraining Loss: 1.285786 \tValidation Loss: 2.560821\n",
      "Epoch: 28907 \tTraining Loss: 1.307680 \tValidation Loss: 2.560430\n",
      "Epoch: 28908 \tTraining Loss: 1.347571 \tValidation Loss: 2.560349\n",
      "Epoch: 28909 \tTraining Loss: 1.347717 \tValidation Loss: 2.558977\n",
      "Epoch: 28910 \tTraining Loss: 1.327687 \tValidation Loss: 2.560397\n",
      "Epoch: 28911 \tTraining Loss: 1.347735 \tValidation Loss: 2.560003\n",
      "Epoch: 28912 \tTraining Loss: 1.336566 \tValidation Loss: 2.560539\n",
      "Epoch: 28913 \tTraining Loss: 1.316988 \tValidation Loss: 2.559294\n",
      "Epoch: 28914 \tTraining Loss: 1.282230 \tValidation Loss: 2.560076\n",
      "Epoch: 28915 \tTraining Loss: 1.346686 \tValidation Loss: 2.559113\n",
      "Epoch: 28916 \tTraining Loss: 1.288816 \tValidation Loss: 2.561190\n",
      "Epoch: 28917 \tTraining Loss: 1.367036 \tValidation Loss: 2.559064\n",
      "Epoch: 28918 \tTraining Loss: 1.323111 \tValidation Loss: 2.560619\n",
      "Epoch: 28919 \tTraining Loss: 1.310313 \tValidation Loss: 2.559914\n",
      "Epoch: 28920 \tTraining Loss: 1.352793 \tValidation Loss: 2.561228\n",
      "Epoch: 28921 \tTraining Loss: 1.369283 \tValidation Loss: 2.561415\n",
      "Epoch: 28922 \tTraining Loss: 1.344403 \tValidation Loss: 2.560452\n",
      "Epoch: 28923 \tTraining Loss: 1.331903 \tValidation Loss: 2.560760\n",
      "Epoch: 28924 \tTraining Loss: 1.332181 \tValidation Loss: 2.560407\n",
      "Epoch: 28925 \tTraining Loss: 1.345975 \tValidation Loss: 2.559997\n",
      "Epoch: 28926 \tTraining Loss: 1.344705 \tValidation Loss: 2.561555\n",
      "Epoch: 28927 \tTraining Loss: 1.328806 \tValidation Loss: 2.561383\n",
      "Epoch: 28928 \tTraining Loss: 1.313309 \tValidation Loss: 2.561553\n",
      "Epoch: 28929 \tTraining Loss: 1.315038 \tValidation Loss: 2.560211\n",
      "Epoch: 28930 \tTraining Loss: 1.291947 \tValidation Loss: 2.561396\n",
      "Epoch: 28931 \tTraining Loss: 1.307020 \tValidation Loss: 2.561626\n",
      "Epoch: 28932 \tTraining Loss: 1.390223 \tValidation Loss: 2.560164\n",
      "Epoch: 28933 \tTraining Loss: 1.319418 \tValidation Loss: 2.561213\n",
      "Epoch: 28934 \tTraining Loss: 1.298018 \tValidation Loss: 2.562144\n",
      "Epoch: 28935 \tTraining Loss: 1.331154 \tValidation Loss: 2.560496\n",
      "Epoch: 28936 \tTraining Loss: 1.368680 \tValidation Loss: 2.561674\n",
      "Epoch: 28937 \tTraining Loss: 1.330873 \tValidation Loss: 2.561629\n",
      "Epoch: 28938 \tTraining Loss: 1.331868 \tValidation Loss: 2.561444\n",
      "Epoch: 28939 \tTraining Loss: 1.347522 \tValidation Loss: 2.562689\n",
      "Epoch: 28940 \tTraining Loss: 1.386166 \tValidation Loss: 2.559948\n",
      "Epoch: 28941 \tTraining Loss: 1.289106 \tValidation Loss: 2.560295\n",
      "Epoch: 28942 \tTraining Loss: 1.362726 \tValidation Loss: 2.560297\n",
      "Epoch: 28943 \tTraining Loss: 1.350882 \tValidation Loss: 2.561644\n",
      "Epoch: 28944 \tTraining Loss: 1.333136 \tValidation Loss: 2.559861\n",
      "Epoch: 28945 \tTraining Loss: 1.363482 \tValidation Loss: 2.560674\n",
      "Epoch: 28946 \tTraining Loss: 1.345908 \tValidation Loss: 2.560749\n",
      "Epoch: 28947 \tTraining Loss: 1.374359 \tValidation Loss: 2.559677\n",
      "Epoch: 28948 \tTraining Loss: 1.353660 \tValidation Loss: 2.560463\n",
      "Epoch: 28949 \tTraining Loss: 1.333350 \tValidation Loss: 2.560766\n",
      "Epoch: 28950 \tTraining Loss: 1.312250 \tValidation Loss: 2.562359\n",
      "Epoch: 28951 \tTraining Loss: 1.347085 \tValidation Loss: 2.560940\n",
      "Epoch: 28952 \tTraining Loss: 1.356771 \tValidation Loss: 2.561542\n",
      "Epoch: 28953 \tTraining Loss: 1.345507 \tValidation Loss: 2.560937\n",
      "Epoch: 28954 \tTraining Loss: 1.355265 \tValidation Loss: 2.560802\n",
      "Epoch: 28955 \tTraining Loss: 1.342589 \tValidation Loss: 2.561601\n",
      "Epoch: 28956 \tTraining Loss: 1.348542 \tValidation Loss: 2.559757\n",
      "Epoch: 28957 \tTraining Loss: 1.304677 \tValidation Loss: 2.560939\n",
      "Epoch: 28958 \tTraining Loss: 1.323922 \tValidation Loss: 2.561369\n",
      "Epoch: 28959 \tTraining Loss: 1.317122 \tValidation Loss: 2.560744\n",
      "Epoch: 28960 \tTraining Loss: 1.347453 \tValidation Loss: 2.560217\n",
      "Epoch: 28961 \tTraining Loss: 1.301568 \tValidation Loss: 2.560519\n",
      "Epoch: 28962 \tTraining Loss: 1.379202 \tValidation Loss: 2.560660\n",
      "Epoch: 28963 \tTraining Loss: 1.340739 \tValidation Loss: 2.561807\n",
      "Epoch: 28964 \tTraining Loss: 1.375239 \tValidation Loss: 2.560192\n",
      "Epoch: 28965 \tTraining Loss: 1.315121 \tValidation Loss: 2.561273\n",
      "Epoch: 28966 \tTraining Loss: 1.336097 \tValidation Loss: 2.560161\n",
      "Epoch: 28967 \tTraining Loss: 1.323339 \tValidation Loss: 2.561040\n",
      "Epoch: 28968 \tTraining Loss: 1.304395 \tValidation Loss: 2.560583\n",
      "Epoch: 28969 \tTraining Loss: 1.325480 \tValidation Loss: 2.562631\n",
      "Epoch: 28970 \tTraining Loss: 1.301022 \tValidation Loss: 2.562104\n",
      "Epoch: 28971 \tTraining Loss: 1.335600 \tValidation Loss: 2.562348\n",
      "Epoch: 28972 \tTraining Loss: 1.328969 \tValidation Loss: 2.561810\n",
      "Epoch: 28973 \tTraining Loss: 1.337461 \tValidation Loss: 2.561965\n",
      "Epoch: 28974 \tTraining Loss: 1.334100 \tValidation Loss: 2.561522\n",
      "Epoch: 28975 \tTraining Loss: 1.339770 \tValidation Loss: 2.560718\n",
      "Epoch: 28976 \tTraining Loss: 1.346725 \tValidation Loss: 2.561938\n",
      "Epoch: 28977 \tTraining Loss: 1.293048 \tValidation Loss: 2.562284\n",
      "Epoch: 28978 \tTraining Loss: 1.393786 \tValidation Loss: 2.560922\n",
      "Epoch: 28979 \tTraining Loss: 1.375209 \tValidation Loss: 2.561216\n",
      "Epoch: 28980 \tTraining Loss: 1.335570 \tValidation Loss: 2.561188\n",
      "Epoch: 28981 \tTraining Loss: 1.414339 \tValidation Loss: 2.559924\n",
      "Epoch: 28982 \tTraining Loss: 1.343856 \tValidation Loss: 2.560811\n",
      "Epoch: 28983 \tTraining Loss: 1.334203 \tValidation Loss: 2.561139\n",
      "Epoch: 28984 \tTraining Loss: 1.381492 \tValidation Loss: 2.560351\n",
      "Epoch: 28985 \tTraining Loss: 1.308906 \tValidation Loss: 2.561065\n",
      "Epoch: 28986 \tTraining Loss: 1.343963 \tValidation Loss: 2.561406\n",
      "Epoch: 28987 \tTraining Loss: 1.330519 \tValidation Loss: 2.561648\n",
      "Epoch: 28988 \tTraining Loss: 1.374682 \tValidation Loss: 2.562430\n",
      "Epoch: 28989 \tTraining Loss: 1.365921 \tValidation Loss: 2.560422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28990 \tTraining Loss: 1.331749 \tValidation Loss: 2.560939\n",
      "Epoch: 28991 \tTraining Loss: 1.316175 \tValidation Loss: 2.560452\n",
      "Epoch: 28992 \tTraining Loss: 1.353543 \tValidation Loss: 2.560175\n",
      "Epoch: 28993 \tTraining Loss: 1.359625 \tValidation Loss: 2.560217\n",
      "Epoch: 28994 \tTraining Loss: 1.361418 \tValidation Loss: 2.560926\n",
      "Epoch: 28995 \tTraining Loss: 1.331244 \tValidation Loss: 2.560916\n",
      "Epoch: 28996 \tTraining Loss: 1.351025 \tValidation Loss: 2.560027\n",
      "Epoch: 28997 \tTraining Loss: 1.349989 \tValidation Loss: 2.561440\n",
      "Epoch: 28998 \tTraining Loss: 1.315862 \tValidation Loss: 2.563773\n",
      "Epoch: 28999 \tTraining Loss: 1.329330 \tValidation Loss: 2.562004\n",
      "Epoch: 29000 \tTraining Loss: 1.349568 \tValidation Loss: 2.562306\n",
      "Epoch: 29001 \tTraining Loss: 1.339397 \tValidation Loss: 2.560977\n",
      "Epoch: 29002 \tTraining Loss: 1.376993 \tValidation Loss: 2.562289\n",
      "Epoch: 29003 \tTraining Loss: 1.325054 \tValidation Loss: 2.562226\n",
      "Epoch: 29004 \tTraining Loss: 1.322438 \tValidation Loss: 2.562013\n",
      "Epoch: 29005 \tTraining Loss: 1.369821 \tValidation Loss: 2.561537\n",
      "Epoch: 29006 \tTraining Loss: 1.331793 \tValidation Loss: 2.561957\n",
      "Epoch: 29007 \tTraining Loss: 1.329656 \tValidation Loss: 2.563313\n",
      "Epoch: 29008 \tTraining Loss: 1.324357 \tValidation Loss: 2.562009\n",
      "Epoch: 29009 \tTraining Loss: 1.298349 \tValidation Loss: 2.563308\n",
      "Epoch: 29010 \tTraining Loss: 1.362784 \tValidation Loss: 2.562231\n",
      "Epoch: 29011 \tTraining Loss: 1.362038 \tValidation Loss: 2.561724\n",
      "Epoch: 29012 \tTraining Loss: 1.334873 \tValidation Loss: 2.563265\n",
      "Epoch: 29013 \tTraining Loss: 1.324315 \tValidation Loss: 2.561577\n",
      "Epoch: 29014 \tTraining Loss: 1.348921 \tValidation Loss: 2.563081\n",
      "Epoch: 29015 \tTraining Loss: 1.309136 \tValidation Loss: 2.563967\n",
      "Epoch: 29016 \tTraining Loss: 1.338391 \tValidation Loss: 2.561780\n",
      "Epoch: 29017 \tTraining Loss: 1.367880 \tValidation Loss: 2.562427\n",
      "Epoch: 29018 \tTraining Loss: 1.361094 \tValidation Loss: 2.562642\n",
      "Epoch: 29019 \tTraining Loss: 1.331688 \tValidation Loss: 2.562511\n",
      "Epoch: 29020 \tTraining Loss: 1.339989 \tValidation Loss: 2.561197\n",
      "Epoch: 29021 \tTraining Loss: 1.303257 \tValidation Loss: 2.562419\n",
      "Epoch: 29022 \tTraining Loss: 1.341479 \tValidation Loss: 2.562468\n",
      "Epoch: 29023 \tTraining Loss: 1.290211 \tValidation Loss: 2.563760\n",
      "Epoch: 29024 \tTraining Loss: 1.334640 \tValidation Loss: 2.563143\n",
      "Epoch: 29025 \tTraining Loss: 1.347259 \tValidation Loss: 2.563621\n",
      "Epoch: 29026 \tTraining Loss: 1.341350 \tValidation Loss: 2.562487\n",
      "Epoch: 29027 \tTraining Loss: 1.327292 \tValidation Loss: 2.562726\n",
      "Epoch: 29028 \tTraining Loss: 1.345864 \tValidation Loss: 2.562872\n",
      "Epoch: 29029 \tTraining Loss: 1.334964 \tValidation Loss: 2.563007\n",
      "Epoch: 29030 \tTraining Loss: 1.345457 \tValidation Loss: 2.560782\n",
      "Epoch: 29031 \tTraining Loss: 1.325599 \tValidation Loss: 2.563528\n",
      "Epoch: 29032 \tTraining Loss: 1.317503 \tValidation Loss: 2.564364\n",
      "Epoch: 29033 \tTraining Loss: 1.308985 \tValidation Loss: 2.563462\n",
      "Epoch: 29034 \tTraining Loss: 1.344813 \tValidation Loss: 2.562729\n",
      "Epoch: 29035 \tTraining Loss: 1.315613 \tValidation Loss: 2.563768\n",
      "Epoch: 29036 \tTraining Loss: 1.328081 \tValidation Loss: 2.562469\n",
      "Epoch: 29037 \tTraining Loss: 1.351040 \tValidation Loss: 2.562607\n",
      "Epoch: 29038 \tTraining Loss: 1.344299 \tValidation Loss: 2.563537\n",
      "Epoch: 29039 \tTraining Loss: 1.393045 \tValidation Loss: 2.563273\n",
      "Epoch: 29040 \tTraining Loss: 1.328654 \tValidation Loss: 2.563396\n",
      "Epoch: 29041 \tTraining Loss: 1.281612 \tValidation Loss: 2.563707\n",
      "Epoch: 29042 \tTraining Loss: 1.359128 \tValidation Loss: 2.562630\n",
      "Epoch: 29043 \tTraining Loss: 1.394953 \tValidation Loss: 2.563193\n",
      "Epoch: 29044 \tTraining Loss: 1.306253 \tValidation Loss: 2.563260\n",
      "Epoch: 29045 \tTraining Loss: 1.297604 \tValidation Loss: 2.563249\n",
      "Epoch: 29046 \tTraining Loss: 1.277833 \tValidation Loss: 2.563793\n",
      "Epoch: 29047 \tTraining Loss: 1.279697 \tValidation Loss: 2.564978\n",
      "Epoch: 29048 \tTraining Loss: 1.310751 \tValidation Loss: 2.563658\n",
      "Epoch: 29049 \tTraining Loss: 1.331391 \tValidation Loss: 2.564144\n",
      "Epoch: 29050 \tTraining Loss: 1.293953 \tValidation Loss: 2.565908\n",
      "Epoch: 29051 \tTraining Loss: 1.311694 \tValidation Loss: 2.563244\n",
      "Epoch: 29052 \tTraining Loss: 1.399888 \tValidation Loss: 2.563251\n",
      "Epoch: 29053 \tTraining Loss: 1.373809 \tValidation Loss: 2.562282\n",
      "Epoch: 29054 \tTraining Loss: 1.317989 \tValidation Loss: 2.563629\n",
      "Epoch: 29055 \tTraining Loss: 1.352307 \tValidation Loss: 2.563303\n",
      "Epoch: 29056 \tTraining Loss: 1.357731 \tValidation Loss: 2.563632\n",
      "Epoch: 29057 \tTraining Loss: 1.330272 \tValidation Loss: 2.564056\n",
      "Epoch: 29058 \tTraining Loss: 1.318594 \tValidation Loss: 2.564764\n",
      "Epoch: 29059 \tTraining Loss: 1.311813 \tValidation Loss: 2.563778\n",
      "Epoch: 29060 \tTraining Loss: 1.349442 \tValidation Loss: 2.563270\n",
      "Epoch: 29061 \tTraining Loss: 1.346532 \tValidation Loss: 2.563961\n",
      "Epoch: 29062 \tTraining Loss: 1.317780 \tValidation Loss: 2.564396\n",
      "Epoch: 29063 \tTraining Loss: 1.340892 \tValidation Loss: 2.564891\n",
      "Epoch: 29064 \tTraining Loss: 1.316024 \tValidation Loss: 2.564164\n",
      "Epoch: 29065 \tTraining Loss: 1.329468 \tValidation Loss: 2.564749\n",
      "Epoch: 29066 \tTraining Loss: 1.321048 \tValidation Loss: 2.563620\n",
      "Epoch: 29067 \tTraining Loss: 1.360493 \tValidation Loss: 2.565196\n",
      "Epoch: 29068 \tTraining Loss: 1.315979 \tValidation Loss: 2.564279\n",
      "Epoch: 29069 \tTraining Loss: 1.367714 \tValidation Loss: 2.563421\n",
      "Epoch: 29070 \tTraining Loss: 1.334413 \tValidation Loss: 2.563733\n",
      "Epoch: 29071 \tTraining Loss: 1.368360 \tValidation Loss: 2.563453\n",
      "Epoch: 29072 \tTraining Loss: 1.323793 \tValidation Loss: 2.563846\n",
      "Epoch: 29073 \tTraining Loss: 1.297495 \tValidation Loss: 2.564429\n",
      "Epoch: 29074 \tTraining Loss: 1.341259 \tValidation Loss: 2.565036\n",
      "Epoch: 29075 \tTraining Loss: 1.331554 \tValidation Loss: 2.565068\n",
      "Epoch: 29076 \tTraining Loss: 1.333635 \tValidation Loss: 2.564056\n",
      "Epoch: 29077 \tTraining Loss: 1.378843 \tValidation Loss: 2.563890\n",
      "Epoch: 29078 \tTraining Loss: 1.306468 \tValidation Loss: 2.563289\n",
      "Epoch: 29079 \tTraining Loss: 1.339876 \tValidation Loss: 2.564160\n",
      "Epoch: 29080 \tTraining Loss: 1.337152 \tValidation Loss: 2.562261\n",
      "Epoch: 29081 \tTraining Loss: 1.321884 \tValidation Loss: 2.564138\n",
      "Epoch: 29082 \tTraining Loss: 1.375989 \tValidation Loss: 2.563682\n",
      "Epoch: 29083 \tTraining Loss: 1.344166 \tValidation Loss: 2.564276\n",
      "Epoch: 29084 \tTraining Loss: 1.351253 \tValidation Loss: 2.564259\n",
      "Epoch: 29085 \tTraining Loss: 1.289465 \tValidation Loss: 2.564394\n",
      "Epoch: 29086 \tTraining Loss: 1.299446 \tValidation Loss: 2.564537\n",
      "Epoch: 29087 \tTraining Loss: 1.312714 \tValidation Loss: 2.565393\n",
      "Epoch: 29088 \tTraining Loss: 1.297376 \tValidation Loss: 2.564399\n",
      "Epoch: 29089 \tTraining Loss: 1.333087 \tValidation Loss: 2.564502\n",
      "Epoch: 29090 \tTraining Loss: 1.341820 \tValidation Loss: 2.566053\n",
      "Epoch: 29091 \tTraining Loss: 1.320551 \tValidation Loss: 2.564823\n",
      "Epoch: 29092 \tTraining Loss: 1.314639 \tValidation Loss: 2.564055\n",
      "Epoch: 29093 \tTraining Loss: 1.340672 \tValidation Loss: 2.564021\n",
      "Epoch: 29094 \tTraining Loss: 1.316707 \tValidation Loss: 2.563956\n",
      "Epoch: 29095 \tTraining Loss: 1.315729 \tValidation Loss: 2.562677\n",
      "Epoch: 29096 \tTraining Loss: 1.335001 \tValidation Loss: 2.562485\n",
      "Epoch: 29097 \tTraining Loss: 1.280841 \tValidation Loss: 2.563366\n",
      "Epoch: 29098 \tTraining Loss: 1.341092 \tValidation Loss: 2.562907\n",
      "Epoch: 29099 \tTraining Loss: 1.334775 \tValidation Loss: 2.564899\n",
      "Epoch: 29100 \tTraining Loss: 1.357980 \tValidation Loss: 2.563089\n",
      "Epoch: 29101 \tTraining Loss: 1.316572 \tValidation Loss: 2.564406\n",
      "Epoch: 29102 \tTraining Loss: 1.300230 \tValidation Loss: 2.566386\n",
      "Epoch: 29103 \tTraining Loss: 1.318611 \tValidation Loss: 2.565360\n",
      "Epoch: 29104 \tTraining Loss: 1.290235 \tValidation Loss: 2.564808\n",
      "Epoch: 29105 \tTraining Loss: 1.306902 \tValidation Loss: 2.563990\n",
      "Epoch: 29106 \tTraining Loss: 1.348692 \tValidation Loss: 2.565897\n",
      "Epoch: 29107 \tTraining Loss: 1.341515 \tValidation Loss: 2.565471\n",
      "Epoch: 29108 \tTraining Loss: 1.314717 \tValidation Loss: 2.563870\n",
      "Epoch: 29109 \tTraining Loss: 1.355272 \tValidation Loss: 2.564251\n",
      "Epoch: 29110 \tTraining Loss: 1.371101 \tValidation Loss: 2.564182\n",
      "Epoch: 29111 \tTraining Loss: 1.306738 \tValidation Loss: 2.564879\n",
      "Epoch: 29112 \tTraining Loss: 1.351369 \tValidation Loss: 2.564965\n",
      "Epoch: 29113 \tTraining Loss: 1.341894 \tValidation Loss: 2.564398\n",
      "Epoch: 29114 \tTraining Loss: 1.311568 \tValidation Loss: 2.564774\n",
      "Epoch: 29115 \tTraining Loss: 1.334350 \tValidation Loss: 2.564183\n",
      "Epoch: 29116 \tTraining Loss: 1.294818 \tValidation Loss: 2.565253\n",
      "Epoch: 29117 \tTraining Loss: 1.315343 \tValidation Loss: 2.565143\n",
      "Epoch: 29118 \tTraining Loss: 1.331014 \tValidation Loss: 2.564759\n",
      "Epoch: 29119 \tTraining Loss: 1.366090 \tValidation Loss: 2.565548\n",
      "Epoch: 29120 \tTraining Loss: 1.302926 \tValidation Loss: 2.566326\n",
      "Epoch: 29121 \tTraining Loss: 1.351865 \tValidation Loss: 2.565152\n",
      "Epoch: 29122 \tTraining Loss: 1.335425 \tValidation Loss: 2.564772\n",
      "Epoch: 29123 \tTraining Loss: 1.306801 \tValidation Loss: 2.565023\n",
      "Epoch: 29124 \tTraining Loss: 1.333546 \tValidation Loss: 2.565813\n",
      "Epoch: 29125 \tTraining Loss: 1.352624 \tValidation Loss: 2.563901\n",
      "Epoch: 29126 \tTraining Loss: 1.318095 \tValidation Loss: 2.565110\n",
      "Epoch: 29127 \tTraining Loss: 1.323897 \tValidation Loss: 2.563694\n",
      "Epoch: 29128 \tTraining Loss: 1.276565 \tValidation Loss: 2.564923\n",
      "Epoch: 29129 \tTraining Loss: 1.341822 \tValidation Loss: 2.564252\n",
      "Epoch: 29130 \tTraining Loss: 1.332674 \tValidation Loss: 2.565384\n",
      "Epoch: 29131 \tTraining Loss: 1.290283 \tValidation Loss: 2.564546\n",
      "Epoch: 29132 \tTraining Loss: 1.357837 \tValidation Loss: 2.563788\n",
      "Epoch: 29133 \tTraining Loss: 1.336050 \tValidation Loss: 2.564924\n",
      "Epoch: 29134 \tTraining Loss: 1.336939 \tValidation Loss: 2.564095\n",
      "Epoch: 29135 \tTraining Loss: 1.322789 \tValidation Loss: 2.565169\n",
      "Epoch: 29136 \tTraining Loss: 1.353233 \tValidation Loss: 2.564080\n",
      "Epoch: 29137 \tTraining Loss: 1.322231 \tValidation Loss: 2.564769\n",
      "Epoch: 29138 \tTraining Loss: 1.343848 \tValidation Loss: 2.564842\n",
      "Epoch: 29139 \tTraining Loss: 1.315724 \tValidation Loss: 2.565845\n",
      "Epoch: 29140 \tTraining Loss: 1.359606 \tValidation Loss: 2.565859\n",
      "Epoch: 29141 \tTraining Loss: 1.377562 \tValidation Loss: 2.565018\n",
      "Epoch: 29142 \tTraining Loss: 1.311625 \tValidation Loss: 2.566635\n",
      "Epoch: 29143 \tTraining Loss: 1.342723 \tValidation Loss: 2.566058\n",
      "Epoch: 29144 \tTraining Loss: 1.341228 \tValidation Loss: 2.564640\n",
      "Epoch: 29145 \tTraining Loss: 1.309687 \tValidation Loss: 2.566618\n",
      "Epoch: 29146 \tTraining Loss: 1.329800 \tValidation Loss: 2.565499\n",
      "Epoch: 29147 \tTraining Loss: 1.364921 \tValidation Loss: 2.564055\n",
      "Epoch: 29148 \tTraining Loss: 1.318960 \tValidation Loss: 2.565572\n",
      "Epoch: 29149 \tTraining Loss: 1.315169 \tValidation Loss: 2.567120\n",
      "Epoch: 29150 \tTraining Loss: 1.357509 \tValidation Loss: 2.566477\n",
      "Epoch: 29151 \tTraining Loss: 1.338426 \tValidation Loss: 2.566143\n",
      "Epoch: 29152 \tTraining Loss: 1.294868 \tValidation Loss: 2.565761\n",
      "Epoch: 29153 \tTraining Loss: 1.297876 \tValidation Loss: 2.567108\n",
      "Epoch: 29154 \tTraining Loss: 1.328656 \tValidation Loss: 2.566240\n",
      "Epoch: 29155 \tTraining Loss: 1.335222 \tValidation Loss: 2.565853\n",
      "Epoch: 29156 \tTraining Loss: 1.342481 \tValidation Loss: 2.564930\n",
      "Epoch: 29157 \tTraining Loss: 1.286836 \tValidation Loss: 2.565133\n",
      "Epoch: 29158 \tTraining Loss: 1.345643 \tValidation Loss: 2.565448\n",
      "Epoch: 29159 \tTraining Loss: 1.361676 \tValidation Loss: 2.565204\n",
      "Epoch: 29160 \tTraining Loss: 1.354272 \tValidation Loss: 2.565086\n",
      "Epoch: 29161 \tTraining Loss: 1.354662 \tValidation Loss: 2.565857\n",
      "Epoch: 29162 \tTraining Loss: 1.329719 \tValidation Loss: 2.566208\n",
      "Epoch: 29163 \tTraining Loss: 1.303636 \tValidation Loss: 2.564167\n",
      "Epoch: 29164 \tTraining Loss: 1.379159 \tValidation Loss: 2.564490\n",
      "Epoch: 29165 \tTraining Loss: 1.303126 \tValidation Loss: 2.565176\n",
      "Epoch: 29166 \tTraining Loss: 1.328213 \tValidation Loss: 2.565904\n",
      "Epoch: 29167 \tTraining Loss: 1.356372 \tValidation Loss: 2.565753\n",
      "Epoch: 29168 \tTraining Loss: 1.365285 \tValidation Loss: 2.565849\n",
      "Epoch: 29169 \tTraining Loss: 1.320177 \tValidation Loss: 2.566959\n",
      "Epoch: 29170 \tTraining Loss: 1.350943 \tValidation Loss: 2.566136\n",
      "Epoch: 29171 \tTraining Loss: 1.339965 \tValidation Loss: 2.566405\n",
      "Epoch: 29172 \tTraining Loss: 1.291256 \tValidation Loss: 2.567227\n",
      "Epoch: 29173 \tTraining Loss: 1.371140 \tValidation Loss: 2.564818\n",
      "Epoch: 29174 \tTraining Loss: 1.353545 \tValidation Loss: 2.565452\n",
      "Epoch: 29175 \tTraining Loss: 1.330073 \tValidation Loss: 2.565713\n",
      "Epoch: 29176 \tTraining Loss: 1.346662 \tValidation Loss: 2.567171\n",
      "Epoch: 29177 \tTraining Loss: 1.296462 \tValidation Loss: 2.567228\n",
      "Epoch: 29178 \tTraining Loss: 1.334416 \tValidation Loss: 2.566644\n",
      "Epoch: 29179 \tTraining Loss: 1.318057 \tValidation Loss: 2.565270\n",
      "Epoch: 29180 \tTraining Loss: 1.333426 \tValidation Loss: 2.566166\n",
      "Epoch: 29181 \tTraining Loss: 1.289920 \tValidation Loss: 2.567319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29182 \tTraining Loss: 1.307498 \tValidation Loss: 2.566762\n",
      "Epoch: 29183 \tTraining Loss: 1.298494 \tValidation Loss: 2.566762\n",
      "Epoch: 29184 \tTraining Loss: 1.364288 \tValidation Loss: 2.565623\n",
      "Epoch: 29185 \tTraining Loss: 1.328291 \tValidation Loss: 2.566423\n",
      "Epoch: 29186 \tTraining Loss: 1.335283 \tValidation Loss: 2.565845\n",
      "Epoch: 29187 \tTraining Loss: 1.344762 \tValidation Loss: 2.567144\n",
      "Epoch: 29188 \tTraining Loss: 1.362572 \tValidation Loss: 2.566909\n",
      "Epoch: 29189 \tTraining Loss: 1.297753 \tValidation Loss: 2.567774\n",
      "Epoch: 29190 \tTraining Loss: 1.328368 \tValidation Loss: 2.565803\n",
      "Epoch: 29191 \tTraining Loss: 1.344592 \tValidation Loss: 2.565661\n",
      "Epoch: 29192 \tTraining Loss: 1.317947 \tValidation Loss: 2.567161\n",
      "Epoch: 29193 \tTraining Loss: 1.364246 \tValidation Loss: 2.565908\n",
      "Epoch: 29194 \tTraining Loss: 1.322207 \tValidation Loss: 2.567806\n",
      "Epoch: 29195 \tTraining Loss: 1.330648 \tValidation Loss: 2.566682\n",
      "Epoch: 29196 \tTraining Loss: 1.336967 \tValidation Loss: 2.567091\n",
      "Epoch: 29197 \tTraining Loss: 1.323522 \tValidation Loss: 2.566197\n",
      "Epoch: 29198 \tTraining Loss: 1.346603 \tValidation Loss: 2.567420\n",
      "Epoch: 29199 \tTraining Loss: 1.310441 \tValidation Loss: 2.567244\n",
      "Epoch: 29200 \tTraining Loss: 1.326945 \tValidation Loss: 2.566396\n",
      "Epoch: 29201 \tTraining Loss: 1.336553 \tValidation Loss: 2.567087\n",
      "Epoch: 29202 \tTraining Loss: 1.327126 \tValidation Loss: 2.566078\n",
      "Epoch: 29203 \tTraining Loss: 1.334625 \tValidation Loss: 2.566295\n",
      "Epoch: 29204 \tTraining Loss: 1.333254 \tValidation Loss: 2.567044\n",
      "Epoch: 29205 \tTraining Loss: 1.354277 \tValidation Loss: 2.566179\n",
      "Epoch: 29206 \tTraining Loss: 1.331168 \tValidation Loss: 2.566474\n",
      "Epoch: 29207 \tTraining Loss: 1.299382 \tValidation Loss: 2.568058\n",
      "Epoch: 29208 \tTraining Loss: 1.318581 \tValidation Loss: 2.566387\n",
      "Epoch: 29209 \tTraining Loss: 1.331176 \tValidation Loss: 2.568103\n",
      "Epoch: 29210 \tTraining Loss: 1.333304 \tValidation Loss: 2.567343\n",
      "Epoch: 29211 \tTraining Loss: 1.303370 \tValidation Loss: 2.567661\n",
      "Epoch: 29212 \tTraining Loss: 1.355964 \tValidation Loss: 2.566269\n",
      "Epoch: 29213 \tTraining Loss: 1.354320 \tValidation Loss: 2.566647\n",
      "Epoch: 29214 \tTraining Loss: 1.322818 \tValidation Loss: 2.567748\n",
      "Epoch: 29215 \tTraining Loss: 1.343675 \tValidation Loss: 2.566960\n",
      "Epoch: 29216 \tTraining Loss: 1.322843 \tValidation Loss: 2.567133\n",
      "Epoch: 29217 \tTraining Loss: 1.341931 \tValidation Loss: 2.565873\n",
      "Epoch: 29218 \tTraining Loss: 1.278579 \tValidation Loss: 2.566844\n",
      "Epoch: 29219 \tTraining Loss: 1.331198 \tValidation Loss: 2.566919\n",
      "Epoch: 29220 \tTraining Loss: 1.338885 \tValidation Loss: 2.566819\n",
      "Epoch: 29221 \tTraining Loss: 1.340729 \tValidation Loss: 2.566779\n",
      "Epoch: 29222 \tTraining Loss: 1.322521 \tValidation Loss: 2.566181\n",
      "Epoch: 29223 \tTraining Loss: 1.347928 \tValidation Loss: 2.567036\n",
      "Epoch: 29224 \tTraining Loss: 1.334422 \tValidation Loss: 2.566685\n",
      "Epoch: 29225 \tTraining Loss: 1.348094 \tValidation Loss: 2.566111\n",
      "Epoch: 29226 \tTraining Loss: 1.301134 \tValidation Loss: 2.567343\n",
      "Epoch: 29227 \tTraining Loss: 1.340676 \tValidation Loss: 2.567567\n",
      "Epoch: 29228 \tTraining Loss: 1.368037 \tValidation Loss: 2.566262\n",
      "Epoch: 29229 \tTraining Loss: 1.342395 \tValidation Loss: 2.566072\n",
      "Epoch: 29230 \tTraining Loss: 1.343641 \tValidation Loss: 2.567647\n",
      "Epoch: 29231 \tTraining Loss: 1.349667 \tValidation Loss: 2.566691\n",
      "Epoch: 29232 \tTraining Loss: 1.308948 \tValidation Loss: 2.568522\n",
      "Epoch: 29233 \tTraining Loss: 1.348894 \tValidation Loss: 2.565304\n",
      "Epoch: 29234 \tTraining Loss: 1.320924 \tValidation Loss: 2.567201\n",
      "Epoch: 29235 \tTraining Loss: 1.329402 \tValidation Loss: 2.566603\n",
      "Epoch: 29236 \tTraining Loss: 1.329380 \tValidation Loss: 2.567891\n",
      "Epoch: 29237 \tTraining Loss: 1.351594 \tValidation Loss: 2.567532\n",
      "Epoch: 29238 \tTraining Loss: 1.342120 \tValidation Loss: 2.566986\n",
      "Epoch: 29239 \tTraining Loss: 1.325862 \tValidation Loss: 2.566171\n",
      "Epoch: 29240 \tTraining Loss: 1.325657 \tValidation Loss: 2.567285\n",
      "Epoch: 29241 \tTraining Loss: 1.327349 \tValidation Loss: 2.567063\n",
      "Epoch: 29242 \tTraining Loss: 1.285669 \tValidation Loss: 2.566838\n",
      "Epoch: 29243 \tTraining Loss: 1.354588 \tValidation Loss: 2.566502\n",
      "Epoch: 29244 \tTraining Loss: 1.326340 \tValidation Loss: 2.567340\n",
      "Epoch: 29245 \tTraining Loss: 1.297655 \tValidation Loss: 2.567216\n",
      "Epoch: 29246 \tTraining Loss: 1.330339 \tValidation Loss: 2.568485\n",
      "Epoch: 29247 \tTraining Loss: 1.309596 \tValidation Loss: 2.568230\n",
      "Epoch: 29248 \tTraining Loss: 1.365889 \tValidation Loss: 2.568450\n",
      "Epoch: 29249 \tTraining Loss: 1.328493 \tValidation Loss: 2.568017\n",
      "Epoch: 29250 \tTraining Loss: 1.352731 \tValidation Loss: 2.568163\n",
      "Epoch: 29251 \tTraining Loss: 1.295437 \tValidation Loss: 2.567436\n",
      "Epoch: 29252 \tTraining Loss: 1.330723 \tValidation Loss: 2.567111\n",
      "Epoch: 29253 \tTraining Loss: 1.322679 \tValidation Loss: 2.567483\n",
      "Epoch: 29254 \tTraining Loss: 1.371022 \tValidation Loss: 2.566777\n",
      "Epoch: 29255 \tTraining Loss: 1.299863 \tValidation Loss: 2.567883\n",
      "Epoch: 29256 \tTraining Loss: 1.337262 \tValidation Loss: 2.567925\n",
      "Epoch: 29257 \tTraining Loss: 1.346675 \tValidation Loss: 2.565403\n",
      "Epoch: 29258 \tTraining Loss: 1.319630 \tValidation Loss: 2.567518\n",
      "Epoch: 29259 \tTraining Loss: 1.321824 \tValidation Loss: 2.567789\n",
      "Epoch: 29260 \tTraining Loss: 1.321858 \tValidation Loss: 2.567112\n",
      "Epoch: 29261 \tTraining Loss: 1.287849 \tValidation Loss: 2.567698\n",
      "Epoch: 29262 \tTraining Loss: 1.341180 \tValidation Loss: 2.567793\n",
      "Epoch: 29263 \tTraining Loss: 1.330353 \tValidation Loss: 2.566708\n",
      "Epoch: 29264 \tTraining Loss: 1.343055 \tValidation Loss: 2.567096\n",
      "Epoch: 29265 \tTraining Loss: 1.338209 \tValidation Loss: 2.567464\n",
      "Epoch: 29266 \tTraining Loss: 1.366832 \tValidation Loss: 2.566990\n",
      "Epoch: 29267 \tTraining Loss: 1.308739 \tValidation Loss: 2.566735\n",
      "Epoch: 29268 \tTraining Loss: 1.289231 \tValidation Loss: 2.567140\n",
      "Epoch: 29269 \tTraining Loss: 1.352986 \tValidation Loss: 2.567280\n",
      "Epoch: 29270 \tTraining Loss: 1.314620 \tValidation Loss: 2.568802\n",
      "Epoch: 29271 \tTraining Loss: 1.348260 \tValidation Loss: 2.567216\n",
      "Epoch: 29272 \tTraining Loss: 1.329346 \tValidation Loss: 2.567688\n",
      "Epoch: 29273 \tTraining Loss: 1.340841 \tValidation Loss: 2.568643\n",
      "Epoch: 29274 \tTraining Loss: 1.308749 \tValidation Loss: 2.567000\n",
      "Epoch: 29275 \tTraining Loss: 1.294262 \tValidation Loss: 2.567655\n",
      "Epoch: 29276 \tTraining Loss: 1.317095 \tValidation Loss: 2.567557\n",
      "Epoch: 29277 \tTraining Loss: 1.345475 \tValidation Loss: 2.567929\n",
      "Epoch: 29278 \tTraining Loss: 1.281095 \tValidation Loss: 2.567511\n",
      "Epoch: 29279 \tTraining Loss: 1.307617 \tValidation Loss: 2.569827\n",
      "Epoch: 29280 \tTraining Loss: 1.309520 \tValidation Loss: 2.568254\n",
      "Epoch: 29281 \tTraining Loss: 1.360268 \tValidation Loss: 2.567538\n",
      "Epoch: 29282 \tTraining Loss: 1.350795 \tValidation Loss: 2.566839\n",
      "Epoch: 29283 \tTraining Loss: 1.309388 \tValidation Loss: 2.567567\n",
      "Epoch: 29284 \tTraining Loss: 1.337177 \tValidation Loss: 2.567794\n",
      "Epoch: 29285 \tTraining Loss: 1.357944 \tValidation Loss: 2.567513\n",
      "Epoch: 29286 \tTraining Loss: 1.357509 \tValidation Loss: 2.567219\n",
      "Epoch: 29287 \tTraining Loss: 1.307016 \tValidation Loss: 2.567982\n",
      "Epoch: 29288 \tTraining Loss: 1.361178 \tValidation Loss: 2.567483\n",
      "Epoch: 29289 \tTraining Loss: 1.338034 \tValidation Loss: 2.567815\n",
      "Epoch: 29290 \tTraining Loss: 1.285385 \tValidation Loss: 2.567931\n",
      "Epoch: 29291 \tTraining Loss: 1.345215 \tValidation Loss: 2.567748\n",
      "Epoch: 29292 \tTraining Loss: 1.377437 \tValidation Loss: 2.567877\n",
      "Epoch: 29293 \tTraining Loss: 1.362863 \tValidation Loss: 2.568633\n",
      "Epoch: 29294 \tTraining Loss: 1.313810 \tValidation Loss: 2.567966\n",
      "Epoch: 29295 \tTraining Loss: 1.337125 \tValidation Loss: 2.568157\n",
      "Epoch: 29296 \tTraining Loss: 1.338569 \tValidation Loss: 2.568986\n",
      "Epoch: 29297 \tTraining Loss: 1.289873 \tValidation Loss: 2.570029\n",
      "Epoch: 29298 \tTraining Loss: 1.340821 \tValidation Loss: 2.568060\n",
      "Epoch: 29299 \tTraining Loss: 1.318329 \tValidation Loss: 2.569338\n",
      "Epoch: 29300 \tTraining Loss: 1.349513 \tValidation Loss: 2.567783\n",
      "Epoch: 29301 \tTraining Loss: 1.330226 \tValidation Loss: 2.567332\n",
      "Epoch: 29302 \tTraining Loss: 1.365605 \tValidation Loss: 2.567434\n",
      "Epoch: 29303 \tTraining Loss: 1.317301 \tValidation Loss: 2.567968\n",
      "Epoch: 29304 \tTraining Loss: 1.315680 \tValidation Loss: 2.569575\n",
      "Epoch: 29305 \tTraining Loss: 1.318807 \tValidation Loss: 2.567977\n",
      "Epoch: 29306 \tTraining Loss: 1.298667 \tValidation Loss: 2.569402\n",
      "Epoch: 29307 \tTraining Loss: 1.332547 \tValidation Loss: 2.570028\n",
      "Epoch: 29308 \tTraining Loss: 1.309049 \tValidation Loss: 2.567629\n",
      "Epoch: 29309 \tTraining Loss: 1.321473 \tValidation Loss: 2.569333\n",
      "Epoch: 29310 \tTraining Loss: 1.284695 \tValidation Loss: 2.569790\n",
      "Epoch: 29311 \tTraining Loss: 1.361330 \tValidation Loss: 2.568920\n",
      "Epoch: 29312 \tTraining Loss: 1.301672 \tValidation Loss: 2.568313\n",
      "Epoch: 29313 \tTraining Loss: 1.282230 \tValidation Loss: 2.568889\n",
      "Epoch: 29314 \tTraining Loss: 1.303273 \tValidation Loss: 2.568360\n",
      "Epoch: 29315 \tTraining Loss: 1.313312 \tValidation Loss: 2.569933\n",
      "Epoch: 29316 \tTraining Loss: 1.303580 \tValidation Loss: 2.568378\n",
      "Epoch: 29317 \tTraining Loss: 1.300057 \tValidation Loss: 2.568494\n",
      "Epoch: 29318 \tTraining Loss: 1.333670 \tValidation Loss: 2.568748\n",
      "Epoch: 29319 \tTraining Loss: 1.319297 \tValidation Loss: 2.569198\n",
      "Epoch: 29320 \tTraining Loss: 1.320341 \tValidation Loss: 2.569106\n",
      "Epoch: 29321 \tTraining Loss: 1.325838 \tValidation Loss: 2.569816\n",
      "Epoch: 29322 \tTraining Loss: 1.324424 \tValidation Loss: 2.569602\n",
      "Epoch: 29323 \tTraining Loss: 1.384237 \tValidation Loss: 2.569059\n",
      "Epoch: 29324 \tTraining Loss: 1.353522 \tValidation Loss: 2.567890\n",
      "Epoch: 29325 \tTraining Loss: 1.322836 \tValidation Loss: 2.569796\n",
      "Epoch: 29326 \tTraining Loss: 1.308730 \tValidation Loss: 2.569721\n",
      "Epoch: 29327 \tTraining Loss: 1.336351 \tValidation Loss: 2.568683\n",
      "Epoch: 29328 \tTraining Loss: 1.322211 \tValidation Loss: 2.568793\n",
      "Epoch: 29329 \tTraining Loss: 1.312017 \tValidation Loss: 2.569124\n",
      "Epoch: 29330 \tTraining Loss: 1.371557 \tValidation Loss: 2.568210\n",
      "Epoch: 29331 \tTraining Loss: 1.285767 \tValidation Loss: 2.568623\n",
      "Epoch: 29332 \tTraining Loss: 1.351945 \tValidation Loss: 2.568118\n",
      "Epoch: 29333 \tTraining Loss: 1.309476 \tValidation Loss: 2.569107\n",
      "Epoch: 29334 \tTraining Loss: 1.291855 \tValidation Loss: 2.569910\n",
      "Epoch: 29335 \tTraining Loss: 1.367202 \tValidation Loss: 2.569306\n",
      "Epoch: 29336 \tTraining Loss: 1.352175 \tValidation Loss: 2.570165\n",
      "Epoch: 29337 \tTraining Loss: 1.332544 \tValidation Loss: 2.569430\n",
      "Epoch: 29338 \tTraining Loss: 1.370520 \tValidation Loss: 2.569172\n",
      "Epoch: 29339 \tTraining Loss: 1.331019 \tValidation Loss: 2.567932\n",
      "Epoch: 29340 \tTraining Loss: 1.320474 \tValidation Loss: 2.568721\n",
      "Epoch: 29341 \tTraining Loss: 1.341207 \tValidation Loss: 2.568844\n",
      "Epoch: 29342 \tTraining Loss: 1.351179 \tValidation Loss: 2.568888\n",
      "Epoch: 29343 \tTraining Loss: 1.323215 \tValidation Loss: 2.568668\n",
      "Epoch: 29344 \tTraining Loss: 1.329897 \tValidation Loss: 2.569291\n",
      "Epoch: 29345 \tTraining Loss: 1.320098 \tValidation Loss: 2.568848\n",
      "Epoch: 29346 \tTraining Loss: 1.328904 \tValidation Loss: 2.568630\n",
      "Epoch: 29347 \tTraining Loss: 1.308398 \tValidation Loss: 2.568728\n",
      "Epoch: 29348 \tTraining Loss: 1.352724 \tValidation Loss: 2.569640\n",
      "Epoch: 29349 \tTraining Loss: 1.348597 \tValidation Loss: 2.568789\n",
      "Epoch: 29350 \tTraining Loss: 1.335289 \tValidation Loss: 2.568088\n",
      "Epoch: 29351 \tTraining Loss: 1.312342 \tValidation Loss: 2.568404\n",
      "Epoch: 29352 \tTraining Loss: 1.290969 \tValidation Loss: 2.569745\n",
      "Epoch: 29353 \tTraining Loss: 1.372363 \tValidation Loss: 2.569114\n",
      "Epoch: 29354 \tTraining Loss: 1.363735 \tValidation Loss: 2.568670\n",
      "Epoch: 29355 \tTraining Loss: 1.306980 \tValidation Loss: 2.570203\n",
      "Epoch: 29356 \tTraining Loss: 1.284869 \tValidation Loss: 2.570207\n",
      "Epoch: 29357 \tTraining Loss: 1.372882 \tValidation Loss: 2.568845\n",
      "Epoch: 29358 \tTraining Loss: 1.307307 \tValidation Loss: 2.569891\n",
      "Epoch: 29359 \tTraining Loss: 1.370448 \tValidation Loss: 2.568678\n",
      "Epoch: 29360 \tTraining Loss: 1.288833 \tValidation Loss: 2.569385\n",
      "Epoch: 29361 \tTraining Loss: 1.338986 \tValidation Loss: 2.569533\n",
      "Epoch: 29362 \tTraining Loss: 1.304258 \tValidation Loss: 2.569675\n",
      "Epoch: 29363 \tTraining Loss: 1.357482 \tValidation Loss: 2.569103\n",
      "Epoch: 29364 \tTraining Loss: 1.322887 \tValidation Loss: 2.568862\n",
      "Epoch: 29365 \tTraining Loss: 1.327944 \tValidation Loss: 2.570665\n",
      "Epoch: 29366 \tTraining Loss: 1.313011 \tValidation Loss: 2.569728\n",
      "Epoch: 29367 \tTraining Loss: 1.330203 \tValidation Loss: 2.569661\n",
      "Epoch: 29368 \tTraining Loss: 1.333117 \tValidation Loss: 2.569879\n",
      "Epoch: 29369 \tTraining Loss: 1.309036 \tValidation Loss: 2.569710\n",
      "Epoch: 29370 \tTraining Loss: 1.306607 \tValidation Loss: 2.570023\n",
      "Epoch: 29371 \tTraining Loss: 1.335653 \tValidation Loss: 2.569170\n",
      "Epoch: 29372 \tTraining Loss: 1.331895 \tValidation Loss: 2.569589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29373 \tTraining Loss: 1.321901 \tValidation Loss: 2.570413\n",
      "Epoch: 29374 \tTraining Loss: 1.323923 \tValidation Loss: 2.570703\n",
      "Epoch: 29375 \tTraining Loss: 1.328152 \tValidation Loss: 2.568567\n",
      "Epoch: 29376 \tTraining Loss: 1.350667 \tValidation Loss: 2.568767\n",
      "Epoch: 29377 \tTraining Loss: 1.357595 \tValidation Loss: 2.568487\n",
      "Epoch: 29378 \tTraining Loss: 1.259151 \tValidation Loss: 2.571103\n",
      "Epoch: 29379 \tTraining Loss: 1.331463 \tValidation Loss: 2.569796\n",
      "Epoch: 29380 \tTraining Loss: 1.342521 \tValidation Loss: 2.569715\n",
      "Epoch: 29381 \tTraining Loss: 1.322768 \tValidation Loss: 2.569893\n",
      "Epoch: 29382 \tTraining Loss: 1.310466 \tValidation Loss: 2.571074\n",
      "Epoch: 29383 \tTraining Loss: 1.316404 \tValidation Loss: 2.570528\n",
      "Epoch: 29384 \tTraining Loss: 1.337181 \tValidation Loss: 2.570593\n",
      "Epoch: 29385 \tTraining Loss: 1.283918 \tValidation Loss: 2.569978\n",
      "Epoch: 29386 \tTraining Loss: 1.308194 \tValidation Loss: 2.569417\n",
      "Epoch: 29387 \tTraining Loss: 1.294065 \tValidation Loss: 2.569984\n",
      "Epoch: 29388 \tTraining Loss: 1.347625 \tValidation Loss: 2.569525\n",
      "Epoch: 29389 \tTraining Loss: 1.323352 \tValidation Loss: 2.570365\n",
      "Epoch: 29390 \tTraining Loss: 1.322873 \tValidation Loss: 2.569412\n",
      "Epoch: 29391 \tTraining Loss: 1.285482 \tValidation Loss: 2.570889\n",
      "Epoch: 29392 \tTraining Loss: 1.305790 \tValidation Loss: 2.570118\n",
      "Epoch: 29393 \tTraining Loss: 1.338301 \tValidation Loss: 2.569115\n",
      "Epoch: 29394 \tTraining Loss: 1.314764 \tValidation Loss: 2.571501\n",
      "Epoch: 29395 \tTraining Loss: 1.290252 \tValidation Loss: 2.570528\n",
      "Epoch: 29396 \tTraining Loss: 1.344526 \tValidation Loss: 2.570017\n",
      "Epoch: 29397 \tTraining Loss: 1.360411 \tValidation Loss: 2.570314\n",
      "Epoch: 29398 \tTraining Loss: 1.328880 \tValidation Loss: 2.569849\n",
      "Epoch: 29399 \tTraining Loss: 1.301908 \tValidation Loss: 2.570008\n",
      "Epoch: 29400 \tTraining Loss: 1.348949 \tValidation Loss: 2.569190\n",
      "Epoch: 29401 \tTraining Loss: 1.352939 \tValidation Loss: 2.570454\n",
      "Epoch: 29402 \tTraining Loss: 1.368970 \tValidation Loss: 2.569515\n",
      "Epoch: 29403 \tTraining Loss: 1.285218 \tValidation Loss: 2.571470\n",
      "Epoch: 29404 \tTraining Loss: 1.322142 \tValidation Loss: 2.570922\n",
      "Epoch: 29405 \tTraining Loss: 1.288914 \tValidation Loss: 2.571192\n",
      "Epoch: 29406 \tTraining Loss: 1.341206 \tValidation Loss: 2.570608\n",
      "Epoch: 29407 \tTraining Loss: 1.293919 \tValidation Loss: 2.570951\n",
      "Epoch: 29408 \tTraining Loss: 1.354576 \tValidation Loss: 2.570846\n",
      "Epoch: 29409 \tTraining Loss: 1.291725 \tValidation Loss: 2.571906\n",
      "Epoch: 29410 \tTraining Loss: 1.340782 \tValidation Loss: 2.569141\n",
      "Epoch: 29411 \tTraining Loss: 1.342537 \tValidation Loss: 2.568943\n",
      "Epoch: 29412 \tTraining Loss: 1.318819 \tValidation Loss: 2.570239\n",
      "Epoch: 29413 \tTraining Loss: 1.304161 \tValidation Loss: 2.570770\n",
      "Epoch: 29414 \tTraining Loss: 1.283784 \tValidation Loss: 2.570886\n",
      "Epoch: 29415 \tTraining Loss: 1.328575 \tValidation Loss: 2.570402\n",
      "Epoch: 29416 \tTraining Loss: 1.319013 \tValidation Loss: 2.570835\n",
      "Epoch: 29417 \tTraining Loss: 1.341267 \tValidation Loss: 2.570352\n",
      "Epoch: 29418 \tTraining Loss: 1.339985 \tValidation Loss: 2.569940\n",
      "Epoch: 29419 \tTraining Loss: 1.315083 \tValidation Loss: 2.571241\n",
      "Epoch: 29420 \tTraining Loss: 1.339241 \tValidation Loss: 2.570320\n",
      "Epoch: 29421 \tTraining Loss: 1.316711 \tValidation Loss: 2.570493\n",
      "Epoch: 29422 \tTraining Loss: 1.326342 \tValidation Loss: 2.571370\n",
      "Epoch: 29423 \tTraining Loss: 1.307328 \tValidation Loss: 2.572039\n",
      "Epoch: 29424 \tTraining Loss: 1.379300 \tValidation Loss: 2.570070\n",
      "Epoch: 29425 \tTraining Loss: 1.313424 \tValidation Loss: 2.569594\n",
      "Epoch: 29426 \tTraining Loss: 1.345285 \tValidation Loss: 2.569702\n",
      "Epoch: 29427 \tTraining Loss: 1.307436 \tValidation Loss: 2.571385\n",
      "Epoch: 29428 \tTraining Loss: 1.300093 \tValidation Loss: 2.571381\n",
      "Epoch: 29429 \tTraining Loss: 1.316167 \tValidation Loss: 2.571623\n",
      "Epoch: 29430 \tTraining Loss: 1.290782 \tValidation Loss: 2.571965\n",
      "Epoch: 29431 \tTraining Loss: 1.310864 \tValidation Loss: 2.571353\n",
      "Epoch: 29432 \tTraining Loss: 1.333504 \tValidation Loss: 2.570868\n",
      "Epoch: 29433 \tTraining Loss: 1.336890 \tValidation Loss: 2.571431\n",
      "Epoch: 29434 \tTraining Loss: 1.357496 \tValidation Loss: 2.572163\n",
      "Epoch: 29435 \tTraining Loss: 1.335874 \tValidation Loss: 2.570319\n",
      "Epoch: 29436 \tTraining Loss: 1.330674 \tValidation Loss: 2.571431\n",
      "Epoch: 29437 \tTraining Loss: 1.292695 \tValidation Loss: 2.570852\n",
      "Epoch: 29438 \tTraining Loss: 1.315607 \tValidation Loss: 2.571645\n",
      "Epoch: 29439 \tTraining Loss: 1.318737 \tValidation Loss: 2.572488\n",
      "Epoch: 29440 \tTraining Loss: 1.354674 \tValidation Loss: 2.571159\n",
      "Epoch: 29441 \tTraining Loss: 1.371444 \tValidation Loss: 2.570503\n",
      "Epoch: 29442 \tTraining Loss: 1.407622 \tValidation Loss: 2.569364\n",
      "Epoch: 29443 \tTraining Loss: 1.327172 \tValidation Loss: 2.571367\n",
      "Epoch: 29444 \tTraining Loss: 1.348764 \tValidation Loss: 2.569692\n",
      "Epoch: 29445 \tTraining Loss: 1.296443 \tValidation Loss: 2.570740\n",
      "Epoch: 29446 \tTraining Loss: 1.306107 \tValidation Loss: 2.570726\n",
      "Epoch: 29447 \tTraining Loss: 1.310666 \tValidation Loss: 2.571563\n",
      "Epoch: 29448 \tTraining Loss: 1.324091 \tValidation Loss: 2.571328\n",
      "Epoch: 29449 \tTraining Loss: 1.332716 \tValidation Loss: 2.571478\n",
      "Epoch: 29450 \tTraining Loss: 1.357042 \tValidation Loss: 2.570864\n",
      "Epoch: 29451 \tTraining Loss: 1.309077 \tValidation Loss: 2.572404\n",
      "Epoch: 29452 \tTraining Loss: 1.341410 \tValidation Loss: 2.572351\n",
      "Epoch: 29453 \tTraining Loss: 1.328384 \tValidation Loss: 2.572267\n",
      "Epoch: 29454 \tTraining Loss: 1.314605 \tValidation Loss: 2.570859\n",
      "Epoch: 29455 \tTraining Loss: 1.387109 \tValidation Loss: 2.569138\n",
      "Epoch: 29456 \tTraining Loss: 1.304407 \tValidation Loss: 2.570527\n",
      "Epoch: 29457 \tTraining Loss: 1.316876 \tValidation Loss: 2.571511\n",
      "Epoch: 29458 \tTraining Loss: 1.328221 \tValidation Loss: 2.571959\n",
      "Epoch: 29459 \tTraining Loss: 1.329107 \tValidation Loss: 2.571885\n",
      "Epoch: 29460 \tTraining Loss: 1.331076 \tValidation Loss: 2.571546\n",
      "Epoch: 29461 \tTraining Loss: 1.334262 \tValidation Loss: 2.571906\n",
      "Epoch: 29462 \tTraining Loss: 1.329871 \tValidation Loss: 2.573187\n",
      "Epoch: 29463 \tTraining Loss: 1.362818 \tValidation Loss: 2.571364\n",
      "Epoch: 29464 \tTraining Loss: 1.281238 \tValidation Loss: 2.573125\n",
      "Epoch: 29465 \tTraining Loss: 1.376995 \tValidation Loss: 2.571109\n",
      "Epoch: 29466 \tTraining Loss: 1.308609 \tValidation Loss: 2.572237\n",
      "Epoch: 29467 \tTraining Loss: 1.364819 \tValidation Loss: 2.571543\n",
      "Epoch: 29468 \tTraining Loss: 1.295131 \tValidation Loss: 2.574203\n",
      "Epoch: 29469 \tTraining Loss: 1.309045 \tValidation Loss: 2.571666\n",
      "Epoch: 29470 \tTraining Loss: 1.323390 \tValidation Loss: 2.572941\n",
      "Epoch: 29471 \tTraining Loss: 1.342275 \tValidation Loss: 2.572900\n",
      "Epoch: 29472 \tTraining Loss: 1.276201 \tValidation Loss: 2.572912\n",
      "Epoch: 29473 \tTraining Loss: 1.340476 \tValidation Loss: 2.571921\n",
      "Epoch: 29474 \tTraining Loss: 1.343673 \tValidation Loss: 2.572691\n",
      "Epoch: 29475 \tTraining Loss: 1.382467 \tValidation Loss: 2.571586\n",
      "Epoch: 29476 \tTraining Loss: 1.341715 \tValidation Loss: 2.571104\n",
      "Epoch: 29477 \tTraining Loss: 1.332843 \tValidation Loss: 2.573344\n",
      "Epoch: 29478 \tTraining Loss: 1.319498 \tValidation Loss: 2.571982\n",
      "Epoch: 29479 \tTraining Loss: 1.351508 \tValidation Loss: 2.571332\n",
      "Epoch: 29480 \tTraining Loss: 1.338773 \tValidation Loss: 2.572515\n",
      "Epoch: 29481 \tTraining Loss: 1.333389 \tValidation Loss: 2.571164\n",
      "Epoch: 29482 \tTraining Loss: 1.331466 \tValidation Loss: 2.572770\n",
      "Epoch: 29483 \tTraining Loss: 1.353160 \tValidation Loss: 2.571670\n",
      "Epoch: 29484 \tTraining Loss: 1.323539 \tValidation Loss: 2.571968\n",
      "Epoch: 29485 \tTraining Loss: 1.348274 \tValidation Loss: 2.572930\n",
      "Epoch: 29486 \tTraining Loss: 1.310265 \tValidation Loss: 2.572226\n",
      "Epoch: 29487 \tTraining Loss: 1.314435 \tValidation Loss: 2.572595\n",
      "Epoch: 29488 \tTraining Loss: 1.347560 \tValidation Loss: 2.572369\n",
      "Epoch: 29489 \tTraining Loss: 1.324777 \tValidation Loss: 2.573003\n",
      "Epoch: 29490 \tTraining Loss: 1.356261 \tValidation Loss: 2.571469\n",
      "Epoch: 29491 \tTraining Loss: 1.371439 \tValidation Loss: 2.572097\n",
      "Epoch: 29492 \tTraining Loss: 1.360774 \tValidation Loss: 2.571768\n",
      "Epoch: 29493 \tTraining Loss: 1.313577 \tValidation Loss: 2.574265\n",
      "Epoch: 29494 \tTraining Loss: 1.370875 \tValidation Loss: 2.572514\n",
      "Epoch: 29495 \tTraining Loss: 1.308214 \tValidation Loss: 2.571836\n",
      "Epoch: 29496 \tTraining Loss: 1.299624 \tValidation Loss: 2.572217\n",
      "Epoch: 29497 \tTraining Loss: 1.291922 \tValidation Loss: 2.573802\n",
      "Epoch: 29498 \tTraining Loss: 1.319975 \tValidation Loss: 2.572814\n",
      "Epoch: 29499 \tTraining Loss: 1.310688 \tValidation Loss: 2.573457\n",
      "Epoch: 29500 \tTraining Loss: 1.315773 \tValidation Loss: 2.573049\n",
      "Epoch: 29501 \tTraining Loss: 1.306853 \tValidation Loss: 2.573295\n",
      "Epoch: 29502 \tTraining Loss: 1.308777 \tValidation Loss: 2.572449\n",
      "Epoch: 29503 \tTraining Loss: 1.316591 \tValidation Loss: 2.572500\n",
      "Epoch: 29504 \tTraining Loss: 1.310410 \tValidation Loss: 2.572538\n",
      "Epoch: 29505 \tTraining Loss: 1.286165 \tValidation Loss: 2.573659\n",
      "Epoch: 29506 \tTraining Loss: 1.313708 \tValidation Loss: 2.574400\n",
      "Epoch: 29507 \tTraining Loss: 1.350592 \tValidation Loss: 2.572238\n",
      "Epoch: 29508 \tTraining Loss: 1.324997 \tValidation Loss: 2.573040\n",
      "Epoch: 29509 \tTraining Loss: 1.329200 \tValidation Loss: 2.573262\n",
      "Epoch: 29510 \tTraining Loss: 1.337059 \tValidation Loss: 2.571603\n",
      "Epoch: 29511 \tTraining Loss: 1.306535 \tValidation Loss: 2.572870\n",
      "Epoch: 29512 \tTraining Loss: 1.320174 \tValidation Loss: 2.572557\n",
      "Epoch: 29513 \tTraining Loss: 1.281608 \tValidation Loss: 2.573675\n",
      "Epoch: 29514 \tTraining Loss: 1.311995 \tValidation Loss: 2.573659\n",
      "Epoch: 29515 \tTraining Loss: 1.293778 \tValidation Loss: 2.573297\n",
      "Epoch: 29516 \tTraining Loss: 1.299611 \tValidation Loss: 2.573369\n",
      "Epoch: 29517 \tTraining Loss: 1.313901 \tValidation Loss: 2.573531\n",
      "Epoch: 29518 \tTraining Loss: 1.331438 \tValidation Loss: 2.573049\n",
      "Epoch: 29519 \tTraining Loss: 1.341142 \tValidation Loss: 2.571653\n",
      "Epoch: 29520 \tTraining Loss: 1.368308 \tValidation Loss: 2.571425\n",
      "Epoch: 29521 \tTraining Loss: 1.323309 \tValidation Loss: 2.573236\n",
      "Epoch: 29522 \tTraining Loss: 1.308372 \tValidation Loss: 2.573339\n",
      "Epoch: 29523 \tTraining Loss: 1.358300 \tValidation Loss: 2.572824\n",
      "Epoch: 29524 \tTraining Loss: 1.282789 \tValidation Loss: 2.575079\n",
      "Epoch: 29525 \tTraining Loss: 1.305661 \tValidation Loss: 2.574159\n",
      "Epoch: 29526 \tTraining Loss: 1.346216 \tValidation Loss: 2.572293\n",
      "Epoch: 29527 \tTraining Loss: 1.341897 \tValidation Loss: 2.573482\n",
      "Epoch: 29528 \tTraining Loss: 1.292554 \tValidation Loss: 2.575408\n",
      "Epoch: 29529 \tTraining Loss: 1.341959 \tValidation Loss: 2.572764\n",
      "Epoch: 29530 \tTraining Loss: 1.351765 \tValidation Loss: 2.574213\n",
      "Epoch: 29531 \tTraining Loss: 1.356920 \tValidation Loss: 2.572605\n",
      "Epoch: 29532 \tTraining Loss: 1.318196 \tValidation Loss: 2.573208\n",
      "Epoch: 29533 \tTraining Loss: 1.291524 \tValidation Loss: 2.574007\n",
      "Epoch: 29534 \tTraining Loss: 1.306852 \tValidation Loss: 2.574545\n",
      "Epoch: 29535 \tTraining Loss: 1.303287 \tValidation Loss: 2.574031\n",
      "Epoch: 29536 \tTraining Loss: 1.276367 \tValidation Loss: 2.574187\n",
      "Epoch: 29537 \tTraining Loss: 1.341347 \tValidation Loss: 2.573153\n",
      "Epoch: 29538 \tTraining Loss: 1.359296 \tValidation Loss: 2.572824\n",
      "Epoch: 29539 \tTraining Loss: 1.379874 \tValidation Loss: 2.573585\n",
      "Epoch: 29540 \tTraining Loss: 1.328024 \tValidation Loss: 2.574049\n",
      "Epoch: 29541 \tTraining Loss: 1.295557 \tValidation Loss: 2.572892\n",
      "Epoch: 29542 \tTraining Loss: 1.287993 \tValidation Loss: 2.574224\n",
      "Epoch: 29543 \tTraining Loss: 1.332230 \tValidation Loss: 2.575086\n",
      "Epoch: 29544 \tTraining Loss: 1.325429 \tValidation Loss: 2.575459\n",
      "Epoch: 29545 \tTraining Loss: 1.350978 \tValidation Loss: 2.573544\n",
      "Epoch: 29546 \tTraining Loss: 1.357044 \tValidation Loss: 2.573077\n",
      "Epoch: 29547 \tTraining Loss: 1.307129 \tValidation Loss: 2.573712\n",
      "Epoch: 29548 \tTraining Loss: 1.338443 \tValidation Loss: 2.574493\n",
      "Epoch: 29549 \tTraining Loss: 1.385102 \tValidation Loss: 2.575131\n",
      "Epoch: 29550 \tTraining Loss: 1.325973 \tValidation Loss: 2.572909\n",
      "Epoch: 29551 \tTraining Loss: 1.347613 \tValidation Loss: 2.572999\n",
      "Epoch: 29552 \tTraining Loss: 1.337416 \tValidation Loss: 2.573805\n",
      "Epoch: 29553 \tTraining Loss: 1.319513 \tValidation Loss: 2.574616\n",
      "Epoch: 29554 \tTraining Loss: 1.315704 \tValidation Loss: 2.573266\n",
      "Epoch: 29555 \tTraining Loss: 1.315419 \tValidation Loss: 2.574263\n",
      "Epoch: 29556 \tTraining Loss: 1.325793 \tValidation Loss: 2.573689\n",
      "Epoch: 29557 \tTraining Loss: 1.305532 \tValidation Loss: 2.574704\n",
      "Epoch: 29558 \tTraining Loss: 1.299123 \tValidation Loss: 2.573519\n",
      "Epoch: 29559 \tTraining Loss: 1.340956 \tValidation Loss: 2.573160\n",
      "Epoch: 29560 \tTraining Loss: 1.325155 \tValidation Loss: 2.574329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29561 \tTraining Loss: 1.326695 \tValidation Loss: 2.575885\n",
      "Epoch: 29562 \tTraining Loss: 1.286471 \tValidation Loss: 2.574145\n",
      "Epoch: 29563 \tTraining Loss: 1.315149 \tValidation Loss: 2.574121\n",
      "Epoch: 29564 \tTraining Loss: 1.325783 \tValidation Loss: 2.575306\n",
      "Epoch: 29565 \tTraining Loss: 1.343697 \tValidation Loss: 2.575061\n",
      "Epoch: 29566 \tTraining Loss: 1.315267 \tValidation Loss: 2.574760\n",
      "Epoch: 29567 \tTraining Loss: 1.351317 \tValidation Loss: 2.573878\n",
      "Epoch: 29568 \tTraining Loss: 1.336863 \tValidation Loss: 2.572348\n",
      "Epoch: 29569 \tTraining Loss: 1.315316 \tValidation Loss: 2.574901\n",
      "Epoch: 29570 \tTraining Loss: 1.291407 \tValidation Loss: 2.575541\n",
      "Epoch: 29571 \tTraining Loss: 1.337711 \tValidation Loss: 2.575169\n",
      "Epoch: 29572 \tTraining Loss: 1.288414 \tValidation Loss: 2.574827\n",
      "Epoch: 29573 \tTraining Loss: 1.342973 \tValidation Loss: 2.575093\n",
      "Epoch: 29574 \tTraining Loss: 1.364750 \tValidation Loss: 2.574744\n",
      "Epoch: 29575 \tTraining Loss: 1.317371 \tValidation Loss: 2.575086\n",
      "Epoch: 29576 \tTraining Loss: 1.329977 \tValidation Loss: 2.575227\n",
      "Epoch: 29577 \tTraining Loss: 1.314169 \tValidation Loss: 2.575596\n",
      "Epoch: 29578 \tTraining Loss: 1.295398 \tValidation Loss: 2.575773\n",
      "Epoch: 29579 \tTraining Loss: 1.333367 \tValidation Loss: 2.575051\n",
      "Epoch: 29580 \tTraining Loss: 1.364639 \tValidation Loss: 2.574198\n",
      "Epoch: 29581 \tTraining Loss: 1.356838 \tValidation Loss: 2.572624\n",
      "Epoch: 29582 \tTraining Loss: 1.332584 \tValidation Loss: 2.573267\n",
      "Epoch: 29583 \tTraining Loss: 1.300979 \tValidation Loss: 2.574303\n",
      "Epoch: 29584 \tTraining Loss: 1.276267 \tValidation Loss: 2.574976\n",
      "Epoch: 29585 \tTraining Loss: 1.334332 \tValidation Loss: 2.575189\n",
      "Epoch: 29586 \tTraining Loss: 1.312923 \tValidation Loss: 2.575862\n",
      "Epoch: 29587 \tTraining Loss: 1.351478 \tValidation Loss: 2.574107\n",
      "Epoch: 29588 \tTraining Loss: 1.340110 \tValidation Loss: 2.574458\n",
      "Epoch: 29589 \tTraining Loss: 1.278813 \tValidation Loss: 2.575169\n",
      "Epoch: 29590 \tTraining Loss: 1.309170 \tValidation Loss: 2.576229\n",
      "Epoch: 29591 \tTraining Loss: 1.306005 \tValidation Loss: 2.575358\n",
      "Epoch: 29592 \tTraining Loss: 1.322391 \tValidation Loss: 2.575732\n",
      "Epoch: 29593 \tTraining Loss: 1.317694 \tValidation Loss: 2.575277\n",
      "Epoch: 29594 \tTraining Loss: 1.314680 \tValidation Loss: 2.575662\n",
      "Epoch: 29595 \tTraining Loss: 1.339227 \tValidation Loss: 2.574903\n",
      "Epoch: 29596 \tTraining Loss: 1.319605 \tValidation Loss: 2.574964\n",
      "Epoch: 29597 \tTraining Loss: 1.295876 \tValidation Loss: 2.576375\n",
      "Epoch: 29598 \tTraining Loss: 1.310586 \tValidation Loss: 2.575208\n",
      "Epoch: 29599 \tTraining Loss: 1.250115 \tValidation Loss: 2.576364\n",
      "Epoch: 29600 \tTraining Loss: 1.329527 \tValidation Loss: 2.575859\n",
      "Epoch: 29601 \tTraining Loss: 1.344327 \tValidation Loss: 2.575689\n",
      "Epoch: 29602 \tTraining Loss: 1.283803 \tValidation Loss: 2.575609\n",
      "Epoch: 29603 \tTraining Loss: 1.394856 \tValidation Loss: 2.575052\n",
      "Epoch: 29604 \tTraining Loss: 1.315438 \tValidation Loss: 2.575894\n",
      "Epoch: 29605 \tTraining Loss: 1.319294 \tValidation Loss: 2.575659\n",
      "Epoch: 29606 \tTraining Loss: 1.331905 \tValidation Loss: 2.575759\n",
      "Epoch: 29607 \tTraining Loss: 1.388824 \tValidation Loss: 2.574478\n",
      "Epoch: 29608 \tTraining Loss: 1.288077 \tValidation Loss: 2.575220\n",
      "Epoch: 29609 \tTraining Loss: 1.324572 \tValidation Loss: 2.576565\n",
      "Epoch: 29610 \tTraining Loss: 1.330991 \tValidation Loss: 2.575512\n",
      "Epoch: 29611 \tTraining Loss: 1.356268 \tValidation Loss: 2.574620\n",
      "Epoch: 29612 \tTraining Loss: 1.313540 \tValidation Loss: 2.574568\n",
      "Epoch: 29613 \tTraining Loss: 1.327416 \tValidation Loss: 2.575049\n",
      "Epoch: 29614 \tTraining Loss: 1.304298 \tValidation Loss: 2.575938\n",
      "Epoch: 29615 \tTraining Loss: 1.308229 \tValidation Loss: 2.575998\n",
      "Epoch: 29616 \tTraining Loss: 1.297315 \tValidation Loss: 2.575377\n",
      "Epoch: 29617 \tTraining Loss: 1.312264 \tValidation Loss: 2.575540\n",
      "Epoch: 29618 \tTraining Loss: 1.367041 \tValidation Loss: 2.575254\n",
      "Epoch: 29619 \tTraining Loss: 1.299949 \tValidation Loss: 2.577259\n",
      "Epoch: 29620 \tTraining Loss: 1.313577 \tValidation Loss: 2.576342\n",
      "Epoch: 29621 \tTraining Loss: 1.302215 \tValidation Loss: 2.575630\n",
      "Epoch: 29622 \tTraining Loss: 1.288816 \tValidation Loss: 2.577163\n",
      "Epoch: 29623 \tTraining Loss: 1.298042 \tValidation Loss: 2.576392\n",
      "Epoch: 29624 \tTraining Loss: 1.331613 \tValidation Loss: 2.577479\n",
      "Epoch: 29625 \tTraining Loss: 1.304926 \tValidation Loss: 2.577301\n",
      "Epoch: 29626 \tTraining Loss: 1.299705 \tValidation Loss: 2.576797\n",
      "Epoch: 29627 \tTraining Loss: 1.318768 \tValidation Loss: 2.576720\n",
      "Epoch: 29628 \tTraining Loss: 1.290833 \tValidation Loss: 2.576876\n",
      "Epoch: 29629 \tTraining Loss: 1.348783 \tValidation Loss: 2.576754\n",
      "Epoch: 29630 \tTraining Loss: 1.344322 \tValidation Loss: 2.576116\n",
      "Epoch: 29631 \tTraining Loss: 1.291714 \tValidation Loss: 2.576716\n",
      "Epoch: 29632 \tTraining Loss: 1.316250 \tValidation Loss: 2.576672\n",
      "Epoch: 29633 \tTraining Loss: 1.306677 \tValidation Loss: 2.575616\n",
      "Epoch: 29634 \tTraining Loss: 1.318057 \tValidation Loss: 2.576595\n",
      "Epoch: 29635 \tTraining Loss: 1.317081 \tValidation Loss: 2.576835\n",
      "Epoch: 29636 \tTraining Loss: 1.295863 \tValidation Loss: 2.576794\n",
      "Epoch: 29637 \tTraining Loss: 1.299047 \tValidation Loss: 2.577347\n",
      "Epoch: 29638 \tTraining Loss: 1.356320 \tValidation Loss: 2.576077\n",
      "Epoch: 29639 \tTraining Loss: 1.303869 \tValidation Loss: 2.576645\n",
      "Epoch: 29640 \tTraining Loss: 1.274950 \tValidation Loss: 2.576742\n",
      "Epoch: 29641 \tTraining Loss: 1.276922 \tValidation Loss: 2.577211\n",
      "Epoch: 29642 \tTraining Loss: 1.328462 \tValidation Loss: 2.577152\n",
      "Epoch: 29643 \tTraining Loss: 1.334442 \tValidation Loss: 2.576848\n",
      "Epoch: 29644 \tTraining Loss: 1.336145 \tValidation Loss: 2.576827\n",
      "Epoch: 29645 \tTraining Loss: 1.317117 \tValidation Loss: 2.576713\n",
      "Epoch: 29646 \tTraining Loss: 1.299664 \tValidation Loss: 2.576601\n",
      "Epoch: 29647 \tTraining Loss: 1.287659 \tValidation Loss: 2.577114\n",
      "Epoch: 29648 \tTraining Loss: 1.307944 \tValidation Loss: 2.577145\n",
      "Epoch: 29649 \tTraining Loss: 1.341832 \tValidation Loss: 2.575959\n",
      "Epoch: 29650 \tTraining Loss: 1.276023 \tValidation Loss: 2.577089\n",
      "Epoch: 29651 \tTraining Loss: 1.368434 \tValidation Loss: 2.574478\n",
      "Epoch: 29652 \tTraining Loss: 1.313839 \tValidation Loss: 2.577179\n",
      "Epoch: 29653 \tTraining Loss: 1.350589 \tValidation Loss: 2.575930\n",
      "Epoch: 29654 \tTraining Loss: 1.283697 \tValidation Loss: 2.576593\n",
      "Epoch: 29655 \tTraining Loss: 1.354955 \tValidation Loss: 2.576750\n",
      "Epoch: 29656 \tTraining Loss: 1.310686 \tValidation Loss: 2.576511\n",
      "Epoch: 29657 \tTraining Loss: 1.294516 \tValidation Loss: 2.577221\n",
      "Epoch: 29658 \tTraining Loss: 1.317621 \tValidation Loss: 2.576521\n",
      "Epoch: 29659 \tTraining Loss: 1.331357 \tValidation Loss: 2.577157\n",
      "Epoch: 29660 \tTraining Loss: 1.264079 \tValidation Loss: 2.577394\n",
      "Epoch: 29661 \tTraining Loss: 1.309204 \tValidation Loss: 2.578623\n",
      "Epoch: 29662 \tTraining Loss: 1.293577 \tValidation Loss: 2.577416\n",
      "Epoch: 29663 \tTraining Loss: 1.311375 \tValidation Loss: 2.577124\n",
      "Epoch: 29664 \tTraining Loss: 1.319792 \tValidation Loss: 2.576987\n",
      "Epoch: 29665 \tTraining Loss: 1.352061 \tValidation Loss: 2.577001\n",
      "Epoch: 29666 \tTraining Loss: 1.305557 \tValidation Loss: 2.578424\n",
      "Epoch: 29667 \tTraining Loss: 1.332682 \tValidation Loss: 2.577478\n",
      "Epoch: 29668 \tTraining Loss: 1.296921 \tValidation Loss: 2.578190\n",
      "Epoch: 29669 \tTraining Loss: 1.328298 \tValidation Loss: 2.577280\n",
      "Epoch: 29670 \tTraining Loss: 1.301645 \tValidation Loss: 2.578007\n",
      "Epoch: 29671 \tTraining Loss: 1.293276 \tValidation Loss: 2.577855\n",
      "Epoch: 29672 \tTraining Loss: 1.339626 \tValidation Loss: 2.578105\n",
      "Epoch: 29673 \tTraining Loss: 1.316297 \tValidation Loss: 2.576912\n",
      "Epoch: 29674 \tTraining Loss: 1.285280 \tValidation Loss: 2.576328\n",
      "Epoch: 29675 \tTraining Loss: 1.314173 \tValidation Loss: 2.576512\n",
      "Epoch: 29676 \tTraining Loss: 1.322887 \tValidation Loss: 2.576547\n",
      "Epoch: 29677 \tTraining Loss: 1.298116 \tValidation Loss: 2.576971\n",
      "Epoch: 29678 \tTraining Loss: 1.309706 \tValidation Loss: 2.578048\n",
      "Epoch: 29679 \tTraining Loss: 1.317820 \tValidation Loss: 2.575589\n",
      "Epoch: 29680 \tTraining Loss: 1.305751 \tValidation Loss: 2.577241\n",
      "Epoch: 29681 \tTraining Loss: 1.300925 \tValidation Loss: 2.576839\n",
      "Epoch: 29682 \tTraining Loss: 1.305015 \tValidation Loss: 2.577867\n",
      "Epoch: 29683 \tTraining Loss: 1.354723 \tValidation Loss: 2.578665\n",
      "Epoch: 29684 \tTraining Loss: 1.320585 \tValidation Loss: 2.577858\n",
      "Epoch: 29685 \tTraining Loss: 1.361767 \tValidation Loss: 2.577017\n",
      "Epoch: 29686 \tTraining Loss: 1.321027 \tValidation Loss: 2.576525\n",
      "Epoch: 29687 \tTraining Loss: 1.320179 \tValidation Loss: 2.577594\n",
      "Epoch: 29688 \tTraining Loss: 1.275573 \tValidation Loss: 2.577686\n",
      "Epoch: 29689 \tTraining Loss: 1.285422 \tValidation Loss: 2.577427\n",
      "Epoch: 29690 \tTraining Loss: 1.373471 \tValidation Loss: 2.577890\n",
      "Epoch: 29691 \tTraining Loss: 1.345667 \tValidation Loss: 2.578381\n",
      "Epoch: 29692 \tTraining Loss: 1.321770 \tValidation Loss: 2.578115\n",
      "Epoch: 29693 \tTraining Loss: 1.275898 \tValidation Loss: 2.578317\n",
      "Epoch: 29694 \tTraining Loss: 1.313804 \tValidation Loss: 2.578938\n",
      "Epoch: 29695 \tTraining Loss: 1.377544 \tValidation Loss: 2.578115\n",
      "Epoch: 29696 \tTraining Loss: 1.351477 \tValidation Loss: 2.578116\n",
      "Epoch: 29697 \tTraining Loss: 1.299818 \tValidation Loss: 2.577916\n",
      "Epoch: 29698 \tTraining Loss: 1.256430 \tValidation Loss: 2.577968\n",
      "Epoch: 29699 \tTraining Loss: 1.329069 \tValidation Loss: 2.576833\n",
      "Epoch: 29700 \tTraining Loss: 1.332605 \tValidation Loss: 2.577358\n",
      "Epoch: 29701 \tTraining Loss: 1.321482 \tValidation Loss: 2.578490\n",
      "Epoch: 29702 \tTraining Loss: 1.331904 \tValidation Loss: 2.577336\n",
      "Epoch: 29703 \tTraining Loss: 1.316139 \tValidation Loss: 2.578147\n",
      "Epoch: 29704 \tTraining Loss: 1.304183 \tValidation Loss: 2.576096\n",
      "Epoch: 29705 \tTraining Loss: 1.319198 \tValidation Loss: 2.577807\n",
      "Epoch: 29706 \tTraining Loss: 1.307356 \tValidation Loss: 2.577974\n",
      "Epoch: 29707 \tTraining Loss: 1.309127 \tValidation Loss: 2.578433\n",
      "Epoch: 29708 \tTraining Loss: 1.327367 \tValidation Loss: 2.577219\n",
      "Epoch: 29709 \tTraining Loss: 1.325896 \tValidation Loss: 2.577601\n",
      "Epoch: 29710 \tTraining Loss: 1.318648 \tValidation Loss: 2.578485\n",
      "Epoch: 29711 \tTraining Loss: 1.354395 \tValidation Loss: 2.577661\n",
      "Epoch: 29712 \tTraining Loss: 1.325116 \tValidation Loss: 2.577926\n",
      "Epoch: 29713 \tTraining Loss: 1.283982 \tValidation Loss: 2.578280\n",
      "Epoch: 29714 \tTraining Loss: 1.309509 \tValidation Loss: 2.577456\n",
      "Epoch: 29715 \tTraining Loss: 1.325943 \tValidation Loss: 2.578401\n",
      "Epoch: 29716 \tTraining Loss: 1.311602 \tValidation Loss: 2.577905\n",
      "Epoch: 29717 \tTraining Loss: 1.290352 \tValidation Loss: 2.578700\n",
      "Epoch: 29718 \tTraining Loss: 1.361898 \tValidation Loss: 2.577559\n",
      "Epoch: 29719 \tTraining Loss: 1.273940 \tValidation Loss: 2.578933\n",
      "Epoch: 29720 \tTraining Loss: 1.305809 \tValidation Loss: 2.579048\n",
      "Epoch: 29721 \tTraining Loss: 1.319506 \tValidation Loss: 2.578631\n",
      "Epoch: 29722 \tTraining Loss: 1.328070 \tValidation Loss: 2.578651\n",
      "Epoch: 29723 \tTraining Loss: 1.289568 \tValidation Loss: 2.579817\n",
      "Epoch: 29724 \tTraining Loss: 1.303076 \tValidation Loss: 2.576758\n",
      "Epoch: 29725 \tTraining Loss: 1.362375 \tValidation Loss: 2.577695\n",
      "Epoch: 29726 \tTraining Loss: 1.311146 \tValidation Loss: 2.577723\n",
      "Epoch: 29727 \tTraining Loss: 1.360484 \tValidation Loss: 2.576688\n",
      "Epoch: 29728 \tTraining Loss: 1.337121 \tValidation Loss: 2.578575\n",
      "Epoch: 29729 \tTraining Loss: 1.379383 \tValidation Loss: 2.577674\n",
      "Epoch: 29730 \tTraining Loss: 1.331718 \tValidation Loss: 2.578627\n",
      "Epoch: 29731 \tTraining Loss: 1.323959 \tValidation Loss: 2.578610\n",
      "Epoch: 29732 \tTraining Loss: 1.332488 \tValidation Loss: 2.578323\n",
      "Epoch: 29733 \tTraining Loss: 1.348952 \tValidation Loss: 2.578044\n",
      "Epoch: 29734 \tTraining Loss: 1.330081 \tValidation Loss: 2.577272\n",
      "Epoch: 29735 \tTraining Loss: 1.318950 \tValidation Loss: 2.578985\n",
      "Epoch: 29736 \tTraining Loss: 1.276928 \tValidation Loss: 2.580262\n",
      "Epoch: 29737 \tTraining Loss: 1.300115 \tValidation Loss: 2.579544\n",
      "Epoch: 29738 \tTraining Loss: 1.299249 \tValidation Loss: 2.579952\n",
      "Epoch: 29739 \tTraining Loss: 1.322818 \tValidation Loss: 2.578989\n",
      "Epoch: 29740 \tTraining Loss: 1.369732 \tValidation Loss: 2.577357\n",
      "Epoch: 29741 \tTraining Loss: 1.312993 \tValidation Loss: 2.578732\n",
      "Epoch: 29742 \tTraining Loss: 1.294975 \tValidation Loss: 2.578315\n",
      "Epoch: 29743 \tTraining Loss: 1.294619 \tValidation Loss: 2.578999\n",
      "Epoch: 29744 \tTraining Loss: 1.329527 \tValidation Loss: 2.578869\n",
      "Epoch: 29745 \tTraining Loss: 1.333200 \tValidation Loss: 2.580251\n",
      "Epoch: 29746 \tTraining Loss: 1.287766 \tValidation Loss: 2.579490\n",
      "Epoch: 29747 \tTraining Loss: 1.302090 \tValidation Loss: 2.579471\n",
      "Epoch: 29748 \tTraining Loss: 1.320337 \tValidation Loss: 2.579268\n",
      "Epoch: 29749 \tTraining Loss: 1.320293 \tValidation Loss: 2.578957\n",
      "Epoch: 29750 \tTraining Loss: 1.301340 \tValidation Loss: 2.580595\n",
      "Epoch: 29751 \tTraining Loss: 1.289830 \tValidation Loss: 2.579482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29752 \tTraining Loss: 1.298794 \tValidation Loss: 2.579056\n",
      "Epoch: 29753 \tTraining Loss: 1.323932 \tValidation Loss: 2.579820\n",
      "Epoch: 29754 \tTraining Loss: 1.291143 \tValidation Loss: 2.579651\n",
      "Epoch: 29755 \tTraining Loss: 1.335565 \tValidation Loss: 2.579734\n",
      "Epoch: 29756 \tTraining Loss: 1.328827 \tValidation Loss: 2.578981\n",
      "Epoch: 29757 \tTraining Loss: 1.327750 \tValidation Loss: 2.578825\n",
      "Epoch: 29758 \tTraining Loss: 1.299660 \tValidation Loss: 2.579743\n",
      "Epoch: 29759 \tTraining Loss: 1.324090 \tValidation Loss: 2.580096\n",
      "Epoch: 29760 \tTraining Loss: 1.286603 \tValidation Loss: 2.580536\n",
      "Epoch: 29761 \tTraining Loss: 1.331083 \tValidation Loss: 2.578422\n",
      "Epoch: 29762 \tTraining Loss: 1.334585 \tValidation Loss: 2.577871\n",
      "Epoch: 29763 \tTraining Loss: 1.261879 \tValidation Loss: 2.580624\n",
      "Epoch: 29764 \tTraining Loss: 1.312597 \tValidation Loss: 2.578970\n",
      "Epoch: 29765 \tTraining Loss: 1.295016 \tValidation Loss: 2.579129\n",
      "Epoch: 29766 \tTraining Loss: 1.269585 \tValidation Loss: 2.579365\n",
      "Epoch: 29767 \tTraining Loss: 1.309202 \tValidation Loss: 2.579728\n",
      "Epoch: 29768 \tTraining Loss: 1.321642 \tValidation Loss: 2.578305\n",
      "Epoch: 29769 \tTraining Loss: 1.337999 \tValidation Loss: 2.580488\n",
      "Epoch: 29770 \tTraining Loss: 1.355073 \tValidation Loss: 2.578108\n",
      "Epoch: 29771 \tTraining Loss: 1.347488 \tValidation Loss: 2.578441\n",
      "Epoch: 29772 \tTraining Loss: 1.380850 \tValidation Loss: 2.579488\n",
      "Epoch: 29773 \tTraining Loss: 1.313784 \tValidation Loss: 2.578948\n",
      "Epoch: 29774 \tTraining Loss: 1.339994 \tValidation Loss: 2.579380\n",
      "Epoch: 29775 \tTraining Loss: 1.338768 \tValidation Loss: 2.578970\n",
      "Epoch: 29776 \tTraining Loss: 1.287626 \tValidation Loss: 2.580803\n",
      "Epoch: 29777 \tTraining Loss: 1.300832 \tValidation Loss: 2.579703\n",
      "Epoch: 29778 \tTraining Loss: 1.363038 \tValidation Loss: 2.580235\n",
      "Epoch: 29779 \tTraining Loss: 1.308240 \tValidation Loss: 2.580375\n",
      "Epoch: 29780 \tTraining Loss: 1.318558 \tValidation Loss: 2.580187\n",
      "Epoch: 29781 \tTraining Loss: 1.293375 \tValidation Loss: 2.580237\n",
      "Epoch: 29782 \tTraining Loss: 1.322817 \tValidation Loss: 2.579149\n",
      "Epoch: 29783 \tTraining Loss: 1.271928 \tValidation Loss: 2.579330\n",
      "Epoch: 29784 \tTraining Loss: 1.358607 \tValidation Loss: 2.580303\n",
      "Epoch: 29785 \tTraining Loss: 1.360473 \tValidation Loss: 2.579530\n",
      "Epoch: 29786 \tTraining Loss: 1.334665 \tValidation Loss: 2.580666\n",
      "Epoch: 29787 \tTraining Loss: 1.294712 \tValidation Loss: 2.580616\n",
      "Epoch: 29788 \tTraining Loss: 1.337561 \tValidation Loss: 2.580926\n",
      "Epoch: 29789 \tTraining Loss: 1.291862 \tValidation Loss: 2.581861\n",
      "Epoch: 29790 \tTraining Loss: 1.308750 \tValidation Loss: 2.580419\n",
      "Epoch: 29791 \tTraining Loss: 1.291058 \tValidation Loss: 2.580672\n",
      "Epoch: 29792 \tTraining Loss: 1.374877 \tValidation Loss: 2.578928\n",
      "Epoch: 29793 \tTraining Loss: 1.316791 \tValidation Loss: 2.580418\n",
      "Epoch: 29794 \tTraining Loss: 1.352748 \tValidation Loss: 2.579228\n",
      "Epoch: 29795 \tTraining Loss: 1.321106 \tValidation Loss: 2.579159\n",
      "Epoch: 29796 \tTraining Loss: 1.297393 \tValidation Loss: 2.580261\n",
      "Epoch: 29797 \tTraining Loss: 1.307509 \tValidation Loss: 2.579552\n",
      "Epoch: 29798 \tTraining Loss: 1.311507 \tValidation Loss: 2.580055\n",
      "Epoch: 29799 \tTraining Loss: 1.368950 \tValidation Loss: 2.579147\n",
      "Epoch: 29800 \tTraining Loss: 1.290207 \tValidation Loss: 2.579863\n",
      "Epoch: 29801 \tTraining Loss: 1.315853 \tValidation Loss: 2.578910\n",
      "Epoch: 29802 \tTraining Loss: 1.324669 \tValidation Loss: 2.580291\n",
      "Epoch: 29803 \tTraining Loss: 1.327529 \tValidation Loss: 2.578920\n",
      "Epoch: 29804 \tTraining Loss: 1.302964 \tValidation Loss: 2.581075\n",
      "Epoch: 29805 \tTraining Loss: 1.322502 \tValidation Loss: 2.580622\n",
      "Epoch: 29806 \tTraining Loss: 1.300637 \tValidation Loss: 2.580770\n",
      "Epoch: 29807 \tTraining Loss: 1.284054 \tValidation Loss: 2.581769\n",
      "Epoch: 29808 \tTraining Loss: 1.311127 \tValidation Loss: 2.580305\n",
      "Epoch: 29809 \tTraining Loss: 1.298327 \tValidation Loss: 2.581333\n",
      "Epoch: 29810 \tTraining Loss: 1.322549 \tValidation Loss: 2.580588\n",
      "Epoch: 29811 \tTraining Loss: 1.299619 \tValidation Loss: 2.578883\n",
      "Epoch: 29812 \tTraining Loss: 1.262169 \tValidation Loss: 2.581168\n",
      "Epoch: 29813 \tTraining Loss: 1.324081 \tValidation Loss: 2.581090\n",
      "Epoch: 29814 \tTraining Loss: 1.343785 \tValidation Loss: 2.580699\n",
      "Epoch: 29815 \tTraining Loss: 1.274665 \tValidation Loss: 2.581614\n",
      "Epoch: 29816 \tTraining Loss: 1.293423 \tValidation Loss: 2.581628\n",
      "Epoch: 29817 \tTraining Loss: 1.403615 \tValidation Loss: 2.578720\n",
      "Epoch: 29818 \tTraining Loss: 1.310431 \tValidation Loss: 2.580478\n",
      "Epoch: 29819 \tTraining Loss: 1.309752 \tValidation Loss: 2.581690\n",
      "Epoch: 29820 \tTraining Loss: 1.342354 \tValidation Loss: 2.580441\n",
      "Epoch: 29821 \tTraining Loss: 1.309198 \tValidation Loss: 2.580769\n",
      "Epoch: 29822 \tTraining Loss: 1.335625 \tValidation Loss: 2.581435\n",
      "Epoch: 29823 \tTraining Loss: 1.298503 \tValidation Loss: 2.581553\n",
      "Epoch: 29824 \tTraining Loss: 1.296479 \tValidation Loss: 2.580133\n",
      "Epoch: 29825 \tTraining Loss: 1.347204 \tValidation Loss: 2.579142\n",
      "Epoch: 29826 \tTraining Loss: 1.337230 \tValidation Loss: 2.579642\n",
      "Epoch: 29827 \tTraining Loss: 1.315239 \tValidation Loss: 2.580368\n",
      "Epoch: 29828 \tTraining Loss: 1.337703 \tValidation Loss: 2.580058\n",
      "Epoch: 29829 \tTraining Loss: 1.260143 \tValidation Loss: 2.582269\n",
      "Epoch: 29830 \tTraining Loss: 1.298354 \tValidation Loss: 2.581436\n",
      "Epoch: 29831 \tTraining Loss: 1.309951 \tValidation Loss: 2.581095\n",
      "Epoch: 29832 \tTraining Loss: 1.351989 \tValidation Loss: 2.580388\n",
      "Epoch: 29833 \tTraining Loss: 1.296646 \tValidation Loss: 2.581748\n",
      "Epoch: 29834 \tTraining Loss: 1.311711 \tValidation Loss: 2.582185\n",
      "Epoch: 29835 \tTraining Loss: 1.325677 \tValidation Loss: 2.580986\n",
      "Epoch: 29836 \tTraining Loss: 1.301520 \tValidation Loss: 2.582644\n",
      "Epoch: 29837 \tTraining Loss: 1.291666 \tValidation Loss: 2.582264\n",
      "Epoch: 29838 \tTraining Loss: 1.347358 \tValidation Loss: 2.581017\n",
      "Epoch: 29839 \tTraining Loss: 1.297378 \tValidation Loss: 2.582036\n",
      "Epoch: 29840 \tTraining Loss: 1.345023 \tValidation Loss: 2.579916\n",
      "Epoch: 29841 \tTraining Loss: 1.326236 \tValidation Loss: 2.580363\n",
      "Epoch: 29842 \tTraining Loss: 1.265487 \tValidation Loss: 2.582685\n",
      "Epoch: 29843 \tTraining Loss: 1.302606 \tValidation Loss: 2.581493\n",
      "Epoch: 29844 \tTraining Loss: 1.340376 \tValidation Loss: 2.580954\n",
      "Epoch: 29845 \tTraining Loss: 1.321311 \tValidation Loss: 2.581461\n",
      "Epoch: 29846 \tTraining Loss: 1.369731 \tValidation Loss: 2.580864\n",
      "Epoch: 29847 \tTraining Loss: 1.336070 \tValidation Loss: 2.580952\n",
      "Epoch: 29848 \tTraining Loss: 1.379623 \tValidation Loss: 2.580010\n",
      "Epoch: 29849 \tTraining Loss: 1.342488 \tValidation Loss: 2.581224\n",
      "Epoch: 29850 \tTraining Loss: 1.331571 \tValidation Loss: 2.580663\n",
      "Epoch: 29851 \tTraining Loss: 1.322649 \tValidation Loss: 2.581183\n",
      "Epoch: 29852 \tTraining Loss: 1.285024 \tValidation Loss: 2.582238\n",
      "Epoch: 29853 \tTraining Loss: 1.317609 \tValidation Loss: 2.581936\n",
      "Epoch: 29854 \tTraining Loss: 1.310607 \tValidation Loss: 2.582440\n",
      "Epoch: 29855 \tTraining Loss: 1.314304 \tValidation Loss: 2.580240\n",
      "Epoch: 29856 \tTraining Loss: 1.340783 \tValidation Loss: 2.582025\n",
      "Epoch: 29857 \tTraining Loss: 1.301747 \tValidation Loss: 2.582362\n",
      "Epoch: 29858 \tTraining Loss: 1.286893 \tValidation Loss: 2.583694\n",
      "Epoch: 29859 \tTraining Loss: 1.323784 \tValidation Loss: 2.582928\n",
      "Epoch: 29860 \tTraining Loss: 1.325340 \tValidation Loss: 2.580806\n",
      "Epoch: 29861 \tTraining Loss: 1.320578 \tValidation Loss: 2.581981\n",
      "Epoch: 29862 \tTraining Loss: 1.295797 \tValidation Loss: 2.581960\n",
      "Epoch: 29863 \tTraining Loss: 1.318043 \tValidation Loss: 2.582360\n",
      "Epoch: 29864 \tTraining Loss: 1.351861 \tValidation Loss: 2.580257\n",
      "Epoch: 29865 \tTraining Loss: 1.303697 \tValidation Loss: 2.580536\n",
      "Epoch: 29866 \tTraining Loss: 1.273723 \tValidation Loss: 2.581206\n",
      "Epoch: 29867 \tTraining Loss: 1.381374 \tValidation Loss: 2.580675\n",
      "Epoch: 29868 \tTraining Loss: 1.291065 \tValidation Loss: 2.581837\n",
      "Epoch: 29869 \tTraining Loss: 1.306656 \tValidation Loss: 2.581313\n",
      "Epoch: 29870 \tTraining Loss: 1.265316 \tValidation Loss: 2.582310\n",
      "Epoch: 29871 \tTraining Loss: 1.321504 \tValidation Loss: 2.582035\n",
      "Epoch: 29872 \tTraining Loss: 1.331577 \tValidation Loss: 2.582231\n",
      "Epoch: 29873 \tTraining Loss: 1.316404 \tValidation Loss: 2.583350\n",
      "Epoch: 29874 \tTraining Loss: 1.292988 \tValidation Loss: 2.582689\n",
      "Epoch: 29875 \tTraining Loss: 1.296468 \tValidation Loss: 2.583069\n",
      "Epoch: 29876 \tTraining Loss: 1.310663 \tValidation Loss: 2.582368\n",
      "Epoch: 29877 \tTraining Loss: 1.308012 \tValidation Loss: 2.582836\n",
      "Epoch: 29878 \tTraining Loss: 1.263568 \tValidation Loss: 2.583240\n",
      "Epoch: 29879 \tTraining Loss: 1.317217 \tValidation Loss: 2.582069\n",
      "Epoch: 29880 \tTraining Loss: 1.300914 \tValidation Loss: 2.581307\n",
      "Epoch: 29881 \tTraining Loss: 1.304988 \tValidation Loss: 2.582789\n",
      "Epoch: 29882 \tTraining Loss: 1.372490 \tValidation Loss: 2.581357\n",
      "Epoch: 29883 \tTraining Loss: 1.286524 \tValidation Loss: 2.581922\n",
      "Epoch: 29884 \tTraining Loss: 1.332425 \tValidation Loss: 2.581462\n",
      "Epoch: 29885 \tTraining Loss: 1.299536 \tValidation Loss: 2.581878\n",
      "Epoch: 29886 \tTraining Loss: 1.383079 \tValidation Loss: 2.580759\n",
      "Epoch: 29887 \tTraining Loss: 1.260511 \tValidation Loss: 2.582790\n",
      "Epoch: 29888 \tTraining Loss: 1.289868 \tValidation Loss: 2.582667\n",
      "Epoch: 29889 \tTraining Loss: 1.280714 \tValidation Loss: 2.583205\n",
      "Epoch: 29890 \tTraining Loss: 1.318978 \tValidation Loss: 2.582283\n",
      "Epoch: 29891 \tTraining Loss: 1.314437 \tValidation Loss: 2.581883\n",
      "Epoch: 29892 \tTraining Loss: 1.342697 \tValidation Loss: 2.581915\n",
      "Epoch: 29893 \tTraining Loss: 1.318746 \tValidation Loss: 2.582666\n",
      "Epoch: 29894 \tTraining Loss: 1.303249 \tValidation Loss: 2.582616\n",
      "Epoch: 29895 \tTraining Loss: 1.264813 \tValidation Loss: 2.582659\n",
      "Epoch: 29896 \tTraining Loss: 1.324676 \tValidation Loss: 2.582114\n",
      "Epoch: 29897 \tTraining Loss: 1.327418 \tValidation Loss: 2.580985\n",
      "Epoch: 29898 \tTraining Loss: 1.307985 \tValidation Loss: 2.581884\n",
      "Epoch: 29899 \tTraining Loss: 1.347049 \tValidation Loss: 2.582113\n",
      "Epoch: 29900 \tTraining Loss: 1.325347 \tValidation Loss: 2.582351\n",
      "Epoch: 29901 \tTraining Loss: 1.308602 \tValidation Loss: 2.582837\n",
      "Epoch: 29902 \tTraining Loss: 1.326275 \tValidation Loss: 2.582864\n",
      "Epoch: 29903 \tTraining Loss: 1.320021 \tValidation Loss: 2.583454\n",
      "Epoch: 29904 \tTraining Loss: 1.305712 \tValidation Loss: 2.584575\n",
      "Epoch: 29905 \tTraining Loss: 1.310339 \tValidation Loss: 2.583635\n",
      "Epoch: 29906 \tTraining Loss: 1.369840 \tValidation Loss: 2.583428\n",
      "Epoch: 29907 \tTraining Loss: 1.339983 \tValidation Loss: 2.582500\n",
      "Epoch: 29908 \tTraining Loss: 1.290542 \tValidation Loss: 2.583326\n",
      "Epoch: 29909 \tTraining Loss: 1.285211 \tValidation Loss: 2.583823\n",
      "Epoch: 29910 \tTraining Loss: 1.347574 \tValidation Loss: 2.582500\n",
      "Epoch: 29911 \tTraining Loss: 1.317514 \tValidation Loss: 2.582687\n",
      "Epoch: 29912 \tTraining Loss: 1.323810 \tValidation Loss: 2.582691\n",
      "Epoch: 29913 \tTraining Loss: 1.307077 \tValidation Loss: 2.582876\n",
      "Epoch: 29914 \tTraining Loss: 1.309750 \tValidation Loss: 2.582633\n",
      "Epoch: 29915 \tTraining Loss: 1.373864 \tValidation Loss: 2.583693\n",
      "Epoch: 29916 \tTraining Loss: 1.308918 \tValidation Loss: 2.583562\n",
      "Epoch: 29917 \tTraining Loss: 1.310280 \tValidation Loss: 2.583181\n",
      "Epoch: 29918 \tTraining Loss: 1.286892 \tValidation Loss: 2.584523\n",
      "Epoch: 29919 \tTraining Loss: 1.322924 \tValidation Loss: 2.583689\n",
      "Epoch: 29920 \tTraining Loss: 1.258310 \tValidation Loss: 2.584637\n",
      "Epoch: 29921 \tTraining Loss: 1.319150 \tValidation Loss: 2.583027\n",
      "Epoch: 29922 \tTraining Loss: 1.298814 \tValidation Loss: 2.583488\n",
      "Epoch: 29923 \tTraining Loss: 1.314402 \tValidation Loss: 2.581815\n",
      "Epoch: 29924 \tTraining Loss: 1.290155 \tValidation Loss: 2.582406\n",
      "Epoch: 29925 \tTraining Loss: 1.347941 \tValidation Loss: 2.583160\n",
      "Epoch: 29926 \tTraining Loss: 1.332426 \tValidation Loss: 2.584200\n",
      "Epoch: 29927 \tTraining Loss: 1.379320 \tValidation Loss: 2.582619\n",
      "Epoch: 29928 \tTraining Loss: 1.301024 \tValidation Loss: 2.583706\n",
      "Epoch: 29929 \tTraining Loss: 1.306274 \tValidation Loss: 2.582977\n",
      "Epoch: 29930 \tTraining Loss: 1.318473 \tValidation Loss: 2.583459\n",
      "Epoch: 29931 \tTraining Loss: 1.300949 \tValidation Loss: 2.584124\n",
      "Epoch: 29932 \tTraining Loss: 1.334934 \tValidation Loss: 2.583745\n",
      "Epoch: 29933 \tTraining Loss: 1.278790 \tValidation Loss: 2.584455\n",
      "Epoch: 29934 \tTraining Loss: 1.346965 \tValidation Loss: 2.583467\n",
      "Epoch: 29935 \tTraining Loss: 1.282928 \tValidation Loss: 2.584018\n",
      "Epoch: 29936 \tTraining Loss: 1.341629 \tValidation Loss: 2.583907\n",
      "Epoch: 29937 \tTraining Loss: 1.329581 \tValidation Loss: 2.582979\n",
      "Epoch: 29938 \tTraining Loss: 1.318776 \tValidation Loss: 2.582336\n",
      "Epoch: 29939 \tTraining Loss: 1.289140 \tValidation Loss: 2.582279\n",
      "Epoch: 29940 \tTraining Loss: 1.339469 \tValidation Loss: 2.582927\n",
      "Epoch: 29941 \tTraining Loss: 1.303824 \tValidation Loss: 2.583767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29942 \tTraining Loss: 1.308944 \tValidation Loss: 2.583134\n",
      "Epoch: 29943 \tTraining Loss: 1.338251 \tValidation Loss: 2.582687\n",
      "Epoch: 29944 \tTraining Loss: 1.354483 \tValidation Loss: 2.582916\n",
      "Epoch: 29945 \tTraining Loss: 1.282488 \tValidation Loss: 2.583416\n",
      "Epoch: 29946 \tTraining Loss: 1.332112 \tValidation Loss: 2.584528\n",
      "Epoch: 29947 \tTraining Loss: 1.303167 \tValidation Loss: 2.583441\n",
      "Epoch: 29948 \tTraining Loss: 1.343675 \tValidation Loss: 2.585080\n",
      "Epoch: 29949 \tTraining Loss: 1.302540 \tValidation Loss: 2.583454\n",
      "Epoch: 29950 \tTraining Loss: 1.306672 \tValidation Loss: 2.583400\n",
      "Epoch: 29951 \tTraining Loss: 1.297571 \tValidation Loss: 2.583748\n",
      "Epoch: 29952 \tTraining Loss: 1.294423 \tValidation Loss: 2.583016\n",
      "Epoch: 29953 \tTraining Loss: 1.346196 \tValidation Loss: 2.583532\n",
      "Epoch: 29954 \tTraining Loss: 1.331001 \tValidation Loss: 2.583283\n",
      "Epoch: 29955 \tTraining Loss: 1.298403 \tValidation Loss: 2.583642\n",
      "Epoch: 29956 \tTraining Loss: 1.284764 \tValidation Loss: 2.584829\n",
      "Epoch: 29957 \tTraining Loss: 1.301672 \tValidation Loss: 2.583750\n",
      "Epoch: 29958 \tTraining Loss: 1.303312 \tValidation Loss: 2.582687\n",
      "Epoch: 29959 \tTraining Loss: 1.279902 \tValidation Loss: 2.582994\n",
      "Epoch: 29960 \tTraining Loss: 1.278572 \tValidation Loss: 2.584612\n",
      "Epoch: 29961 \tTraining Loss: 1.319498 \tValidation Loss: 2.583261\n",
      "Epoch: 29962 \tTraining Loss: 1.322019 \tValidation Loss: 2.583682\n",
      "Epoch: 29963 \tTraining Loss: 1.317568 \tValidation Loss: 2.583375\n",
      "Epoch: 29964 \tTraining Loss: 1.305989 \tValidation Loss: 2.584193\n",
      "Epoch: 29965 \tTraining Loss: 1.344661 \tValidation Loss: 2.583494\n",
      "Epoch: 29966 \tTraining Loss: 1.301281 \tValidation Loss: 2.582926\n",
      "Epoch: 29967 \tTraining Loss: 1.319284 \tValidation Loss: 2.583932\n",
      "Epoch: 29968 \tTraining Loss: 1.337521 \tValidation Loss: 2.584357\n",
      "Epoch: 29969 \tTraining Loss: 1.318339 \tValidation Loss: 2.584121\n",
      "Epoch: 29970 \tTraining Loss: 1.295527 \tValidation Loss: 2.584960\n",
      "Epoch: 29971 \tTraining Loss: 1.323910 \tValidation Loss: 2.584212\n",
      "Epoch: 29972 \tTraining Loss: 1.287360 \tValidation Loss: 2.584554\n",
      "Epoch: 29973 \tTraining Loss: 1.280788 \tValidation Loss: 2.583792\n",
      "Epoch: 29974 \tTraining Loss: 1.348787 \tValidation Loss: 2.584057\n",
      "Epoch: 29975 \tTraining Loss: 1.315888 \tValidation Loss: 2.584117\n",
      "Epoch: 29976 \tTraining Loss: 1.310941 \tValidation Loss: 2.585100\n",
      "Epoch: 29977 \tTraining Loss: 1.293160 \tValidation Loss: 2.584470\n",
      "Epoch: 29978 \tTraining Loss: 1.273086 \tValidation Loss: 2.584564\n",
      "Epoch: 29979 \tTraining Loss: 1.285257 \tValidation Loss: 2.585040\n",
      "Epoch: 29980 \tTraining Loss: 1.295333 \tValidation Loss: 2.584979\n",
      "Epoch: 29981 \tTraining Loss: 1.299304 \tValidation Loss: 2.583641\n",
      "Epoch: 29982 \tTraining Loss: 1.315554 \tValidation Loss: 2.584890\n",
      "Epoch: 29983 \tTraining Loss: 1.323288 \tValidation Loss: 2.584254\n",
      "Epoch: 29984 \tTraining Loss: 1.341844 \tValidation Loss: 2.584674\n",
      "Epoch: 29985 \tTraining Loss: 1.302450 \tValidation Loss: 2.586004\n",
      "Epoch: 29986 \tTraining Loss: 1.262904 \tValidation Loss: 2.585440\n",
      "Epoch: 29987 \tTraining Loss: 1.289800 \tValidation Loss: 2.585767\n",
      "Epoch: 29988 \tTraining Loss: 1.287805 \tValidation Loss: 2.585625\n",
      "Epoch: 29989 \tTraining Loss: 1.323663 \tValidation Loss: 2.585129\n",
      "Epoch: 29990 \tTraining Loss: 1.310010 \tValidation Loss: 2.585001\n",
      "Epoch: 29991 \tTraining Loss: 1.310651 \tValidation Loss: 2.584532\n",
      "Epoch: 29992 \tTraining Loss: 1.255254 \tValidation Loss: 2.584755\n",
      "Epoch: 29993 \tTraining Loss: 1.329649 \tValidation Loss: 2.584525\n",
      "Epoch: 29994 \tTraining Loss: 1.306442 \tValidation Loss: 2.584136\n",
      "Epoch: 29995 \tTraining Loss: 1.273628 \tValidation Loss: 2.585572\n",
      "Epoch: 29996 \tTraining Loss: 1.317891 \tValidation Loss: 2.583750\n",
      "Epoch: 29997 \tTraining Loss: 1.282380 \tValidation Loss: 2.585418\n",
      "Epoch: 29998 \tTraining Loss: 1.315204 \tValidation Loss: 2.585295\n",
      "Epoch: 29999 \tTraining Loss: 1.319229 \tValidation Loss: 2.584127\n",
      "Epoch: 30000 \tTraining Loss: 1.359145 \tValidation Loss: 2.583446\n",
      "Epoch: 30001 \tTraining Loss: 1.307206 \tValidation Loss: 2.583130\n",
      "Epoch: 30002 \tTraining Loss: 1.346469 \tValidation Loss: 2.585196\n",
      "Epoch: 30003 \tTraining Loss: 1.312859 \tValidation Loss: 2.584433\n",
      "Epoch: 30004 \tTraining Loss: 1.286672 \tValidation Loss: 2.585310\n",
      "Epoch: 30005 \tTraining Loss: 1.337132 \tValidation Loss: 2.584613\n",
      "Epoch: 30006 \tTraining Loss: 1.290564 \tValidation Loss: 2.585338\n",
      "Epoch: 30007 \tTraining Loss: 1.291932 \tValidation Loss: 2.586532\n",
      "Epoch: 30008 \tTraining Loss: 1.297385 \tValidation Loss: 2.585346\n",
      "Epoch: 30009 \tTraining Loss: 1.312490 \tValidation Loss: 2.585667\n",
      "Epoch: 30010 \tTraining Loss: 1.259254 \tValidation Loss: 2.585734\n",
      "Epoch: 30011 \tTraining Loss: 1.328680 \tValidation Loss: 2.585441\n",
      "Epoch: 30012 \tTraining Loss: 1.306691 \tValidation Loss: 2.586764\n",
      "Epoch: 30013 \tTraining Loss: 1.346833 \tValidation Loss: 2.584863\n",
      "Epoch: 30014 \tTraining Loss: 1.324811 \tValidation Loss: 2.586159\n",
      "Epoch: 30015 \tTraining Loss: 1.294441 \tValidation Loss: 2.585868\n",
      "Epoch: 30016 \tTraining Loss: 1.296642 \tValidation Loss: 2.587317\n",
      "Epoch: 30017 \tTraining Loss: 1.321613 \tValidation Loss: 2.585791\n",
      "Epoch: 30018 \tTraining Loss: 1.330502 \tValidation Loss: 2.585850\n",
      "Epoch: 30019 \tTraining Loss: 1.310511 \tValidation Loss: 2.585972\n",
      "Epoch: 30020 \tTraining Loss: 1.312219 \tValidation Loss: 2.585734\n",
      "Epoch: 30021 \tTraining Loss: 1.325666 \tValidation Loss: 2.584364\n",
      "Epoch: 30022 \tTraining Loss: 1.265807 \tValidation Loss: 2.584437\n",
      "Epoch: 30023 \tTraining Loss: 1.276330 \tValidation Loss: 2.584916\n",
      "Epoch: 30024 \tTraining Loss: 1.361879 \tValidation Loss: 2.584209\n",
      "Epoch: 30025 \tTraining Loss: 1.271721 \tValidation Loss: 2.585637\n",
      "Epoch: 30026 \tTraining Loss: 1.342832 \tValidation Loss: 2.584542\n",
      "Epoch: 30027 \tTraining Loss: 1.307514 \tValidation Loss: 2.584932\n",
      "Epoch: 30028 \tTraining Loss: 1.338432 \tValidation Loss: 2.584119\n",
      "Epoch: 30029 \tTraining Loss: 1.296574 \tValidation Loss: 2.585639\n",
      "Epoch: 30030 \tTraining Loss: 1.329361 \tValidation Loss: 2.585234\n",
      "Epoch: 30031 \tTraining Loss: 1.263011 \tValidation Loss: 2.585736\n",
      "Epoch: 30032 \tTraining Loss: 1.321020 \tValidation Loss: 2.585481\n",
      "Epoch: 30033 \tTraining Loss: 1.282591 \tValidation Loss: 2.586361\n",
      "Epoch: 30034 \tTraining Loss: 1.298634 \tValidation Loss: 2.584939\n",
      "Epoch: 30035 \tTraining Loss: 1.300383 \tValidation Loss: 2.587687\n",
      "Epoch: 30036 \tTraining Loss: 1.295379 \tValidation Loss: 2.586254\n",
      "Epoch: 30037 \tTraining Loss: 1.337910 \tValidation Loss: 2.585389\n",
      "Epoch: 30038 \tTraining Loss: 1.288017 \tValidation Loss: 2.586141\n",
      "Epoch: 30039 \tTraining Loss: 1.341630 \tValidation Loss: 2.586988\n",
      "Epoch: 30040 \tTraining Loss: 1.325498 \tValidation Loss: 2.584915\n",
      "Epoch: 30041 \tTraining Loss: 1.296429 \tValidation Loss: 2.584706\n",
      "Epoch: 30042 \tTraining Loss: 1.305934 \tValidation Loss: 2.585394\n",
      "Epoch: 30043 \tTraining Loss: 1.287504 \tValidation Loss: 2.587095\n",
      "Epoch: 30044 \tTraining Loss: 1.273543 \tValidation Loss: 2.586240\n",
      "Epoch: 30045 \tTraining Loss: 1.369029 \tValidation Loss: 2.585138\n",
      "Epoch: 30046 \tTraining Loss: 1.320648 \tValidation Loss: 2.585710\n",
      "Epoch: 30047 \tTraining Loss: 1.311280 \tValidation Loss: 2.585120\n",
      "Epoch: 30048 \tTraining Loss: 1.317668 \tValidation Loss: 2.585458\n",
      "Epoch: 30049 \tTraining Loss: 1.268896 \tValidation Loss: 2.586477\n",
      "Epoch: 30050 \tTraining Loss: 1.375301 \tValidation Loss: 2.585443\n",
      "Epoch: 30051 \tTraining Loss: 1.300323 \tValidation Loss: 2.586069\n",
      "Epoch: 30052 \tTraining Loss: 1.276525 \tValidation Loss: 2.587358\n",
      "Epoch: 30053 \tTraining Loss: 1.305980 \tValidation Loss: 2.585690\n",
      "Epoch: 30054 \tTraining Loss: 1.294969 \tValidation Loss: 2.585624\n",
      "Epoch: 30055 \tTraining Loss: 1.320943 \tValidation Loss: 2.586257\n",
      "Epoch: 30056 \tTraining Loss: 1.329675 \tValidation Loss: 2.586499\n",
      "Epoch: 30057 \tTraining Loss: 1.309135 \tValidation Loss: 2.586608\n",
      "Epoch: 30058 \tTraining Loss: 1.285164 \tValidation Loss: 2.584940\n",
      "Epoch: 30059 \tTraining Loss: 1.297963 \tValidation Loss: 2.586046\n",
      "Epoch: 30060 \tTraining Loss: 1.318535 \tValidation Loss: 2.586028\n",
      "Epoch: 30061 \tTraining Loss: 1.339445 \tValidation Loss: 2.587463\n",
      "Epoch: 30062 \tTraining Loss: 1.293350 \tValidation Loss: 2.585703\n",
      "Epoch: 30063 \tTraining Loss: 1.306041 \tValidation Loss: 2.586934\n",
      "Epoch: 30064 \tTraining Loss: 1.321540 \tValidation Loss: 2.584909\n",
      "Epoch: 30065 \tTraining Loss: 1.296115 \tValidation Loss: 2.586234\n",
      "Epoch: 30066 \tTraining Loss: 1.304680 \tValidation Loss: 2.585778\n",
      "Epoch: 30067 \tTraining Loss: 1.345800 \tValidation Loss: 2.586359\n",
      "Epoch: 30068 \tTraining Loss: 1.283826 \tValidation Loss: 2.586468\n",
      "Epoch: 30069 \tTraining Loss: 1.359930 \tValidation Loss: 2.585245\n",
      "Epoch: 30070 \tTraining Loss: 1.323133 \tValidation Loss: 2.586172\n",
      "Epoch: 30071 \tTraining Loss: 1.317673 \tValidation Loss: 2.585935\n",
      "Epoch: 30072 \tTraining Loss: 1.322127 \tValidation Loss: 2.586055\n",
      "Epoch: 30073 \tTraining Loss: 1.330648 \tValidation Loss: 2.585602\n",
      "Epoch: 30074 \tTraining Loss: 1.313632 \tValidation Loss: 2.586091\n",
      "Epoch: 30075 \tTraining Loss: 1.357298 \tValidation Loss: 2.586214\n",
      "Epoch: 30076 \tTraining Loss: 1.326624 \tValidation Loss: 2.587067\n",
      "Epoch: 30077 \tTraining Loss: 1.268072 \tValidation Loss: 2.586312\n",
      "Epoch: 30078 \tTraining Loss: 1.337620 \tValidation Loss: 2.586544\n",
      "Epoch: 30079 \tTraining Loss: 1.312074 \tValidation Loss: 2.585708\n",
      "Epoch: 30080 \tTraining Loss: 1.319081 \tValidation Loss: 2.587085\n",
      "Epoch: 30081 \tTraining Loss: 1.292908 \tValidation Loss: 2.585858\n",
      "Epoch: 30082 \tTraining Loss: 1.321869 \tValidation Loss: 2.586101\n",
      "Epoch: 30083 \tTraining Loss: 1.342771 \tValidation Loss: 2.586132\n",
      "Epoch: 30084 \tTraining Loss: 1.283315 \tValidation Loss: 2.587497\n",
      "Epoch: 30085 \tTraining Loss: 1.233162 \tValidation Loss: 2.588327\n",
      "Epoch: 30086 \tTraining Loss: 1.314211 \tValidation Loss: 2.585523\n",
      "Epoch: 30087 \tTraining Loss: 1.331295 \tValidation Loss: 2.585728\n",
      "Epoch: 30088 \tTraining Loss: 1.337990 \tValidation Loss: 2.586611\n",
      "Epoch: 30089 \tTraining Loss: 1.316415 \tValidation Loss: 2.585205\n",
      "Epoch: 30090 \tTraining Loss: 1.332705 \tValidation Loss: 2.585576\n",
      "Epoch: 30091 \tTraining Loss: 1.352276 \tValidation Loss: 2.584533\n",
      "Epoch: 30092 \tTraining Loss: 1.336256 \tValidation Loss: 2.586053\n",
      "Epoch: 30093 \tTraining Loss: 1.293097 \tValidation Loss: 2.586278\n",
      "Epoch: 30094 \tTraining Loss: 1.306317 \tValidation Loss: 2.587049\n",
      "Epoch: 30095 \tTraining Loss: 1.277046 \tValidation Loss: 2.587146\n",
      "Epoch: 30096 \tTraining Loss: 1.366563 \tValidation Loss: 2.586338\n",
      "Epoch: 30097 \tTraining Loss: 1.306191 \tValidation Loss: 2.586321\n",
      "Epoch: 30098 \tTraining Loss: 1.306107 \tValidation Loss: 2.587008\n",
      "Epoch: 30099 \tTraining Loss: 1.366627 \tValidation Loss: 2.587046\n",
      "Epoch: 30100 \tTraining Loss: 1.330293 \tValidation Loss: 2.587281\n",
      "Epoch: 30101 \tTraining Loss: 1.302555 \tValidation Loss: 2.586628\n",
      "Epoch: 30102 \tTraining Loss: 1.348523 \tValidation Loss: 2.585434\n",
      "Epoch: 30103 \tTraining Loss: 1.321233 \tValidation Loss: 2.585133\n",
      "Epoch: 30104 \tTraining Loss: 1.323836 \tValidation Loss: 2.585719\n",
      "Epoch: 30105 \tTraining Loss: 1.322072 \tValidation Loss: 2.586120\n",
      "Epoch: 30106 \tTraining Loss: 1.300556 \tValidation Loss: 2.587726\n",
      "Epoch: 30107 \tTraining Loss: 1.271940 \tValidation Loss: 2.587551\n",
      "Epoch: 30108 \tTraining Loss: 1.300506 \tValidation Loss: 2.585847\n",
      "Epoch: 30109 \tTraining Loss: 1.262373 \tValidation Loss: 2.587949\n",
      "Epoch: 30110 \tTraining Loss: 1.303968 \tValidation Loss: 2.585835\n",
      "Epoch: 30111 \tTraining Loss: 1.310776 \tValidation Loss: 2.586748\n",
      "Epoch: 30112 \tTraining Loss: 1.320196 \tValidation Loss: 2.588431\n",
      "Epoch: 30113 \tTraining Loss: 1.315178 \tValidation Loss: 2.587333\n",
      "Epoch: 30114 \tTraining Loss: 1.320471 \tValidation Loss: 2.586679\n",
      "Epoch: 30115 \tTraining Loss: 1.293892 \tValidation Loss: 2.587050\n",
      "Epoch: 30116 \tTraining Loss: 1.329349 \tValidation Loss: 2.586583\n",
      "Epoch: 30117 \tTraining Loss: 1.318553 \tValidation Loss: 2.587199\n",
      "Epoch: 30118 \tTraining Loss: 1.355067 \tValidation Loss: 2.586657\n",
      "Epoch: 30119 \tTraining Loss: 1.331696 \tValidation Loss: 2.586142\n",
      "Epoch: 30120 \tTraining Loss: 1.278055 \tValidation Loss: 2.588096\n",
      "Epoch: 30121 \tTraining Loss: 1.315817 \tValidation Loss: 2.587927\n",
      "Epoch: 30122 \tTraining Loss: 1.319614 \tValidation Loss: 2.586838\n",
      "Epoch: 30123 \tTraining Loss: 1.285996 \tValidation Loss: 2.587454\n",
      "Epoch: 30124 \tTraining Loss: 1.310727 \tValidation Loss: 2.587722\n",
      "Epoch: 30125 \tTraining Loss: 1.275815 \tValidation Loss: 2.586916\n",
      "Epoch: 30126 \tTraining Loss: 1.321619 \tValidation Loss: 2.586234\n",
      "Epoch: 30127 \tTraining Loss: 1.300363 \tValidation Loss: 2.587966\n",
      "Epoch: 30128 \tTraining Loss: 1.310505 \tValidation Loss: 2.587828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30129 \tTraining Loss: 1.311962 \tValidation Loss: 2.587483\n",
      "Epoch: 30130 \tTraining Loss: 1.310460 \tValidation Loss: 2.587739\n",
      "Epoch: 30131 \tTraining Loss: 1.310218 \tValidation Loss: 2.586700\n",
      "Epoch: 30132 \tTraining Loss: 1.285515 \tValidation Loss: 2.587469\n",
      "Epoch: 30133 \tTraining Loss: 1.275946 \tValidation Loss: 2.588840\n",
      "Epoch: 30134 \tTraining Loss: 1.320473 \tValidation Loss: 2.588456\n",
      "Epoch: 30135 \tTraining Loss: 1.371922 \tValidation Loss: 2.586618\n",
      "Epoch: 30136 \tTraining Loss: 1.321653 \tValidation Loss: 2.587879\n",
      "Epoch: 30137 \tTraining Loss: 1.293810 \tValidation Loss: 2.586794\n",
      "Epoch: 30138 \tTraining Loss: 1.311925 \tValidation Loss: 2.587291\n",
      "Epoch: 30139 \tTraining Loss: 1.334617 \tValidation Loss: 2.587566\n",
      "Epoch: 30140 \tTraining Loss: 1.296955 \tValidation Loss: 2.588313\n",
      "Epoch: 30141 \tTraining Loss: 1.284845 \tValidation Loss: 2.588832\n",
      "Epoch: 30142 \tTraining Loss: 1.317073 \tValidation Loss: 2.587636\n",
      "Epoch: 30143 \tTraining Loss: 1.337092 \tValidation Loss: 2.586959\n",
      "Epoch: 30144 \tTraining Loss: 1.296182 \tValidation Loss: 2.587802\n",
      "Epoch: 30145 \tTraining Loss: 1.323222 \tValidation Loss: 2.587754\n",
      "Epoch: 30146 \tTraining Loss: 1.329145 \tValidation Loss: 2.586823\n",
      "Epoch: 30147 \tTraining Loss: 1.352499 \tValidation Loss: 2.587447\n",
      "Epoch: 30148 \tTraining Loss: 1.305352 \tValidation Loss: 2.587599\n",
      "Epoch: 30149 \tTraining Loss: 1.262890 \tValidation Loss: 2.588822\n",
      "Epoch: 30150 \tTraining Loss: 1.276612 \tValidation Loss: 2.587983\n",
      "Epoch: 30151 \tTraining Loss: 1.342935 \tValidation Loss: 2.587947\n",
      "Epoch: 30152 \tTraining Loss: 1.302951 \tValidation Loss: 2.587465\n",
      "Epoch: 30153 \tTraining Loss: 1.337034 \tValidation Loss: 2.586559\n",
      "Epoch: 30154 \tTraining Loss: 1.317747 \tValidation Loss: 2.588435\n",
      "Epoch: 30155 \tTraining Loss: 1.289740 \tValidation Loss: 2.587433\n",
      "Epoch: 30156 \tTraining Loss: 1.273107 \tValidation Loss: 2.589032\n",
      "Epoch: 30157 \tTraining Loss: 1.331637 \tValidation Loss: 2.587824\n",
      "Epoch: 30158 \tTraining Loss: 1.343525 \tValidation Loss: 2.588471\n",
      "Epoch: 30159 \tTraining Loss: 1.335714 \tValidation Loss: 2.587627\n",
      "Epoch: 30160 \tTraining Loss: 1.285334 \tValidation Loss: 2.588279\n",
      "Epoch: 30161 \tTraining Loss: 1.323298 \tValidation Loss: 2.587568\n",
      "Epoch: 30162 \tTraining Loss: 1.310981 \tValidation Loss: 2.588079\n",
      "Epoch: 30163 \tTraining Loss: 1.280425 \tValidation Loss: 2.588859\n",
      "Epoch: 30164 \tTraining Loss: 1.343685 \tValidation Loss: 2.587705\n",
      "Epoch: 30165 \tTraining Loss: 1.271435 \tValidation Loss: 2.588465\n",
      "Epoch: 30166 \tTraining Loss: 1.329373 \tValidation Loss: 2.588628\n",
      "Epoch: 30167 \tTraining Loss: 1.338599 \tValidation Loss: 2.588438\n",
      "Epoch: 30168 \tTraining Loss: 1.341504 \tValidation Loss: 2.588398\n",
      "Epoch: 30169 \tTraining Loss: 1.272852 \tValidation Loss: 2.588432\n",
      "Epoch: 30170 \tTraining Loss: 1.328903 \tValidation Loss: 2.588642\n",
      "Epoch: 30171 \tTraining Loss: 1.337060 \tValidation Loss: 2.587882\n",
      "Epoch: 30172 \tTraining Loss: 1.326290 \tValidation Loss: 2.589084\n",
      "Epoch: 30173 \tTraining Loss: 1.351483 \tValidation Loss: 2.588215\n",
      "Epoch: 30174 \tTraining Loss: 1.337527 \tValidation Loss: 2.587014\n",
      "Epoch: 30175 \tTraining Loss: 1.323215 \tValidation Loss: 2.587259\n",
      "Epoch: 30176 \tTraining Loss: 1.314772 \tValidation Loss: 2.588722\n",
      "Epoch: 30177 \tTraining Loss: 1.251810 \tValidation Loss: 2.589611\n",
      "Epoch: 30178 \tTraining Loss: 1.300653 \tValidation Loss: 2.589354\n",
      "Epoch: 30179 \tTraining Loss: 1.321508 \tValidation Loss: 2.588606\n",
      "Epoch: 30180 \tTraining Loss: 1.281840 \tValidation Loss: 2.589175\n",
      "Epoch: 30181 \tTraining Loss: 1.320305 \tValidation Loss: 2.587753\n",
      "Epoch: 30182 \tTraining Loss: 1.304986 \tValidation Loss: 2.589494\n",
      "Epoch: 30183 \tTraining Loss: 1.334664 \tValidation Loss: 2.588881\n",
      "Epoch: 30184 \tTraining Loss: 1.344935 \tValidation Loss: 2.588404\n",
      "Epoch: 30185 \tTraining Loss: 1.305891 \tValidation Loss: 2.588475\n",
      "Epoch: 30186 \tTraining Loss: 1.325045 \tValidation Loss: 2.587239\n",
      "Epoch: 30187 \tTraining Loss: 1.286932 \tValidation Loss: 2.589448\n",
      "Epoch: 30188 \tTraining Loss: 1.274221 \tValidation Loss: 2.590728\n",
      "Epoch: 30189 \tTraining Loss: 1.338412 \tValidation Loss: 2.589126\n",
      "Epoch: 30190 \tTraining Loss: 1.317396 \tValidation Loss: 2.589941\n",
      "Epoch: 30191 \tTraining Loss: 1.362606 \tValidation Loss: 2.588440\n",
      "Epoch: 30192 \tTraining Loss: 1.303521 \tValidation Loss: 2.589607\n",
      "Epoch: 30193 \tTraining Loss: 1.286523 \tValidation Loss: 2.589507\n",
      "Epoch: 30194 \tTraining Loss: 1.302590 \tValidation Loss: 2.588975\n",
      "Epoch: 30195 \tTraining Loss: 1.294673 \tValidation Loss: 2.589837\n",
      "Epoch: 30196 \tTraining Loss: 1.307445 \tValidation Loss: 2.589395\n",
      "Epoch: 30197 \tTraining Loss: 1.277609 \tValidation Loss: 2.589284\n",
      "Epoch: 30198 \tTraining Loss: 1.315433 \tValidation Loss: 2.588479\n",
      "Epoch: 30199 \tTraining Loss: 1.314756 \tValidation Loss: 2.588809\n",
      "Epoch: 30200 \tTraining Loss: 1.288835 \tValidation Loss: 2.589488\n",
      "Epoch: 30201 \tTraining Loss: 1.288343 \tValidation Loss: 2.588581\n",
      "Epoch: 30202 \tTraining Loss: 1.289535 \tValidation Loss: 2.589787\n",
      "Epoch: 30203 \tTraining Loss: 1.347473 \tValidation Loss: 2.588279\n",
      "Epoch: 30204 \tTraining Loss: 1.305365 \tValidation Loss: 2.588785\n",
      "Epoch: 30205 \tTraining Loss: 1.330632 \tValidation Loss: 2.589316\n",
      "Epoch: 30206 \tTraining Loss: 1.326867 \tValidation Loss: 2.589112\n",
      "Epoch: 30207 \tTraining Loss: 1.284815 \tValidation Loss: 2.589792\n",
      "Epoch: 30208 \tTraining Loss: 1.282857 \tValidation Loss: 2.590526\n",
      "Epoch: 30209 \tTraining Loss: 1.343929 \tValidation Loss: 2.590258\n",
      "Epoch: 30210 \tTraining Loss: 1.284890 \tValidation Loss: 2.590811\n",
      "Epoch: 30211 \tTraining Loss: 1.286100 \tValidation Loss: 2.589972\n",
      "Epoch: 30212 \tTraining Loss: 1.314778 \tValidation Loss: 2.590040\n",
      "Epoch: 30213 \tTraining Loss: 1.325114 \tValidation Loss: 2.588892\n",
      "Epoch: 30214 \tTraining Loss: 1.293232 \tValidation Loss: 2.589517\n",
      "Epoch: 30215 \tTraining Loss: 1.343218 \tValidation Loss: 2.588324\n",
      "Epoch: 30216 \tTraining Loss: 1.308906 \tValidation Loss: 2.589843\n",
      "Epoch: 30217 \tTraining Loss: 1.326700 \tValidation Loss: 2.589522\n",
      "Epoch: 30218 \tTraining Loss: 1.338970 \tValidation Loss: 2.589367\n",
      "Epoch: 30219 \tTraining Loss: 1.322913 \tValidation Loss: 2.591067\n",
      "Epoch: 30220 \tTraining Loss: 1.274085 \tValidation Loss: 2.590899\n",
      "Epoch: 30221 \tTraining Loss: 1.293453 \tValidation Loss: 2.591371\n",
      "Epoch: 30222 \tTraining Loss: 1.355244 \tValidation Loss: 2.589870\n",
      "Epoch: 30223 \tTraining Loss: 1.307354 \tValidation Loss: 2.590286\n",
      "Epoch: 30224 \tTraining Loss: 1.353512 \tValidation Loss: 2.590503\n",
      "Epoch: 30225 \tTraining Loss: 1.296591 \tValidation Loss: 2.590482\n",
      "Epoch: 30226 \tTraining Loss: 1.276171 \tValidation Loss: 2.590312\n",
      "Epoch: 30227 \tTraining Loss: 1.330708 \tValidation Loss: 2.590551\n",
      "Epoch: 30228 \tTraining Loss: 1.307843 \tValidation Loss: 2.591207\n",
      "Epoch: 30229 \tTraining Loss: 1.287532 \tValidation Loss: 2.590618\n",
      "Epoch: 30230 \tTraining Loss: 1.335053 \tValidation Loss: 2.590315\n",
      "Epoch: 30231 \tTraining Loss: 1.365956 \tValidation Loss: 2.588843\n",
      "Epoch: 30232 \tTraining Loss: 1.267928 \tValidation Loss: 2.590752\n",
      "Epoch: 30233 \tTraining Loss: 1.373345 \tValidation Loss: 2.589845\n",
      "Epoch: 30234 \tTraining Loss: 1.292170 \tValidation Loss: 2.590202\n",
      "Epoch: 30235 \tTraining Loss: 1.344952 \tValidation Loss: 2.590292\n",
      "Epoch: 30236 \tTraining Loss: 1.334288 \tValidation Loss: 2.590031\n",
      "Epoch: 30237 \tTraining Loss: 1.346805 \tValidation Loss: 2.589149\n",
      "Epoch: 30238 \tTraining Loss: 1.262024 \tValidation Loss: 2.590374\n",
      "Epoch: 30239 \tTraining Loss: 1.293364 \tValidation Loss: 2.589706\n",
      "Epoch: 30240 \tTraining Loss: 1.297518 \tValidation Loss: 2.590320\n",
      "Epoch: 30241 \tTraining Loss: 1.271015 \tValidation Loss: 2.589930\n",
      "Epoch: 30242 \tTraining Loss: 1.375604 \tValidation Loss: 2.589031\n",
      "Epoch: 30243 \tTraining Loss: 1.321361 \tValidation Loss: 2.589423\n",
      "Epoch: 30244 \tTraining Loss: 1.342615 \tValidation Loss: 2.590286\n",
      "Epoch: 30245 \tTraining Loss: 1.287129 \tValidation Loss: 2.589072\n",
      "Epoch: 30246 \tTraining Loss: 1.299026 \tValidation Loss: 2.591138\n",
      "Epoch: 30247 \tTraining Loss: 1.305541 \tValidation Loss: 2.589818\n",
      "Epoch: 30248 \tTraining Loss: 1.306979 \tValidation Loss: 2.590996\n",
      "Epoch: 30249 \tTraining Loss: 1.304114 \tValidation Loss: 2.590334\n",
      "Epoch: 30250 \tTraining Loss: 1.315074 \tValidation Loss: 2.591068\n",
      "Epoch: 30251 \tTraining Loss: 1.292758 \tValidation Loss: 2.591897\n",
      "Epoch: 30252 \tTraining Loss: 1.320936 \tValidation Loss: 2.589427\n",
      "Epoch: 30253 \tTraining Loss: 1.311170 \tValidation Loss: 2.588891\n",
      "Epoch: 30254 \tTraining Loss: 1.296644 \tValidation Loss: 2.590846\n",
      "Epoch: 30255 \tTraining Loss: 1.297192 \tValidation Loss: 2.591057\n",
      "Epoch: 30256 \tTraining Loss: 1.308244 \tValidation Loss: 2.590719\n",
      "Epoch: 30257 \tTraining Loss: 1.270687 \tValidation Loss: 2.591831\n",
      "Epoch: 30258 \tTraining Loss: 1.286413 \tValidation Loss: 2.591015\n",
      "Epoch: 30259 \tTraining Loss: 1.276251 \tValidation Loss: 2.590537\n",
      "Epoch: 30260 \tTraining Loss: 1.316120 \tValidation Loss: 2.590364\n",
      "Epoch: 30261 \tTraining Loss: 1.311518 \tValidation Loss: 2.591160\n",
      "Epoch: 30262 \tTraining Loss: 1.345405 \tValidation Loss: 2.589772\n",
      "Epoch: 30263 \tTraining Loss: 1.342836 \tValidation Loss: 2.590779\n",
      "Epoch: 30264 \tTraining Loss: 1.303109 \tValidation Loss: 2.589103\n",
      "Epoch: 30265 \tTraining Loss: 1.319927 \tValidation Loss: 2.591142\n",
      "Epoch: 30266 \tTraining Loss: 1.307453 \tValidation Loss: 2.591454\n",
      "Epoch: 30267 \tTraining Loss: 1.292952 \tValidation Loss: 2.590238\n",
      "Epoch: 30268 \tTraining Loss: 1.292368 \tValidation Loss: 2.590490\n",
      "Epoch: 30269 \tTraining Loss: 1.267615 \tValidation Loss: 2.591347\n",
      "Epoch: 30270 \tTraining Loss: 1.314444 \tValidation Loss: 2.590432\n",
      "Epoch: 30271 \tTraining Loss: 1.323954 \tValidation Loss: 2.590998\n",
      "Epoch: 30272 \tTraining Loss: 1.301080 \tValidation Loss: 2.592824\n",
      "Epoch: 30273 \tTraining Loss: 1.286720 \tValidation Loss: 2.590708\n",
      "Epoch: 30274 \tTraining Loss: 1.271214 \tValidation Loss: 2.592015\n",
      "Epoch: 30275 \tTraining Loss: 1.305909 \tValidation Loss: 2.590515\n",
      "Epoch: 30276 \tTraining Loss: 1.311795 \tValidation Loss: 2.591179\n",
      "Epoch: 30277 \tTraining Loss: 1.295087 \tValidation Loss: 2.590574\n",
      "Epoch: 30278 \tTraining Loss: 1.289961 \tValidation Loss: 2.591996\n",
      "Epoch: 30279 \tTraining Loss: 1.339774 \tValidation Loss: 2.591184\n",
      "Epoch: 30280 \tTraining Loss: 1.305342 \tValidation Loss: 2.592657\n",
      "Epoch: 30281 \tTraining Loss: 1.306106 \tValidation Loss: 2.590383\n",
      "Epoch: 30282 \tTraining Loss: 1.306392 \tValidation Loss: 2.591940\n",
      "Epoch: 30283 \tTraining Loss: 1.308551 \tValidation Loss: 2.590953\n",
      "Epoch: 30284 \tTraining Loss: 1.324704 \tValidation Loss: 2.591043\n",
      "Epoch: 30285 \tTraining Loss: 1.277042 \tValidation Loss: 2.591093\n",
      "Epoch: 30286 \tTraining Loss: 1.298515 \tValidation Loss: 2.591701\n",
      "Epoch: 30287 \tTraining Loss: 1.282899 \tValidation Loss: 2.590710\n",
      "Epoch: 30288 \tTraining Loss: 1.297669 \tValidation Loss: 2.592571\n",
      "Epoch: 30289 \tTraining Loss: 1.294352 \tValidation Loss: 2.592774\n",
      "Epoch: 30290 \tTraining Loss: 1.292016 \tValidation Loss: 2.591824\n",
      "Epoch: 30291 \tTraining Loss: 1.318409 \tValidation Loss: 2.591284\n",
      "Epoch: 30292 \tTraining Loss: 1.352377 \tValidation Loss: 2.590270\n",
      "Epoch: 30293 \tTraining Loss: 1.320932 \tValidation Loss: 2.591715\n",
      "Epoch: 30294 \tTraining Loss: 1.296877 \tValidation Loss: 2.592011\n",
      "Epoch: 30295 \tTraining Loss: 1.276101 \tValidation Loss: 2.592956\n",
      "Epoch: 30296 \tTraining Loss: 1.305939 \tValidation Loss: 2.591781\n",
      "Epoch: 30297 \tTraining Loss: 1.315321 \tValidation Loss: 2.591481\n",
      "Epoch: 30298 \tTraining Loss: 1.331076 \tValidation Loss: 2.591315\n",
      "Epoch: 30299 \tTraining Loss: 1.276894 \tValidation Loss: 2.591265\n",
      "Epoch: 30300 \tTraining Loss: 1.289026 \tValidation Loss: 2.592454\n",
      "Epoch: 30301 \tTraining Loss: 1.337362 \tValidation Loss: 2.590438\n",
      "Epoch: 30302 \tTraining Loss: 1.284616 \tValidation Loss: 2.592719\n",
      "Epoch: 30303 \tTraining Loss: 1.358315 \tValidation Loss: 2.591462\n",
      "Epoch: 30304 \tTraining Loss: 1.339660 \tValidation Loss: 2.590953\n",
      "Epoch: 30305 \tTraining Loss: 1.269195 \tValidation Loss: 2.593554\n",
      "Epoch: 30306 \tTraining Loss: 1.340399 \tValidation Loss: 2.591498\n",
      "Epoch: 30307 \tTraining Loss: 1.286077 \tValidation Loss: 2.591479\n",
      "Epoch: 30308 \tTraining Loss: 1.327377 \tValidation Loss: 2.591344\n",
      "Epoch: 30309 \tTraining Loss: 1.244852 \tValidation Loss: 2.593593\n",
      "Epoch: 30310 \tTraining Loss: 1.284589 \tValidation Loss: 2.592545\n",
      "Epoch: 30311 \tTraining Loss: 1.294444 \tValidation Loss: 2.592111\n",
      "Epoch: 30312 \tTraining Loss: 1.347736 \tValidation Loss: 2.591500\n",
      "Epoch: 30313 \tTraining Loss: 1.314386 \tValidation Loss: 2.591756\n",
      "Epoch: 30314 \tTraining Loss: 1.316342 \tValidation Loss: 2.591596\n",
      "Epoch: 30315 \tTraining Loss: 1.286090 \tValidation Loss: 2.592906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30316 \tTraining Loss: 1.318106 \tValidation Loss: 2.591005\n",
      "Epoch: 30317 \tTraining Loss: 1.345152 \tValidation Loss: 2.592443\n",
      "Epoch: 30318 \tTraining Loss: 1.265695 \tValidation Loss: 2.592721\n",
      "Epoch: 30319 \tTraining Loss: 1.324703 \tValidation Loss: 2.589819\n",
      "Epoch: 30320 \tTraining Loss: 1.280001 \tValidation Loss: 2.590666\n",
      "Epoch: 30321 \tTraining Loss: 1.308548 \tValidation Loss: 2.591074\n",
      "Epoch: 30322 \tTraining Loss: 1.264063 \tValidation Loss: 2.593072\n",
      "Epoch: 30323 \tTraining Loss: 1.281867 \tValidation Loss: 2.593114\n",
      "Epoch: 30324 \tTraining Loss: 1.269358 \tValidation Loss: 2.593899\n",
      "Epoch: 30325 \tTraining Loss: 1.272148 \tValidation Loss: 2.592625\n",
      "Epoch: 30326 \tTraining Loss: 1.294806 \tValidation Loss: 2.592498\n",
      "Epoch: 30327 \tTraining Loss: 1.261580 \tValidation Loss: 2.592938\n",
      "Epoch: 30328 \tTraining Loss: 1.335276 \tValidation Loss: 2.591073\n",
      "Epoch: 30329 \tTraining Loss: 1.303153 \tValidation Loss: 2.590969\n",
      "Epoch: 30330 \tTraining Loss: 1.288698 \tValidation Loss: 2.591910\n",
      "Epoch: 30331 \tTraining Loss: 1.345090 \tValidation Loss: 2.590684\n",
      "Epoch: 30332 \tTraining Loss: 1.302706 \tValidation Loss: 2.592267\n",
      "Epoch: 30333 \tTraining Loss: 1.271681 \tValidation Loss: 2.593684\n",
      "Epoch: 30334 \tTraining Loss: 1.318993 \tValidation Loss: 2.592448\n",
      "Epoch: 30335 \tTraining Loss: 1.329053 \tValidation Loss: 2.593384\n",
      "Epoch: 30336 \tTraining Loss: 1.270809 \tValidation Loss: 2.592308\n",
      "Epoch: 30337 \tTraining Loss: 1.329847 \tValidation Loss: 2.591898\n",
      "Epoch: 30338 \tTraining Loss: 1.319853 \tValidation Loss: 2.591041\n",
      "Epoch: 30339 \tTraining Loss: 1.351875 \tValidation Loss: 2.591823\n",
      "Epoch: 30340 \tTraining Loss: 1.309485 \tValidation Loss: 2.592308\n",
      "Epoch: 30341 \tTraining Loss: 1.266644 \tValidation Loss: 2.592401\n",
      "Epoch: 30342 \tTraining Loss: 1.277846 \tValidation Loss: 2.590872\n",
      "Epoch: 30343 \tTraining Loss: 1.268613 \tValidation Loss: 2.592508\n",
      "Epoch: 30344 \tTraining Loss: 1.280160 \tValidation Loss: 2.592343\n",
      "Epoch: 30345 \tTraining Loss: 1.322375 \tValidation Loss: 2.594349\n",
      "Epoch: 30346 \tTraining Loss: 1.316202 \tValidation Loss: 2.592466\n",
      "Epoch: 30347 \tTraining Loss: 1.333739 \tValidation Loss: 2.592503\n",
      "Epoch: 30348 \tTraining Loss: 1.338560 \tValidation Loss: 2.591655\n",
      "Epoch: 30349 \tTraining Loss: 1.300540 \tValidation Loss: 2.591978\n",
      "Epoch: 30350 \tTraining Loss: 1.347911 \tValidation Loss: 2.591716\n",
      "Epoch: 30351 \tTraining Loss: 1.325281 \tValidation Loss: 2.592615\n",
      "Epoch: 30352 \tTraining Loss: 1.299143 \tValidation Loss: 2.593031\n",
      "Epoch: 30353 \tTraining Loss: 1.275832 \tValidation Loss: 2.594541\n",
      "Epoch: 30354 \tTraining Loss: 1.292503 \tValidation Loss: 2.591750\n",
      "Epoch: 30355 \tTraining Loss: 1.288791 \tValidation Loss: 2.593043\n",
      "Epoch: 30356 \tTraining Loss: 1.297814 \tValidation Loss: 2.593243\n",
      "Epoch: 30357 \tTraining Loss: 1.283169 \tValidation Loss: 2.592690\n",
      "Epoch: 30358 \tTraining Loss: 1.304816 \tValidation Loss: 2.592368\n",
      "Epoch: 30359 \tTraining Loss: 1.288728 \tValidation Loss: 2.592422\n",
      "Epoch: 30360 \tTraining Loss: 1.309593 \tValidation Loss: 2.592728\n",
      "Epoch: 30361 \tTraining Loss: 1.315489 \tValidation Loss: 2.593625\n",
      "Epoch: 30362 \tTraining Loss: 1.320320 \tValidation Loss: 2.592691\n",
      "Epoch: 30363 \tTraining Loss: 1.296263 \tValidation Loss: 2.593522\n",
      "Epoch: 30364 \tTraining Loss: 1.279315 \tValidation Loss: 2.592763\n",
      "Epoch: 30365 \tTraining Loss: 1.247263 \tValidation Loss: 2.593220\n",
      "Epoch: 30366 \tTraining Loss: 1.294131 \tValidation Loss: 2.593056\n",
      "Epoch: 30367 \tTraining Loss: 1.309178 \tValidation Loss: 2.594026\n",
      "Epoch: 30368 \tTraining Loss: 1.289344 \tValidation Loss: 2.593110\n",
      "Epoch: 30369 \tTraining Loss: 1.276347 \tValidation Loss: 2.593318\n",
      "Epoch: 30370 \tTraining Loss: 1.311664 \tValidation Loss: 2.592050\n",
      "Epoch: 30371 \tTraining Loss: 1.272457 \tValidation Loss: 2.594939\n",
      "Epoch: 30372 \tTraining Loss: 1.290876 \tValidation Loss: 2.592662\n",
      "Epoch: 30373 \tTraining Loss: 1.273519 \tValidation Loss: 2.593096\n",
      "Epoch: 30374 \tTraining Loss: 1.274845 \tValidation Loss: 2.593595\n",
      "Epoch: 30375 \tTraining Loss: 1.349797 \tValidation Loss: 2.594206\n",
      "Epoch: 30376 \tTraining Loss: 1.301889 \tValidation Loss: 2.593209\n",
      "Epoch: 30377 \tTraining Loss: 1.289781 \tValidation Loss: 2.594133\n",
      "Epoch: 30378 \tTraining Loss: 1.325929 \tValidation Loss: 2.591394\n",
      "Epoch: 30379 \tTraining Loss: 1.293319 \tValidation Loss: 2.592644\n",
      "Epoch: 30380 \tTraining Loss: 1.264304 \tValidation Loss: 2.594505\n",
      "Epoch: 30381 \tTraining Loss: 1.296907 \tValidation Loss: 2.593438\n",
      "Epoch: 30382 \tTraining Loss: 1.290264 \tValidation Loss: 2.594636\n",
      "Epoch: 30383 \tTraining Loss: 1.317016 \tValidation Loss: 2.594325\n",
      "Epoch: 30384 \tTraining Loss: 1.292333 \tValidation Loss: 2.593915\n",
      "Epoch: 30385 \tTraining Loss: 1.305646 \tValidation Loss: 2.594284\n",
      "Epoch: 30386 \tTraining Loss: 1.295064 \tValidation Loss: 2.593888\n",
      "Epoch: 30387 \tTraining Loss: 1.357403 \tValidation Loss: 2.591647\n",
      "Epoch: 30388 \tTraining Loss: 1.312019 \tValidation Loss: 2.593439\n",
      "Epoch: 30389 \tTraining Loss: 1.304221 \tValidation Loss: 2.593631\n",
      "Epoch: 30390 \tTraining Loss: 1.333235 \tValidation Loss: 2.593737\n",
      "Epoch: 30391 \tTraining Loss: 1.308204 \tValidation Loss: 2.594083\n",
      "Epoch: 30392 \tTraining Loss: 1.299775 \tValidation Loss: 2.593636\n",
      "Epoch: 30393 \tTraining Loss: 1.261720 \tValidation Loss: 2.595227\n",
      "Epoch: 30394 \tTraining Loss: 1.281386 \tValidation Loss: 2.594951\n",
      "Epoch: 30395 \tTraining Loss: 1.311493 \tValidation Loss: 2.593336\n",
      "Epoch: 30396 \tTraining Loss: 1.309408 \tValidation Loss: 2.594631\n",
      "Epoch: 30397 \tTraining Loss: 1.281198 \tValidation Loss: 2.594628\n",
      "Epoch: 30398 \tTraining Loss: 1.296747 \tValidation Loss: 2.593757\n",
      "Epoch: 30399 \tTraining Loss: 1.309950 \tValidation Loss: 2.594925\n",
      "Epoch: 30400 \tTraining Loss: 1.272585 \tValidation Loss: 2.594231\n",
      "Epoch: 30401 \tTraining Loss: 1.278396 \tValidation Loss: 2.594863\n",
      "Epoch: 30402 \tTraining Loss: 1.335662 \tValidation Loss: 2.592692\n",
      "Epoch: 30403 \tTraining Loss: 1.325935 \tValidation Loss: 2.594407\n",
      "Epoch: 30404 \tTraining Loss: 1.270475 \tValidation Loss: 2.595215\n",
      "Epoch: 30405 \tTraining Loss: 1.291507 \tValidation Loss: 2.594311\n",
      "Epoch: 30406 \tTraining Loss: 1.353302 \tValidation Loss: 2.594171\n",
      "Epoch: 30407 \tTraining Loss: 1.312886 \tValidation Loss: 2.592933\n",
      "Epoch: 30408 \tTraining Loss: 1.301287 \tValidation Loss: 2.594450\n",
      "Epoch: 30409 \tTraining Loss: 1.283336 \tValidation Loss: 2.593803\n",
      "Epoch: 30410 \tTraining Loss: 1.294419 \tValidation Loss: 2.593881\n",
      "Epoch: 30411 \tTraining Loss: 1.346047 \tValidation Loss: 2.593084\n",
      "Epoch: 30412 \tTraining Loss: 1.298504 \tValidation Loss: 2.593298\n",
      "Epoch: 30413 \tTraining Loss: 1.309739 \tValidation Loss: 2.593652\n",
      "Epoch: 30414 \tTraining Loss: 1.338493 \tValidation Loss: 2.592944\n",
      "Epoch: 30415 \tTraining Loss: 1.291426 \tValidation Loss: 2.593050\n",
      "Epoch: 30416 \tTraining Loss: 1.288905 \tValidation Loss: 2.593803\n",
      "Epoch: 30417 \tTraining Loss: 1.307420 \tValidation Loss: 2.592960\n",
      "Epoch: 30418 \tTraining Loss: 1.286036 \tValidation Loss: 2.593604\n",
      "Epoch: 30419 \tTraining Loss: 1.297329 \tValidation Loss: 2.594589\n",
      "Epoch: 30420 \tTraining Loss: 1.311629 \tValidation Loss: 2.595228\n",
      "Epoch: 30421 \tTraining Loss: 1.329553 \tValidation Loss: 2.592366\n",
      "Epoch: 30422 \tTraining Loss: 1.318615 \tValidation Loss: 2.593419\n",
      "Epoch: 30423 \tTraining Loss: 1.317181 \tValidation Loss: 2.593428\n",
      "Epoch: 30424 \tTraining Loss: 1.305438 \tValidation Loss: 2.594099\n",
      "Epoch: 30425 \tTraining Loss: 1.320890 \tValidation Loss: 2.593954\n",
      "Epoch: 30426 \tTraining Loss: 1.302549 \tValidation Loss: 2.594752\n",
      "Epoch: 30427 \tTraining Loss: 1.307120 \tValidation Loss: 2.595406\n",
      "Epoch: 30428 \tTraining Loss: 1.316286 \tValidation Loss: 2.594382\n",
      "Epoch: 30429 \tTraining Loss: 1.328429 \tValidation Loss: 2.594130\n",
      "Epoch: 30430 \tTraining Loss: 1.327555 \tValidation Loss: 2.595381\n",
      "Epoch: 30431 \tTraining Loss: 1.302214 \tValidation Loss: 2.594977\n",
      "Epoch: 30432 \tTraining Loss: 1.260739 \tValidation Loss: 2.596307\n",
      "Epoch: 30433 \tTraining Loss: 1.249216 \tValidation Loss: 2.595068\n",
      "Epoch: 30434 \tTraining Loss: 1.321602 \tValidation Loss: 2.593546\n",
      "Epoch: 30435 \tTraining Loss: 1.317893 \tValidation Loss: 2.594556\n",
      "Epoch: 30436 \tTraining Loss: 1.303255 \tValidation Loss: 2.594024\n",
      "Epoch: 30437 \tTraining Loss: 1.268025 \tValidation Loss: 2.595266\n",
      "Epoch: 30438 \tTraining Loss: 1.258943 \tValidation Loss: 2.594531\n",
      "Epoch: 30439 \tTraining Loss: 1.313025 \tValidation Loss: 2.594733\n",
      "Epoch: 30440 \tTraining Loss: 1.281172 \tValidation Loss: 2.594576\n",
      "Epoch: 30441 \tTraining Loss: 1.325351 \tValidation Loss: 2.595021\n",
      "Epoch: 30442 \tTraining Loss: 1.284942 \tValidation Loss: 2.595731\n",
      "Epoch: 30443 \tTraining Loss: 1.326700 \tValidation Loss: 2.594124\n",
      "Epoch: 30444 \tTraining Loss: 1.301984 \tValidation Loss: 2.595597\n",
      "Epoch: 30445 \tTraining Loss: 1.294222 \tValidation Loss: 2.594689\n",
      "Epoch: 30446 \tTraining Loss: 1.303727 \tValidation Loss: 2.594151\n",
      "Epoch: 30447 \tTraining Loss: 1.342517 \tValidation Loss: 2.595095\n",
      "Epoch: 30448 \tTraining Loss: 1.291565 \tValidation Loss: 2.595139\n",
      "Epoch: 30449 \tTraining Loss: 1.286250 \tValidation Loss: 2.594185\n",
      "Epoch: 30450 \tTraining Loss: 1.275824 \tValidation Loss: 2.593825\n",
      "Epoch: 30451 \tTraining Loss: 1.301785 \tValidation Loss: 2.595722\n",
      "Epoch: 30452 \tTraining Loss: 1.306662 \tValidation Loss: 2.593123\n",
      "Epoch: 30453 \tTraining Loss: 1.385522 \tValidation Loss: 2.593397\n",
      "Epoch: 30454 \tTraining Loss: 1.320855 \tValidation Loss: 2.593622\n",
      "Epoch: 30455 \tTraining Loss: 1.315083 \tValidation Loss: 2.595317\n",
      "Epoch: 30456 \tTraining Loss: 1.333994 \tValidation Loss: 2.594810\n",
      "Epoch: 30457 \tTraining Loss: 1.296834 \tValidation Loss: 2.595446\n",
      "Epoch: 30458 \tTraining Loss: 1.350557 \tValidation Loss: 2.595212\n",
      "Epoch: 30459 \tTraining Loss: 1.293964 \tValidation Loss: 2.593450\n",
      "Epoch: 30460 \tTraining Loss: 1.296392 \tValidation Loss: 2.595466\n",
      "Epoch: 30461 \tTraining Loss: 1.290025 \tValidation Loss: 2.595010\n",
      "Epoch: 30462 \tTraining Loss: 1.359715 \tValidation Loss: 2.595434\n",
      "Epoch: 30463 \tTraining Loss: 1.275400 \tValidation Loss: 2.594660\n",
      "Epoch: 30464 \tTraining Loss: 1.338806 \tValidation Loss: 2.592988\n",
      "Epoch: 30465 \tTraining Loss: 1.301597 \tValidation Loss: 2.594471\n",
      "Epoch: 30466 \tTraining Loss: 1.286518 \tValidation Loss: 2.594053\n",
      "Epoch: 30467 \tTraining Loss: 1.342988 \tValidation Loss: 2.594790\n",
      "Epoch: 30468 \tTraining Loss: 1.318935 \tValidation Loss: 2.594113\n",
      "Epoch: 30469 \tTraining Loss: 1.338219 \tValidation Loss: 2.593792\n",
      "Epoch: 30470 \tTraining Loss: 1.314842 \tValidation Loss: 2.594952\n",
      "Epoch: 30471 \tTraining Loss: 1.277454 \tValidation Loss: 2.594463\n",
      "Epoch: 30472 \tTraining Loss: 1.341298 \tValidation Loss: 2.594696\n",
      "Epoch: 30473 \tTraining Loss: 1.333184 \tValidation Loss: 2.595275\n",
      "Epoch: 30474 \tTraining Loss: 1.264749 \tValidation Loss: 2.596028\n",
      "Epoch: 30475 \tTraining Loss: 1.293352 \tValidation Loss: 2.595398\n",
      "Epoch: 30476 \tTraining Loss: 1.276611 \tValidation Loss: 2.596775\n",
      "Epoch: 30477 \tTraining Loss: 1.330297 \tValidation Loss: 2.595217\n",
      "Epoch: 30478 \tTraining Loss: 1.323452 \tValidation Loss: 2.595116\n",
      "Epoch: 30479 \tTraining Loss: 1.305682 \tValidation Loss: 2.596718\n",
      "Epoch: 30480 \tTraining Loss: 1.323322 \tValidation Loss: 2.596168\n",
      "Epoch: 30481 \tTraining Loss: 1.264343 \tValidation Loss: 2.596911\n",
      "Epoch: 30482 \tTraining Loss: 1.308101 \tValidation Loss: 2.596705\n",
      "Epoch: 30483 \tTraining Loss: 1.322291 \tValidation Loss: 2.596849\n",
      "Epoch: 30484 \tTraining Loss: 1.311475 \tValidation Loss: 2.596025\n",
      "Epoch: 30485 \tTraining Loss: 1.277207 \tValidation Loss: 2.596598\n",
      "Epoch: 30486 \tTraining Loss: 1.312860 \tValidation Loss: 2.595913\n",
      "Epoch: 30487 \tTraining Loss: 1.328494 \tValidation Loss: 2.595934\n",
      "Epoch: 30488 \tTraining Loss: 1.277777 \tValidation Loss: 2.596060\n",
      "Epoch: 30489 \tTraining Loss: 1.368912 \tValidation Loss: 2.594825\n",
      "Epoch: 30490 \tTraining Loss: 1.288352 \tValidation Loss: 2.596515\n",
      "Epoch: 30491 \tTraining Loss: 1.263370 \tValidation Loss: 2.597604\n",
      "Epoch: 30492 \tTraining Loss: 1.325931 \tValidation Loss: 2.595148\n",
      "Epoch: 30493 \tTraining Loss: 1.322673 \tValidation Loss: 2.596304\n",
      "Epoch: 30494 \tTraining Loss: 1.237539 \tValidation Loss: 2.597492\n",
      "Epoch: 30495 \tTraining Loss: 1.315266 \tValidation Loss: 2.596972\n",
      "Epoch: 30496 \tTraining Loss: 1.311246 \tValidation Loss: 2.596267\n",
      "Epoch: 30497 \tTraining Loss: 1.353397 \tValidation Loss: 2.595548\n",
      "Epoch: 30498 \tTraining Loss: 1.303088 \tValidation Loss: 2.595962\n",
      "Epoch: 30499 \tTraining Loss: 1.325563 \tValidation Loss: 2.595562\n",
      "Epoch: 30500 \tTraining Loss: 1.294309 \tValidation Loss: 2.595594\n",
      "Epoch: 30501 \tTraining Loss: 1.308646 \tValidation Loss: 2.596527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30502 \tTraining Loss: 1.333737 \tValidation Loss: 2.594908\n",
      "Epoch: 30503 \tTraining Loss: 1.288535 \tValidation Loss: 2.595745\n",
      "Epoch: 30504 \tTraining Loss: 1.290344 \tValidation Loss: 2.596578\n",
      "Epoch: 30505 \tTraining Loss: 1.335884 \tValidation Loss: 2.596664\n",
      "Epoch: 30506 \tTraining Loss: 1.278688 \tValidation Loss: 2.596378\n",
      "Epoch: 30507 \tTraining Loss: 1.306346 \tValidation Loss: 2.595474\n",
      "Epoch: 30508 \tTraining Loss: 1.304491 \tValidation Loss: 2.594799\n",
      "Epoch: 30509 \tTraining Loss: 1.320564 \tValidation Loss: 2.597016\n",
      "Epoch: 30510 \tTraining Loss: 1.295534 \tValidation Loss: 2.595863\n",
      "Epoch: 30511 \tTraining Loss: 1.322231 \tValidation Loss: 2.594730\n",
      "Epoch: 30512 \tTraining Loss: 1.296260 \tValidation Loss: 2.594735\n",
      "Epoch: 30513 \tTraining Loss: 1.352869 \tValidation Loss: 2.594452\n",
      "Epoch: 30514 \tTraining Loss: 1.364159 \tValidation Loss: 2.595250\n",
      "Epoch: 30515 \tTraining Loss: 1.300489 \tValidation Loss: 2.597487\n",
      "Epoch: 30516 \tTraining Loss: 1.289056 \tValidation Loss: 2.597802\n",
      "Epoch: 30517 \tTraining Loss: 1.288350 \tValidation Loss: 2.598170\n",
      "Epoch: 30518 \tTraining Loss: 1.276219 \tValidation Loss: 2.596832\n",
      "Epoch: 30519 \tTraining Loss: 1.291932 \tValidation Loss: 2.596057\n",
      "Epoch: 30520 \tTraining Loss: 1.333048 \tValidation Loss: 2.596591\n",
      "Epoch: 30521 \tTraining Loss: 1.283809 \tValidation Loss: 2.596531\n",
      "Epoch: 30522 \tTraining Loss: 1.315402 \tValidation Loss: 2.596277\n",
      "Epoch: 30523 \tTraining Loss: 1.327790 \tValidation Loss: 2.596410\n",
      "Epoch: 30524 \tTraining Loss: 1.305971 \tValidation Loss: 2.596486\n",
      "Epoch: 30525 \tTraining Loss: 1.316617 \tValidation Loss: 2.596237\n",
      "Epoch: 30526 \tTraining Loss: 1.311721 \tValidation Loss: 2.597242\n",
      "Epoch: 30527 \tTraining Loss: 1.279784 \tValidation Loss: 2.596863\n",
      "Epoch: 30528 \tTraining Loss: 1.287720 \tValidation Loss: 2.598020\n",
      "Epoch: 30529 \tTraining Loss: 1.281869 \tValidation Loss: 2.596236\n",
      "Epoch: 30530 \tTraining Loss: 1.319242 \tValidation Loss: 2.595464\n",
      "Epoch: 30531 \tTraining Loss: 1.257114 \tValidation Loss: 2.597250\n",
      "Epoch: 30532 \tTraining Loss: 1.321966 \tValidation Loss: 2.596570\n",
      "Epoch: 30533 \tTraining Loss: 1.278443 \tValidation Loss: 2.598243\n",
      "Epoch: 30534 \tTraining Loss: 1.270807 \tValidation Loss: 2.597049\n",
      "Epoch: 30535 \tTraining Loss: 1.269019 \tValidation Loss: 2.595403\n",
      "Epoch: 30536 \tTraining Loss: 1.301165 \tValidation Loss: 2.597584\n",
      "Epoch: 30537 \tTraining Loss: 1.295531 \tValidation Loss: 2.597229\n",
      "Epoch: 30538 \tTraining Loss: 1.298133 \tValidation Loss: 2.596718\n",
      "Epoch: 30539 \tTraining Loss: 1.250502 \tValidation Loss: 2.598190\n",
      "Epoch: 30540 \tTraining Loss: 1.340824 \tValidation Loss: 2.597100\n",
      "Epoch: 30541 \tTraining Loss: 1.307946 \tValidation Loss: 2.597252\n",
      "Epoch: 30542 \tTraining Loss: 1.297679 \tValidation Loss: 2.595233\n",
      "Epoch: 30543 \tTraining Loss: 1.262636 \tValidation Loss: 2.596993\n",
      "Epoch: 30544 \tTraining Loss: 1.339110 \tValidation Loss: 2.596829\n",
      "Epoch: 30545 \tTraining Loss: 1.290536 \tValidation Loss: 2.596985\n",
      "Epoch: 30546 \tTraining Loss: 1.298033 \tValidation Loss: 2.596506\n",
      "Epoch: 30547 \tTraining Loss: 1.290967 \tValidation Loss: 2.597813\n",
      "Epoch: 30548 \tTraining Loss: 1.316014 \tValidation Loss: 2.597527\n",
      "Epoch: 30549 \tTraining Loss: 1.305099 \tValidation Loss: 2.596658\n",
      "Epoch: 30550 \tTraining Loss: 1.303083 \tValidation Loss: 2.597416\n",
      "Epoch: 30551 \tTraining Loss: 1.286193 \tValidation Loss: 2.596347\n",
      "Epoch: 30552 \tTraining Loss: 1.336825 \tValidation Loss: 2.596563\n",
      "Epoch: 30553 \tTraining Loss: 1.257664 \tValidation Loss: 2.598749\n",
      "Epoch: 30554 \tTraining Loss: 1.299746 \tValidation Loss: 2.597482\n",
      "Epoch: 30555 \tTraining Loss: 1.280311 \tValidation Loss: 2.597209\n",
      "Epoch: 30556 \tTraining Loss: 1.289532 \tValidation Loss: 2.596699\n",
      "Epoch: 30557 \tTraining Loss: 1.324411 \tValidation Loss: 2.594961\n",
      "Epoch: 30558 \tTraining Loss: 1.281614 \tValidation Loss: 2.595374\n",
      "Epoch: 30559 \tTraining Loss: 1.305818 \tValidation Loss: 2.596915\n",
      "Epoch: 30560 \tTraining Loss: 1.282151 \tValidation Loss: 2.597315\n",
      "Epoch: 30561 \tTraining Loss: 1.318889 \tValidation Loss: 2.597069\n",
      "Epoch: 30562 \tTraining Loss: 1.266321 \tValidation Loss: 2.598039\n",
      "Epoch: 30563 \tTraining Loss: 1.292665 \tValidation Loss: 2.598156\n",
      "Epoch: 30564 \tTraining Loss: 1.296274 \tValidation Loss: 2.597345\n",
      "Epoch: 30565 \tTraining Loss: 1.314150 \tValidation Loss: 2.597525\n",
      "Epoch: 30566 \tTraining Loss: 1.325160 \tValidation Loss: 2.598921\n",
      "Epoch: 30567 \tTraining Loss: 1.314694 \tValidation Loss: 2.596323\n",
      "Epoch: 30568 \tTraining Loss: 1.314010 \tValidation Loss: 2.597024\n",
      "Epoch: 30569 \tTraining Loss: 1.319334 \tValidation Loss: 2.598006\n",
      "Epoch: 30570 \tTraining Loss: 1.301239 \tValidation Loss: 2.598062\n",
      "Epoch: 30571 \tTraining Loss: 1.265706 \tValidation Loss: 2.597575\n",
      "Epoch: 30572 \tTraining Loss: 1.284052 \tValidation Loss: 2.598406\n",
      "Epoch: 30573 \tTraining Loss: 1.333379 \tValidation Loss: 2.596526\n",
      "Epoch: 30574 \tTraining Loss: 1.289710 \tValidation Loss: 2.597594\n",
      "Epoch: 30575 \tTraining Loss: 1.308624 \tValidation Loss: 2.596265\n",
      "Epoch: 30576 \tTraining Loss: 1.310108 \tValidation Loss: 2.596759\n",
      "Epoch: 30577 \tTraining Loss: 1.321060 \tValidation Loss: 2.596813\n",
      "Epoch: 30578 \tTraining Loss: 1.284076 \tValidation Loss: 2.596209\n",
      "Epoch: 30579 \tTraining Loss: 1.328412 \tValidation Loss: 2.595370\n",
      "Epoch: 30580 \tTraining Loss: 1.280173 \tValidation Loss: 2.597690\n",
      "Epoch: 30581 \tTraining Loss: 1.308725 \tValidation Loss: 2.597168\n",
      "Epoch: 30582 \tTraining Loss: 1.319055 \tValidation Loss: 2.597690\n",
      "Epoch: 30583 \tTraining Loss: 1.302312 \tValidation Loss: 2.596627\n",
      "Epoch: 30584 \tTraining Loss: 1.273665 \tValidation Loss: 2.597665\n",
      "Epoch: 30585 \tTraining Loss: 1.291110 \tValidation Loss: 2.598500\n",
      "Epoch: 30586 \tTraining Loss: 1.328761 \tValidation Loss: 2.596831\n",
      "Epoch: 30587 \tTraining Loss: 1.335853 \tValidation Loss: 2.596858\n",
      "Epoch: 30588 \tTraining Loss: 1.328618 \tValidation Loss: 2.596668\n",
      "Epoch: 30589 \tTraining Loss: 1.292130 \tValidation Loss: 2.598299\n",
      "Epoch: 30590 \tTraining Loss: 1.344314 \tValidation Loss: 2.597540\n",
      "Epoch: 30591 \tTraining Loss: 1.343030 \tValidation Loss: 2.597035\n",
      "Epoch: 30592 \tTraining Loss: 1.324682 \tValidation Loss: 2.597856\n",
      "Epoch: 30593 \tTraining Loss: 1.328960 \tValidation Loss: 2.596942\n",
      "Epoch: 30594 \tTraining Loss: 1.287159 \tValidation Loss: 2.597941\n",
      "Epoch: 30595 \tTraining Loss: 1.257527 \tValidation Loss: 2.598307\n",
      "Epoch: 30596 \tTraining Loss: 1.305842 \tValidation Loss: 2.597151\n",
      "Epoch: 30597 \tTraining Loss: 1.315617 \tValidation Loss: 2.598615\n",
      "Epoch: 30598 \tTraining Loss: 1.318008 \tValidation Loss: 2.598164\n",
      "Epoch: 30599 \tTraining Loss: 1.284275 \tValidation Loss: 2.598319\n",
      "Epoch: 30600 \tTraining Loss: 1.303431 \tValidation Loss: 2.597359\n",
      "Epoch: 30601 \tTraining Loss: 1.304329 \tValidation Loss: 2.597494\n",
      "Epoch: 30602 \tTraining Loss: 1.317710 \tValidation Loss: 2.597427\n",
      "Epoch: 30603 \tTraining Loss: 1.342841 \tValidation Loss: 2.597239\n",
      "Epoch: 30604 \tTraining Loss: 1.259936 \tValidation Loss: 2.598610\n",
      "Epoch: 30605 \tTraining Loss: 1.302918 \tValidation Loss: 2.597458\n",
      "Epoch: 30606 \tTraining Loss: 1.298556 \tValidation Loss: 2.599765\n",
      "Epoch: 30607 \tTraining Loss: 1.289365 \tValidation Loss: 2.598815\n",
      "Epoch: 30608 \tTraining Loss: 1.283485 \tValidation Loss: 2.598741\n",
      "Epoch: 30609 \tTraining Loss: 1.259905 \tValidation Loss: 2.599392\n",
      "Epoch: 30610 \tTraining Loss: 1.295656 \tValidation Loss: 2.599683\n",
      "Epoch: 30611 \tTraining Loss: 1.275184 \tValidation Loss: 2.599987\n",
      "Epoch: 30612 \tTraining Loss: 1.262318 \tValidation Loss: 2.598996\n",
      "Epoch: 30613 \tTraining Loss: 1.338573 \tValidation Loss: 2.598338\n",
      "Epoch: 30614 \tTraining Loss: 1.322810 \tValidation Loss: 2.597904\n",
      "Epoch: 30615 \tTraining Loss: 1.326060 \tValidation Loss: 2.597791\n",
      "Epoch: 30616 \tTraining Loss: 1.296224 \tValidation Loss: 2.597879\n",
      "Epoch: 30617 \tTraining Loss: 1.259338 \tValidation Loss: 2.598848\n",
      "Epoch: 30618 \tTraining Loss: 1.292047 \tValidation Loss: 2.598717\n",
      "Epoch: 30619 \tTraining Loss: 1.277271 \tValidation Loss: 2.598130\n",
      "Epoch: 30620 \tTraining Loss: 1.286503 \tValidation Loss: 2.599181\n",
      "Epoch: 30621 \tTraining Loss: 1.336570 \tValidation Loss: 2.597845\n",
      "Epoch: 30622 \tTraining Loss: 1.307472 \tValidation Loss: 2.597905\n",
      "Epoch: 30623 \tTraining Loss: 1.304556 \tValidation Loss: 2.599370\n",
      "Epoch: 30624 \tTraining Loss: 1.319691 \tValidation Loss: 2.598385\n",
      "Epoch: 30625 \tTraining Loss: 1.295023 \tValidation Loss: 2.599307\n",
      "Epoch: 30626 \tTraining Loss: 1.273684 \tValidation Loss: 2.599864\n",
      "Epoch: 30627 \tTraining Loss: 1.302984 \tValidation Loss: 2.599799\n",
      "Epoch: 30628 \tTraining Loss: 1.270091 \tValidation Loss: 2.598148\n",
      "Epoch: 30629 \tTraining Loss: 1.287889 \tValidation Loss: 2.598170\n",
      "Epoch: 30630 \tTraining Loss: 1.256327 \tValidation Loss: 2.599557\n",
      "Epoch: 30631 \tTraining Loss: 1.302779 \tValidation Loss: 2.598668\n",
      "Epoch: 30632 \tTraining Loss: 1.333267 \tValidation Loss: 2.597289\n",
      "Epoch: 30633 \tTraining Loss: 1.314487 \tValidation Loss: 2.598355\n",
      "Epoch: 30634 \tTraining Loss: 1.336754 \tValidation Loss: 2.597642\n",
      "Epoch: 30635 \tTraining Loss: 1.290082 \tValidation Loss: 2.598288\n",
      "Epoch: 30636 \tTraining Loss: 1.266689 \tValidation Loss: 2.599056\n",
      "Epoch: 30637 \tTraining Loss: 1.294417 \tValidation Loss: 2.600325\n",
      "Epoch: 30638 \tTraining Loss: 1.282101 \tValidation Loss: 2.599970\n",
      "Epoch: 30639 \tTraining Loss: 1.287830 \tValidation Loss: 2.599856\n",
      "Epoch: 30640 \tTraining Loss: 1.253926 \tValidation Loss: 2.598949\n",
      "Epoch: 30641 \tTraining Loss: 1.321622 \tValidation Loss: 2.599515\n",
      "Epoch: 30642 \tTraining Loss: 1.347739 \tValidation Loss: 2.598106\n",
      "Epoch: 30643 \tTraining Loss: 1.326689 \tValidation Loss: 2.599216\n",
      "Epoch: 30644 \tTraining Loss: 1.272929 \tValidation Loss: 2.599233\n",
      "Epoch: 30645 \tTraining Loss: 1.340061 \tValidation Loss: 2.599928\n",
      "Epoch: 30646 \tTraining Loss: 1.300071 \tValidation Loss: 2.600298\n",
      "Epoch: 30647 \tTraining Loss: 1.321666 \tValidation Loss: 2.599151\n",
      "Epoch: 30648 \tTraining Loss: 1.278132 \tValidation Loss: 2.599955\n",
      "Epoch: 30649 \tTraining Loss: 1.284473 \tValidation Loss: 2.599642\n",
      "Epoch: 30650 \tTraining Loss: 1.288614 \tValidation Loss: 2.600083\n",
      "Epoch: 30651 \tTraining Loss: 1.358220 \tValidation Loss: 2.597899\n",
      "Epoch: 30652 \tTraining Loss: 1.293966 \tValidation Loss: 2.600582\n",
      "Epoch: 30653 \tTraining Loss: 1.312348 \tValidation Loss: 2.598535\n",
      "Epoch: 30654 \tTraining Loss: 1.313360 \tValidation Loss: 2.598512\n",
      "Epoch: 30655 \tTraining Loss: 1.298630 \tValidation Loss: 2.598946\n",
      "Epoch: 30656 \tTraining Loss: 1.280517 \tValidation Loss: 2.599685\n",
      "Epoch: 30657 \tTraining Loss: 1.301172 \tValidation Loss: 2.599592\n",
      "Epoch: 30658 \tTraining Loss: 1.294538 \tValidation Loss: 2.599828\n",
      "Epoch: 30659 \tTraining Loss: 1.325868 \tValidation Loss: 2.600050\n",
      "Epoch: 30660 \tTraining Loss: 1.293027 \tValidation Loss: 2.599603\n",
      "Epoch: 30661 \tTraining Loss: 1.326483 \tValidation Loss: 2.600154\n",
      "Epoch: 30662 \tTraining Loss: 1.292596 \tValidation Loss: 2.599971\n",
      "Epoch: 30663 \tTraining Loss: 1.336611 \tValidation Loss: 2.598992\n",
      "Epoch: 30664 \tTraining Loss: 1.308919 \tValidation Loss: 2.599733\n",
      "Epoch: 30665 \tTraining Loss: 1.257558 \tValidation Loss: 2.600065\n",
      "Epoch: 30666 \tTraining Loss: 1.335558 \tValidation Loss: 2.599104\n",
      "Epoch: 30667 \tTraining Loss: 1.290373 \tValidation Loss: 2.599609\n",
      "Epoch: 30668 \tTraining Loss: 1.327652 \tValidation Loss: 2.600338\n",
      "Epoch: 30669 \tTraining Loss: 1.313296 \tValidation Loss: 2.598721\n",
      "Epoch: 30670 \tTraining Loss: 1.307531 \tValidation Loss: 2.600137\n",
      "Epoch: 30671 \tTraining Loss: 1.313860 \tValidation Loss: 2.599815\n",
      "Epoch: 30672 \tTraining Loss: 1.286750 \tValidation Loss: 2.600878\n",
      "Epoch: 30673 \tTraining Loss: 1.325213 \tValidation Loss: 2.599004\n",
      "Epoch: 30674 \tTraining Loss: 1.299786 \tValidation Loss: 2.600111\n",
      "Epoch: 30675 \tTraining Loss: 1.318555 \tValidation Loss: 2.600835\n",
      "Epoch: 30676 \tTraining Loss: 1.313681 \tValidation Loss: 2.599681\n",
      "Epoch: 30677 \tTraining Loss: 1.273174 \tValidation Loss: 2.600311\n",
      "Epoch: 30678 \tTraining Loss: 1.317995 \tValidation Loss: 2.599443\n",
      "Epoch: 30679 \tTraining Loss: 1.324792 \tValidation Loss: 2.598968\n",
      "Epoch: 30680 \tTraining Loss: 1.274776 \tValidation Loss: 2.599597\n",
      "Epoch: 30681 \tTraining Loss: 1.296156 \tValidation Loss: 2.600868\n",
      "Epoch: 30682 \tTraining Loss: 1.338161 \tValidation Loss: 2.601272\n",
      "Epoch: 30683 \tTraining Loss: 1.317392 \tValidation Loss: 2.600812\n",
      "Epoch: 30684 \tTraining Loss: 1.313034 \tValidation Loss: 2.600023\n",
      "Epoch: 30685 \tTraining Loss: 1.307401 \tValidation Loss: 2.599622\n",
      "Epoch: 30686 \tTraining Loss: 1.330755 \tValidation Loss: 2.600038\n",
      "Epoch: 30687 \tTraining Loss: 1.305250 \tValidation Loss: 2.601482\n",
      "Epoch: 30688 \tTraining Loss: 1.338611 \tValidation Loss: 2.600841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30689 \tTraining Loss: 1.303929 \tValidation Loss: 2.600040\n",
      "Epoch: 30690 \tTraining Loss: 1.306353 \tValidation Loss: 2.598736\n",
      "Epoch: 30691 \tTraining Loss: 1.252965 \tValidation Loss: 2.601387\n",
      "Epoch: 30692 \tTraining Loss: 1.316443 \tValidation Loss: 2.601303\n",
      "Epoch: 30693 \tTraining Loss: 1.293662 \tValidation Loss: 2.600918\n",
      "Epoch: 30694 \tTraining Loss: 1.294747 \tValidation Loss: 2.600230\n",
      "Epoch: 30695 \tTraining Loss: 1.352785 \tValidation Loss: 2.599749\n",
      "Epoch: 30696 \tTraining Loss: 1.268955 \tValidation Loss: 2.600677\n",
      "Epoch: 30697 \tTraining Loss: 1.304179 \tValidation Loss: 2.599934\n",
      "Epoch: 30698 \tTraining Loss: 1.298593 \tValidation Loss: 2.599849\n",
      "Epoch: 30699 \tTraining Loss: 1.312168 \tValidation Loss: 2.600071\n",
      "Epoch: 30700 \tTraining Loss: 1.286902 \tValidation Loss: 2.599477\n",
      "Epoch: 30701 \tTraining Loss: 1.283040 \tValidation Loss: 2.600002\n",
      "Epoch: 30702 \tTraining Loss: 1.308227 \tValidation Loss: 2.600594\n",
      "Epoch: 30703 \tTraining Loss: 1.305476 \tValidation Loss: 2.600299\n",
      "Epoch: 30704 \tTraining Loss: 1.305032 \tValidation Loss: 2.600019\n",
      "Epoch: 30705 \tTraining Loss: 1.310930 \tValidation Loss: 2.600036\n",
      "Epoch: 30706 \tTraining Loss: 1.268167 \tValidation Loss: 2.601185\n",
      "Epoch: 30707 \tTraining Loss: 1.317565 \tValidation Loss: 2.600596\n",
      "Epoch: 30708 \tTraining Loss: 1.341282 \tValidation Loss: 2.600577\n",
      "Epoch: 30709 \tTraining Loss: 1.284849 \tValidation Loss: 2.600557\n",
      "Epoch: 30710 \tTraining Loss: 1.329358 \tValidation Loss: 2.601211\n",
      "Epoch: 30711 \tTraining Loss: 1.308866 \tValidation Loss: 2.601687\n",
      "Epoch: 30712 \tTraining Loss: 1.371708 \tValidation Loss: 2.599327\n",
      "Epoch: 30713 \tTraining Loss: 1.328965 \tValidation Loss: 2.599251\n",
      "Epoch: 30714 \tTraining Loss: 1.314903 \tValidation Loss: 2.600137\n",
      "Epoch: 30715 \tTraining Loss: 1.274943 \tValidation Loss: 2.602028\n",
      "Epoch: 30716 \tTraining Loss: 1.299340 \tValidation Loss: 2.600823\n",
      "Epoch: 30717 \tTraining Loss: 1.236556 \tValidation Loss: 2.601600\n",
      "Epoch: 30718 \tTraining Loss: 1.248743 \tValidation Loss: 2.600820\n",
      "Epoch: 30719 \tTraining Loss: 1.294856 \tValidation Loss: 2.601392\n",
      "Epoch: 30720 \tTraining Loss: 1.316418 \tValidation Loss: 2.601867\n",
      "Epoch: 30721 \tTraining Loss: 1.294088 \tValidation Loss: 2.600767\n",
      "Epoch: 30722 \tTraining Loss: 1.304980 \tValidation Loss: 2.600836\n",
      "Epoch: 30723 \tTraining Loss: 1.275322 \tValidation Loss: 2.601845\n",
      "Epoch: 30724 \tTraining Loss: 1.296195 \tValidation Loss: 2.599022\n",
      "Epoch: 30725 \tTraining Loss: 1.296119 \tValidation Loss: 2.601335\n",
      "Epoch: 30726 \tTraining Loss: 1.270400 \tValidation Loss: 2.602462\n",
      "Epoch: 30727 \tTraining Loss: 1.328746 \tValidation Loss: 2.601055\n",
      "Epoch: 30728 \tTraining Loss: 1.319949 \tValidation Loss: 2.600463\n",
      "Epoch: 30729 \tTraining Loss: 1.273170 \tValidation Loss: 2.600450\n",
      "Epoch: 30730 \tTraining Loss: 1.310806 \tValidation Loss: 2.601705\n",
      "Epoch: 30731 \tTraining Loss: 1.291304 \tValidation Loss: 2.601302\n",
      "Epoch: 30732 \tTraining Loss: 1.300488 \tValidation Loss: 2.601649\n",
      "Epoch: 30733 \tTraining Loss: 1.321155 \tValidation Loss: 2.602224\n",
      "Epoch: 30734 \tTraining Loss: 1.298834 \tValidation Loss: 2.601861\n",
      "Epoch: 30735 \tTraining Loss: 1.312300 \tValidation Loss: 2.601306\n",
      "Epoch: 30736 \tTraining Loss: 1.293207 \tValidation Loss: 2.601078\n",
      "Epoch: 30737 \tTraining Loss: 1.282294 \tValidation Loss: 2.600621\n",
      "Epoch: 30738 \tTraining Loss: 1.285990 \tValidation Loss: 2.601433\n",
      "Epoch: 30739 \tTraining Loss: 1.312872 \tValidation Loss: 2.601441\n",
      "Epoch: 30740 \tTraining Loss: 1.310645 \tValidation Loss: 2.601882\n",
      "Epoch: 30741 \tTraining Loss: 1.320627 \tValidation Loss: 2.600587\n",
      "Epoch: 30742 \tTraining Loss: 1.287007 \tValidation Loss: 2.601358\n",
      "Epoch: 30743 \tTraining Loss: 1.297765 \tValidation Loss: 2.601102\n",
      "Epoch: 30744 \tTraining Loss: 1.314448 \tValidation Loss: 2.600857\n",
      "Epoch: 30745 \tTraining Loss: 1.288576 \tValidation Loss: 2.601517\n",
      "Epoch: 30746 \tTraining Loss: 1.294314 \tValidation Loss: 2.602221\n",
      "Epoch: 30747 \tTraining Loss: 1.303971 \tValidation Loss: 2.600914\n",
      "Epoch: 30748 \tTraining Loss: 1.322693 \tValidation Loss: 2.601849\n",
      "Epoch: 30749 \tTraining Loss: 1.323543 \tValidation Loss: 2.599475\n",
      "Epoch: 30750 \tTraining Loss: 1.371332 \tValidation Loss: 2.599836\n",
      "Epoch: 30751 \tTraining Loss: 1.323820 \tValidation Loss: 2.600309\n",
      "Epoch: 30752 \tTraining Loss: 1.295391 \tValidation Loss: 2.600349\n",
      "Epoch: 30753 \tTraining Loss: 1.292094 \tValidation Loss: 2.601055\n",
      "Epoch: 30754 \tTraining Loss: 1.300212 \tValidation Loss: 2.600409\n",
      "Epoch: 30755 \tTraining Loss: 1.291914 \tValidation Loss: 2.602328\n",
      "Epoch: 30756 \tTraining Loss: 1.263559 \tValidation Loss: 2.602008\n",
      "Epoch: 30757 \tTraining Loss: 1.312120 \tValidation Loss: 2.601431\n",
      "Epoch: 30758 \tTraining Loss: 1.297134 \tValidation Loss: 2.602953\n",
      "Epoch: 30759 \tTraining Loss: 1.292042 \tValidation Loss: 2.600603\n",
      "Epoch: 30760 \tTraining Loss: 1.309729 \tValidation Loss: 2.600455\n",
      "Epoch: 30761 \tTraining Loss: 1.297765 \tValidation Loss: 2.601563\n",
      "Epoch: 30762 \tTraining Loss: 1.248379 \tValidation Loss: 2.602698\n",
      "Epoch: 30763 \tTraining Loss: 1.329021 \tValidation Loss: 2.600824\n",
      "Epoch: 30764 \tTraining Loss: 1.330276 \tValidation Loss: 2.600525\n",
      "Epoch: 30765 \tTraining Loss: 1.297248 \tValidation Loss: 2.601790\n",
      "Epoch: 30766 \tTraining Loss: 1.344511 \tValidation Loss: 2.600899\n",
      "Epoch: 30767 \tTraining Loss: 1.301783 \tValidation Loss: 2.601594\n",
      "Epoch: 30768 \tTraining Loss: 1.335167 \tValidation Loss: 2.601294\n",
      "Epoch: 30769 \tTraining Loss: 1.305715 \tValidation Loss: 2.602060\n",
      "Epoch: 30770 \tTraining Loss: 1.284444 \tValidation Loss: 2.602609\n",
      "Epoch: 30771 \tTraining Loss: 1.334779 \tValidation Loss: 2.600549\n",
      "Epoch: 30772 \tTraining Loss: 1.290061 \tValidation Loss: 2.601021\n",
      "Epoch: 30773 \tTraining Loss: 1.355814 \tValidation Loss: 2.601504\n",
      "Epoch: 30774 \tTraining Loss: 1.343307 \tValidation Loss: 2.601140\n",
      "Epoch: 30775 \tTraining Loss: 1.288591 \tValidation Loss: 2.601296\n",
      "Epoch: 30776 \tTraining Loss: 1.305521 \tValidation Loss: 2.601867\n",
      "Epoch: 30777 \tTraining Loss: 1.299700 \tValidation Loss: 2.602416\n",
      "Epoch: 30778 \tTraining Loss: 1.262154 \tValidation Loss: 2.601878\n",
      "Epoch: 30779 \tTraining Loss: 1.286300 \tValidation Loss: 2.601727\n",
      "Epoch: 30780 \tTraining Loss: 1.303344 \tValidation Loss: 2.601142\n",
      "Epoch: 30781 \tTraining Loss: 1.304330 \tValidation Loss: 2.600246\n",
      "Epoch: 30782 \tTraining Loss: 1.306322 \tValidation Loss: 2.601269\n",
      "Epoch: 30783 \tTraining Loss: 1.288611 \tValidation Loss: 2.602185\n",
      "Epoch: 30784 \tTraining Loss: 1.254404 \tValidation Loss: 2.602529\n",
      "Epoch: 30785 \tTraining Loss: 1.265262 \tValidation Loss: 2.602560\n",
      "Epoch: 30786 \tTraining Loss: 1.307347 \tValidation Loss: 2.603085\n",
      "Epoch: 30787 \tTraining Loss: 1.287576 \tValidation Loss: 2.602263\n",
      "Epoch: 30788 \tTraining Loss: 1.289727 \tValidation Loss: 2.602021\n",
      "Epoch: 30789 \tTraining Loss: 1.284048 \tValidation Loss: 2.602936\n",
      "Epoch: 30790 \tTraining Loss: 1.329542 \tValidation Loss: 2.601135\n",
      "Epoch: 30791 \tTraining Loss: 1.284969 \tValidation Loss: 2.602594\n",
      "Epoch: 30792 \tTraining Loss: 1.269617 \tValidation Loss: 2.603531\n",
      "Epoch: 30793 \tTraining Loss: 1.353783 \tValidation Loss: 2.601670\n",
      "Epoch: 30794 \tTraining Loss: 1.295194 \tValidation Loss: 2.601171\n",
      "Epoch: 30795 \tTraining Loss: 1.296198 \tValidation Loss: 2.600245\n",
      "Epoch: 30796 \tTraining Loss: 1.260069 \tValidation Loss: 2.601920\n",
      "Epoch: 30797 \tTraining Loss: 1.327766 \tValidation Loss: 2.601176\n",
      "Epoch: 30798 \tTraining Loss: 1.285405 \tValidation Loss: 2.602231\n",
      "Epoch: 30799 \tTraining Loss: 1.288162 \tValidation Loss: 2.601430\n",
      "Epoch: 30800 \tTraining Loss: 1.289596 \tValidation Loss: 2.601660\n",
      "Epoch: 30801 \tTraining Loss: 1.287690 \tValidation Loss: 2.603541\n",
      "Epoch: 30802 \tTraining Loss: 1.318231 \tValidation Loss: 2.603996\n",
      "Epoch: 30803 \tTraining Loss: 1.337992 \tValidation Loss: 2.601146\n",
      "Epoch: 30804 \tTraining Loss: 1.307926 \tValidation Loss: 2.601730\n",
      "Epoch: 30805 \tTraining Loss: 1.324396 \tValidation Loss: 2.603120\n",
      "Epoch: 30806 \tTraining Loss: 1.335023 \tValidation Loss: 2.600332\n",
      "Epoch: 30807 \tTraining Loss: 1.273568 \tValidation Loss: 2.603172\n",
      "Epoch: 30808 \tTraining Loss: 1.293469 \tValidation Loss: 2.603513\n",
      "Epoch: 30809 \tTraining Loss: 1.270789 \tValidation Loss: 2.602702\n",
      "Epoch: 30810 \tTraining Loss: 1.294848 \tValidation Loss: 2.601565\n",
      "Epoch: 30811 \tTraining Loss: 1.245137 \tValidation Loss: 2.602120\n",
      "Epoch: 30812 \tTraining Loss: 1.291187 \tValidation Loss: 2.602774\n",
      "Epoch: 30813 \tTraining Loss: 1.325557 \tValidation Loss: 2.603039\n",
      "Epoch: 30814 \tTraining Loss: 1.330769 \tValidation Loss: 2.602659\n",
      "Epoch: 30815 \tTraining Loss: 1.291337 \tValidation Loss: 2.601448\n",
      "Epoch: 30816 \tTraining Loss: 1.310658 \tValidation Loss: 2.601855\n",
      "Epoch: 30817 \tTraining Loss: 1.327874 \tValidation Loss: 2.601759\n",
      "Epoch: 30818 \tTraining Loss: 1.265917 \tValidation Loss: 2.603738\n",
      "Epoch: 30819 \tTraining Loss: 1.288764 \tValidation Loss: 2.603160\n",
      "Epoch: 30820 \tTraining Loss: 1.307334 \tValidation Loss: 2.603480\n",
      "Epoch: 30821 \tTraining Loss: 1.272020 \tValidation Loss: 2.602496\n",
      "Epoch: 30822 \tTraining Loss: 1.301864 \tValidation Loss: 2.601775\n",
      "Epoch: 30823 \tTraining Loss: 1.286305 \tValidation Loss: 2.602412\n",
      "Epoch: 30824 \tTraining Loss: 1.267107 \tValidation Loss: 2.603217\n",
      "Epoch: 30825 \tTraining Loss: 1.312978 \tValidation Loss: 2.602593\n",
      "Epoch: 30826 \tTraining Loss: 1.304305 \tValidation Loss: 2.603579\n",
      "Epoch: 30827 \tTraining Loss: 1.272518 \tValidation Loss: 2.602662\n",
      "Epoch: 30828 \tTraining Loss: 1.276364 \tValidation Loss: 2.602253\n",
      "Epoch: 30829 \tTraining Loss: 1.306585 \tValidation Loss: 2.602086\n",
      "Epoch: 30830 \tTraining Loss: 1.298966 \tValidation Loss: 2.602515\n",
      "Epoch: 30831 \tTraining Loss: 1.256971 \tValidation Loss: 2.602483\n",
      "Epoch: 30832 \tTraining Loss: 1.290635 \tValidation Loss: 2.600250\n",
      "Epoch: 30833 \tTraining Loss: 1.288008 \tValidation Loss: 2.600833\n",
      "Epoch: 30834 \tTraining Loss: 1.272816 \tValidation Loss: 2.602111\n",
      "Epoch: 30835 \tTraining Loss: 1.293255 \tValidation Loss: 2.601368\n",
      "Epoch: 30836 \tTraining Loss: 1.302861 \tValidation Loss: 2.602908\n",
      "Epoch: 30837 \tTraining Loss: 1.269750 \tValidation Loss: 2.603761\n",
      "Epoch: 30838 \tTraining Loss: 1.286932 \tValidation Loss: 2.602469\n",
      "Epoch: 30839 \tTraining Loss: 1.279178 \tValidation Loss: 2.601921\n",
      "Epoch: 30840 \tTraining Loss: 1.272693 \tValidation Loss: 2.602313\n",
      "Epoch: 30841 \tTraining Loss: 1.293983 \tValidation Loss: 2.601392\n",
      "Epoch: 30842 \tTraining Loss: 1.320354 \tValidation Loss: 2.601274\n",
      "Epoch: 30843 \tTraining Loss: 1.314911 \tValidation Loss: 2.602950\n",
      "Epoch: 30844 \tTraining Loss: 1.313481 \tValidation Loss: 2.603517\n",
      "Epoch: 30845 \tTraining Loss: 1.291263 \tValidation Loss: 2.602886\n",
      "Epoch: 30846 \tTraining Loss: 1.315561 \tValidation Loss: 2.603186\n",
      "Epoch: 30847 \tTraining Loss: 1.302549 \tValidation Loss: 2.603946\n",
      "Epoch: 30848 \tTraining Loss: 1.301886 \tValidation Loss: 2.602702\n",
      "Epoch: 30849 \tTraining Loss: 1.311966 \tValidation Loss: 2.602901\n",
      "Epoch: 30850 \tTraining Loss: 1.284077 \tValidation Loss: 2.601771\n",
      "Epoch: 30851 \tTraining Loss: 1.281171 \tValidation Loss: 2.604834\n",
      "Epoch: 30852 \tTraining Loss: 1.290956 \tValidation Loss: 2.601816\n",
      "Epoch: 30853 \tTraining Loss: 1.333843 \tValidation Loss: 2.602168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30854 \tTraining Loss: 1.315720 \tValidation Loss: 2.602108\n",
      "Epoch: 30855 \tTraining Loss: 1.320633 \tValidation Loss: 2.603453\n",
      "Epoch: 30856 \tTraining Loss: 1.294902 \tValidation Loss: 2.603405\n",
      "Epoch: 30857 \tTraining Loss: 1.237464 \tValidation Loss: 2.603669\n",
      "Epoch: 30858 \tTraining Loss: 1.308137 \tValidation Loss: 2.602981\n",
      "Epoch: 30859 \tTraining Loss: 1.279693 \tValidation Loss: 2.603232\n",
      "Epoch: 30860 \tTraining Loss: 1.266908 \tValidation Loss: 2.602552\n",
      "Epoch: 30861 \tTraining Loss: 1.289106 \tValidation Loss: 2.603672\n",
      "Epoch: 30862 \tTraining Loss: 1.277335 \tValidation Loss: 2.604280\n",
      "Epoch: 30863 \tTraining Loss: 1.297802 \tValidation Loss: 2.603785\n",
      "Epoch: 30864 \tTraining Loss: 1.343936 \tValidation Loss: 2.603715\n",
      "Epoch: 30865 \tTraining Loss: 1.280334 \tValidation Loss: 2.601699\n",
      "Epoch: 30866 \tTraining Loss: 1.287185 \tValidation Loss: 2.603070\n",
      "Epoch: 30867 \tTraining Loss: 1.273682 \tValidation Loss: 2.602714\n",
      "Epoch: 30868 \tTraining Loss: 1.290699 \tValidation Loss: 2.604998\n",
      "Epoch: 30869 \tTraining Loss: 1.304496 \tValidation Loss: 2.603268\n",
      "Epoch: 30870 \tTraining Loss: 1.307345 \tValidation Loss: 2.603739\n",
      "Epoch: 30871 \tTraining Loss: 1.253336 \tValidation Loss: 2.604293\n",
      "Epoch: 30872 \tTraining Loss: 1.283290 \tValidation Loss: 2.604292\n",
      "Epoch: 30873 \tTraining Loss: 1.294621 \tValidation Loss: 2.603546\n",
      "Epoch: 30874 \tTraining Loss: 1.327670 \tValidation Loss: 2.603427\n",
      "Epoch: 30875 \tTraining Loss: 1.307801 \tValidation Loss: 2.602575\n",
      "Epoch: 30876 \tTraining Loss: 1.248881 \tValidation Loss: 2.605275\n",
      "Epoch: 30877 \tTraining Loss: 1.337489 \tValidation Loss: 2.603405\n",
      "Epoch: 30878 \tTraining Loss: 1.274787 \tValidation Loss: 2.603969\n",
      "Epoch: 30879 \tTraining Loss: 1.293432 \tValidation Loss: 2.603266\n",
      "Epoch: 30880 \tTraining Loss: 1.301964 \tValidation Loss: 2.604410\n",
      "Epoch: 30881 \tTraining Loss: 1.348560 \tValidation Loss: 2.602204\n",
      "Epoch: 30882 \tTraining Loss: 1.310414 \tValidation Loss: 2.602786\n",
      "Epoch: 30883 \tTraining Loss: 1.287686 \tValidation Loss: 2.604403\n",
      "Epoch: 30884 \tTraining Loss: 1.304221 \tValidation Loss: 2.603181\n",
      "Epoch: 30885 \tTraining Loss: 1.287258 \tValidation Loss: 2.604469\n",
      "Epoch: 30886 \tTraining Loss: 1.277179 \tValidation Loss: 2.604345\n",
      "Epoch: 30887 \tTraining Loss: 1.375434 \tValidation Loss: 2.602795\n",
      "Epoch: 30888 \tTraining Loss: 1.333696 \tValidation Loss: 2.603274\n",
      "Epoch: 30889 \tTraining Loss: 1.313347 \tValidation Loss: 2.604775\n",
      "Epoch: 30890 \tTraining Loss: 1.290965 \tValidation Loss: 2.603682\n",
      "Epoch: 30891 \tTraining Loss: 1.301288 \tValidation Loss: 2.603369\n",
      "Epoch: 30892 \tTraining Loss: 1.301327 \tValidation Loss: 2.602061\n",
      "Epoch: 30893 \tTraining Loss: 1.332063 \tValidation Loss: 2.602991\n",
      "Epoch: 30894 \tTraining Loss: 1.310597 \tValidation Loss: 2.603618\n",
      "Epoch: 30895 \tTraining Loss: 1.308767 \tValidation Loss: 2.604631\n",
      "Epoch: 30896 \tTraining Loss: 1.307606 \tValidation Loss: 2.603955\n",
      "Epoch: 30897 \tTraining Loss: 1.267906 \tValidation Loss: 2.604594\n",
      "Epoch: 30898 \tTraining Loss: 1.314455 \tValidation Loss: 2.604785\n",
      "Epoch: 30899 \tTraining Loss: 1.286358 \tValidation Loss: 2.604420\n",
      "Epoch: 30900 \tTraining Loss: 1.321743 \tValidation Loss: 2.605107\n",
      "Epoch: 30901 \tTraining Loss: 1.285238 \tValidation Loss: 2.602534\n",
      "Epoch: 30902 \tTraining Loss: 1.305268 \tValidation Loss: 2.603851\n",
      "Epoch: 30903 \tTraining Loss: 1.296491 \tValidation Loss: 2.605583\n",
      "Epoch: 30904 \tTraining Loss: 1.267065 \tValidation Loss: 2.603776\n",
      "Epoch: 30905 \tTraining Loss: 1.303746 \tValidation Loss: 2.605018\n",
      "Epoch: 30906 \tTraining Loss: 1.299214 \tValidation Loss: 2.604424\n",
      "Epoch: 30907 \tTraining Loss: 1.271711 \tValidation Loss: 2.603595\n",
      "Epoch: 30908 \tTraining Loss: 1.286207 \tValidation Loss: 2.604723\n",
      "Epoch: 30909 \tTraining Loss: 1.327606 \tValidation Loss: 2.602783\n",
      "Epoch: 30910 \tTraining Loss: 1.296336 \tValidation Loss: 2.603608\n",
      "Epoch: 30911 \tTraining Loss: 1.266911 \tValidation Loss: 2.604562\n",
      "Epoch: 30912 \tTraining Loss: 1.309885 \tValidation Loss: 2.602834\n",
      "Epoch: 30913 \tTraining Loss: 1.275354 \tValidation Loss: 2.604951\n",
      "Epoch: 30914 \tTraining Loss: 1.295665 \tValidation Loss: 2.603494\n",
      "Epoch: 30915 \tTraining Loss: 1.259190 \tValidation Loss: 2.603563\n",
      "Epoch: 30916 \tTraining Loss: 1.275553 \tValidation Loss: 2.602748\n",
      "Epoch: 30917 \tTraining Loss: 1.287140 \tValidation Loss: 2.603389\n",
      "Epoch: 30918 \tTraining Loss: 1.257177 \tValidation Loss: 2.605078\n",
      "Epoch: 30919 \tTraining Loss: 1.318225 \tValidation Loss: 2.604039\n",
      "Epoch: 30920 \tTraining Loss: 1.289915 \tValidation Loss: 2.605004\n",
      "Epoch: 30921 \tTraining Loss: 1.313459 \tValidation Loss: 2.603448\n",
      "Epoch: 30922 \tTraining Loss: 1.292249 \tValidation Loss: 2.605660\n",
      "Epoch: 30923 \tTraining Loss: 1.293273 \tValidation Loss: 2.604387\n",
      "Epoch: 30924 \tTraining Loss: 1.272612 \tValidation Loss: 2.604420\n",
      "Epoch: 30925 \tTraining Loss: 1.256407 \tValidation Loss: 2.605361\n",
      "Epoch: 30926 \tTraining Loss: 1.254215 \tValidation Loss: 2.605217\n",
      "Epoch: 30927 \tTraining Loss: 1.299095 \tValidation Loss: 2.606263\n",
      "Epoch: 30928 \tTraining Loss: 1.289442 \tValidation Loss: 2.605208\n",
      "Epoch: 30929 \tTraining Loss: 1.311339 \tValidation Loss: 2.605157\n",
      "Epoch: 30930 \tTraining Loss: 1.273363 \tValidation Loss: 2.604539\n",
      "Epoch: 30931 \tTraining Loss: 1.286996 \tValidation Loss: 2.605604\n",
      "Epoch: 30932 \tTraining Loss: 1.317431 \tValidation Loss: 2.605479\n",
      "Epoch: 30933 \tTraining Loss: 1.315630 \tValidation Loss: 2.605151\n",
      "Epoch: 30934 \tTraining Loss: 1.272982 \tValidation Loss: 2.605242\n",
      "Epoch: 30935 \tTraining Loss: 1.290069 \tValidation Loss: 2.605013\n",
      "Epoch: 30936 \tTraining Loss: 1.277135 \tValidation Loss: 2.605561\n",
      "Epoch: 30937 \tTraining Loss: 1.280270 \tValidation Loss: 2.604743\n",
      "Epoch: 30938 \tTraining Loss: 1.325668 \tValidation Loss: 2.605037\n",
      "Epoch: 30939 \tTraining Loss: 1.266213 \tValidation Loss: 2.605081\n",
      "Epoch: 30940 \tTraining Loss: 1.310709 \tValidation Loss: 2.605229\n",
      "Epoch: 30941 \tTraining Loss: 1.296196 \tValidation Loss: 2.605203\n",
      "Epoch: 30942 \tTraining Loss: 1.288911 \tValidation Loss: 2.606589\n",
      "Epoch: 30943 \tTraining Loss: 1.266802 \tValidation Loss: 2.605416\n",
      "Epoch: 30944 \tTraining Loss: 1.289310 \tValidation Loss: 2.606151\n",
      "Epoch: 30945 \tTraining Loss: 1.289744 \tValidation Loss: 2.606273\n",
      "Epoch: 30946 \tTraining Loss: 1.304248 \tValidation Loss: 2.605945\n",
      "Epoch: 30947 \tTraining Loss: 1.312555 \tValidation Loss: 2.606093\n",
      "Epoch: 30948 \tTraining Loss: 1.298978 \tValidation Loss: 2.604553\n",
      "Epoch: 30949 \tTraining Loss: 1.300565 \tValidation Loss: 2.605716\n",
      "Epoch: 30950 \tTraining Loss: 1.294309 \tValidation Loss: 2.606021\n",
      "Epoch: 30951 \tTraining Loss: 1.296595 \tValidation Loss: 2.605887\n",
      "Epoch: 30952 \tTraining Loss: 1.290611 \tValidation Loss: 2.605834\n",
      "Epoch: 30953 \tTraining Loss: 1.351256 \tValidation Loss: 2.604641\n",
      "Epoch: 30954 \tTraining Loss: 1.294764 \tValidation Loss: 2.605913\n",
      "Epoch: 30955 \tTraining Loss: 1.306329 \tValidation Loss: 2.604925\n",
      "Epoch: 30956 \tTraining Loss: 1.288569 \tValidation Loss: 2.604578\n",
      "Epoch: 30957 \tTraining Loss: 1.270816 \tValidation Loss: 2.605520\n",
      "Epoch: 30958 \tTraining Loss: 1.281480 \tValidation Loss: 2.607055\n",
      "Epoch: 30959 \tTraining Loss: 1.281630 \tValidation Loss: 2.604611\n",
      "Epoch: 30960 \tTraining Loss: 1.281344 \tValidation Loss: 2.607211\n",
      "Epoch: 30961 \tTraining Loss: 1.290809 \tValidation Loss: 2.606061\n",
      "Epoch: 30962 \tTraining Loss: 1.324301 \tValidation Loss: 2.604294\n",
      "Epoch: 30963 \tTraining Loss: 1.271268 \tValidation Loss: 2.605414\n",
      "Epoch: 30964 \tTraining Loss: 1.297829 \tValidation Loss: 2.604681\n",
      "Epoch: 30965 \tTraining Loss: 1.302015 \tValidation Loss: 2.604973\n",
      "Epoch: 30966 \tTraining Loss: 1.312125 \tValidation Loss: 2.605362\n",
      "Epoch: 30967 \tTraining Loss: 1.316799 \tValidation Loss: 2.606687\n",
      "Epoch: 30968 \tTraining Loss: 1.318339 \tValidation Loss: 2.606799\n",
      "Epoch: 30969 \tTraining Loss: 1.317953 \tValidation Loss: 2.604110\n",
      "Epoch: 30970 \tTraining Loss: 1.285529 \tValidation Loss: 2.606341\n",
      "Epoch: 30971 \tTraining Loss: 1.280304 \tValidation Loss: 2.608088\n",
      "Epoch: 30972 \tTraining Loss: 1.275141 \tValidation Loss: 2.605213\n",
      "Epoch: 30973 \tTraining Loss: 1.313204 \tValidation Loss: 2.605467\n",
      "Epoch: 30974 \tTraining Loss: 1.346734 \tValidation Loss: 2.605614\n",
      "Epoch: 30975 \tTraining Loss: 1.337479 \tValidation Loss: 2.604251\n",
      "Epoch: 30976 \tTraining Loss: 1.329006 \tValidation Loss: 2.603147\n",
      "Epoch: 30977 \tTraining Loss: 1.284402 \tValidation Loss: 2.605483\n",
      "Epoch: 30978 \tTraining Loss: 1.309232 \tValidation Loss: 2.604643\n",
      "Epoch: 30979 \tTraining Loss: 1.236985 \tValidation Loss: 2.606219\n",
      "Epoch: 30980 \tTraining Loss: 1.274542 \tValidation Loss: 2.605396\n",
      "Epoch: 30981 \tTraining Loss: 1.258271 \tValidation Loss: 2.606122\n",
      "Epoch: 30982 \tTraining Loss: 1.292710 \tValidation Loss: 2.606415\n",
      "Epoch: 30983 \tTraining Loss: 1.286203 \tValidation Loss: 2.606525\n",
      "Epoch: 30984 \tTraining Loss: 1.315250 \tValidation Loss: 2.606155\n",
      "Epoch: 30985 \tTraining Loss: 1.284986 \tValidation Loss: 2.606741\n",
      "Epoch: 30986 \tTraining Loss: 1.290062 \tValidation Loss: 2.606050\n",
      "Epoch: 30987 \tTraining Loss: 1.277667 \tValidation Loss: 2.607564\n",
      "Epoch: 30988 \tTraining Loss: 1.285033 \tValidation Loss: 2.606948\n",
      "Epoch: 30989 \tTraining Loss: 1.321804 \tValidation Loss: 2.605332\n",
      "Epoch: 30990 \tTraining Loss: 1.281330 \tValidation Loss: 2.605527\n",
      "Epoch: 30991 \tTraining Loss: 1.267638 \tValidation Loss: 2.607710\n",
      "Epoch: 30992 \tTraining Loss: 1.279327 \tValidation Loss: 2.606758\n",
      "Epoch: 30993 \tTraining Loss: 1.341012 \tValidation Loss: 2.605900\n",
      "Epoch: 30994 \tTraining Loss: 1.303397 \tValidation Loss: 2.605483\n",
      "Epoch: 30995 \tTraining Loss: 1.335298 \tValidation Loss: 2.606556\n",
      "Epoch: 30996 \tTraining Loss: 1.311926 \tValidation Loss: 2.606702\n",
      "Epoch: 30997 \tTraining Loss: 1.235117 \tValidation Loss: 2.607505\n",
      "Epoch: 30998 \tTraining Loss: 1.271956 \tValidation Loss: 2.606647\n",
      "Epoch: 30999 \tTraining Loss: 1.308924 \tValidation Loss: 2.606290\n",
      "Epoch: 31000 \tTraining Loss: 1.319282 \tValidation Loss: 2.605404\n",
      "Epoch: 31001 \tTraining Loss: 1.332986 \tValidation Loss: 2.606051\n",
      "Epoch: 31002 \tTraining Loss: 1.287098 \tValidation Loss: 2.606601\n",
      "Epoch: 31003 \tTraining Loss: 1.285485 \tValidation Loss: 2.605952\n",
      "Epoch: 31004 \tTraining Loss: 1.296047 \tValidation Loss: 2.606196\n",
      "Epoch: 31005 \tTraining Loss: 1.263482 \tValidation Loss: 2.606610\n",
      "Epoch: 31006 \tTraining Loss: 1.304306 \tValidation Loss: 2.607481\n",
      "Epoch: 31007 \tTraining Loss: 1.333041 \tValidation Loss: 2.606008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31008 \tTraining Loss: 1.266726 \tValidation Loss: 2.607603\n",
      "Epoch: 31009 \tTraining Loss: 1.313773 \tValidation Loss: 2.606102\n",
      "Epoch: 31010 \tTraining Loss: 1.292477 \tValidation Loss: 2.604794\n",
      "Epoch: 31011 \tTraining Loss: 1.325679 \tValidation Loss: 2.605275\n",
      "Epoch: 31012 \tTraining Loss: 1.302341 \tValidation Loss: 2.606994\n",
      "Epoch: 31013 \tTraining Loss: 1.336750 \tValidation Loss: 2.605714\n",
      "Epoch: 31014 \tTraining Loss: 1.324564 \tValidation Loss: 2.605078\n",
      "Epoch: 31015 \tTraining Loss: 1.254713 \tValidation Loss: 2.606579\n",
      "Epoch: 31016 \tTraining Loss: 1.292675 \tValidation Loss: 2.605200\n",
      "Epoch: 31017 \tTraining Loss: 1.294385 \tValidation Loss: 2.606726\n",
      "Epoch: 31018 \tTraining Loss: 1.276932 \tValidation Loss: 2.605461\n",
      "Epoch: 31019 \tTraining Loss: 1.249332 \tValidation Loss: 2.607582\n",
      "Epoch: 31020 \tTraining Loss: 1.284777 \tValidation Loss: 2.607527\n",
      "Epoch: 31021 \tTraining Loss: 1.306128 \tValidation Loss: 2.606514\n",
      "Epoch: 31022 \tTraining Loss: 1.305890 \tValidation Loss: 2.607361\n",
      "Epoch: 31023 \tTraining Loss: 1.258573 \tValidation Loss: 2.606434\n",
      "Epoch: 31024 \tTraining Loss: 1.278735 \tValidation Loss: 2.607472\n",
      "Epoch: 31025 \tTraining Loss: 1.337641 \tValidation Loss: 2.607096\n",
      "Epoch: 31026 \tTraining Loss: 1.276769 \tValidation Loss: 2.607383\n",
      "Epoch: 31027 \tTraining Loss: 1.311149 \tValidation Loss: 2.608585\n",
      "Epoch: 31028 \tTraining Loss: 1.235969 \tValidation Loss: 2.606882\n",
      "Epoch: 31029 \tTraining Loss: 1.324229 \tValidation Loss: 2.606579\n",
      "Epoch: 31030 \tTraining Loss: 1.260815 \tValidation Loss: 2.607469\n",
      "Epoch: 31031 \tTraining Loss: 1.259931 \tValidation Loss: 2.609146\n",
      "Epoch: 31032 \tTraining Loss: 1.297231 \tValidation Loss: 2.606958\n",
      "Epoch: 31033 \tTraining Loss: 1.281872 \tValidation Loss: 2.606175\n",
      "Epoch: 31034 \tTraining Loss: 1.298598 \tValidation Loss: 2.606086\n",
      "Epoch: 31035 \tTraining Loss: 1.305498 \tValidation Loss: 2.607210\n",
      "Epoch: 31036 \tTraining Loss: 1.284722 \tValidation Loss: 2.605682\n",
      "Epoch: 31037 \tTraining Loss: 1.287497 \tValidation Loss: 2.606666\n",
      "Epoch: 31038 \tTraining Loss: 1.311780 \tValidation Loss: 2.606884\n",
      "Epoch: 31039 \tTraining Loss: 1.304488 \tValidation Loss: 2.606768\n",
      "Epoch: 31040 \tTraining Loss: 1.286650 \tValidation Loss: 2.606589\n",
      "Epoch: 31041 \tTraining Loss: 1.331074 \tValidation Loss: 2.606423\n",
      "Epoch: 31042 \tTraining Loss: 1.313383 \tValidation Loss: 2.607942\n",
      "Epoch: 31043 \tTraining Loss: 1.243939 \tValidation Loss: 2.607705\n",
      "Epoch: 31044 \tTraining Loss: 1.276505 \tValidation Loss: 2.606740\n",
      "Epoch: 31045 \tTraining Loss: 1.277267 \tValidation Loss: 2.608444\n",
      "Epoch: 31046 \tTraining Loss: 1.295972 \tValidation Loss: 2.606370\n",
      "Epoch: 31047 \tTraining Loss: 1.304677 \tValidation Loss: 2.606652\n",
      "Epoch: 31048 \tTraining Loss: 1.297736 \tValidation Loss: 2.607133\n",
      "Epoch: 31049 \tTraining Loss: 1.309766 \tValidation Loss: 2.606273\n",
      "Epoch: 31050 \tTraining Loss: 1.302959 \tValidation Loss: 2.606420\n",
      "Epoch: 31051 \tTraining Loss: 1.267195 \tValidation Loss: 2.608191\n",
      "Epoch: 31052 \tTraining Loss: 1.281423 \tValidation Loss: 2.608559\n",
      "Epoch: 31053 \tTraining Loss: 1.313431 \tValidation Loss: 2.605499\n",
      "Epoch: 31054 \tTraining Loss: 1.335642 \tValidation Loss: 2.607219\n",
      "Epoch: 31055 \tTraining Loss: 1.320245 \tValidation Loss: 2.606280\n",
      "Epoch: 31056 \tTraining Loss: 1.312036 \tValidation Loss: 2.606261\n",
      "Epoch: 31057 \tTraining Loss: 1.291284 \tValidation Loss: 2.607460\n",
      "Epoch: 31058 \tTraining Loss: 1.301763 \tValidation Loss: 2.608525\n",
      "Epoch: 31059 \tTraining Loss: 1.283949 \tValidation Loss: 2.608100\n",
      "Epoch: 31060 \tTraining Loss: 1.304653 \tValidation Loss: 2.608651\n",
      "Epoch: 31061 \tTraining Loss: 1.277626 \tValidation Loss: 2.608344\n",
      "Epoch: 31062 \tTraining Loss: 1.291020 \tValidation Loss: 2.607716\n",
      "Epoch: 31063 \tTraining Loss: 1.292017 \tValidation Loss: 2.607306\n",
      "Epoch: 31064 \tTraining Loss: 1.300345 \tValidation Loss: 2.608069\n",
      "Epoch: 31065 \tTraining Loss: 1.271377 \tValidation Loss: 2.607947\n",
      "Epoch: 31066 \tTraining Loss: 1.292770 \tValidation Loss: 2.607763\n",
      "Epoch: 31067 \tTraining Loss: 1.291667 \tValidation Loss: 2.608133\n",
      "Epoch: 31068 \tTraining Loss: 1.264078 \tValidation Loss: 2.607480\n",
      "Epoch: 31069 \tTraining Loss: 1.222442 \tValidation Loss: 2.608727\n",
      "Epoch: 31070 \tTraining Loss: 1.294795 \tValidation Loss: 2.608415\n",
      "Epoch: 31071 \tTraining Loss: 1.265450 \tValidation Loss: 2.607823\n",
      "Epoch: 31072 \tTraining Loss: 1.317013 \tValidation Loss: 2.607187\n",
      "Epoch: 31073 \tTraining Loss: 1.349989 \tValidation Loss: 2.607224\n",
      "Epoch: 31074 \tTraining Loss: 1.305122 \tValidation Loss: 2.607033\n",
      "Epoch: 31075 \tTraining Loss: 1.297284 \tValidation Loss: 2.606302\n",
      "Epoch: 31076 \tTraining Loss: 1.261802 \tValidation Loss: 2.607367\n",
      "Epoch: 31077 \tTraining Loss: 1.300310 \tValidation Loss: 2.607879\n",
      "Epoch: 31078 \tTraining Loss: 1.337909 \tValidation Loss: 2.607873\n",
      "Epoch: 31079 \tTraining Loss: 1.295317 \tValidation Loss: 2.608272\n",
      "Epoch: 31080 \tTraining Loss: 1.329251 \tValidation Loss: 2.607121\n",
      "Epoch: 31081 \tTraining Loss: 1.322799 \tValidation Loss: 2.607079\n",
      "Epoch: 31082 \tTraining Loss: 1.260066 \tValidation Loss: 2.609127\n",
      "Epoch: 31083 \tTraining Loss: 1.282531 \tValidation Loss: 2.607902\n",
      "Epoch: 31084 \tTraining Loss: 1.266065 \tValidation Loss: 2.607765\n",
      "Epoch: 31085 \tTraining Loss: 1.333424 \tValidation Loss: 2.607792\n",
      "Epoch: 31086 \tTraining Loss: 1.330515 \tValidation Loss: 2.606437\n",
      "Epoch: 31087 \tTraining Loss: 1.252214 \tValidation Loss: 2.608898\n",
      "Epoch: 31088 \tTraining Loss: 1.265389 \tValidation Loss: 2.607391\n",
      "Epoch: 31089 \tTraining Loss: 1.244459 \tValidation Loss: 2.608659\n",
      "Epoch: 31090 \tTraining Loss: 1.318891 \tValidation Loss: 2.608353\n",
      "Epoch: 31091 \tTraining Loss: 1.306195 \tValidation Loss: 2.608595\n",
      "Epoch: 31092 \tTraining Loss: 1.269522 \tValidation Loss: 2.609999\n",
      "Epoch: 31093 \tTraining Loss: 1.307368 \tValidation Loss: 2.607489\n",
      "Epoch: 31094 \tTraining Loss: 1.303982 \tValidation Loss: 2.606758\n",
      "Epoch: 31095 \tTraining Loss: 1.295611 \tValidation Loss: 2.607187\n",
      "Epoch: 31096 \tTraining Loss: 1.291928 \tValidation Loss: 2.607709\n",
      "Epoch: 31097 \tTraining Loss: 1.317307 \tValidation Loss: 2.609236\n",
      "Epoch: 31098 \tTraining Loss: 1.300264 \tValidation Loss: 2.609880\n",
      "Epoch: 31099 \tTraining Loss: 1.323462 \tValidation Loss: 2.608816\n",
      "Epoch: 31100 \tTraining Loss: 1.324261 \tValidation Loss: 2.607309\n",
      "Epoch: 31101 \tTraining Loss: 1.299050 \tValidation Loss: 2.607371\n",
      "Epoch: 31102 \tTraining Loss: 1.312026 \tValidation Loss: 2.607449\n",
      "Epoch: 31103 \tTraining Loss: 1.309601 \tValidation Loss: 2.607447\n",
      "Epoch: 31104 \tTraining Loss: 1.299577 \tValidation Loss: 2.607952\n",
      "Epoch: 31105 \tTraining Loss: 1.319988 \tValidation Loss: 2.608096\n",
      "Epoch: 31106 \tTraining Loss: 1.285560 \tValidation Loss: 2.608779\n",
      "Epoch: 31107 \tTraining Loss: 1.305806 \tValidation Loss: 2.608268\n",
      "Epoch: 31108 \tTraining Loss: 1.299406 \tValidation Loss: 2.609435\n",
      "Epoch: 31109 \tTraining Loss: 1.285031 \tValidation Loss: 2.608378\n",
      "Epoch: 31110 \tTraining Loss: 1.306734 \tValidation Loss: 2.608693\n",
      "Epoch: 31111 \tTraining Loss: 1.288033 \tValidation Loss: 2.608134\n",
      "Epoch: 31112 \tTraining Loss: 1.254262 \tValidation Loss: 2.608742\n",
      "Epoch: 31113 \tTraining Loss: 1.314601 \tValidation Loss: 2.607903\n",
      "Epoch: 31114 \tTraining Loss: 1.348143 \tValidation Loss: 2.606142\n",
      "Epoch: 31115 \tTraining Loss: 1.280210 \tValidation Loss: 2.606839\n",
      "Epoch: 31116 \tTraining Loss: 1.293584 \tValidation Loss: 2.608234\n",
      "Epoch: 31117 \tTraining Loss: 1.320616 \tValidation Loss: 2.608247\n",
      "Epoch: 31118 \tTraining Loss: 1.319931 \tValidation Loss: 2.608436\n",
      "Epoch: 31119 \tTraining Loss: 1.284268 \tValidation Loss: 2.609801\n",
      "Epoch: 31120 \tTraining Loss: 1.300279 \tValidation Loss: 2.608065\n",
      "Epoch: 31121 \tTraining Loss: 1.326738 \tValidation Loss: 2.608491\n",
      "Epoch: 31122 \tTraining Loss: 1.331525 \tValidation Loss: 2.608315\n",
      "Epoch: 31123 \tTraining Loss: 1.293005 \tValidation Loss: 2.609275\n",
      "Epoch: 31124 \tTraining Loss: 1.272286 \tValidation Loss: 2.609396\n",
      "Epoch: 31125 \tTraining Loss: 1.271723 \tValidation Loss: 2.608949\n",
      "Epoch: 31126 \tTraining Loss: 1.316914 \tValidation Loss: 2.608479\n",
      "Epoch: 31127 \tTraining Loss: 1.278886 \tValidation Loss: 2.609846\n",
      "Epoch: 31128 \tTraining Loss: 1.296049 \tValidation Loss: 2.607723\n",
      "Epoch: 31129 \tTraining Loss: 1.322017 \tValidation Loss: 2.608212\n",
      "Epoch: 31130 \tTraining Loss: 1.278599 \tValidation Loss: 2.609524\n",
      "Epoch: 31131 \tTraining Loss: 1.297764 \tValidation Loss: 2.609396\n",
      "Epoch: 31132 \tTraining Loss: 1.297726 \tValidation Loss: 2.609135\n",
      "Epoch: 31133 \tTraining Loss: 1.300984 \tValidation Loss: 2.608977\n",
      "Epoch: 31134 \tTraining Loss: 1.267486 \tValidation Loss: 2.610399\n",
      "Epoch: 31135 \tTraining Loss: 1.303667 \tValidation Loss: 2.609507\n",
      "Epoch: 31136 \tTraining Loss: 1.247768 \tValidation Loss: 2.610895\n",
      "Epoch: 31137 \tTraining Loss: 1.326431 \tValidation Loss: 2.608181\n",
      "Epoch: 31138 \tTraining Loss: 1.292054 \tValidation Loss: 2.609377\n",
      "Epoch: 31139 \tTraining Loss: 1.295151 \tValidation Loss: 2.608017\n",
      "Epoch: 31140 \tTraining Loss: 1.299876 \tValidation Loss: 2.608553\n",
      "Epoch: 31141 \tTraining Loss: 1.260671 \tValidation Loss: 2.609038\n",
      "Epoch: 31142 \tTraining Loss: 1.280281 \tValidation Loss: 2.607953\n",
      "Epoch: 31143 \tTraining Loss: 1.289095 \tValidation Loss: 2.608911\n",
      "Epoch: 31144 \tTraining Loss: 1.335464 \tValidation Loss: 2.608068\n",
      "Epoch: 31145 \tTraining Loss: 1.314506 \tValidation Loss: 2.607014\n",
      "Epoch: 31146 \tTraining Loss: 1.273396 \tValidation Loss: 2.609093\n",
      "Epoch: 31147 \tTraining Loss: 1.250394 \tValidation Loss: 2.610068\n",
      "Epoch: 31148 \tTraining Loss: 1.306834 \tValidation Loss: 2.610032\n",
      "Epoch: 31149 \tTraining Loss: 1.310371 \tValidation Loss: 2.609707\n",
      "Epoch: 31150 \tTraining Loss: 1.282071 \tValidation Loss: 2.610514\n",
      "Epoch: 31151 \tTraining Loss: 1.310588 \tValidation Loss: 2.609084\n",
      "Epoch: 31152 \tTraining Loss: 1.303977 \tValidation Loss: 2.609348\n",
      "Epoch: 31153 \tTraining Loss: 1.254917 \tValidation Loss: 2.610249\n",
      "Epoch: 31154 \tTraining Loss: 1.294690 \tValidation Loss: 2.608825\n",
      "Epoch: 31155 \tTraining Loss: 1.307694 \tValidation Loss: 2.608274\n",
      "Epoch: 31156 \tTraining Loss: 1.281089 \tValidation Loss: 2.608570\n",
      "Epoch: 31157 \tTraining Loss: 1.296155 \tValidation Loss: 2.609918\n",
      "Epoch: 31158 \tTraining Loss: 1.273507 \tValidation Loss: 2.610966\n",
      "Epoch: 31159 \tTraining Loss: 1.261291 \tValidation Loss: 2.610096\n",
      "Epoch: 31160 \tTraining Loss: 1.322122 \tValidation Loss: 2.609456\n",
      "Epoch: 31161 \tTraining Loss: 1.267020 \tValidation Loss: 2.609895\n",
      "Epoch: 31162 \tTraining Loss: 1.294055 \tValidation Loss: 2.609751\n",
      "Epoch: 31163 \tTraining Loss: 1.267439 \tValidation Loss: 2.610096\n",
      "Epoch: 31164 \tTraining Loss: 1.272248 \tValidation Loss: 2.611164\n",
      "Epoch: 31165 \tTraining Loss: 1.305589 \tValidation Loss: 2.609715\n",
      "Epoch: 31166 \tTraining Loss: 1.333327 \tValidation Loss: 2.608562\n",
      "Epoch: 31167 \tTraining Loss: 1.289515 \tValidation Loss: 2.610090\n",
      "Epoch: 31168 \tTraining Loss: 1.271304 \tValidation Loss: 2.610094\n",
      "Epoch: 31169 \tTraining Loss: 1.317286 \tValidation Loss: 2.608938\n",
      "Epoch: 31170 \tTraining Loss: 1.275851 \tValidation Loss: 2.609684\n",
      "Epoch: 31171 \tTraining Loss: 1.304561 \tValidation Loss: 2.609333\n",
      "Epoch: 31172 \tTraining Loss: 1.282446 \tValidation Loss: 2.610281\n",
      "Epoch: 31173 \tTraining Loss: 1.281124 \tValidation Loss: 2.608953\n",
      "Epoch: 31174 \tTraining Loss: 1.226367 \tValidation Loss: 2.610726\n",
      "Epoch: 31175 \tTraining Loss: 1.313317 \tValidation Loss: 2.610301\n",
      "Epoch: 31176 \tTraining Loss: 1.288962 \tValidation Loss: 2.609711\n",
      "Epoch: 31177 \tTraining Loss: 1.261679 \tValidation Loss: 2.610038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31178 \tTraining Loss: 1.299927 \tValidation Loss: 2.609541\n",
      "Epoch: 31179 \tTraining Loss: 1.252672 \tValidation Loss: 2.610709\n",
      "Epoch: 31180 \tTraining Loss: 1.283958 \tValidation Loss: 2.609543\n",
      "Epoch: 31181 \tTraining Loss: 1.302314 \tValidation Loss: 2.610538\n",
      "Epoch: 31182 \tTraining Loss: 1.302043 \tValidation Loss: 2.611368\n",
      "Epoch: 31183 \tTraining Loss: 1.321361 \tValidation Loss: 2.610584\n",
      "Epoch: 31184 \tTraining Loss: 1.310109 \tValidation Loss: 2.609036\n",
      "Epoch: 31185 \tTraining Loss: 1.331311 \tValidation Loss: 2.608713\n",
      "Epoch: 31186 \tTraining Loss: 1.298911 \tValidation Loss: 2.608291\n",
      "Epoch: 31187 \tTraining Loss: 1.274053 \tValidation Loss: 2.610015\n",
      "Epoch: 31188 \tTraining Loss: 1.268813 \tValidation Loss: 2.610189\n",
      "Epoch: 31189 \tTraining Loss: 1.274765 \tValidation Loss: 2.611453\n",
      "Epoch: 31190 \tTraining Loss: 1.287320 \tValidation Loss: 2.610841\n",
      "Epoch: 31191 \tTraining Loss: 1.271234 \tValidation Loss: 2.612010\n",
      "Epoch: 31192 \tTraining Loss: 1.253639 \tValidation Loss: 2.610739\n",
      "Epoch: 31193 \tTraining Loss: 1.251525 \tValidation Loss: 2.611565\n",
      "Epoch: 31194 \tTraining Loss: 1.316383 \tValidation Loss: 2.609992\n",
      "Epoch: 31195 \tTraining Loss: 1.286163 \tValidation Loss: 2.610218\n",
      "Epoch: 31196 \tTraining Loss: 1.283171 \tValidation Loss: 2.610681\n",
      "Epoch: 31197 \tTraining Loss: 1.264415 \tValidation Loss: 2.610792\n",
      "Epoch: 31198 \tTraining Loss: 1.335098 \tValidation Loss: 2.610533\n",
      "Epoch: 31199 \tTraining Loss: 1.256682 \tValidation Loss: 2.610608\n",
      "Epoch: 31200 \tTraining Loss: 1.310262 \tValidation Loss: 2.609976\n",
      "Epoch: 31201 \tTraining Loss: 1.297276 \tValidation Loss: 2.610694\n",
      "Epoch: 31202 \tTraining Loss: 1.284659 \tValidation Loss: 2.609015\n",
      "Epoch: 31203 \tTraining Loss: 1.309819 \tValidation Loss: 2.610306\n",
      "Epoch: 31204 \tTraining Loss: 1.282910 \tValidation Loss: 2.610059\n",
      "Epoch: 31205 \tTraining Loss: 1.260651 \tValidation Loss: 2.610931\n",
      "Epoch: 31206 \tTraining Loss: 1.282964 \tValidation Loss: 2.611194\n",
      "Epoch: 31207 \tTraining Loss: 1.310999 \tValidation Loss: 2.610561\n",
      "Epoch: 31208 \tTraining Loss: 1.319865 \tValidation Loss: 2.610310\n",
      "Epoch: 31209 \tTraining Loss: 1.313956 \tValidation Loss: 2.611006\n",
      "Epoch: 31210 \tTraining Loss: 1.286385 \tValidation Loss: 2.612125\n",
      "Epoch: 31211 \tTraining Loss: 1.253566 \tValidation Loss: 2.611909\n",
      "Epoch: 31212 \tTraining Loss: 1.293394 \tValidation Loss: 2.611716\n",
      "Epoch: 31213 \tTraining Loss: 1.262740 \tValidation Loss: 2.611638\n",
      "Epoch: 31214 \tTraining Loss: 1.340405 \tValidation Loss: 2.609740\n",
      "Epoch: 31215 \tTraining Loss: 1.264673 \tValidation Loss: 2.611654\n",
      "Epoch: 31216 \tTraining Loss: 1.240602 \tValidation Loss: 2.611374\n",
      "Epoch: 31217 \tTraining Loss: 1.267244 \tValidation Loss: 2.611751\n",
      "Epoch: 31218 \tTraining Loss: 1.297000 \tValidation Loss: 2.610334\n",
      "Epoch: 31219 \tTraining Loss: 1.254902 \tValidation Loss: 2.612200\n",
      "Epoch: 31220 \tTraining Loss: 1.279489 \tValidation Loss: 2.609680\n",
      "Epoch: 31221 \tTraining Loss: 1.250565 \tValidation Loss: 2.611898\n",
      "Epoch: 31222 \tTraining Loss: 1.299626 \tValidation Loss: 2.611356\n",
      "Epoch: 31223 \tTraining Loss: 1.284091 \tValidation Loss: 2.611270\n",
      "Epoch: 31224 \tTraining Loss: 1.274434 \tValidation Loss: 2.611143\n",
      "Epoch: 31225 \tTraining Loss: 1.309019 \tValidation Loss: 2.610637\n",
      "Epoch: 31226 \tTraining Loss: 1.234996 \tValidation Loss: 2.611979\n",
      "Epoch: 31227 \tTraining Loss: 1.260162 \tValidation Loss: 2.611823\n",
      "Epoch: 31228 \tTraining Loss: 1.263842 \tValidation Loss: 2.612562\n",
      "Epoch: 31229 \tTraining Loss: 1.307549 \tValidation Loss: 2.611996\n",
      "Epoch: 31230 \tTraining Loss: 1.340619 \tValidation Loss: 2.610892\n",
      "Epoch: 31231 \tTraining Loss: 1.285473 \tValidation Loss: 2.612313\n",
      "Epoch: 31232 \tTraining Loss: 1.291663 \tValidation Loss: 2.612871\n",
      "Epoch: 31233 \tTraining Loss: 1.289254 \tValidation Loss: 2.610388\n",
      "Epoch: 31234 \tTraining Loss: 1.288001 \tValidation Loss: 2.611824\n",
      "Epoch: 31235 \tTraining Loss: 1.296515 \tValidation Loss: 2.611507\n",
      "Epoch: 31236 \tTraining Loss: 1.301282 \tValidation Loss: 2.612375\n",
      "Epoch: 31237 \tTraining Loss: 1.250326 \tValidation Loss: 2.612207\n",
      "Epoch: 31238 \tTraining Loss: 1.307685 \tValidation Loss: 2.610413\n",
      "Epoch: 31239 \tTraining Loss: 1.304109 \tValidation Loss: 2.610605\n",
      "Epoch: 31240 \tTraining Loss: 1.276299 \tValidation Loss: 2.611347\n",
      "Epoch: 31241 \tTraining Loss: 1.267891 \tValidation Loss: 2.611445\n",
      "Epoch: 31242 \tTraining Loss: 1.290349 \tValidation Loss: 2.611501\n",
      "Epoch: 31243 \tTraining Loss: 1.316047 \tValidation Loss: 2.611047\n",
      "Epoch: 31244 \tTraining Loss: 1.324454 \tValidation Loss: 2.610912\n",
      "Epoch: 31245 \tTraining Loss: 1.261647 \tValidation Loss: 2.611670\n",
      "Epoch: 31246 \tTraining Loss: 1.278667 \tValidation Loss: 2.611735\n",
      "Epoch: 31247 \tTraining Loss: 1.330842 \tValidation Loss: 2.611248\n",
      "Epoch: 31248 \tTraining Loss: 1.277324 \tValidation Loss: 2.612272\n",
      "Epoch: 31249 \tTraining Loss: 1.311907 \tValidation Loss: 2.611291\n",
      "Epoch: 31250 \tTraining Loss: 1.266153 \tValidation Loss: 2.612125\n",
      "Epoch: 31251 \tTraining Loss: 1.303435 \tValidation Loss: 2.611408\n",
      "Epoch: 31252 \tTraining Loss: 1.286401 \tValidation Loss: 2.611811\n",
      "Epoch: 31253 \tTraining Loss: 1.299652 \tValidation Loss: 2.612394\n",
      "Epoch: 31254 \tTraining Loss: 1.250880 \tValidation Loss: 2.612328\n",
      "Epoch: 31255 \tTraining Loss: 1.319187 \tValidation Loss: 2.612563\n",
      "Epoch: 31256 \tTraining Loss: 1.289361 \tValidation Loss: 2.612349\n",
      "Epoch: 31257 \tTraining Loss: 1.299298 \tValidation Loss: 2.612207\n",
      "Epoch: 31258 \tTraining Loss: 1.318123 \tValidation Loss: 2.610799\n",
      "Epoch: 31259 \tTraining Loss: 1.303187 \tValidation Loss: 2.610480\n",
      "Epoch: 31260 \tTraining Loss: 1.294055 \tValidation Loss: 2.611363\n",
      "Epoch: 31261 \tTraining Loss: 1.275442 \tValidation Loss: 2.611878\n",
      "Epoch: 31262 \tTraining Loss: 1.291915 \tValidation Loss: 2.611695\n",
      "Epoch: 31263 \tTraining Loss: 1.293373 \tValidation Loss: 2.610934\n",
      "Epoch: 31264 \tTraining Loss: 1.287383 \tValidation Loss: 2.610445\n",
      "Epoch: 31265 \tTraining Loss: 1.304915 \tValidation Loss: 2.611778\n",
      "Epoch: 31266 \tTraining Loss: 1.238586 \tValidation Loss: 2.611362\n",
      "Epoch: 31267 \tTraining Loss: 1.294214 \tValidation Loss: 2.611493\n",
      "Epoch: 31268 \tTraining Loss: 1.295079 \tValidation Loss: 2.612176\n",
      "Epoch: 31269 \tTraining Loss: 1.289102 \tValidation Loss: 2.611160\n",
      "Epoch: 31270 \tTraining Loss: 1.255209 \tValidation Loss: 2.610363\n",
      "Epoch: 31271 \tTraining Loss: 1.258648 \tValidation Loss: 2.611684\n",
      "Epoch: 31272 \tTraining Loss: 1.285141 \tValidation Loss: 2.613286\n",
      "Epoch: 31273 \tTraining Loss: 1.265870 \tValidation Loss: 2.611404\n",
      "Epoch: 31274 \tTraining Loss: 1.301529 \tValidation Loss: 2.612515\n",
      "Epoch: 31275 \tTraining Loss: 1.319668 \tValidation Loss: 2.612895\n",
      "Epoch: 31276 \tTraining Loss: 1.302066 \tValidation Loss: 2.611365\n",
      "Epoch: 31277 \tTraining Loss: 1.317003 \tValidation Loss: 2.610535\n",
      "Epoch: 31278 \tTraining Loss: 1.311011 \tValidation Loss: 2.611588\n",
      "Epoch: 31279 \tTraining Loss: 1.253419 \tValidation Loss: 2.611733\n",
      "Epoch: 31280 \tTraining Loss: 1.252804 \tValidation Loss: 2.613037\n",
      "Epoch: 31281 \tTraining Loss: 1.267924 \tValidation Loss: 2.611374\n",
      "Epoch: 31282 \tTraining Loss: 1.290741 \tValidation Loss: 2.611686\n",
      "Epoch: 31283 \tTraining Loss: 1.275199 \tValidation Loss: 2.611754\n",
      "Epoch: 31284 \tTraining Loss: 1.293297 \tValidation Loss: 2.611625\n",
      "Epoch: 31285 \tTraining Loss: 1.279402 \tValidation Loss: 2.612563\n",
      "Epoch: 31286 \tTraining Loss: 1.292074 \tValidation Loss: 2.613318\n",
      "Epoch: 31287 \tTraining Loss: 1.295362 \tValidation Loss: 2.612691\n",
      "Epoch: 31288 \tTraining Loss: 1.338508 \tValidation Loss: 2.611951\n",
      "Epoch: 31289 \tTraining Loss: 1.291663 \tValidation Loss: 2.611826\n",
      "Epoch: 31290 \tTraining Loss: 1.295585 \tValidation Loss: 2.612145\n",
      "Epoch: 31291 \tTraining Loss: 1.295520 \tValidation Loss: 2.613151\n",
      "Epoch: 31292 \tTraining Loss: 1.304431 \tValidation Loss: 2.613736\n",
      "Epoch: 31293 \tTraining Loss: 1.297542 \tValidation Loss: 2.611711\n",
      "Epoch: 31294 \tTraining Loss: 1.287403 \tValidation Loss: 2.612824\n",
      "Epoch: 31295 \tTraining Loss: 1.275078 \tValidation Loss: 2.612278\n",
      "Epoch: 31296 \tTraining Loss: 1.279408 \tValidation Loss: 2.612376\n",
      "Epoch: 31297 \tTraining Loss: 1.310543 \tValidation Loss: 2.611549\n",
      "Epoch: 31298 \tTraining Loss: 1.278174 \tValidation Loss: 2.613899\n",
      "Epoch: 31299 \tTraining Loss: 1.319263 \tValidation Loss: 2.611858\n",
      "Epoch: 31300 \tTraining Loss: 1.316731 \tValidation Loss: 2.611885\n",
      "Epoch: 31301 \tTraining Loss: 1.306263 \tValidation Loss: 2.611807\n",
      "Epoch: 31302 \tTraining Loss: 1.279507 \tValidation Loss: 2.613608\n",
      "Epoch: 31303 \tTraining Loss: 1.279754 \tValidation Loss: 2.613034\n",
      "Epoch: 31304 \tTraining Loss: 1.277324 \tValidation Loss: 2.614112\n",
      "Epoch: 31305 \tTraining Loss: 1.257174 \tValidation Loss: 2.613055\n",
      "Epoch: 31306 \tTraining Loss: 1.306525 \tValidation Loss: 2.612798\n",
      "Epoch: 31307 \tTraining Loss: 1.300870 \tValidation Loss: 2.612949\n",
      "Epoch: 31308 \tTraining Loss: 1.285027 \tValidation Loss: 2.611509\n",
      "Epoch: 31309 \tTraining Loss: 1.300200 \tValidation Loss: 2.612926\n",
      "Epoch: 31310 \tTraining Loss: 1.298921 \tValidation Loss: 2.613935\n",
      "Epoch: 31311 \tTraining Loss: 1.279553 \tValidation Loss: 2.613292\n",
      "Epoch: 31312 \tTraining Loss: 1.264405 \tValidation Loss: 2.614704\n",
      "Epoch: 31313 \tTraining Loss: 1.290104 \tValidation Loss: 2.614388\n",
      "Epoch: 31314 \tTraining Loss: 1.295774 \tValidation Loss: 2.613670\n",
      "Epoch: 31315 \tTraining Loss: 1.283563 \tValidation Loss: 2.614003\n",
      "Epoch: 31316 \tTraining Loss: 1.275586 \tValidation Loss: 2.613514\n",
      "Epoch: 31317 \tTraining Loss: 1.259388 \tValidation Loss: 2.613693\n",
      "Epoch: 31318 \tTraining Loss: 1.254873 \tValidation Loss: 2.614599\n",
      "Epoch: 31319 \tTraining Loss: 1.261126 \tValidation Loss: 2.614580\n",
      "Epoch: 31320 \tTraining Loss: 1.284040 \tValidation Loss: 2.614657\n",
      "Epoch: 31321 \tTraining Loss: 1.284352 \tValidation Loss: 2.614496\n",
      "Epoch: 31322 \tTraining Loss: 1.288994 \tValidation Loss: 2.615197\n",
      "Epoch: 31323 \tTraining Loss: 1.249244 \tValidation Loss: 2.614236\n",
      "Epoch: 31324 \tTraining Loss: 1.302501 \tValidation Loss: 2.614331\n",
      "Epoch: 31325 \tTraining Loss: 1.274490 \tValidation Loss: 2.613246\n",
      "Epoch: 31326 \tTraining Loss: 1.292334 \tValidation Loss: 2.613125\n",
      "Epoch: 31327 \tTraining Loss: 1.281821 \tValidation Loss: 2.612817\n",
      "Epoch: 31328 \tTraining Loss: 1.297529 \tValidation Loss: 2.613463\n",
      "Epoch: 31329 \tTraining Loss: 1.320734 \tValidation Loss: 2.612231\n",
      "Epoch: 31330 \tTraining Loss: 1.328129 \tValidation Loss: 2.613392\n",
      "Epoch: 31331 \tTraining Loss: 1.264271 \tValidation Loss: 2.612581\n",
      "Epoch: 31332 \tTraining Loss: 1.276675 \tValidation Loss: 2.613615\n",
      "Epoch: 31333 \tTraining Loss: 1.268328 \tValidation Loss: 2.613887\n",
      "Epoch: 31334 \tTraining Loss: 1.289987 \tValidation Loss: 2.613260\n",
      "Epoch: 31335 \tTraining Loss: 1.259852 \tValidation Loss: 2.614536\n",
      "Epoch: 31336 \tTraining Loss: 1.322648 \tValidation Loss: 2.612442\n",
      "Epoch: 31337 \tTraining Loss: 1.298673 \tValidation Loss: 2.613237\n",
      "Epoch: 31338 \tTraining Loss: 1.288404 \tValidation Loss: 2.613877\n",
      "Epoch: 31339 \tTraining Loss: 1.288198 \tValidation Loss: 2.613588\n",
      "Epoch: 31340 \tTraining Loss: 1.265538 \tValidation Loss: 2.612687\n",
      "Epoch: 31341 \tTraining Loss: 1.305917 \tValidation Loss: 2.613819\n",
      "Epoch: 31342 \tTraining Loss: 1.264578 \tValidation Loss: 2.613715\n",
      "Epoch: 31343 \tTraining Loss: 1.325120 \tValidation Loss: 2.612679\n",
      "Epoch: 31344 \tTraining Loss: 1.249126 \tValidation Loss: 2.612967\n",
      "Epoch: 31345 \tTraining Loss: 1.257722 \tValidation Loss: 2.613531\n",
      "Epoch: 31346 \tTraining Loss: 1.248064 \tValidation Loss: 2.613298\n",
      "Epoch: 31347 \tTraining Loss: 1.260994 \tValidation Loss: 2.613905\n",
      "Epoch: 31348 \tTraining Loss: 1.299634 \tValidation Loss: 2.614560\n",
      "Epoch: 31349 \tTraining Loss: 1.296344 \tValidation Loss: 2.613348\n",
      "Epoch: 31350 \tTraining Loss: 1.257872 \tValidation Loss: 2.612947\n",
      "Epoch: 31351 \tTraining Loss: 1.307966 \tValidation Loss: 2.613680\n",
      "Epoch: 31352 \tTraining Loss: 1.268081 \tValidation Loss: 2.612843\n",
      "Epoch: 31353 \tTraining Loss: 1.285060 \tValidation Loss: 2.614546\n",
      "Epoch: 31354 \tTraining Loss: 1.307100 \tValidation Loss: 2.613516\n",
      "Epoch: 31355 \tTraining Loss: 1.270544 \tValidation Loss: 2.612574\n",
      "Epoch: 31356 \tTraining Loss: 1.304640 \tValidation Loss: 2.613061\n",
      "Epoch: 31357 \tTraining Loss: 1.287457 \tValidation Loss: 2.614705\n",
      "Epoch: 31358 \tTraining Loss: 1.271194 \tValidation Loss: 2.615221\n",
      "Epoch: 31359 \tTraining Loss: 1.298463 \tValidation Loss: 2.613998\n",
      "Epoch: 31360 \tTraining Loss: 1.306309 \tValidation Loss: 2.614004\n",
      "Epoch: 31361 \tTraining Loss: 1.313716 \tValidation Loss: 2.613897\n",
      "Epoch: 31362 \tTraining Loss: 1.301767 \tValidation Loss: 2.614968\n",
      "Epoch: 31363 \tTraining Loss: 1.289948 \tValidation Loss: 2.613796\n",
      "Epoch: 31364 \tTraining Loss: 1.300583 \tValidation Loss: 2.612493\n",
      "Epoch: 31365 \tTraining Loss: 1.256702 \tValidation Loss: 2.614581\n",
      "Epoch: 31366 \tTraining Loss: 1.303988 \tValidation Loss: 2.614236\n",
      "Epoch: 31367 \tTraining Loss: 1.303107 \tValidation Loss: 2.614517\n",
      "Epoch: 31368 \tTraining Loss: 1.302224 \tValidation Loss: 2.614902\n",
      "Epoch: 31369 \tTraining Loss: 1.314124 \tValidation Loss: 2.613814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31370 \tTraining Loss: 1.281666 \tValidation Loss: 2.615115\n",
      "Epoch: 31371 \tTraining Loss: 1.248652 \tValidation Loss: 2.615681\n",
      "Epoch: 31372 \tTraining Loss: 1.230405 \tValidation Loss: 2.615354\n",
      "Epoch: 31373 \tTraining Loss: 1.266642 \tValidation Loss: 2.615206\n",
      "Epoch: 31374 \tTraining Loss: 1.322755 \tValidation Loss: 2.614084\n",
      "Epoch: 31375 \tTraining Loss: 1.262329 \tValidation Loss: 2.614069\n",
      "Epoch: 31376 \tTraining Loss: 1.276621 \tValidation Loss: 2.614534\n",
      "Epoch: 31377 \tTraining Loss: 1.265332 \tValidation Loss: 2.614096\n",
      "Epoch: 31378 \tTraining Loss: 1.265968 \tValidation Loss: 2.615525\n",
      "Epoch: 31379 \tTraining Loss: 1.262159 \tValidation Loss: 2.614878\n",
      "Epoch: 31380 \tTraining Loss: 1.284166 \tValidation Loss: 2.611493\n",
      "Epoch: 31381 \tTraining Loss: 1.227342 \tValidation Loss: 2.615500\n",
      "Epoch: 31382 \tTraining Loss: 1.267061 \tValidation Loss: 2.615368\n",
      "Epoch: 31383 \tTraining Loss: 1.317073 \tValidation Loss: 2.614942\n",
      "Epoch: 31384 \tTraining Loss: 1.274055 \tValidation Loss: 2.614793\n",
      "Epoch: 31385 \tTraining Loss: 1.305407 \tValidation Loss: 2.615138\n",
      "Epoch: 31386 \tTraining Loss: 1.284871 \tValidation Loss: 2.613686\n",
      "Epoch: 31387 \tTraining Loss: 1.262008 \tValidation Loss: 2.615885\n",
      "Epoch: 31388 \tTraining Loss: 1.291594 \tValidation Loss: 2.614143\n",
      "Epoch: 31389 \tTraining Loss: 1.263256 \tValidation Loss: 2.615760\n",
      "Epoch: 31390 \tTraining Loss: 1.272984 \tValidation Loss: 2.614054\n",
      "Epoch: 31391 \tTraining Loss: 1.266769 \tValidation Loss: 2.615154\n",
      "Epoch: 31392 \tTraining Loss: 1.285242 \tValidation Loss: 2.614680\n",
      "Epoch: 31393 \tTraining Loss: 1.291486 \tValidation Loss: 2.613874\n",
      "Epoch: 31394 \tTraining Loss: 1.270283 \tValidation Loss: 2.614468\n",
      "Epoch: 31395 \tTraining Loss: 1.258004 \tValidation Loss: 2.614987\n",
      "Epoch: 31396 \tTraining Loss: 1.338386 \tValidation Loss: 2.612979\n",
      "Epoch: 31397 \tTraining Loss: 1.239043 \tValidation Loss: 2.615646\n",
      "Epoch: 31398 \tTraining Loss: 1.268206 \tValidation Loss: 2.612969\n",
      "Epoch: 31399 \tTraining Loss: 1.249280 \tValidation Loss: 2.614528\n",
      "Epoch: 31400 \tTraining Loss: 1.244156 \tValidation Loss: 2.614849\n",
      "Epoch: 31401 \tTraining Loss: 1.264696 \tValidation Loss: 2.616454\n",
      "Epoch: 31402 \tTraining Loss: 1.287498 \tValidation Loss: 2.615859\n",
      "Epoch: 31403 \tTraining Loss: 1.305986 \tValidation Loss: 2.615887\n",
      "Epoch: 31404 \tTraining Loss: 1.246710 \tValidation Loss: 2.614573\n",
      "Epoch: 31405 \tTraining Loss: 1.318821 \tValidation Loss: 2.614529\n",
      "Epoch: 31406 \tTraining Loss: 1.308703 \tValidation Loss: 2.614971\n",
      "Epoch: 31407 \tTraining Loss: 1.260566 \tValidation Loss: 2.615563\n",
      "Epoch: 31408 \tTraining Loss: 1.293273 \tValidation Loss: 2.613550\n",
      "Epoch: 31409 \tTraining Loss: 1.324005 \tValidation Loss: 2.614169\n",
      "Epoch: 31410 \tTraining Loss: 1.292874 \tValidation Loss: 2.614472\n",
      "Epoch: 31411 \tTraining Loss: 1.295017 \tValidation Loss: 2.613425\n",
      "Epoch: 31412 \tTraining Loss: 1.248148 \tValidation Loss: 2.614171\n",
      "Epoch: 31413 \tTraining Loss: 1.305693 \tValidation Loss: 2.616008\n",
      "Epoch: 31414 \tTraining Loss: 1.298673 \tValidation Loss: 2.614883\n",
      "Epoch: 31415 \tTraining Loss: 1.303770 \tValidation Loss: 2.614537\n",
      "Epoch: 31416 \tTraining Loss: 1.298403 \tValidation Loss: 2.614378\n",
      "Epoch: 31417 \tTraining Loss: 1.290024 \tValidation Loss: 2.615020\n",
      "Epoch: 31418 \tTraining Loss: 1.302667 \tValidation Loss: 2.615376\n",
      "Epoch: 31419 \tTraining Loss: 1.293312 \tValidation Loss: 2.615941\n",
      "Epoch: 31420 \tTraining Loss: 1.328906 \tValidation Loss: 2.613872\n",
      "Epoch: 31421 \tTraining Loss: 1.296619 \tValidation Loss: 2.613307\n",
      "Epoch: 31422 \tTraining Loss: 1.260231 \tValidation Loss: 2.614689\n",
      "Epoch: 31423 \tTraining Loss: 1.277541 \tValidation Loss: 2.614500\n",
      "Epoch: 31424 \tTraining Loss: 1.298706 \tValidation Loss: 2.614591\n",
      "Epoch: 31425 \tTraining Loss: 1.307238 \tValidation Loss: 2.614441\n",
      "Epoch: 31426 \tTraining Loss: 1.307340 \tValidation Loss: 2.614877\n",
      "Epoch: 31427 \tTraining Loss: 1.250052 \tValidation Loss: 2.615911\n",
      "Epoch: 31428 \tTraining Loss: 1.358155 \tValidation Loss: 2.615510\n",
      "Epoch: 31429 \tTraining Loss: 1.267513 \tValidation Loss: 2.615421\n",
      "Epoch: 31430 \tTraining Loss: 1.230071 \tValidation Loss: 2.616230\n",
      "Epoch: 31431 \tTraining Loss: 1.310202 \tValidation Loss: 2.615194\n",
      "Epoch: 31432 \tTraining Loss: 1.281583 \tValidation Loss: 2.615200\n",
      "Epoch: 31433 \tTraining Loss: 1.340847 \tValidation Loss: 2.616535\n",
      "Epoch: 31434 \tTraining Loss: 1.294361 \tValidation Loss: 2.616058\n",
      "Epoch: 31435 \tTraining Loss: 1.308223 \tValidation Loss: 2.615608\n",
      "Epoch: 31436 \tTraining Loss: 1.315978 \tValidation Loss: 2.614827\n",
      "Epoch: 31437 \tTraining Loss: 1.320982 \tValidation Loss: 2.616228\n",
      "Epoch: 31438 \tTraining Loss: 1.274146 \tValidation Loss: 2.616939\n",
      "Epoch: 31439 \tTraining Loss: 1.267621 \tValidation Loss: 2.616997\n",
      "Epoch: 31440 \tTraining Loss: 1.231095 \tValidation Loss: 2.614984\n",
      "Epoch: 31441 \tTraining Loss: 1.272971 \tValidation Loss: 2.616838\n",
      "Epoch: 31442 \tTraining Loss: 1.282902 \tValidation Loss: 2.616789\n",
      "Epoch: 31443 \tTraining Loss: 1.295304 \tValidation Loss: 2.617199\n",
      "Epoch: 31444 \tTraining Loss: 1.289759 \tValidation Loss: 2.616900\n",
      "Epoch: 31445 \tTraining Loss: 1.266479 \tValidation Loss: 2.616298\n",
      "Epoch: 31446 \tTraining Loss: 1.251113 \tValidation Loss: 2.615950\n",
      "Epoch: 31447 \tTraining Loss: 1.258397 \tValidation Loss: 2.616573\n",
      "Epoch: 31448 \tTraining Loss: 1.316927 \tValidation Loss: 2.614780\n",
      "Epoch: 31449 \tTraining Loss: 1.292616 \tValidation Loss: 2.615087\n",
      "Epoch: 31450 \tTraining Loss: 1.306360 \tValidation Loss: 2.616202\n",
      "Epoch: 31451 \tTraining Loss: 1.274903 \tValidation Loss: 2.617552\n",
      "Epoch: 31452 \tTraining Loss: 1.313844 \tValidation Loss: 2.614998\n",
      "Epoch: 31453 \tTraining Loss: 1.276911 \tValidation Loss: 2.616586\n",
      "Epoch: 31454 \tTraining Loss: 1.309047 \tValidation Loss: 2.617343\n",
      "Epoch: 31455 \tTraining Loss: 1.350751 \tValidation Loss: 2.615566\n",
      "Epoch: 31456 \tTraining Loss: 1.248165 \tValidation Loss: 2.617153\n",
      "Epoch: 31457 \tTraining Loss: 1.295449 \tValidation Loss: 2.615418\n",
      "Epoch: 31458 \tTraining Loss: 1.278208 \tValidation Loss: 2.616401\n",
      "Epoch: 31459 \tTraining Loss: 1.279504 \tValidation Loss: 2.615579\n",
      "Epoch: 31460 \tTraining Loss: 1.318753 \tValidation Loss: 2.615704\n",
      "Epoch: 31461 \tTraining Loss: 1.317679 \tValidation Loss: 2.615162\n",
      "Epoch: 31462 \tTraining Loss: 1.321326 \tValidation Loss: 2.615005\n",
      "Epoch: 31463 \tTraining Loss: 1.231017 \tValidation Loss: 2.616667\n",
      "Epoch: 31464 \tTraining Loss: 1.305789 \tValidation Loss: 2.615865\n",
      "Epoch: 31465 \tTraining Loss: 1.324795 \tValidation Loss: 2.616520\n",
      "Epoch: 31466 \tTraining Loss: 1.270042 \tValidation Loss: 2.616888\n",
      "Epoch: 31467 \tTraining Loss: 1.289683 \tValidation Loss: 2.617181\n",
      "Epoch: 31468 \tTraining Loss: 1.269192 \tValidation Loss: 2.617010\n",
      "Epoch: 31469 \tTraining Loss: 1.287791 \tValidation Loss: 2.616379\n",
      "Epoch: 31470 \tTraining Loss: 1.258794 \tValidation Loss: 2.616580\n",
      "Epoch: 31471 \tTraining Loss: 1.332856 \tValidation Loss: 2.615222\n",
      "Epoch: 31472 \tTraining Loss: 1.288180 \tValidation Loss: 2.617414\n",
      "Epoch: 31473 \tTraining Loss: 1.251778 \tValidation Loss: 2.616242\n",
      "Epoch: 31474 \tTraining Loss: 1.240815 \tValidation Loss: 2.616544\n",
      "Epoch: 31475 \tTraining Loss: 1.276488 \tValidation Loss: 2.616276\n",
      "Epoch: 31476 \tTraining Loss: 1.295422 \tValidation Loss: 2.617498\n",
      "Epoch: 31477 \tTraining Loss: 1.254807 \tValidation Loss: 2.615803\n",
      "Epoch: 31478 \tTraining Loss: 1.280481 \tValidation Loss: 2.615533\n",
      "Epoch: 31479 \tTraining Loss: 1.293152 \tValidation Loss: 2.616645\n",
      "Epoch: 31480 \tTraining Loss: 1.255729 \tValidation Loss: 2.617689\n",
      "Epoch: 31481 \tTraining Loss: 1.279050 \tValidation Loss: 2.617139\n",
      "Epoch: 31482 \tTraining Loss: 1.302466 \tValidation Loss: 2.615311\n",
      "Epoch: 31483 \tTraining Loss: 1.260156 \tValidation Loss: 2.617286\n",
      "Epoch: 31484 \tTraining Loss: 1.269344 \tValidation Loss: 2.616361\n",
      "Epoch: 31485 \tTraining Loss: 1.330516 \tValidation Loss: 2.614821\n",
      "Epoch: 31486 \tTraining Loss: 1.270408 \tValidation Loss: 2.616715\n",
      "Epoch: 31487 \tTraining Loss: 1.295710 \tValidation Loss: 2.614486\n",
      "Epoch: 31488 \tTraining Loss: 1.253982 \tValidation Loss: 2.615692\n",
      "Epoch: 31489 \tTraining Loss: 1.276438 \tValidation Loss: 2.615726\n",
      "Epoch: 31490 \tTraining Loss: 1.274273 \tValidation Loss: 2.616717\n",
      "Epoch: 31491 \tTraining Loss: 1.298996 \tValidation Loss: 2.616334\n",
      "Epoch: 31492 \tTraining Loss: 1.279925 \tValidation Loss: 2.617768\n",
      "Epoch: 31493 \tTraining Loss: 1.292276 \tValidation Loss: 2.616450\n",
      "Epoch: 31494 \tTraining Loss: 1.303647 \tValidation Loss: 2.617043\n",
      "Epoch: 31495 \tTraining Loss: 1.290995 \tValidation Loss: 2.615780\n",
      "Epoch: 31496 \tTraining Loss: 1.273044 \tValidation Loss: 2.617100\n",
      "Epoch: 31497 \tTraining Loss: 1.282447 \tValidation Loss: 2.615810\n",
      "Epoch: 31498 \tTraining Loss: 1.263480 \tValidation Loss: 2.618174\n",
      "Epoch: 31499 \tTraining Loss: 1.274710 \tValidation Loss: 2.617440\n",
      "Epoch: 31500 \tTraining Loss: 1.254589 \tValidation Loss: 2.616701\n",
      "Epoch: 31501 \tTraining Loss: 1.268484 \tValidation Loss: 2.616981\n",
      "Epoch: 31502 \tTraining Loss: 1.279489 \tValidation Loss: 2.619024\n",
      "Epoch: 31503 \tTraining Loss: 1.259074 \tValidation Loss: 2.617846\n",
      "Epoch: 31504 \tTraining Loss: 1.257757 \tValidation Loss: 2.616401\n",
      "Epoch: 31505 \tTraining Loss: 1.282461 \tValidation Loss: 2.617048\n",
      "Epoch: 31506 \tTraining Loss: 1.252306 \tValidation Loss: 2.617895\n",
      "Epoch: 31507 \tTraining Loss: 1.295038 \tValidation Loss: 2.617033\n",
      "Epoch: 31508 \tTraining Loss: 1.305070 \tValidation Loss: 2.617020\n",
      "Epoch: 31509 \tTraining Loss: 1.298640 \tValidation Loss: 2.616733\n",
      "Epoch: 31510 \tTraining Loss: 1.273288 \tValidation Loss: 2.614784\n",
      "Epoch: 31511 \tTraining Loss: 1.311290 \tValidation Loss: 2.617118\n",
      "Epoch: 31512 \tTraining Loss: 1.273094 \tValidation Loss: 2.617454\n",
      "Epoch: 31513 \tTraining Loss: 1.283215 \tValidation Loss: 2.616886\n",
      "Epoch: 31514 \tTraining Loss: 1.266201 \tValidation Loss: 2.617842\n",
      "Epoch: 31515 \tTraining Loss: 1.280686 \tValidation Loss: 2.618023\n",
      "Epoch: 31516 \tTraining Loss: 1.296009 \tValidation Loss: 2.617891\n",
      "Epoch: 31517 \tTraining Loss: 1.302755 \tValidation Loss: 2.618335\n",
      "Epoch: 31518 \tTraining Loss: 1.287334 \tValidation Loss: 2.617433\n",
      "Epoch: 31519 \tTraining Loss: 1.281069 \tValidation Loss: 2.616230\n",
      "Epoch: 31520 \tTraining Loss: 1.263406 \tValidation Loss: 2.616862\n",
      "Epoch: 31521 \tTraining Loss: 1.255515 \tValidation Loss: 2.618448\n",
      "Epoch: 31522 \tTraining Loss: 1.267807 \tValidation Loss: 2.617070\n",
      "Epoch: 31523 \tTraining Loss: 1.275481 \tValidation Loss: 2.616658\n",
      "Epoch: 31524 \tTraining Loss: 1.325208 \tValidation Loss: 2.616461\n",
      "Epoch: 31525 \tTraining Loss: 1.259487 \tValidation Loss: 2.616483\n",
      "Epoch: 31526 \tTraining Loss: 1.292067 \tValidation Loss: 2.616840\n",
      "Epoch: 31527 \tTraining Loss: 1.259492 \tValidation Loss: 2.617881\n",
      "Epoch: 31528 \tTraining Loss: 1.273668 \tValidation Loss: 2.617666\n",
      "Epoch: 31529 \tTraining Loss: 1.300976 \tValidation Loss: 2.617435\n",
      "Epoch: 31530 \tTraining Loss: 1.339785 \tValidation Loss: 2.616695\n",
      "Epoch: 31531 \tTraining Loss: 1.254478 \tValidation Loss: 2.617682\n",
      "Epoch: 31532 \tTraining Loss: 1.273126 \tValidation Loss: 2.616230\n",
      "Epoch: 31533 \tTraining Loss: 1.261732 \tValidation Loss: 2.617872\n",
      "Epoch: 31534 \tTraining Loss: 1.245306 \tValidation Loss: 2.618972\n",
      "Epoch: 31535 \tTraining Loss: 1.263342 \tValidation Loss: 2.618073\n",
      "Epoch: 31536 \tTraining Loss: 1.299700 \tValidation Loss: 2.616285\n",
      "Epoch: 31537 \tTraining Loss: 1.312290 \tValidation Loss: 2.615861\n",
      "Epoch: 31538 \tTraining Loss: 1.295138 \tValidation Loss: 2.616095\n",
      "Epoch: 31539 \tTraining Loss: 1.258889 \tValidation Loss: 2.616922\n",
      "Epoch: 31540 \tTraining Loss: 1.277133 \tValidation Loss: 2.617925\n",
      "Epoch: 31541 \tTraining Loss: 1.228245 \tValidation Loss: 2.617499\n",
      "Epoch: 31542 \tTraining Loss: 1.309276 \tValidation Loss: 2.617205\n",
      "Epoch: 31543 \tTraining Loss: 1.277441 \tValidation Loss: 2.618139\n",
      "Epoch: 31544 \tTraining Loss: 1.312467 \tValidation Loss: 2.617693\n",
      "Epoch: 31545 \tTraining Loss: 1.283016 \tValidation Loss: 2.617501\n",
      "Epoch: 31546 \tTraining Loss: 1.291835 \tValidation Loss: 2.617802\n",
      "Epoch: 31547 \tTraining Loss: 1.283640 \tValidation Loss: 2.617488\n",
      "Epoch: 31548 \tTraining Loss: 1.289251 \tValidation Loss: 2.617086\n",
      "Epoch: 31549 \tTraining Loss: 1.260572 \tValidation Loss: 2.616959\n",
      "Epoch: 31550 \tTraining Loss: 1.313262 \tValidation Loss: 2.615907\n",
      "Epoch: 31551 \tTraining Loss: 1.290196 \tValidation Loss: 2.617883\n",
      "Epoch: 31552 \tTraining Loss: 1.333805 \tValidation Loss: 2.614778\n",
      "Epoch: 31553 \tTraining Loss: 1.278034 \tValidation Loss: 2.616594\n",
      "Epoch: 31554 \tTraining Loss: 1.277296 \tValidation Loss: 2.618486\n",
      "Epoch: 31555 \tTraining Loss: 1.268667 \tValidation Loss: 2.616884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31556 \tTraining Loss: 1.313616 \tValidation Loss: 2.617549\n",
      "Epoch: 31557 \tTraining Loss: 1.301213 \tValidation Loss: 2.619016\n",
      "Epoch: 31558 \tTraining Loss: 1.270309 \tValidation Loss: 2.619686\n",
      "Epoch: 31559 \tTraining Loss: 1.269043 \tValidation Loss: 2.619107\n",
      "Epoch: 31560 \tTraining Loss: 1.260770 \tValidation Loss: 2.617344\n",
      "Epoch: 31561 \tTraining Loss: 1.306143 \tValidation Loss: 2.618312\n",
      "Epoch: 31562 \tTraining Loss: 1.238433 \tValidation Loss: 2.618257\n",
      "Epoch: 31563 \tTraining Loss: 1.323542 \tValidation Loss: 2.617750\n",
      "Epoch: 31564 \tTraining Loss: 1.314337 \tValidation Loss: 2.616484\n",
      "Epoch: 31565 \tTraining Loss: 1.317601 \tValidation Loss: 2.617321\n",
      "Epoch: 31566 \tTraining Loss: 1.234997 \tValidation Loss: 2.618119\n",
      "Epoch: 31567 \tTraining Loss: 1.254216 \tValidation Loss: 2.617571\n",
      "Epoch: 31568 \tTraining Loss: 1.300545 \tValidation Loss: 2.617712\n",
      "Epoch: 31569 \tTraining Loss: 1.296529 \tValidation Loss: 2.618371\n",
      "Epoch: 31570 \tTraining Loss: 1.283511 \tValidation Loss: 2.618749\n",
      "Epoch: 31571 \tTraining Loss: 1.313406 \tValidation Loss: 2.616765\n",
      "Epoch: 31572 \tTraining Loss: 1.267278 \tValidation Loss: 2.617829\n",
      "Epoch: 31573 \tTraining Loss: 1.268084 \tValidation Loss: 2.618210\n",
      "Epoch: 31574 \tTraining Loss: 1.333753 \tValidation Loss: 2.618711\n",
      "Epoch: 31575 \tTraining Loss: 1.247880 \tValidation Loss: 2.616992\n",
      "Epoch: 31576 \tTraining Loss: 1.300635 \tValidation Loss: 2.618076\n",
      "Epoch: 31577 \tTraining Loss: 1.314955 \tValidation Loss: 2.617993\n",
      "Epoch: 31578 \tTraining Loss: 1.340741 \tValidation Loss: 2.616902\n",
      "Epoch: 31579 \tTraining Loss: 1.275346 \tValidation Loss: 2.617642\n",
      "Epoch: 31580 \tTraining Loss: 1.297043 \tValidation Loss: 2.618052\n",
      "Epoch: 31581 \tTraining Loss: 1.328924 \tValidation Loss: 2.616550\n",
      "Epoch: 31582 \tTraining Loss: 1.283675 \tValidation Loss: 2.617815\n",
      "Epoch: 31583 \tTraining Loss: 1.271089 \tValidation Loss: 2.619375\n",
      "Epoch: 31584 \tTraining Loss: 1.231313 \tValidation Loss: 2.619420\n",
      "Epoch: 31585 \tTraining Loss: 1.350243 \tValidation Loss: 2.617190\n",
      "Epoch: 31586 \tTraining Loss: 1.297014 \tValidation Loss: 2.618331\n",
      "Epoch: 31587 \tTraining Loss: 1.275583 \tValidation Loss: 2.619608\n",
      "Epoch: 31588 \tTraining Loss: 1.296980 \tValidation Loss: 2.618752\n",
      "Epoch: 31589 \tTraining Loss: 1.271732 \tValidation Loss: 2.618308\n",
      "Epoch: 31590 \tTraining Loss: 1.264941 \tValidation Loss: 2.618221\n",
      "Epoch: 31591 \tTraining Loss: 1.254606 \tValidation Loss: 2.618986\n",
      "Epoch: 31592 \tTraining Loss: 1.294494 \tValidation Loss: 2.620536\n",
      "Epoch: 31593 \tTraining Loss: 1.268665 \tValidation Loss: 2.618250\n",
      "Epoch: 31594 \tTraining Loss: 1.304575 \tValidation Loss: 2.617783\n",
      "Epoch: 31595 \tTraining Loss: 1.306069 \tValidation Loss: 2.618180\n",
      "Epoch: 31596 \tTraining Loss: 1.277082 \tValidation Loss: 2.619334\n",
      "Epoch: 31597 \tTraining Loss: 1.275727 \tValidation Loss: 2.618340\n",
      "Epoch: 31598 \tTraining Loss: 1.279307 \tValidation Loss: 2.618090\n",
      "Epoch: 31599 \tTraining Loss: 1.335827 \tValidation Loss: 2.617687\n",
      "Epoch: 31600 \tTraining Loss: 1.255675 \tValidation Loss: 2.618989\n",
      "Epoch: 31601 \tTraining Loss: 1.267169 \tValidation Loss: 2.619581\n",
      "Epoch: 31602 \tTraining Loss: 1.307838 \tValidation Loss: 2.617411\n",
      "Epoch: 31603 \tTraining Loss: 1.316223 \tValidation Loss: 2.618330\n",
      "Epoch: 31604 \tTraining Loss: 1.293978 \tValidation Loss: 2.617002\n",
      "Epoch: 31605 \tTraining Loss: 1.254585 \tValidation Loss: 2.618138\n",
      "Epoch: 31606 \tTraining Loss: 1.307437 \tValidation Loss: 2.617997\n",
      "Epoch: 31607 \tTraining Loss: 1.274942 \tValidation Loss: 2.618931\n",
      "Epoch: 31608 \tTraining Loss: 1.300459 \tValidation Loss: 2.619110\n",
      "Epoch: 31609 \tTraining Loss: 1.307602 \tValidation Loss: 2.617876\n",
      "Epoch: 31610 \tTraining Loss: 1.282526 \tValidation Loss: 2.618008\n",
      "Epoch: 31611 \tTraining Loss: 1.269139 \tValidation Loss: 2.619476\n",
      "Epoch: 31612 \tTraining Loss: 1.298372 \tValidation Loss: 2.619522\n",
      "Epoch: 31613 \tTraining Loss: 1.287843 \tValidation Loss: 2.619379\n",
      "Epoch: 31614 \tTraining Loss: 1.291183 \tValidation Loss: 2.619508\n",
      "Epoch: 31615 \tTraining Loss: 1.280624 \tValidation Loss: 2.619549\n",
      "Epoch: 31616 \tTraining Loss: 1.259270 \tValidation Loss: 2.620194\n",
      "Epoch: 31617 \tTraining Loss: 1.274858 \tValidation Loss: 2.620486\n",
      "Epoch: 31618 \tTraining Loss: 1.287063 \tValidation Loss: 2.620091\n",
      "Epoch: 31619 \tTraining Loss: 1.298762 \tValidation Loss: 2.619322\n",
      "Epoch: 31620 \tTraining Loss: 1.301279 \tValidation Loss: 2.618784\n",
      "Epoch: 31621 \tTraining Loss: 1.276703 \tValidation Loss: 2.620185\n",
      "Epoch: 31622 \tTraining Loss: 1.280946 \tValidation Loss: 2.618821\n",
      "Epoch: 31623 \tTraining Loss: 1.297257 \tValidation Loss: 2.619770\n",
      "Epoch: 31624 \tTraining Loss: 1.240179 \tValidation Loss: 2.619863\n",
      "Epoch: 31625 \tTraining Loss: 1.283826 \tValidation Loss: 2.620345\n",
      "Epoch: 31626 \tTraining Loss: 1.322285 \tValidation Loss: 2.620936\n",
      "Epoch: 31627 \tTraining Loss: 1.280418 \tValidation Loss: 2.619694\n",
      "Epoch: 31628 \tTraining Loss: 1.304666 \tValidation Loss: 2.618881\n",
      "Epoch: 31629 \tTraining Loss: 1.274290 \tValidation Loss: 2.620877\n",
      "Epoch: 31630 \tTraining Loss: 1.276144 \tValidation Loss: 2.619124\n",
      "Epoch: 31631 \tTraining Loss: 1.300480 \tValidation Loss: 2.619509\n",
      "Epoch: 31632 \tTraining Loss: 1.298838 \tValidation Loss: 2.619339\n",
      "Epoch: 31633 \tTraining Loss: 1.257958 \tValidation Loss: 2.618690\n",
      "Epoch: 31634 \tTraining Loss: 1.313556 \tValidation Loss: 2.618620\n",
      "Epoch: 31635 \tTraining Loss: 1.318467 \tValidation Loss: 2.620241\n",
      "Epoch: 31636 \tTraining Loss: 1.287733 \tValidation Loss: 2.619756\n",
      "Epoch: 31637 \tTraining Loss: 1.328241 \tValidation Loss: 2.618763\n",
      "Epoch: 31638 \tTraining Loss: 1.272800 \tValidation Loss: 2.619255\n",
      "Epoch: 31639 \tTraining Loss: 1.281724 \tValidation Loss: 2.619170\n",
      "Epoch: 31640 \tTraining Loss: 1.259226 \tValidation Loss: 2.620507\n",
      "Epoch: 31641 \tTraining Loss: 1.323550 \tValidation Loss: 2.620735\n",
      "Epoch: 31642 \tTraining Loss: 1.262329 \tValidation Loss: 2.621574\n",
      "Epoch: 31643 \tTraining Loss: 1.285145 \tValidation Loss: 2.619748\n",
      "Epoch: 31644 \tTraining Loss: 1.272315 \tValidation Loss: 2.619589\n",
      "Epoch: 31645 \tTraining Loss: 1.255676 \tValidation Loss: 2.620109\n",
      "Epoch: 31646 \tTraining Loss: 1.304524 \tValidation Loss: 2.621230\n",
      "Epoch: 31647 \tTraining Loss: 1.260219 \tValidation Loss: 2.620174\n",
      "Epoch: 31648 \tTraining Loss: 1.328409 \tValidation Loss: 2.618256\n",
      "Epoch: 31649 \tTraining Loss: 1.284572 \tValidation Loss: 2.619236\n",
      "Epoch: 31650 \tTraining Loss: 1.283425 \tValidation Loss: 2.619348\n",
      "Epoch: 31651 \tTraining Loss: 1.290066 \tValidation Loss: 2.620091\n",
      "Epoch: 31652 \tTraining Loss: 1.245890 \tValidation Loss: 2.620289\n",
      "Epoch: 31653 \tTraining Loss: 1.229202 \tValidation Loss: 2.621765\n",
      "Epoch: 31654 \tTraining Loss: 1.256248 \tValidation Loss: 2.620574\n",
      "Epoch: 31655 \tTraining Loss: 1.287625 \tValidation Loss: 2.620310\n",
      "Epoch: 31656 \tTraining Loss: 1.272865 \tValidation Loss: 2.620606\n",
      "Epoch: 31657 \tTraining Loss: 1.284742 \tValidation Loss: 2.619742\n",
      "Epoch: 31658 \tTraining Loss: 1.291491 \tValidation Loss: 2.619359\n",
      "Epoch: 31659 \tTraining Loss: 1.273573 \tValidation Loss: 2.621385\n",
      "Epoch: 31660 \tTraining Loss: 1.299902 \tValidation Loss: 2.621334\n",
      "Epoch: 31661 \tTraining Loss: 1.324343 \tValidation Loss: 2.619745\n",
      "Epoch: 31662 \tTraining Loss: 1.295096 \tValidation Loss: 2.620967\n",
      "Epoch: 31663 \tTraining Loss: 1.246775 \tValidation Loss: 2.621749\n",
      "Epoch: 31664 \tTraining Loss: 1.294925 \tValidation Loss: 2.621138\n",
      "Epoch: 31665 \tTraining Loss: 1.263009 \tValidation Loss: 2.620722\n",
      "Epoch: 31666 \tTraining Loss: 1.286567 \tValidation Loss: 2.621246\n",
      "Epoch: 31667 \tTraining Loss: 1.304582 \tValidation Loss: 2.621030\n",
      "Epoch: 31668 \tTraining Loss: 1.275833 \tValidation Loss: 2.620751\n",
      "Epoch: 31669 \tTraining Loss: 1.261353 \tValidation Loss: 2.620059\n",
      "Epoch: 31670 \tTraining Loss: 1.281283 \tValidation Loss: 2.620967\n",
      "Epoch: 31671 \tTraining Loss: 1.293448 \tValidation Loss: 2.620531\n",
      "Epoch: 31672 \tTraining Loss: 1.275563 \tValidation Loss: 2.620830\n",
      "Epoch: 31673 \tTraining Loss: 1.247085 \tValidation Loss: 2.621932\n",
      "Epoch: 31674 \tTraining Loss: 1.290976 \tValidation Loss: 2.620936\n",
      "Epoch: 31675 \tTraining Loss: 1.297424 \tValidation Loss: 2.619300\n",
      "Epoch: 31676 \tTraining Loss: 1.230315 \tValidation Loss: 2.621046\n",
      "Epoch: 31677 \tTraining Loss: 1.267905 \tValidation Loss: 2.620670\n",
      "Epoch: 31678 \tTraining Loss: 1.259229 \tValidation Loss: 2.622095\n",
      "Epoch: 31679 \tTraining Loss: 1.313498 \tValidation Loss: 2.621412\n",
      "Epoch: 31680 \tTraining Loss: 1.315269 \tValidation Loss: 2.620411\n",
      "Epoch: 31681 \tTraining Loss: 1.296760 \tValidation Loss: 2.619882\n",
      "Epoch: 31682 \tTraining Loss: 1.273456 \tValidation Loss: 2.620353\n",
      "Epoch: 31683 \tTraining Loss: 1.285093 \tValidation Loss: 2.620560\n",
      "Epoch: 31684 \tTraining Loss: 1.283090 \tValidation Loss: 2.620890\n",
      "Epoch: 31685 \tTraining Loss: 1.271340 \tValidation Loss: 2.621475\n",
      "Epoch: 31686 \tTraining Loss: 1.318903 \tValidation Loss: 2.621117\n",
      "Epoch: 31687 \tTraining Loss: 1.280625 \tValidation Loss: 2.621560\n",
      "Epoch: 31688 \tTraining Loss: 1.248935 \tValidation Loss: 2.622755\n",
      "Epoch: 31689 \tTraining Loss: 1.231233 \tValidation Loss: 2.621388\n",
      "Epoch: 31690 \tTraining Loss: 1.259055 \tValidation Loss: 2.622130\n",
      "Epoch: 31691 \tTraining Loss: 1.246706 \tValidation Loss: 2.620195\n",
      "Epoch: 31692 \tTraining Loss: 1.240995 \tValidation Loss: 2.621459\n",
      "Epoch: 31693 \tTraining Loss: 1.277146 \tValidation Loss: 2.621713\n",
      "Epoch: 31694 \tTraining Loss: 1.292171 \tValidation Loss: 2.619859\n",
      "Epoch: 31695 \tTraining Loss: 1.267156 \tValidation Loss: 2.621463\n",
      "Epoch: 31696 \tTraining Loss: 1.305826 \tValidation Loss: 2.620976\n",
      "Epoch: 31697 \tTraining Loss: 1.301260 \tValidation Loss: 2.619867\n",
      "Epoch: 31698 \tTraining Loss: 1.262053 \tValidation Loss: 2.619839\n",
      "Epoch: 31699 \tTraining Loss: 1.327692 \tValidation Loss: 2.620099\n",
      "Epoch: 31700 \tTraining Loss: 1.267985 \tValidation Loss: 2.621585\n",
      "Epoch: 31701 \tTraining Loss: 1.333399 \tValidation Loss: 2.620502\n",
      "Epoch: 31702 \tTraining Loss: 1.253271 \tValidation Loss: 2.622545\n",
      "Epoch: 31703 \tTraining Loss: 1.262641 \tValidation Loss: 2.622658\n",
      "Epoch: 31704 \tTraining Loss: 1.265228 \tValidation Loss: 2.621812\n",
      "Epoch: 31705 \tTraining Loss: 1.319769 \tValidation Loss: 2.621591\n",
      "Epoch: 31706 \tTraining Loss: 1.264986 \tValidation Loss: 2.621789\n",
      "Epoch: 31707 \tTraining Loss: 1.289605 \tValidation Loss: 2.622615\n",
      "Epoch: 31708 \tTraining Loss: 1.280630 \tValidation Loss: 2.621722\n",
      "Epoch: 31709 \tTraining Loss: 1.260261 \tValidation Loss: 2.622321\n",
      "Epoch: 31710 \tTraining Loss: 1.301374 \tValidation Loss: 2.619900\n",
      "Epoch: 31711 \tTraining Loss: 1.272061 \tValidation Loss: 2.621116\n",
      "Epoch: 31712 \tTraining Loss: 1.267591 \tValidation Loss: 2.620647\n",
      "Epoch: 31713 \tTraining Loss: 1.268601 \tValidation Loss: 2.621906\n",
      "Epoch: 31714 \tTraining Loss: 1.277852 \tValidation Loss: 2.621629\n",
      "Epoch: 31715 \tTraining Loss: 1.267371 \tValidation Loss: 2.621878\n",
      "Epoch: 31716 \tTraining Loss: 1.254143 \tValidation Loss: 2.621694\n",
      "Epoch: 31717 \tTraining Loss: 1.270529 \tValidation Loss: 2.623021\n",
      "Epoch: 31718 \tTraining Loss: 1.287878 \tValidation Loss: 2.622656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31719 \tTraining Loss: 1.242868 \tValidation Loss: 2.621885\n",
      "Epoch: 31720 \tTraining Loss: 1.279903 \tValidation Loss: 2.621297\n",
      "Epoch: 31721 \tTraining Loss: 1.258913 \tValidation Loss: 2.622565\n",
      "Epoch: 31722 \tTraining Loss: 1.259038 \tValidation Loss: 2.622620\n",
      "Epoch: 31723 \tTraining Loss: 1.283726 \tValidation Loss: 2.622100\n",
      "Epoch: 31724 \tTraining Loss: 1.303880 \tValidation Loss: 2.622755\n",
      "Epoch: 31725 \tTraining Loss: 1.271446 \tValidation Loss: 2.620898\n",
      "Epoch: 31726 \tTraining Loss: 1.296111 \tValidation Loss: 2.621238\n",
      "Epoch: 31727 \tTraining Loss: 1.324100 \tValidation Loss: 2.620168\n",
      "Epoch: 31728 \tTraining Loss: 1.269153 \tValidation Loss: 2.621981\n",
      "Epoch: 31729 \tTraining Loss: 1.314985 \tValidation Loss: 2.621952\n",
      "Epoch: 31730 \tTraining Loss: 1.287017 \tValidation Loss: 2.622859\n",
      "Epoch: 31731 \tTraining Loss: 1.252872 \tValidation Loss: 2.622514\n",
      "Epoch: 31732 \tTraining Loss: 1.307547 \tValidation Loss: 2.619979\n",
      "Epoch: 31733 \tTraining Loss: 1.287896 \tValidation Loss: 2.621145\n",
      "Epoch: 31734 \tTraining Loss: 1.287618 \tValidation Loss: 2.620196\n",
      "Epoch: 31735 \tTraining Loss: 1.293032 \tValidation Loss: 2.621287\n",
      "Epoch: 31736 \tTraining Loss: 1.291077 \tValidation Loss: 2.622059\n",
      "Epoch: 31737 \tTraining Loss: 1.306253 \tValidation Loss: 2.623859\n",
      "Epoch: 31738 \tTraining Loss: 1.293745 \tValidation Loss: 2.621926\n",
      "Epoch: 31739 \tTraining Loss: 1.269320 \tValidation Loss: 2.623368\n",
      "Epoch: 31740 \tTraining Loss: 1.285045 \tValidation Loss: 2.620980\n",
      "Epoch: 31741 \tTraining Loss: 1.280186 \tValidation Loss: 2.623000\n",
      "Epoch: 31742 \tTraining Loss: 1.292932 \tValidation Loss: 2.621144\n",
      "Epoch: 31743 \tTraining Loss: 1.298516 \tValidation Loss: 2.622552\n",
      "Epoch: 31744 \tTraining Loss: 1.243457 \tValidation Loss: 2.623167\n",
      "Epoch: 31745 \tTraining Loss: 1.294391 \tValidation Loss: 2.622167\n",
      "Epoch: 31746 \tTraining Loss: 1.264940 \tValidation Loss: 2.622942\n",
      "Epoch: 31747 \tTraining Loss: 1.281130 \tValidation Loss: 2.622884\n",
      "Epoch: 31748 \tTraining Loss: 1.262458 \tValidation Loss: 2.623410\n",
      "Epoch: 31749 \tTraining Loss: 1.260114 \tValidation Loss: 2.623519\n",
      "Epoch: 31750 \tTraining Loss: 1.273567 \tValidation Loss: 2.624321\n",
      "Epoch: 31751 \tTraining Loss: 1.273789 \tValidation Loss: 2.621920\n",
      "Epoch: 31752 \tTraining Loss: 1.250196 \tValidation Loss: 2.623422\n",
      "Epoch: 31753 \tTraining Loss: 1.256317 \tValidation Loss: 2.622873\n",
      "Epoch: 31754 \tTraining Loss: 1.218709 \tValidation Loss: 2.623571\n",
      "Epoch: 31755 \tTraining Loss: 1.249017 \tValidation Loss: 2.622769\n",
      "Epoch: 31756 \tTraining Loss: 1.296447 \tValidation Loss: 2.623509\n",
      "Epoch: 31757 \tTraining Loss: 1.297315 \tValidation Loss: 2.619959\n",
      "Epoch: 31758 \tTraining Loss: 1.244272 \tValidation Loss: 2.622328\n",
      "Epoch: 31759 \tTraining Loss: 1.235471 \tValidation Loss: 2.621369\n",
      "Epoch: 31760 \tTraining Loss: 1.294784 \tValidation Loss: 2.622974\n",
      "Epoch: 31761 \tTraining Loss: 1.285409 \tValidation Loss: 2.622507\n",
      "Epoch: 31762 \tTraining Loss: 1.262661 \tValidation Loss: 2.622453\n",
      "Epoch: 31763 \tTraining Loss: 1.258835 \tValidation Loss: 2.622053\n",
      "Epoch: 31764 \tTraining Loss: 1.337473 \tValidation Loss: 2.621678\n",
      "Epoch: 31765 \tTraining Loss: 1.312913 \tValidation Loss: 2.622707\n",
      "Epoch: 31766 \tTraining Loss: 1.266515 \tValidation Loss: 2.623543\n",
      "Epoch: 31767 \tTraining Loss: 1.257776 \tValidation Loss: 2.623110\n",
      "Epoch: 31768 \tTraining Loss: 1.256186 \tValidation Loss: 2.621926\n",
      "Epoch: 31769 \tTraining Loss: 1.254273 \tValidation Loss: 2.623952\n",
      "Epoch: 31770 \tTraining Loss: 1.270609 \tValidation Loss: 2.622180\n",
      "Epoch: 31771 \tTraining Loss: 1.266656 \tValidation Loss: 2.623103\n",
      "Epoch: 31772 \tTraining Loss: 1.269928 \tValidation Loss: 2.623158\n",
      "Epoch: 31773 \tTraining Loss: 1.277347 \tValidation Loss: 2.622682\n",
      "Epoch: 31774 \tTraining Loss: 1.270339 \tValidation Loss: 2.623260\n",
      "Epoch: 31775 \tTraining Loss: 1.290322 \tValidation Loss: 2.622631\n",
      "Epoch: 31776 \tTraining Loss: 1.245085 \tValidation Loss: 2.624825\n",
      "Epoch: 31777 \tTraining Loss: 1.216666 \tValidation Loss: 2.624699\n",
      "Epoch: 31778 \tTraining Loss: 1.266357 \tValidation Loss: 2.623946\n",
      "Epoch: 31779 \tTraining Loss: 1.262659 \tValidation Loss: 2.623703\n",
      "Epoch: 31780 \tTraining Loss: 1.272797 \tValidation Loss: 2.623125\n",
      "Epoch: 31781 \tTraining Loss: 1.278766 \tValidation Loss: 2.623795\n",
      "Epoch: 31782 \tTraining Loss: 1.270854 \tValidation Loss: 2.623707\n",
      "Epoch: 31783 \tTraining Loss: 1.281201 \tValidation Loss: 2.624024\n",
      "Epoch: 31784 \tTraining Loss: 1.268221 \tValidation Loss: 2.624595\n",
      "Epoch: 31785 \tTraining Loss: 1.260769 \tValidation Loss: 2.624661\n",
      "Epoch: 31786 \tTraining Loss: 1.296950 \tValidation Loss: 2.624386\n",
      "Epoch: 31787 \tTraining Loss: 1.329585 \tValidation Loss: 2.623042\n",
      "Epoch: 31788 \tTraining Loss: 1.341735 \tValidation Loss: 2.623833\n",
      "Epoch: 31789 \tTraining Loss: 1.325304 \tValidation Loss: 2.622571\n",
      "Epoch: 31790 \tTraining Loss: 1.232306 \tValidation Loss: 2.624656\n",
      "Epoch: 31791 \tTraining Loss: 1.261090 \tValidation Loss: 2.623357\n",
      "Epoch: 31792 \tTraining Loss: 1.271977 \tValidation Loss: 2.624062\n",
      "Epoch: 31793 \tTraining Loss: 1.306226 \tValidation Loss: 2.622533\n",
      "Epoch: 31794 \tTraining Loss: 1.303180 \tValidation Loss: 2.622491\n",
      "Epoch: 31795 \tTraining Loss: 1.269506 \tValidation Loss: 2.623497\n",
      "Epoch: 31796 \tTraining Loss: 1.290192 \tValidation Loss: 2.623830\n",
      "Epoch: 31797 \tTraining Loss: 1.257142 \tValidation Loss: 2.624265\n",
      "Epoch: 31798 \tTraining Loss: 1.286317 \tValidation Loss: 2.625361\n",
      "Epoch: 31799 \tTraining Loss: 1.252727 \tValidation Loss: 2.624901\n",
      "Epoch: 31800 \tTraining Loss: 1.286782 \tValidation Loss: 2.624191\n",
      "Epoch: 31801 \tTraining Loss: 1.285543 \tValidation Loss: 2.623176\n",
      "Epoch: 31802 \tTraining Loss: 1.285869 \tValidation Loss: 2.624292\n",
      "Epoch: 31803 \tTraining Loss: 1.276102 \tValidation Loss: 2.623796\n",
      "Epoch: 31804 \tTraining Loss: 1.305102 \tValidation Loss: 2.624324\n",
      "Epoch: 31805 \tTraining Loss: 1.264544 \tValidation Loss: 2.624153\n",
      "Epoch: 31806 \tTraining Loss: 1.266297 \tValidation Loss: 2.624250\n",
      "Epoch: 31807 \tTraining Loss: 1.282687 \tValidation Loss: 2.624849\n",
      "Epoch: 31808 \tTraining Loss: 1.306842 \tValidation Loss: 2.623475\n",
      "Epoch: 31809 \tTraining Loss: 1.263036 \tValidation Loss: 2.624067\n",
      "Epoch: 31810 \tTraining Loss: 1.292439 \tValidation Loss: 2.623898\n",
      "Epoch: 31811 \tTraining Loss: 1.231573 \tValidation Loss: 2.623337\n",
      "Epoch: 31812 \tTraining Loss: 1.305699 \tValidation Loss: 2.624064\n",
      "Epoch: 31813 \tTraining Loss: 1.257566 \tValidation Loss: 2.624063\n",
      "Epoch: 31814 \tTraining Loss: 1.254277 \tValidation Loss: 2.624195\n",
      "Epoch: 31815 \tTraining Loss: 1.302636 \tValidation Loss: 2.624949\n",
      "Epoch: 31816 \tTraining Loss: 1.274946 \tValidation Loss: 2.624214\n",
      "Epoch: 31817 \tTraining Loss: 1.307372 \tValidation Loss: 2.624180\n",
      "Epoch: 31818 \tTraining Loss: 1.304004 \tValidation Loss: 2.624386\n",
      "Epoch: 31819 \tTraining Loss: 1.297679 \tValidation Loss: 2.624365\n",
      "Epoch: 31820 \tTraining Loss: 1.269003 \tValidation Loss: 2.624236\n",
      "Epoch: 31821 \tTraining Loss: 1.274968 \tValidation Loss: 2.624370\n",
      "Epoch: 31822 \tTraining Loss: 1.257468 \tValidation Loss: 2.624284\n",
      "Epoch: 31823 \tTraining Loss: 1.299029 \tValidation Loss: 2.623275\n",
      "Epoch: 31824 \tTraining Loss: 1.242673 \tValidation Loss: 2.623340\n",
      "Epoch: 31825 \tTraining Loss: 1.295771 \tValidation Loss: 2.624203\n",
      "Epoch: 31826 \tTraining Loss: 1.310662 \tValidation Loss: 2.623080\n",
      "Epoch: 31827 \tTraining Loss: 1.310482 \tValidation Loss: 2.623753\n",
      "Epoch: 31828 \tTraining Loss: 1.287571 \tValidation Loss: 2.623639\n",
      "Epoch: 31829 \tTraining Loss: 1.266647 \tValidation Loss: 2.622993\n",
      "Epoch: 31830 \tTraining Loss: 1.306020 \tValidation Loss: 2.623836\n",
      "Epoch: 31831 \tTraining Loss: 1.276062 \tValidation Loss: 2.624230\n",
      "Epoch: 31832 \tTraining Loss: 1.272636 \tValidation Loss: 2.624470\n",
      "Epoch: 31833 \tTraining Loss: 1.262558 \tValidation Loss: 2.625394\n",
      "Epoch: 31834 \tTraining Loss: 1.259405 \tValidation Loss: 2.626421\n",
      "Epoch: 31835 \tTraining Loss: 1.279406 \tValidation Loss: 2.623709\n",
      "Epoch: 31836 \tTraining Loss: 1.317192 \tValidation Loss: 2.625407\n",
      "Epoch: 31837 \tTraining Loss: 1.241748 \tValidation Loss: 2.626457\n",
      "Epoch: 31838 \tTraining Loss: 1.275931 \tValidation Loss: 2.623685\n",
      "Epoch: 31839 \tTraining Loss: 1.255202 \tValidation Loss: 2.624507\n",
      "Epoch: 31840 \tTraining Loss: 1.303230 \tValidation Loss: 2.624923\n",
      "Epoch: 31841 \tTraining Loss: 1.306891 \tValidation Loss: 2.623911\n",
      "Epoch: 31842 \tTraining Loss: 1.272794 \tValidation Loss: 2.624539\n",
      "Epoch: 31843 \tTraining Loss: 1.237374 \tValidation Loss: 2.625127\n",
      "Epoch: 31844 \tTraining Loss: 1.278072 \tValidation Loss: 2.626147\n",
      "Epoch: 31845 \tTraining Loss: 1.280779 \tValidation Loss: 2.624401\n",
      "Epoch: 31846 \tTraining Loss: 1.251265 \tValidation Loss: 2.624591\n",
      "Epoch: 31847 \tTraining Loss: 1.284486 \tValidation Loss: 2.625845\n",
      "Epoch: 31848 \tTraining Loss: 1.234344 \tValidation Loss: 2.626403\n",
      "Epoch: 31849 \tTraining Loss: 1.273567 \tValidation Loss: 2.625756\n",
      "Epoch: 31850 \tTraining Loss: 1.267535 \tValidation Loss: 2.625277\n",
      "Epoch: 31851 \tTraining Loss: 1.294129 \tValidation Loss: 2.625335\n",
      "Epoch: 31852 \tTraining Loss: 1.329147 \tValidation Loss: 2.624596\n",
      "Epoch: 31853 \tTraining Loss: 1.293025 \tValidation Loss: 2.625839\n",
      "Epoch: 31854 \tTraining Loss: 1.285379 \tValidation Loss: 2.624625\n",
      "Epoch: 31855 \tTraining Loss: 1.301258 \tValidation Loss: 2.624428\n",
      "Epoch: 31856 \tTraining Loss: 1.255124 \tValidation Loss: 2.624118\n",
      "Epoch: 31857 \tTraining Loss: 1.300626 \tValidation Loss: 2.625233\n",
      "Epoch: 31858 \tTraining Loss: 1.283049 \tValidation Loss: 2.624969\n",
      "Epoch: 31859 \tTraining Loss: 1.267491 \tValidation Loss: 2.624745\n",
      "Epoch: 31860 \tTraining Loss: 1.321097 \tValidation Loss: 2.625391\n",
      "Epoch: 31861 \tTraining Loss: 1.265575 \tValidation Loss: 2.625428\n",
      "Epoch: 31862 \tTraining Loss: 1.296037 \tValidation Loss: 2.624523\n",
      "Epoch: 31863 \tTraining Loss: 1.243108 \tValidation Loss: 2.625190\n",
      "Epoch: 31864 \tTraining Loss: 1.268796 \tValidation Loss: 2.624455\n",
      "Epoch: 31865 \tTraining Loss: 1.263047 \tValidation Loss: 2.624716\n",
      "Epoch: 31866 \tTraining Loss: 1.298915 \tValidation Loss: 2.624138\n",
      "Epoch: 31867 \tTraining Loss: 1.291081 \tValidation Loss: 2.623538\n",
      "Epoch: 31868 \tTraining Loss: 1.299306 \tValidation Loss: 2.624606\n",
      "Epoch: 31869 \tTraining Loss: 1.292224 \tValidation Loss: 2.624838\n",
      "Epoch: 31870 \tTraining Loss: 1.254852 \tValidation Loss: 2.625959\n",
      "Epoch: 31871 \tTraining Loss: 1.303899 \tValidation Loss: 2.625312\n",
      "Epoch: 31872 \tTraining Loss: 1.331642 \tValidation Loss: 2.625300\n",
      "Epoch: 31873 \tTraining Loss: 1.295293 \tValidation Loss: 2.622966\n",
      "Epoch: 31874 \tTraining Loss: 1.310125 \tValidation Loss: 2.624445\n",
      "Epoch: 31875 \tTraining Loss: 1.263381 \tValidation Loss: 2.625748\n",
      "Epoch: 31876 \tTraining Loss: 1.274164 \tValidation Loss: 2.626705\n",
      "Epoch: 31877 \tTraining Loss: 1.240274 \tValidation Loss: 2.627514\n",
      "Epoch: 31878 \tTraining Loss: 1.225248 \tValidation Loss: 2.627280\n",
      "Epoch: 31879 \tTraining Loss: 1.302634 \tValidation Loss: 2.625115\n",
      "Epoch: 31880 \tTraining Loss: 1.295271 \tValidation Loss: 2.626260\n",
      "Epoch: 31881 \tTraining Loss: 1.284446 \tValidation Loss: 2.626188\n",
      "Epoch: 31882 \tTraining Loss: 1.269814 \tValidation Loss: 2.626842\n",
      "Epoch: 31883 \tTraining Loss: 1.315628 \tValidation Loss: 2.625382\n",
      "Epoch: 31884 \tTraining Loss: 1.219382 \tValidation Loss: 2.627220\n",
      "Epoch: 31885 \tTraining Loss: 1.286822 \tValidation Loss: 2.625605\n",
      "Epoch: 31886 \tTraining Loss: 1.264722 \tValidation Loss: 2.625373\n",
      "Epoch: 31887 \tTraining Loss: 1.249016 \tValidation Loss: 2.626591\n",
      "Epoch: 31888 \tTraining Loss: 1.273416 \tValidation Loss: 2.627367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31889 \tTraining Loss: 1.290024 \tValidation Loss: 2.625353\n",
      "Epoch: 31890 \tTraining Loss: 1.280828 \tValidation Loss: 2.626915\n",
      "Epoch: 31891 \tTraining Loss: 1.284624 \tValidation Loss: 2.626363\n",
      "Epoch: 31892 \tTraining Loss: 1.275634 \tValidation Loss: 2.627206\n",
      "Epoch: 31893 \tTraining Loss: 1.327573 \tValidation Loss: 2.625758\n",
      "Epoch: 31894 \tTraining Loss: 1.283824 \tValidation Loss: 2.625675\n",
      "Epoch: 31895 \tTraining Loss: 1.322374 \tValidation Loss: 2.625168\n",
      "Epoch: 31896 \tTraining Loss: 1.257311 \tValidation Loss: 2.626706\n",
      "Epoch: 31897 \tTraining Loss: 1.285397 \tValidation Loss: 2.626214\n",
      "Epoch: 31898 \tTraining Loss: 1.277571 \tValidation Loss: 2.627193\n",
      "Epoch: 31899 \tTraining Loss: 1.296933 \tValidation Loss: 2.626633\n",
      "Epoch: 31900 \tTraining Loss: 1.287245 \tValidation Loss: 2.626283\n",
      "Epoch: 31901 \tTraining Loss: 1.294596 \tValidation Loss: 2.626543\n",
      "Epoch: 31902 \tTraining Loss: 1.309380 \tValidation Loss: 2.624857\n",
      "Epoch: 31903 \tTraining Loss: 1.271073 \tValidation Loss: 2.625871\n",
      "Epoch: 31904 \tTraining Loss: 1.249386 \tValidation Loss: 2.624686\n",
      "Epoch: 31905 \tTraining Loss: 1.349179 \tValidation Loss: 2.625213\n",
      "Epoch: 31906 \tTraining Loss: 1.312803 \tValidation Loss: 2.625550\n",
      "Epoch: 31907 \tTraining Loss: 1.264334 \tValidation Loss: 2.627278\n",
      "Epoch: 31908 \tTraining Loss: 1.310248 \tValidation Loss: 2.626532\n",
      "Epoch: 31909 \tTraining Loss: 1.261518 \tValidation Loss: 2.626451\n",
      "Epoch: 31910 \tTraining Loss: 1.274784 \tValidation Loss: 2.627525\n",
      "Epoch: 31911 \tTraining Loss: 1.268117 \tValidation Loss: 2.626786\n",
      "Epoch: 31912 \tTraining Loss: 1.296039 \tValidation Loss: 2.626098\n",
      "Epoch: 31913 \tTraining Loss: 1.286857 \tValidation Loss: 2.626599\n",
      "Epoch: 31914 \tTraining Loss: 1.284506 \tValidation Loss: 2.626457\n",
      "Epoch: 31915 \tTraining Loss: 1.289185 \tValidation Loss: 2.627711\n",
      "Epoch: 31916 \tTraining Loss: 1.305231 \tValidation Loss: 2.628226\n",
      "Epoch: 31917 \tTraining Loss: 1.253196 \tValidation Loss: 2.626812\n",
      "Epoch: 31918 \tTraining Loss: 1.277516 \tValidation Loss: 2.626881\n",
      "Epoch: 31919 \tTraining Loss: 1.269051 \tValidation Loss: 2.626993\n",
      "Epoch: 31920 \tTraining Loss: 1.273903 \tValidation Loss: 2.626090\n",
      "Epoch: 31921 \tTraining Loss: 1.300583 \tValidation Loss: 2.624846\n",
      "Epoch: 31922 \tTraining Loss: 1.250774 \tValidation Loss: 2.627567\n",
      "Epoch: 31923 \tTraining Loss: 1.291455 \tValidation Loss: 2.626625\n",
      "Epoch: 31924 \tTraining Loss: 1.270958 \tValidation Loss: 2.626617\n",
      "Epoch: 31925 \tTraining Loss: 1.266744 \tValidation Loss: 2.627692\n",
      "Epoch: 31926 \tTraining Loss: 1.244557 \tValidation Loss: 2.625896\n",
      "Epoch: 31927 \tTraining Loss: 1.263827 \tValidation Loss: 2.627304\n",
      "Epoch: 31928 \tTraining Loss: 1.283738 \tValidation Loss: 2.627549\n",
      "Epoch: 31929 \tTraining Loss: 1.318172 \tValidation Loss: 2.625980\n",
      "Epoch: 31930 \tTraining Loss: 1.254000 \tValidation Loss: 2.627927\n",
      "Epoch: 31931 \tTraining Loss: 1.249448 \tValidation Loss: 2.627317\n",
      "Epoch: 31932 \tTraining Loss: 1.290308 \tValidation Loss: 2.627172\n",
      "Epoch: 31933 \tTraining Loss: 1.283310 \tValidation Loss: 2.626578\n",
      "Epoch: 31934 \tTraining Loss: 1.259465 \tValidation Loss: 2.628184\n",
      "Epoch: 31935 \tTraining Loss: 1.318644 \tValidation Loss: 2.627665\n",
      "Epoch: 31936 \tTraining Loss: 1.288846 \tValidation Loss: 2.626221\n",
      "Epoch: 31937 \tTraining Loss: 1.268236 \tValidation Loss: 2.625513\n",
      "Epoch: 31938 \tTraining Loss: 1.299705 \tValidation Loss: 2.625692\n",
      "Epoch: 31939 \tTraining Loss: 1.273075 \tValidation Loss: 2.626744\n",
      "Epoch: 31940 \tTraining Loss: 1.266608 \tValidation Loss: 2.627787\n",
      "Epoch: 31941 \tTraining Loss: 1.292778 \tValidation Loss: 2.626863\n",
      "Epoch: 31942 \tTraining Loss: 1.287907 \tValidation Loss: 2.627755\n",
      "Epoch: 31943 \tTraining Loss: 1.235730 \tValidation Loss: 2.627978\n",
      "Epoch: 31944 \tTraining Loss: 1.241171 \tValidation Loss: 2.628336\n",
      "Epoch: 31945 \tTraining Loss: 1.269986 \tValidation Loss: 2.627691\n",
      "Epoch: 31946 \tTraining Loss: 1.229971 \tValidation Loss: 2.627647\n",
      "Epoch: 31947 \tTraining Loss: 1.287842 \tValidation Loss: 2.627215\n",
      "Epoch: 31948 \tTraining Loss: 1.281112 \tValidation Loss: 2.627013\n",
      "Epoch: 31949 \tTraining Loss: 1.234902 \tValidation Loss: 2.627367\n",
      "Epoch: 31950 \tTraining Loss: 1.287857 \tValidation Loss: 2.627035\n",
      "Epoch: 31951 \tTraining Loss: 1.305440 \tValidation Loss: 2.627529\n",
      "Epoch: 31952 \tTraining Loss: 1.276788 \tValidation Loss: 2.626947\n",
      "Epoch: 31953 \tTraining Loss: 1.259001 \tValidation Loss: 2.627577\n",
      "Epoch: 31954 \tTraining Loss: 1.248595 \tValidation Loss: 2.628435\n",
      "Epoch: 31955 \tTraining Loss: 1.293472 \tValidation Loss: 2.627822\n",
      "Epoch: 31956 \tTraining Loss: 1.227189 \tValidation Loss: 2.628047\n",
      "Epoch: 31957 \tTraining Loss: 1.232198 \tValidation Loss: 2.628730\n",
      "Epoch: 31958 \tTraining Loss: 1.268693 \tValidation Loss: 2.628507\n",
      "Epoch: 31959 \tTraining Loss: 1.287552 \tValidation Loss: 2.626542\n",
      "Epoch: 31960 \tTraining Loss: 1.288737 \tValidation Loss: 2.626837\n",
      "Epoch: 31961 \tTraining Loss: 1.263762 \tValidation Loss: 2.627133\n",
      "Epoch: 31962 \tTraining Loss: 1.236696 \tValidation Loss: 2.627622\n",
      "Epoch: 31963 \tTraining Loss: 1.291695 \tValidation Loss: 2.627597\n",
      "Epoch: 31964 \tTraining Loss: 1.285642 \tValidation Loss: 2.628327\n",
      "Epoch: 31965 \tTraining Loss: 1.282119 \tValidation Loss: 2.627210\n",
      "Epoch: 31966 \tTraining Loss: 1.345071 \tValidation Loss: 2.626103\n",
      "Epoch: 31967 \tTraining Loss: 1.264723 \tValidation Loss: 2.628222\n",
      "Epoch: 31968 \tTraining Loss: 1.300677 \tValidation Loss: 2.627094\n",
      "Epoch: 31969 \tTraining Loss: 1.321520 \tValidation Loss: 2.627357\n",
      "Epoch: 31970 \tTraining Loss: 1.264508 \tValidation Loss: 2.628344\n",
      "Epoch: 31971 \tTraining Loss: 1.265964 \tValidation Loss: 2.628811\n",
      "Epoch: 31972 \tTraining Loss: 1.328969 \tValidation Loss: 2.628587\n",
      "Epoch: 31973 \tTraining Loss: 1.220677 \tValidation Loss: 2.630167\n",
      "Epoch: 31974 \tTraining Loss: 1.245674 \tValidation Loss: 2.626538\n",
      "Epoch: 31975 \tTraining Loss: 1.273956 \tValidation Loss: 2.628422\n",
      "Epoch: 31976 \tTraining Loss: 1.302561 \tValidation Loss: 2.627588\n",
      "Epoch: 31977 \tTraining Loss: 1.299906 \tValidation Loss: 2.627548\n",
      "Epoch: 31978 \tTraining Loss: 1.285114 \tValidation Loss: 2.628287\n",
      "Epoch: 31979 \tTraining Loss: 1.253505 \tValidation Loss: 2.628506\n",
      "Epoch: 31980 \tTraining Loss: 1.301702 \tValidation Loss: 2.628147\n",
      "Epoch: 31981 \tTraining Loss: 1.272743 \tValidation Loss: 2.628484\n",
      "Epoch: 31982 \tTraining Loss: 1.271909 \tValidation Loss: 2.626680\n",
      "Epoch: 31983 \tTraining Loss: 1.266258 \tValidation Loss: 2.628537\n",
      "Epoch: 31984 \tTraining Loss: 1.279313 \tValidation Loss: 2.628165\n",
      "Epoch: 31985 \tTraining Loss: 1.280860 \tValidation Loss: 2.628799\n",
      "Epoch: 31986 \tTraining Loss: 1.256834 \tValidation Loss: 2.628802\n",
      "Epoch: 31987 \tTraining Loss: 1.237032 \tValidation Loss: 2.628990\n",
      "Epoch: 31988 \tTraining Loss: 1.287406 \tValidation Loss: 2.629719\n",
      "Epoch: 31989 \tTraining Loss: 1.312564 \tValidation Loss: 2.627893\n",
      "Epoch: 31990 \tTraining Loss: 1.235924 \tValidation Loss: 2.628767\n",
      "Epoch: 31991 \tTraining Loss: 1.295790 \tValidation Loss: 2.628721\n",
      "Epoch: 31992 \tTraining Loss: 1.258882 \tValidation Loss: 2.628903\n",
      "Epoch: 31993 \tTraining Loss: 1.275701 \tValidation Loss: 2.627969\n",
      "Epoch: 31994 \tTraining Loss: 1.239243 \tValidation Loss: 2.628613\n",
      "Epoch: 31995 \tTraining Loss: 1.245979 \tValidation Loss: 2.628248\n",
      "Epoch: 31996 \tTraining Loss: 1.262032 \tValidation Loss: 2.628763\n",
      "Epoch: 31997 \tTraining Loss: 1.273685 \tValidation Loss: 2.629598\n",
      "Epoch: 31998 \tTraining Loss: 1.281532 \tValidation Loss: 2.627945\n",
      "Epoch: 31999 \tTraining Loss: 1.287585 \tValidation Loss: 2.629776\n",
      "Epoch: 32000 \tTraining Loss: 1.259503 \tValidation Loss: 2.628292\n",
      "Epoch: 32001 \tTraining Loss: 1.294621 \tValidation Loss: 2.627606\n",
      "Epoch: 32002 \tTraining Loss: 1.269342 \tValidation Loss: 2.628625\n",
      "Epoch: 32003 \tTraining Loss: 1.276635 \tValidation Loss: 2.627181\n",
      "Epoch: 32004 \tTraining Loss: 1.260983 \tValidation Loss: 2.628335\n",
      "Epoch: 32005 \tTraining Loss: 1.300889 \tValidation Loss: 2.628208\n",
      "Epoch: 32006 \tTraining Loss: 1.310251 \tValidation Loss: 2.627125\n",
      "Epoch: 32007 \tTraining Loss: 1.274715 \tValidation Loss: 2.628306\n",
      "Epoch: 32008 \tTraining Loss: 1.268044 \tValidation Loss: 2.628956\n",
      "Epoch: 32009 \tTraining Loss: 1.252317 \tValidation Loss: 2.629092\n",
      "Epoch: 32010 \tTraining Loss: 1.281677 \tValidation Loss: 2.627474\n",
      "Epoch: 32011 \tTraining Loss: 1.268237 \tValidation Loss: 2.628861\n",
      "Epoch: 32012 \tTraining Loss: 1.296356 \tValidation Loss: 2.626948\n",
      "Epoch: 32013 \tTraining Loss: 1.288268 \tValidation Loss: 2.627705\n",
      "Epoch: 32014 \tTraining Loss: 1.295683 \tValidation Loss: 2.627915\n",
      "Epoch: 32015 \tTraining Loss: 1.258050 \tValidation Loss: 2.628602\n",
      "Epoch: 32016 \tTraining Loss: 1.273933 \tValidation Loss: 2.627300\n",
      "Epoch: 32017 \tTraining Loss: 1.291011 \tValidation Loss: 2.627995\n",
      "Epoch: 32018 \tTraining Loss: 1.310644 \tValidation Loss: 2.628141\n",
      "Epoch: 32019 \tTraining Loss: 1.249574 \tValidation Loss: 2.628944\n",
      "Epoch: 32020 \tTraining Loss: 1.304983 \tValidation Loss: 2.628738\n",
      "Epoch: 32021 \tTraining Loss: 1.235227 \tValidation Loss: 2.630759\n",
      "Epoch: 32022 \tTraining Loss: 1.272138 \tValidation Loss: 2.628599\n",
      "Epoch: 32023 \tTraining Loss: 1.303677 \tValidation Loss: 2.629211\n",
      "Epoch: 32024 \tTraining Loss: 1.269411 \tValidation Loss: 2.629582\n",
      "Epoch: 32025 \tTraining Loss: 1.271401 \tValidation Loss: 2.630810\n",
      "Epoch: 32026 \tTraining Loss: 1.276829 \tValidation Loss: 2.629294\n",
      "Epoch: 32027 \tTraining Loss: 1.305337 \tValidation Loss: 2.631263\n",
      "Epoch: 32028 \tTraining Loss: 1.256007 \tValidation Loss: 2.628826\n",
      "Epoch: 32029 \tTraining Loss: 1.285463 \tValidation Loss: 2.629251\n",
      "Epoch: 32030 \tTraining Loss: 1.258662 \tValidation Loss: 2.628779\n",
      "Epoch: 32031 \tTraining Loss: 1.276109 \tValidation Loss: 2.627712\n",
      "Epoch: 32032 \tTraining Loss: 1.201515 \tValidation Loss: 2.630476\n",
      "Epoch: 32033 \tTraining Loss: 1.255580 \tValidation Loss: 2.629673\n",
      "Epoch: 32034 \tTraining Loss: 1.276433 \tValidation Loss: 2.629922\n",
      "Epoch: 32035 \tTraining Loss: 1.289588 \tValidation Loss: 2.628473\n",
      "Epoch: 32036 \tTraining Loss: 1.292589 \tValidation Loss: 2.628602\n",
      "Epoch: 32037 \tTraining Loss: 1.285816 \tValidation Loss: 2.629430\n",
      "Epoch: 32038 \tTraining Loss: 1.258195 \tValidation Loss: 2.630093\n",
      "Epoch: 32039 \tTraining Loss: 1.275671 \tValidation Loss: 2.628812\n",
      "Epoch: 32040 \tTraining Loss: 1.215628 \tValidation Loss: 2.631508\n",
      "Epoch: 32041 \tTraining Loss: 1.291574 \tValidation Loss: 2.628342\n",
      "Epoch: 32042 \tTraining Loss: 1.217966 \tValidation Loss: 2.629077\n",
      "Epoch: 32043 \tTraining Loss: 1.284281 \tValidation Loss: 2.629117\n",
      "Epoch: 32044 \tTraining Loss: 1.245497 \tValidation Loss: 2.629744\n",
      "Epoch: 32045 \tTraining Loss: 1.292122 \tValidation Loss: 2.629520\n",
      "Epoch: 32046 \tTraining Loss: 1.256582 \tValidation Loss: 2.628867\n",
      "Epoch: 32047 \tTraining Loss: 1.294668 \tValidation Loss: 2.630470\n",
      "Epoch: 32048 \tTraining Loss: 1.265253 \tValidation Loss: 2.629678\n",
      "Epoch: 32049 \tTraining Loss: 1.259646 \tValidation Loss: 2.629627\n",
      "Epoch: 32050 \tTraining Loss: 1.295565 \tValidation Loss: 2.631025\n",
      "Epoch: 32051 \tTraining Loss: 1.294612 \tValidation Loss: 2.629880\n",
      "Epoch: 32052 \tTraining Loss: 1.287786 \tValidation Loss: 2.629298\n",
      "Epoch: 32053 \tTraining Loss: 1.283724 \tValidation Loss: 2.629405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32054 \tTraining Loss: 1.272102 \tValidation Loss: 2.629925\n",
      "Epoch: 32055 \tTraining Loss: 1.282276 \tValidation Loss: 2.629638\n",
      "Epoch: 32056 \tTraining Loss: 1.276050 \tValidation Loss: 2.630031\n",
      "Epoch: 32057 \tTraining Loss: 1.239694 \tValidation Loss: 2.630136\n",
      "Epoch: 32058 \tTraining Loss: 1.249859 \tValidation Loss: 2.630763\n",
      "Epoch: 32059 \tTraining Loss: 1.308507 \tValidation Loss: 2.630020\n",
      "Epoch: 32060 \tTraining Loss: 1.270297 \tValidation Loss: 2.629966\n",
      "Epoch: 32061 \tTraining Loss: 1.284782 \tValidation Loss: 2.631340\n",
      "Epoch: 32062 \tTraining Loss: 1.239758 \tValidation Loss: 2.630671\n",
      "Epoch: 32063 \tTraining Loss: 1.294458 \tValidation Loss: 2.629020\n",
      "Epoch: 32064 \tTraining Loss: 1.295378 \tValidation Loss: 2.629771\n",
      "Epoch: 32065 \tTraining Loss: 1.296314 \tValidation Loss: 2.629630\n",
      "Epoch: 32066 \tTraining Loss: 1.295170 \tValidation Loss: 2.628348\n",
      "Epoch: 32067 \tTraining Loss: 1.278052 \tValidation Loss: 2.629347\n",
      "Epoch: 32068 \tTraining Loss: 1.282255 \tValidation Loss: 2.631560\n",
      "Epoch: 32069 \tTraining Loss: 1.300468 \tValidation Loss: 2.629617\n",
      "Epoch: 32070 \tTraining Loss: 1.299807 \tValidation Loss: 2.629723\n",
      "Epoch: 32071 \tTraining Loss: 1.265104 \tValidation Loss: 2.630802\n",
      "Epoch: 32072 \tTraining Loss: 1.252385 \tValidation Loss: 2.630661\n",
      "Epoch: 32073 \tTraining Loss: 1.293316 \tValidation Loss: 2.630655\n",
      "Epoch: 32074 \tTraining Loss: 1.246238 \tValidation Loss: 2.630539\n",
      "Epoch: 32075 \tTraining Loss: 1.251875 \tValidation Loss: 2.630651\n",
      "Epoch: 32076 \tTraining Loss: 1.296480 \tValidation Loss: 2.629314\n",
      "Epoch: 32077 \tTraining Loss: 1.230780 \tValidation Loss: 2.630404\n",
      "Epoch: 32078 \tTraining Loss: 1.287162 \tValidation Loss: 2.629315\n",
      "Epoch: 32079 \tTraining Loss: 1.260009 \tValidation Loss: 2.630161\n",
      "Epoch: 32080 \tTraining Loss: 1.232797 \tValidation Loss: 2.630604\n",
      "Epoch: 32081 \tTraining Loss: 1.254259 \tValidation Loss: 2.632324\n",
      "Epoch: 32082 \tTraining Loss: 1.272957 \tValidation Loss: 2.632360\n",
      "Epoch: 32083 \tTraining Loss: 1.240445 \tValidation Loss: 2.631463\n",
      "Epoch: 32084 \tTraining Loss: 1.282453 \tValidation Loss: 2.629795\n",
      "Epoch: 32085 \tTraining Loss: 1.255511 \tValidation Loss: 2.630197\n",
      "Epoch: 32086 \tTraining Loss: 1.297462 \tValidation Loss: 2.629951\n",
      "Epoch: 32087 \tTraining Loss: 1.243969 \tValidation Loss: 2.629542\n",
      "Epoch: 32088 \tTraining Loss: 1.316886 \tValidation Loss: 2.629111\n",
      "Epoch: 32089 \tTraining Loss: 1.316511 \tValidation Loss: 2.629216\n",
      "Epoch: 32090 \tTraining Loss: 1.311573 \tValidation Loss: 2.630502\n",
      "Epoch: 32091 \tTraining Loss: 1.288371 \tValidation Loss: 2.628705\n",
      "Epoch: 32092 \tTraining Loss: 1.281568 \tValidation Loss: 2.630365\n",
      "Epoch: 32093 \tTraining Loss: 1.289988 \tValidation Loss: 2.630978\n",
      "Epoch: 32094 \tTraining Loss: 1.216212 \tValidation Loss: 2.631355\n",
      "Epoch: 32095 \tTraining Loss: 1.206617 \tValidation Loss: 2.632788\n",
      "Epoch: 32096 \tTraining Loss: 1.332172 \tValidation Loss: 2.630331\n",
      "Epoch: 32097 \tTraining Loss: 1.244176 \tValidation Loss: 2.631981\n",
      "Epoch: 32098 \tTraining Loss: 1.244873 \tValidation Loss: 2.631390\n",
      "Epoch: 32099 \tTraining Loss: 1.259763 \tValidation Loss: 2.631618\n",
      "Epoch: 32100 \tTraining Loss: 1.261346 \tValidation Loss: 2.631193\n",
      "Epoch: 32101 \tTraining Loss: 1.276953 \tValidation Loss: 2.629379\n",
      "Epoch: 32102 \tTraining Loss: 1.261214 \tValidation Loss: 2.631452\n",
      "Epoch: 32103 \tTraining Loss: 1.251263 \tValidation Loss: 2.632308\n",
      "Epoch: 32104 \tTraining Loss: 1.243111 \tValidation Loss: 2.630820\n",
      "Epoch: 32105 \tTraining Loss: 1.302318 \tValidation Loss: 2.632064\n",
      "Epoch: 32106 \tTraining Loss: 1.266865 \tValidation Loss: 2.632410\n",
      "Epoch: 32107 \tTraining Loss: 1.288094 \tValidation Loss: 2.631954\n",
      "Epoch: 32108 \tTraining Loss: 1.340044 \tValidation Loss: 2.630873\n",
      "Epoch: 32109 \tTraining Loss: 1.254057 \tValidation Loss: 2.631622\n",
      "Epoch: 32110 \tTraining Loss: 1.251728 \tValidation Loss: 2.631507\n",
      "Epoch: 32111 \tTraining Loss: 1.297025 \tValidation Loss: 2.629903\n",
      "Epoch: 32112 \tTraining Loss: 1.199188 \tValidation Loss: 2.631482\n",
      "Epoch: 32113 \tTraining Loss: 1.278080 \tValidation Loss: 2.629810\n",
      "Epoch: 32114 \tTraining Loss: 1.294783 \tValidation Loss: 2.631305\n",
      "Epoch: 32115 \tTraining Loss: 1.230223 \tValidation Loss: 2.630646\n",
      "Epoch: 32116 \tTraining Loss: 1.287050 \tValidation Loss: 2.631399\n",
      "Epoch: 32117 \tTraining Loss: 1.269172 \tValidation Loss: 2.631070\n",
      "Epoch: 32118 \tTraining Loss: 1.253538 \tValidation Loss: 2.631649\n",
      "Epoch: 32119 \tTraining Loss: 1.257825 \tValidation Loss: 2.631706\n",
      "Epoch: 32120 \tTraining Loss: 1.264839 \tValidation Loss: 2.632160\n",
      "Epoch: 32121 \tTraining Loss: 1.267824 \tValidation Loss: 2.630983\n",
      "Epoch: 32122 \tTraining Loss: 1.266172 \tValidation Loss: 2.632616\n",
      "Epoch: 32123 \tTraining Loss: 1.293410 \tValidation Loss: 2.630309\n",
      "Epoch: 32124 \tTraining Loss: 1.251738 \tValidation Loss: 2.631026\n",
      "Epoch: 32125 \tTraining Loss: 1.265863 \tValidation Loss: 2.631953\n",
      "Epoch: 32126 \tTraining Loss: 1.260145 \tValidation Loss: 2.632524\n",
      "Epoch: 32127 \tTraining Loss: 1.278842 \tValidation Loss: 2.630911\n",
      "Epoch: 32128 \tTraining Loss: 1.319841 \tValidation Loss: 2.630480\n",
      "Epoch: 32129 \tTraining Loss: 1.243391 \tValidation Loss: 2.631645\n",
      "Epoch: 32130 \tTraining Loss: 1.257804 \tValidation Loss: 2.631738\n",
      "Epoch: 32131 \tTraining Loss: 1.239244 \tValidation Loss: 2.633076\n",
      "Epoch: 32132 \tTraining Loss: 1.271495 \tValidation Loss: 2.630673\n",
      "Epoch: 32133 \tTraining Loss: 1.305585 \tValidation Loss: 2.631555\n",
      "Epoch: 32134 \tTraining Loss: 1.275930 \tValidation Loss: 2.632311\n",
      "Epoch: 32135 \tTraining Loss: 1.235973 \tValidation Loss: 2.633724\n",
      "Epoch: 32136 \tTraining Loss: 1.210117 \tValidation Loss: 2.631995\n",
      "Epoch: 32137 \tTraining Loss: 1.248815 \tValidation Loss: 2.632380\n",
      "Epoch: 32138 \tTraining Loss: 1.250726 \tValidation Loss: 2.631749\n",
      "Epoch: 32139 \tTraining Loss: 1.218399 \tValidation Loss: 2.633321\n",
      "Epoch: 32140 \tTraining Loss: 1.292263 \tValidation Loss: 2.631120\n",
      "Epoch: 32141 \tTraining Loss: 1.272314 \tValidation Loss: 2.632087\n",
      "Epoch: 32142 \tTraining Loss: 1.252086 \tValidation Loss: 2.631165\n",
      "Epoch: 32143 \tTraining Loss: 1.236054 \tValidation Loss: 2.632445\n",
      "Epoch: 32144 \tTraining Loss: 1.267380 \tValidation Loss: 2.630511\n",
      "Epoch: 32145 \tTraining Loss: 1.288564 \tValidation Loss: 2.631229\n",
      "Epoch: 32146 \tTraining Loss: 1.268905 \tValidation Loss: 2.632699\n",
      "Epoch: 32147 \tTraining Loss: 1.246553 \tValidation Loss: 2.632202\n",
      "Epoch: 32148 \tTraining Loss: 1.240661 \tValidation Loss: 2.633018\n",
      "Epoch: 32149 \tTraining Loss: 1.244554 \tValidation Loss: 2.632921\n",
      "Epoch: 32150 \tTraining Loss: 1.312460 \tValidation Loss: 2.633040\n",
      "Epoch: 32151 \tTraining Loss: 1.273889 \tValidation Loss: 2.632886\n",
      "Epoch: 32152 \tTraining Loss: 1.210525 \tValidation Loss: 2.634198\n",
      "Epoch: 32153 \tTraining Loss: 1.325675 \tValidation Loss: 2.630853\n",
      "Epoch: 32154 \tTraining Loss: 1.294036 \tValidation Loss: 2.631210\n",
      "Epoch: 32155 \tTraining Loss: 1.231080 \tValidation Loss: 2.632239\n",
      "Epoch: 32156 \tTraining Loss: 1.289060 \tValidation Loss: 2.631879\n",
      "Epoch: 32157 \tTraining Loss: 1.282829 \tValidation Loss: 2.631160\n",
      "Epoch: 32158 \tTraining Loss: 1.277882 \tValidation Loss: 2.632347\n",
      "Epoch: 32159 \tTraining Loss: 1.279862 \tValidation Loss: 2.632035\n",
      "Epoch: 32160 \tTraining Loss: 1.298551 \tValidation Loss: 2.632360\n",
      "Epoch: 32161 \tTraining Loss: 1.276888 \tValidation Loss: 2.633048\n",
      "Epoch: 32162 \tTraining Loss: 1.287362 \tValidation Loss: 2.632962\n",
      "Epoch: 32163 \tTraining Loss: 1.251733 \tValidation Loss: 2.633548\n",
      "Epoch: 32164 \tTraining Loss: 1.282476 \tValidation Loss: 2.632175\n",
      "Epoch: 32165 \tTraining Loss: 1.292306 \tValidation Loss: 2.631375\n",
      "Epoch: 32166 \tTraining Loss: 1.292531 \tValidation Loss: 2.632923\n",
      "Epoch: 32167 \tTraining Loss: 1.289935 \tValidation Loss: 2.632311\n",
      "Epoch: 32168 \tTraining Loss: 1.288843 \tValidation Loss: 2.632461\n",
      "Epoch: 32169 \tTraining Loss: 1.281463 \tValidation Loss: 2.631984\n",
      "Epoch: 32170 \tTraining Loss: 1.250660 \tValidation Loss: 2.632589\n",
      "Epoch: 32171 \tTraining Loss: 1.270305 \tValidation Loss: 2.633112\n",
      "Epoch: 32172 \tTraining Loss: 1.252625 \tValidation Loss: 2.633582\n",
      "Epoch: 32173 \tTraining Loss: 1.258679 \tValidation Loss: 2.634478\n",
      "Epoch: 32174 \tTraining Loss: 1.247622 \tValidation Loss: 2.633388\n",
      "Epoch: 32175 \tTraining Loss: 1.249800 \tValidation Loss: 2.633739\n",
      "Epoch: 32176 \tTraining Loss: 1.324672 \tValidation Loss: 2.633188\n",
      "Epoch: 32177 \tTraining Loss: 1.251094 \tValidation Loss: 2.632544\n",
      "Epoch: 32178 \tTraining Loss: 1.251688 \tValidation Loss: 2.631870\n",
      "Epoch: 32179 \tTraining Loss: 1.247077 \tValidation Loss: 2.634137\n",
      "Epoch: 32180 \tTraining Loss: 1.222584 \tValidation Loss: 2.632883\n",
      "Epoch: 32181 \tTraining Loss: 1.236021 \tValidation Loss: 2.632586\n",
      "Epoch: 32182 \tTraining Loss: 1.274428 \tValidation Loss: 2.633803\n",
      "Epoch: 32183 \tTraining Loss: 1.287852 \tValidation Loss: 2.633363\n",
      "Epoch: 32184 \tTraining Loss: 1.284942 \tValidation Loss: 2.634309\n",
      "Epoch: 32185 \tTraining Loss: 1.298777 \tValidation Loss: 2.633007\n",
      "Epoch: 32186 \tTraining Loss: 1.271743 \tValidation Loss: 2.633591\n",
      "Epoch: 32187 \tTraining Loss: 1.264963 \tValidation Loss: 2.632361\n",
      "Epoch: 32188 \tTraining Loss: 1.262252 \tValidation Loss: 2.632445\n",
      "Epoch: 32189 \tTraining Loss: 1.292475 \tValidation Loss: 2.633295\n",
      "Epoch: 32190 \tTraining Loss: 1.276816 \tValidation Loss: 2.632137\n",
      "Epoch: 32191 \tTraining Loss: 1.249904 \tValidation Loss: 2.635016\n",
      "Epoch: 32192 \tTraining Loss: 1.236414 \tValidation Loss: 2.633917\n",
      "Epoch: 32193 \tTraining Loss: 1.272671 \tValidation Loss: 2.635088\n",
      "Epoch: 32194 \tTraining Loss: 1.274158 \tValidation Loss: 2.633134\n",
      "Epoch: 32195 \tTraining Loss: 1.253558 \tValidation Loss: 2.633052\n",
      "Epoch: 32196 \tTraining Loss: 1.232385 \tValidation Loss: 2.634708\n",
      "Epoch: 32197 \tTraining Loss: 1.286076 \tValidation Loss: 2.634280\n",
      "Epoch: 32198 \tTraining Loss: 1.279613 \tValidation Loss: 2.633979\n",
      "Epoch: 32199 \tTraining Loss: 1.299987 \tValidation Loss: 2.635375\n",
      "Epoch: 32200 \tTraining Loss: 1.290145 \tValidation Loss: 2.634214\n",
      "Epoch: 32201 \tTraining Loss: 1.262928 \tValidation Loss: 2.634457\n",
      "Epoch: 32202 \tTraining Loss: 1.277994 \tValidation Loss: 2.632706\n",
      "Epoch: 32203 \tTraining Loss: 1.204463 \tValidation Loss: 2.635015\n",
      "Epoch: 32204 \tTraining Loss: 1.276991 \tValidation Loss: 2.633416\n",
      "Epoch: 32205 \tTraining Loss: 1.299219 \tValidation Loss: 2.632135\n",
      "Epoch: 32206 \tTraining Loss: 1.266973 \tValidation Loss: 2.634000\n",
      "Epoch: 32207 \tTraining Loss: 1.327410 \tValidation Loss: 2.632257\n",
      "Epoch: 32208 \tTraining Loss: 1.267991 \tValidation Loss: 2.634624\n",
      "Epoch: 32209 \tTraining Loss: 1.278317 \tValidation Loss: 2.633600\n",
      "Epoch: 32210 \tTraining Loss: 1.296795 \tValidation Loss: 2.632997\n",
      "Epoch: 32211 \tTraining Loss: 1.315567 \tValidation Loss: 2.632312\n",
      "Epoch: 32212 \tTraining Loss: 1.272565 \tValidation Loss: 2.634039\n",
      "Epoch: 32213 \tTraining Loss: 1.264678 \tValidation Loss: 2.633350\n",
      "Epoch: 32214 \tTraining Loss: 1.326664 \tValidation Loss: 2.632983\n",
      "Epoch: 32215 \tTraining Loss: 1.286764 \tValidation Loss: 2.634275\n",
      "Epoch: 32216 \tTraining Loss: 1.292432 \tValidation Loss: 2.634370\n",
      "Epoch: 32217 \tTraining Loss: 1.225863 \tValidation Loss: 2.634973\n",
      "Epoch: 32218 \tTraining Loss: 1.252939 \tValidation Loss: 2.634832\n",
      "Epoch: 32219 \tTraining Loss: 1.234384 \tValidation Loss: 2.635849\n",
      "Epoch: 32220 \tTraining Loss: 1.273921 \tValidation Loss: 2.635258\n",
      "Epoch: 32221 \tTraining Loss: 1.252209 \tValidation Loss: 2.634660\n",
      "Epoch: 32222 \tTraining Loss: 1.213234 \tValidation Loss: 2.634252\n",
      "Epoch: 32223 \tTraining Loss: 1.255524 \tValidation Loss: 2.634647\n",
      "Epoch: 32224 \tTraining Loss: 1.246898 \tValidation Loss: 2.634042\n",
      "Epoch: 32225 \tTraining Loss: 1.299396 \tValidation Loss: 2.632907\n",
      "Epoch: 32226 \tTraining Loss: 1.311178 \tValidation Loss: 2.633707\n",
      "Epoch: 32227 \tTraining Loss: 1.294903 \tValidation Loss: 2.633401\n",
      "Epoch: 32228 \tTraining Loss: 1.269076 \tValidation Loss: 2.634315\n",
      "Epoch: 32229 \tTraining Loss: 1.220658 \tValidation Loss: 2.635963\n",
      "Epoch: 32230 \tTraining Loss: 1.338161 \tValidation Loss: 2.633311\n",
      "Epoch: 32231 \tTraining Loss: 1.276030 \tValidation Loss: 2.633421\n",
      "Epoch: 32232 \tTraining Loss: 1.278803 \tValidation Loss: 2.634063\n",
      "Epoch: 32233 \tTraining Loss: 1.271224 \tValidation Loss: 2.634746\n",
      "Epoch: 32234 \tTraining Loss: 1.242571 \tValidation Loss: 2.634429\n",
      "Epoch: 32235 \tTraining Loss: 1.322953 \tValidation Loss: 2.635866\n",
      "Epoch: 32236 \tTraining Loss: 1.261153 \tValidation Loss: 2.635305\n",
      "Epoch: 32237 \tTraining Loss: 1.293423 \tValidation Loss: 2.634399\n",
      "Epoch: 32238 \tTraining Loss: 1.285196 \tValidation Loss: 2.634763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32239 \tTraining Loss: 1.291924 \tValidation Loss: 2.633874\n",
      "Epoch: 32240 \tTraining Loss: 1.262158 \tValidation Loss: 2.636082\n",
      "Epoch: 32241 \tTraining Loss: 1.222785 \tValidation Loss: 2.634334\n",
      "Epoch: 32242 \tTraining Loss: 1.316965 \tValidation Loss: 2.633397\n",
      "Epoch: 32243 \tTraining Loss: 1.260386 \tValidation Loss: 2.635897\n",
      "Epoch: 32244 \tTraining Loss: 1.260376 \tValidation Loss: 2.634845\n",
      "Epoch: 32245 \tTraining Loss: 1.251818 \tValidation Loss: 2.634877\n",
      "Epoch: 32246 \tTraining Loss: 1.280532 \tValidation Loss: 2.635466\n",
      "Epoch: 32247 \tTraining Loss: 1.266943 \tValidation Loss: 2.634357\n",
      "Epoch: 32248 \tTraining Loss: 1.287848 \tValidation Loss: 2.635971\n",
      "Epoch: 32249 \tTraining Loss: 1.286253 \tValidation Loss: 2.634722\n",
      "Epoch: 32250 \tTraining Loss: 1.257117 \tValidation Loss: 2.634640\n",
      "Epoch: 32251 \tTraining Loss: 1.352847 \tValidation Loss: 2.633851\n",
      "Epoch: 32252 \tTraining Loss: 1.265285 \tValidation Loss: 2.635286\n",
      "Epoch: 32253 \tTraining Loss: 1.258257 \tValidation Loss: 2.633324\n",
      "Epoch: 32254 \tTraining Loss: 1.276492 \tValidation Loss: 2.634458\n",
      "Epoch: 32255 \tTraining Loss: 1.268870 \tValidation Loss: 2.635737\n",
      "Epoch: 32256 \tTraining Loss: 1.279853 \tValidation Loss: 2.635620\n",
      "Epoch: 32257 \tTraining Loss: 1.231214 \tValidation Loss: 2.635865\n",
      "Epoch: 32258 \tTraining Loss: 1.251244 \tValidation Loss: 2.635647\n",
      "Epoch: 32259 \tTraining Loss: 1.217008 \tValidation Loss: 2.637616\n",
      "Epoch: 32260 \tTraining Loss: 1.315059 \tValidation Loss: 2.634106\n",
      "Epoch: 32261 \tTraining Loss: 1.255934 \tValidation Loss: 2.635359\n",
      "Epoch: 32262 \tTraining Loss: 1.293046 \tValidation Loss: 2.634990\n",
      "Epoch: 32263 \tTraining Loss: 1.310611 \tValidation Loss: 2.635101\n",
      "Epoch: 32264 \tTraining Loss: 1.276823 \tValidation Loss: 2.635185\n",
      "Epoch: 32265 \tTraining Loss: 1.296274 \tValidation Loss: 2.635104\n",
      "Epoch: 32266 \tTraining Loss: 1.267944 \tValidation Loss: 2.634768\n",
      "Epoch: 32267 \tTraining Loss: 1.207088 \tValidation Loss: 2.636441\n",
      "Epoch: 32268 \tTraining Loss: 1.274288 \tValidation Loss: 2.635544\n",
      "Epoch: 32269 \tTraining Loss: 1.260224 \tValidation Loss: 2.636909\n",
      "Epoch: 32270 \tTraining Loss: 1.256468 \tValidation Loss: 2.637254\n",
      "Epoch: 32271 \tTraining Loss: 1.290752 \tValidation Loss: 2.636385\n",
      "Epoch: 32272 \tTraining Loss: 1.296683 \tValidation Loss: 2.634224\n",
      "Epoch: 32273 \tTraining Loss: 1.272714 \tValidation Loss: 2.636354\n",
      "Epoch: 32274 \tTraining Loss: 1.215841 \tValidation Loss: 2.636125\n",
      "Epoch: 32275 \tTraining Loss: 1.303413 \tValidation Loss: 2.636573\n",
      "Epoch: 32276 \tTraining Loss: 1.298507 \tValidation Loss: 2.634908\n",
      "Epoch: 32277 \tTraining Loss: 1.286338 \tValidation Loss: 2.635189\n",
      "Epoch: 32278 \tTraining Loss: 1.297728 \tValidation Loss: 2.634977\n",
      "Epoch: 32279 \tTraining Loss: 1.227991 \tValidation Loss: 2.636185\n",
      "Epoch: 32280 \tTraining Loss: 1.287737 \tValidation Loss: 2.636143\n",
      "Epoch: 32281 \tTraining Loss: 1.276631 \tValidation Loss: 2.636442\n",
      "Epoch: 32282 \tTraining Loss: 1.236812 \tValidation Loss: 2.635960\n",
      "Epoch: 32283 \tTraining Loss: 1.234925 \tValidation Loss: 2.635894\n",
      "Epoch: 32284 \tTraining Loss: 1.266397 \tValidation Loss: 2.635666\n",
      "Epoch: 32285 \tTraining Loss: 1.282197 \tValidation Loss: 2.636534\n",
      "Epoch: 32286 \tTraining Loss: 1.249548 \tValidation Loss: 2.635531\n",
      "Epoch: 32287 \tTraining Loss: 1.304813 \tValidation Loss: 2.635000\n",
      "Epoch: 32288 \tTraining Loss: 1.253842 \tValidation Loss: 2.635507\n",
      "Epoch: 32289 \tTraining Loss: 1.305278 \tValidation Loss: 2.634879\n",
      "Epoch: 32290 \tTraining Loss: 1.239739 \tValidation Loss: 2.636591\n",
      "Epoch: 32291 \tTraining Loss: 1.329260 \tValidation Loss: 2.635034\n",
      "Epoch: 32292 \tTraining Loss: 1.265435 \tValidation Loss: 2.635992\n",
      "Epoch: 32293 \tTraining Loss: 1.239754 \tValidation Loss: 2.636323\n",
      "Epoch: 32294 \tTraining Loss: 1.233593 \tValidation Loss: 2.637108\n",
      "Epoch: 32295 \tTraining Loss: 1.243997 \tValidation Loss: 2.636321\n",
      "Epoch: 32296 \tTraining Loss: 1.277695 \tValidation Loss: 2.636596\n",
      "Epoch: 32297 \tTraining Loss: 1.268998 \tValidation Loss: 2.637054\n",
      "Epoch: 32298 \tTraining Loss: 1.305303 \tValidation Loss: 2.636354\n",
      "Epoch: 32299 \tTraining Loss: 1.302730 \tValidation Loss: 2.636461\n",
      "Epoch: 32300 \tTraining Loss: 1.244458 \tValidation Loss: 2.636331\n",
      "Epoch: 32301 \tTraining Loss: 1.268309 \tValidation Loss: 2.635510\n",
      "Epoch: 32302 \tTraining Loss: 1.251966 \tValidation Loss: 2.635956\n",
      "Epoch: 32303 \tTraining Loss: 1.265092 \tValidation Loss: 2.635952\n",
      "Epoch: 32304 \tTraining Loss: 1.238886 \tValidation Loss: 2.638800\n",
      "Epoch: 32305 \tTraining Loss: 1.230387 \tValidation Loss: 2.636869\n",
      "Epoch: 32306 \tTraining Loss: 1.263271 \tValidation Loss: 2.637471\n",
      "Epoch: 32307 \tTraining Loss: 1.306902 \tValidation Loss: 2.634810\n",
      "Epoch: 32308 \tTraining Loss: 1.250531 \tValidation Loss: 2.636415\n",
      "Epoch: 32309 \tTraining Loss: 1.243858 \tValidation Loss: 2.636910\n",
      "Epoch: 32310 \tTraining Loss: 1.271504 \tValidation Loss: 2.636076\n",
      "Epoch: 32311 \tTraining Loss: 1.309387 \tValidation Loss: 2.636386\n",
      "Epoch: 32312 \tTraining Loss: 1.269322 \tValidation Loss: 2.635635\n",
      "Epoch: 32313 \tTraining Loss: 1.243616 \tValidation Loss: 2.636044\n",
      "Epoch: 32314 \tTraining Loss: 1.240430 \tValidation Loss: 2.636744\n",
      "Epoch: 32315 \tTraining Loss: 1.265799 \tValidation Loss: 2.637827\n",
      "Epoch: 32316 \tTraining Loss: 1.237133 \tValidation Loss: 2.637116\n",
      "Epoch: 32317 \tTraining Loss: 1.297085 \tValidation Loss: 2.636985\n",
      "Epoch: 32318 \tTraining Loss: 1.272018 \tValidation Loss: 2.636567\n",
      "Epoch: 32319 \tTraining Loss: 1.266938 \tValidation Loss: 2.636423\n",
      "Epoch: 32320 \tTraining Loss: 1.275026 \tValidation Loss: 2.636378\n",
      "Epoch: 32321 \tTraining Loss: 1.243240 \tValidation Loss: 2.637241\n",
      "Epoch: 32322 \tTraining Loss: 1.300756 \tValidation Loss: 2.636297\n",
      "Epoch: 32323 \tTraining Loss: 1.254496 \tValidation Loss: 2.637829\n",
      "Epoch: 32324 \tTraining Loss: 1.322752 \tValidation Loss: 2.636985\n",
      "Epoch: 32325 \tTraining Loss: 1.282864 \tValidation Loss: 2.637418\n",
      "Epoch: 32326 \tTraining Loss: 1.278529 \tValidation Loss: 2.634436\n",
      "Epoch: 32327 \tTraining Loss: 1.277870 \tValidation Loss: 2.635916\n",
      "Epoch: 32328 \tTraining Loss: 1.254724 \tValidation Loss: 2.636857\n",
      "Epoch: 32329 \tTraining Loss: 1.275536 \tValidation Loss: 2.636611\n",
      "Epoch: 32330 \tTraining Loss: 1.312645 \tValidation Loss: 2.634804\n",
      "Epoch: 32331 \tTraining Loss: 1.251881 \tValidation Loss: 2.636017\n",
      "Epoch: 32332 \tTraining Loss: 1.289746 \tValidation Loss: 2.635451\n",
      "Epoch: 32333 \tTraining Loss: 1.274064 \tValidation Loss: 2.635651\n",
      "Epoch: 32334 \tTraining Loss: 1.275188 \tValidation Loss: 2.637864\n",
      "Epoch: 32335 \tTraining Loss: 1.277566 \tValidation Loss: 2.635024\n",
      "Epoch: 32336 \tTraining Loss: 1.259963 \tValidation Loss: 2.636770\n",
      "Epoch: 32337 \tTraining Loss: 1.258374 \tValidation Loss: 2.635942\n",
      "Epoch: 32338 \tTraining Loss: 1.253365 \tValidation Loss: 2.637034\n",
      "Epoch: 32339 \tTraining Loss: 1.260321 \tValidation Loss: 2.636545\n",
      "Epoch: 32340 \tTraining Loss: 1.304810 \tValidation Loss: 2.636827\n",
      "Epoch: 32341 \tTraining Loss: 1.226783 \tValidation Loss: 2.638281\n",
      "Epoch: 32342 \tTraining Loss: 1.273232 \tValidation Loss: 2.636958\n",
      "Epoch: 32343 \tTraining Loss: 1.239567 \tValidation Loss: 2.636785\n",
      "Epoch: 32344 \tTraining Loss: 1.269636 \tValidation Loss: 2.637690\n",
      "Epoch: 32345 \tTraining Loss: 1.248561 \tValidation Loss: 2.637522\n",
      "Epoch: 32346 \tTraining Loss: 1.255775 \tValidation Loss: 2.637473\n",
      "Epoch: 32347 \tTraining Loss: 1.255522 \tValidation Loss: 2.638217\n",
      "Epoch: 32348 \tTraining Loss: 1.263046 \tValidation Loss: 2.636560\n",
      "Epoch: 32349 \tTraining Loss: 1.245914 \tValidation Loss: 2.637279\n",
      "Epoch: 32350 \tTraining Loss: 1.290489 \tValidation Loss: 2.636809\n",
      "Epoch: 32351 \tTraining Loss: 1.242656 \tValidation Loss: 2.636519\n",
      "Epoch: 32352 \tTraining Loss: 1.261073 \tValidation Loss: 2.636698\n",
      "Epoch: 32353 \tTraining Loss: 1.249959 \tValidation Loss: 2.636460\n",
      "Epoch: 32354 \tTraining Loss: 1.282050 \tValidation Loss: 2.636520\n",
      "Epoch: 32355 \tTraining Loss: 1.284348 \tValidation Loss: 2.636968\n",
      "Epoch: 32356 \tTraining Loss: 1.274679 \tValidation Loss: 2.636100\n",
      "Epoch: 32357 \tTraining Loss: 1.234477 \tValidation Loss: 2.637930\n",
      "Epoch: 32358 \tTraining Loss: 1.279431 \tValidation Loss: 2.637505\n",
      "Epoch: 32359 \tTraining Loss: 1.264283 \tValidation Loss: 2.639148\n",
      "Epoch: 32360 \tTraining Loss: 1.264578 \tValidation Loss: 2.636171\n",
      "Epoch: 32361 \tTraining Loss: 1.285102 \tValidation Loss: 2.637541\n",
      "Epoch: 32362 \tTraining Loss: 1.272110 \tValidation Loss: 2.637632\n",
      "Epoch: 32363 \tTraining Loss: 1.233210 \tValidation Loss: 2.637534\n",
      "Epoch: 32364 \tTraining Loss: 1.322567 \tValidation Loss: 2.637606\n",
      "Epoch: 32365 \tTraining Loss: 1.270573 \tValidation Loss: 2.637129\n",
      "Epoch: 32366 \tTraining Loss: 1.302937 \tValidation Loss: 2.636854\n",
      "Epoch: 32367 \tTraining Loss: 1.280521 \tValidation Loss: 2.636924\n",
      "Epoch: 32368 \tTraining Loss: 1.318140 \tValidation Loss: 2.637637\n",
      "Epoch: 32369 \tTraining Loss: 1.232358 \tValidation Loss: 2.639135\n",
      "Epoch: 32370 \tTraining Loss: 1.308860 \tValidation Loss: 2.636311\n",
      "Epoch: 32371 \tTraining Loss: 1.284008 \tValidation Loss: 2.638185\n",
      "Epoch: 32372 \tTraining Loss: 1.298327 \tValidation Loss: 2.636711\n",
      "Epoch: 32373 \tTraining Loss: 1.259397 \tValidation Loss: 2.638088\n",
      "Epoch: 32374 \tTraining Loss: 1.297534 \tValidation Loss: 2.637295\n",
      "Epoch: 32375 \tTraining Loss: 1.280744 \tValidation Loss: 2.634897\n",
      "Epoch: 32376 \tTraining Loss: 1.268646 \tValidation Loss: 2.636539\n",
      "Epoch: 32377 \tTraining Loss: 1.252671 \tValidation Loss: 2.637828\n",
      "Epoch: 32378 \tTraining Loss: 1.289370 \tValidation Loss: 2.637961\n",
      "Epoch: 32379 \tTraining Loss: 1.283235 \tValidation Loss: 2.637926\n",
      "Epoch: 32380 \tTraining Loss: 1.268772 \tValidation Loss: 2.637267\n",
      "Epoch: 32381 \tTraining Loss: 1.215213 \tValidation Loss: 2.638822\n",
      "Epoch: 32382 \tTraining Loss: 1.288274 \tValidation Loss: 2.637881\n",
      "Epoch: 32383 \tTraining Loss: 1.250458 \tValidation Loss: 2.638113\n",
      "Epoch: 32384 \tTraining Loss: 1.268086 \tValidation Loss: 2.638574\n",
      "Epoch: 32385 \tTraining Loss: 1.293918 \tValidation Loss: 2.637823\n",
      "Epoch: 32386 \tTraining Loss: 1.304600 \tValidation Loss: 2.638475\n",
      "Epoch: 32387 \tTraining Loss: 1.265213 \tValidation Loss: 2.638302\n",
      "Epoch: 32388 \tTraining Loss: 1.282811 \tValidation Loss: 2.639430\n",
      "Epoch: 32389 \tTraining Loss: 1.285957 \tValidation Loss: 2.638467\n",
      "Epoch: 32390 \tTraining Loss: 1.281947 \tValidation Loss: 2.637144\n",
      "Epoch: 32391 \tTraining Loss: 1.267540 \tValidation Loss: 2.636466\n",
      "Epoch: 32392 \tTraining Loss: 1.277305 \tValidation Loss: 2.637743\n",
      "Epoch: 32393 \tTraining Loss: 1.262761 \tValidation Loss: 2.638187\n",
      "Epoch: 32394 \tTraining Loss: 1.287309 \tValidation Loss: 2.638743\n",
      "Epoch: 32395 \tTraining Loss: 1.251918 \tValidation Loss: 2.639197\n",
      "Epoch: 32396 \tTraining Loss: 1.289693 \tValidation Loss: 2.638433\n",
      "Epoch: 32397 \tTraining Loss: 1.243549 \tValidation Loss: 2.638968\n",
      "Epoch: 32398 \tTraining Loss: 1.282728 \tValidation Loss: 2.637493\n",
      "Epoch: 32399 \tTraining Loss: 1.267473 \tValidation Loss: 2.638105\n",
      "Epoch: 32400 \tTraining Loss: 1.272644 \tValidation Loss: 2.638153\n",
      "Epoch: 32401 \tTraining Loss: 1.338506 \tValidation Loss: 2.638245\n",
      "Epoch: 32402 \tTraining Loss: 1.267291 \tValidation Loss: 2.640131\n",
      "Epoch: 32403 \tTraining Loss: 1.323743 \tValidation Loss: 2.637362\n",
      "Epoch: 32404 \tTraining Loss: 1.250521 \tValidation Loss: 2.638729\n",
      "Epoch: 32405 \tTraining Loss: 1.263966 \tValidation Loss: 2.637663\n",
      "Epoch: 32406 \tTraining Loss: 1.314195 \tValidation Loss: 2.636786\n",
      "Epoch: 32407 \tTraining Loss: 1.303254 \tValidation Loss: 2.637845\n",
      "Epoch: 32408 \tTraining Loss: 1.258511 \tValidation Loss: 2.638540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32409 \tTraining Loss: 1.279245 \tValidation Loss: 2.638775\n",
      "Epoch: 32410 \tTraining Loss: 1.260104 \tValidation Loss: 2.638664\n",
      "Epoch: 32411 \tTraining Loss: 1.253170 \tValidation Loss: 2.639147\n",
      "Epoch: 32412 \tTraining Loss: 1.262337 \tValidation Loss: 2.638530\n",
      "Epoch: 32413 \tTraining Loss: 1.286212 \tValidation Loss: 2.638657\n",
      "Epoch: 32414 \tTraining Loss: 1.313574 \tValidation Loss: 2.637827\n",
      "Epoch: 32415 \tTraining Loss: 1.343388 \tValidation Loss: 2.636517\n",
      "Epoch: 32416 \tTraining Loss: 1.265468 \tValidation Loss: 2.638277\n",
      "Epoch: 32417 \tTraining Loss: 1.276509 \tValidation Loss: 2.637733\n",
      "Epoch: 32418 \tTraining Loss: 1.251886 \tValidation Loss: 2.638102\n",
      "Epoch: 32419 \tTraining Loss: 1.265685 \tValidation Loss: 2.640017\n",
      "Epoch: 32420 \tTraining Loss: 1.257060 \tValidation Loss: 2.637750\n",
      "Epoch: 32421 \tTraining Loss: 1.241005 \tValidation Loss: 2.639389\n",
      "Epoch: 32422 \tTraining Loss: 1.272179 \tValidation Loss: 2.637959\n",
      "Epoch: 32423 \tTraining Loss: 1.292262 \tValidation Loss: 2.638643\n",
      "Epoch: 32424 \tTraining Loss: 1.233108 \tValidation Loss: 2.638917\n",
      "Epoch: 32425 \tTraining Loss: 1.304387 \tValidation Loss: 2.637060\n",
      "Epoch: 32426 \tTraining Loss: 1.300005 \tValidation Loss: 2.637508\n",
      "Epoch: 32427 \tTraining Loss: 1.304211 \tValidation Loss: 2.638970\n",
      "Epoch: 32428 \tTraining Loss: 1.247617 \tValidation Loss: 2.639574\n",
      "Epoch: 32429 \tTraining Loss: 1.292135 \tValidation Loss: 2.638431\n",
      "Epoch: 32430 \tTraining Loss: 1.283775 \tValidation Loss: 2.639251\n",
      "Epoch: 32431 \tTraining Loss: 1.286004 \tValidation Loss: 2.639004\n",
      "Epoch: 32432 \tTraining Loss: 1.297571 \tValidation Loss: 2.639576\n",
      "Epoch: 32433 \tTraining Loss: 1.272020 \tValidation Loss: 2.640338\n",
      "Epoch: 32434 \tTraining Loss: 1.281271 \tValidation Loss: 2.638747\n",
      "Epoch: 32435 \tTraining Loss: 1.272757 \tValidation Loss: 2.638708\n",
      "Epoch: 32436 \tTraining Loss: 1.298154 \tValidation Loss: 2.639807\n",
      "Epoch: 32437 \tTraining Loss: 1.250687 \tValidation Loss: 2.639002\n",
      "Epoch: 32438 \tTraining Loss: 1.217718 \tValidation Loss: 2.639781\n",
      "Epoch: 32439 \tTraining Loss: 1.279687 \tValidation Loss: 2.640354\n",
      "Epoch: 32440 \tTraining Loss: 1.244888 \tValidation Loss: 2.639536\n",
      "Epoch: 32441 \tTraining Loss: 1.268698 \tValidation Loss: 2.638708\n",
      "Epoch: 32442 \tTraining Loss: 1.292621 \tValidation Loss: 2.639051\n",
      "Epoch: 32443 \tTraining Loss: 1.258444 \tValidation Loss: 2.639527\n",
      "Epoch: 32444 \tTraining Loss: 1.211404 \tValidation Loss: 2.640021\n",
      "Epoch: 32445 \tTraining Loss: 1.255940 \tValidation Loss: 2.639518\n",
      "Epoch: 32446 \tTraining Loss: 1.245522 \tValidation Loss: 2.638923\n",
      "Epoch: 32447 \tTraining Loss: 1.275542 \tValidation Loss: 2.639893\n",
      "Epoch: 32448 \tTraining Loss: 1.277417 \tValidation Loss: 2.638856\n",
      "Epoch: 32449 \tTraining Loss: 1.284451 \tValidation Loss: 2.639937\n",
      "Epoch: 32450 \tTraining Loss: 1.296307 \tValidation Loss: 2.638799\n",
      "Epoch: 32451 \tTraining Loss: 1.264468 \tValidation Loss: 2.639191\n",
      "Epoch: 32452 \tTraining Loss: 1.292250 \tValidation Loss: 2.639957\n",
      "Epoch: 32453 \tTraining Loss: 1.298264 \tValidation Loss: 2.640091\n",
      "Epoch: 32454 \tTraining Loss: 1.296338 \tValidation Loss: 2.639720\n",
      "Epoch: 32455 \tTraining Loss: 1.256572 \tValidation Loss: 2.638454\n",
      "Epoch: 32456 \tTraining Loss: 1.268013 \tValidation Loss: 2.638906\n",
      "Epoch: 32457 \tTraining Loss: 1.277860 \tValidation Loss: 2.640231\n",
      "Epoch: 32458 \tTraining Loss: 1.301153 \tValidation Loss: 2.639015\n",
      "Epoch: 32459 \tTraining Loss: 1.243832 \tValidation Loss: 2.638905\n",
      "Epoch: 32460 \tTraining Loss: 1.269603 \tValidation Loss: 2.639273\n",
      "Epoch: 32461 \tTraining Loss: 1.269074 \tValidation Loss: 2.639078\n",
      "Epoch: 32462 \tTraining Loss: 1.286923 \tValidation Loss: 2.641524\n",
      "Epoch: 32463 \tTraining Loss: 1.274215 \tValidation Loss: 2.640437\n",
      "Epoch: 32464 \tTraining Loss: 1.250337 \tValidation Loss: 2.640830\n",
      "Epoch: 32465 \tTraining Loss: 1.274871 \tValidation Loss: 2.641605\n",
      "Epoch: 32466 \tTraining Loss: 1.267228 \tValidation Loss: 2.641054\n",
      "Epoch: 32467 \tTraining Loss: 1.223352 \tValidation Loss: 2.642104\n",
      "Epoch: 32468 \tTraining Loss: 1.263481 \tValidation Loss: 2.640377\n",
      "Epoch: 32469 \tTraining Loss: 1.304897 \tValidation Loss: 2.640579\n",
      "Epoch: 32470 \tTraining Loss: 1.263284 \tValidation Loss: 2.639962\n",
      "Epoch: 32471 \tTraining Loss: 1.278953 \tValidation Loss: 2.639888\n",
      "Epoch: 32472 \tTraining Loss: 1.303237 \tValidation Loss: 2.639868\n",
      "Epoch: 32473 \tTraining Loss: 1.219219 \tValidation Loss: 2.640050\n",
      "Epoch: 32474 \tTraining Loss: 1.248558 \tValidation Loss: 2.641821\n",
      "Epoch: 32475 \tTraining Loss: 1.296573 \tValidation Loss: 2.640216\n",
      "Epoch: 32476 \tTraining Loss: 1.268421 \tValidation Loss: 2.640576\n",
      "Epoch: 32477 \tTraining Loss: 1.270324 \tValidation Loss: 2.639532\n",
      "Epoch: 32478 \tTraining Loss: 1.279640 \tValidation Loss: 2.639836\n",
      "Epoch: 32479 \tTraining Loss: 1.234229 \tValidation Loss: 2.641752\n",
      "Epoch: 32480 \tTraining Loss: 1.254680 \tValidation Loss: 2.642098\n",
      "Epoch: 32481 \tTraining Loss: 1.258502 \tValidation Loss: 2.641356\n",
      "Epoch: 32482 \tTraining Loss: 1.281258 \tValidation Loss: 2.641533\n",
      "Epoch: 32483 \tTraining Loss: 1.245900 \tValidation Loss: 2.641451\n",
      "Epoch: 32484 \tTraining Loss: 1.240903 \tValidation Loss: 2.641197\n",
      "Epoch: 32485 \tTraining Loss: 1.279702 \tValidation Loss: 2.641188\n",
      "Epoch: 32486 \tTraining Loss: 1.306569 \tValidation Loss: 2.639497\n",
      "Epoch: 32487 \tTraining Loss: 1.225969 \tValidation Loss: 2.642150\n",
      "Epoch: 32488 \tTraining Loss: 1.251080 \tValidation Loss: 2.640182\n",
      "Epoch: 32489 \tTraining Loss: 1.255531 \tValidation Loss: 2.641952\n",
      "Epoch: 32490 \tTraining Loss: 1.272500 \tValidation Loss: 2.642645\n",
      "Epoch: 32491 \tTraining Loss: 1.228431 \tValidation Loss: 2.640709\n",
      "Epoch: 32492 \tTraining Loss: 1.230026 \tValidation Loss: 2.640531\n",
      "Epoch: 32493 \tTraining Loss: 1.276844 \tValidation Loss: 2.641441\n",
      "Epoch: 32494 \tTraining Loss: 1.288887 \tValidation Loss: 2.640519\n",
      "Epoch: 32495 \tTraining Loss: 1.243341 \tValidation Loss: 2.640575\n",
      "Epoch: 32496 \tTraining Loss: 1.262778 \tValidation Loss: 2.642127\n",
      "Epoch: 32497 \tTraining Loss: 1.305007 \tValidation Loss: 2.641489\n",
      "Epoch: 32498 \tTraining Loss: 1.290635 \tValidation Loss: 2.641792\n",
      "Epoch: 32499 \tTraining Loss: 1.291480 \tValidation Loss: 2.642286\n",
      "Epoch: 32500 \tTraining Loss: 1.267939 \tValidation Loss: 2.641440\n",
      "Epoch: 32501 \tTraining Loss: 1.279552 \tValidation Loss: 2.642497\n",
      "Epoch: 32502 \tTraining Loss: 1.221173 \tValidation Loss: 2.642009\n",
      "Epoch: 32503 \tTraining Loss: 1.266862 \tValidation Loss: 2.642080\n",
      "Epoch: 32504 \tTraining Loss: 1.309836 \tValidation Loss: 2.641590\n",
      "Epoch: 32505 \tTraining Loss: 1.283504 \tValidation Loss: 2.640683\n",
      "Epoch: 32506 \tTraining Loss: 1.290828 \tValidation Loss: 2.641123\n",
      "Epoch: 32507 \tTraining Loss: 1.277288 \tValidation Loss: 2.642263\n",
      "Epoch: 32508 \tTraining Loss: 1.236452 \tValidation Loss: 2.641765\n",
      "Epoch: 32509 \tTraining Loss: 1.287448 \tValidation Loss: 2.642399\n",
      "Epoch: 32510 \tTraining Loss: 1.278111 \tValidation Loss: 2.642500\n",
      "Epoch: 32511 \tTraining Loss: 1.247988 \tValidation Loss: 2.642531\n",
      "Epoch: 32512 \tTraining Loss: 1.233902 \tValidation Loss: 2.641836\n",
      "Epoch: 32513 \tTraining Loss: 1.278381 \tValidation Loss: 2.641622\n",
      "Epoch: 32514 \tTraining Loss: 1.250463 \tValidation Loss: 2.642430\n",
      "Epoch: 32515 \tTraining Loss: 1.254918 \tValidation Loss: 2.641751\n",
      "Epoch: 32516 \tTraining Loss: 1.234414 \tValidation Loss: 2.643293\n",
      "Epoch: 32517 \tTraining Loss: 1.290242 \tValidation Loss: 2.641833\n",
      "Epoch: 32518 \tTraining Loss: 1.247064 \tValidation Loss: 2.643459\n",
      "Epoch: 32519 \tTraining Loss: 1.279215 \tValidation Loss: 2.641595\n",
      "Epoch: 32520 \tTraining Loss: 1.289012 \tValidation Loss: 2.641927\n",
      "Epoch: 32521 \tTraining Loss: 1.268485 \tValidation Loss: 2.642526\n",
      "Epoch: 32522 \tTraining Loss: 1.268462 \tValidation Loss: 2.640887\n",
      "Epoch: 32523 \tTraining Loss: 1.319760 \tValidation Loss: 2.641857\n",
      "Epoch: 32524 \tTraining Loss: 1.251155 \tValidation Loss: 2.642561\n",
      "Epoch: 32525 \tTraining Loss: 1.227116 \tValidation Loss: 2.643238\n",
      "Epoch: 32526 \tTraining Loss: 1.289146 \tValidation Loss: 2.641532\n",
      "Epoch: 32527 \tTraining Loss: 1.277029 \tValidation Loss: 2.641732\n",
      "Epoch: 32528 \tTraining Loss: 1.233550 \tValidation Loss: 2.642198\n",
      "Epoch: 32529 \tTraining Loss: 1.252096 \tValidation Loss: 2.641449\n",
      "Epoch: 32530 \tTraining Loss: 1.305181 \tValidation Loss: 2.640332\n",
      "Epoch: 32531 \tTraining Loss: 1.272781 \tValidation Loss: 2.642064\n",
      "Epoch: 32532 \tTraining Loss: 1.268811 \tValidation Loss: 2.642574\n",
      "Epoch: 32533 \tTraining Loss: 1.263461 \tValidation Loss: 2.640114\n",
      "Epoch: 32534 \tTraining Loss: 1.252284 \tValidation Loss: 2.641308\n",
      "Epoch: 32535 \tTraining Loss: 1.217974 \tValidation Loss: 2.640984\n",
      "Epoch: 32536 \tTraining Loss: 1.282767 \tValidation Loss: 2.641531\n",
      "Epoch: 32537 \tTraining Loss: 1.253265 \tValidation Loss: 2.641786\n",
      "Epoch: 32538 \tTraining Loss: 1.277286 \tValidation Loss: 2.643296\n",
      "Epoch: 32539 \tTraining Loss: 1.261958 \tValidation Loss: 2.643032\n",
      "Epoch: 32540 \tTraining Loss: 1.310989 \tValidation Loss: 2.641890\n",
      "Epoch: 32541 \tTraining Loss: 1.265514 \tValidation Loss: 2.641860\n",
      "Epoch: 32542 \tTraining Loss: 1.278520 \tValidation Loss: 2.642694\n",
      "Epoch: 32543 \tTraining Loss: 1.239777 \tValidation Loss: 2.641892\n",
      "Epoch: 32544 \tTraining Loss: 1.288130 \tValidation Loss: 2.641497\n",
      "Epoch: 32545 \tTraining Loss: 1.262091 \tValidation Loss: 2.642602\n",
      "Epoch: 32546 \tTraining Loss: 1.284464 \tValidation Loss: 2.641484\n",
      "Epoch: 32547 \tTraining Loss: 1.262593 \tValidation Loss: 2.641720\n",
      "Epoch: 32548 \tTraining Loss: 1.299336 \tValidation Loss: 2.641258\n",
      "Epoch: 32549 \tTraining Loss: 1.277620 \tValidation Loss: 2.642624\n",
      "Epoch: 32550 \tTraining Loss: 1.306330 \tValidation Loss: 2.641018\n",
      "Epoch: 32551 \tTraining Loss: 1.266268 \tValidation Loss: 2.640007\n",
      "Epoch: 32552 \tTraining Loss: 1.221229 \tValidation Loss: 2.642183\n",
      "Epoch: 32553 \tTraining Loss: 1.233239 \tValidation Loss: 2.641853\n",
      "Epoch: 32554 \tTraining Loss: 1.248770 \tValidation Loss: 2.642373\n",
      "Epoch: 32555 \tTraining Loss: 1.270026 \tValidation Loss: 2.640922\n",
      "Epoch: 32556 \tTraining Loss: 1.250393 \tValidation Loss: 2.642164\n",
      "Epoch: 32557 \tTraining Loss: 1.266853 \tValidation Loss: 2.643202\n",
      "Epoch: 32558 \tTraining Loss: 1.248004 \tValidation Loss: 2.642253\n",
      "Epoch: 32559 \tTraining Loss: 1.279516 \tValidation Loss: 2.642374\n",
      "Epoch: 32560 \tTraining Loss: 1.277201 \tValidation Loss: 2.642050\n",
      "Epoch: 32561 \tTraining Loss: 1.284635 \tValidation Loss: 2.640783\n",
      "Epoch: 32562 \tTraining Loss: 1.273637 \tValidation Loss: 2.642362\n",
      "Epoch: 32563 \tTraining Loss: 1.285178 \tValidation Loss: 2.642946\n",
      "Epoch: 32564 \tTraining Loss: 1.217296 \tValidation Loss: 2.644199\n",
      "Epoch: 32565 \tTraining Loss: 1.262112 \tValidation Loss: 2.642592\n",
      "Epoch: 32566 \tTraining Loss: 1.263452 \tValidation Loss: 2.641951\n",
      "Epoch: 32567 \tTraining Loss: 1.259496 \tValidation Loss: 2.643511\n",
      "Epoch: 32568 \tTraining Loss: 1.277062 \tValidation Loss: 2.642330\n",
      "Epoch: 32569 \tTraining Loss: 1.228481 \tValidation Loss: 2.643557\n",
      "Epoch: 32570 \tTraining Loss: 1.275921 \tValidation Loss: 2.643330\n",
      "Epoch: 32571 \tTraining Loss: 1.265051 \tValidation Loss: 2.642814\n",
      "Epoch: 32572 \tTraining Loss: 1.222669 \tValidation Loss: 2.642525\n",
      "Epoch: 32573 \tTraining Loss: 1.315441 \tValidation Loss: 2.643652\n",
      "Epoch: 32574 \tTraining Loss: 1.276872 \tValidation Loss: 2.643337\n",
      "Epoch: 32575 \tTraining Loss: 1.272630 \tValidation Loss: 2.641809\n",
      "Epoch: 32576 \tTraining Loss: 1.215935 \tValidation Loss: 2.643162\n",
      "Epoch: 32577 \tTraining Loss: 1.267657 \tValidation Loss: 2.642917\n",
      "Epoch: 32578 \tTraining Loss: 1.255049 \tValidation Loss: 2.643084\n",
      "Epoch: 32579 \tTraining Loss: 1.311237 \tValidation Loss: 2.641030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32580 \tTraining Loss: 1.310734 \tValidation Loss: 2.642854\n",
      "Epoch: 32581 \tTraining Loss: 1.283440 \tValidation Loss: 2.642187\n",
      "Epoch: 32582 \tTraining Loss: 1.247581 \tValidation Loss: 2.642292\n",
      "Epoch: 32583 \tTraining Loss: 1.258310 \tValidation Loss: 2.642483\n",
      "Epoch: 32584 \tTraining Loss: 1.296836 \tValidation Loss: 2.643231\n",
      "Epoch: 32585 \tTraining Loss: 1.227826 \tValidation Loss: 2.641576\n",
      "Epoch: 32586 \tTraining Loss: 1.254949 \tValidation Loss: 2.643970\n",
      "Epoch: 32587 \tTraining Loss: 1.233197 \tValidation Loss: 2.642468\n",
      "Epoch: 32588 \tTraining Loss: 1.214502 \tValidation Loss: 2.643609\n",
      "Epoch: 32589 \tTraining Loss: 1.281829 \tValidation Loss: 2.643081\n",
      "Epoch: 32590 \tTraining Loss: 1.243796 \tValidation Loss: 2.642272\n",
      "Epoch: 32591 \tTraining Loss: 1.262686 \tValidation Loss: 2.644756\n",
      "Epoch: 32592 \tTraining Loss: 1.292156 \tValidation Loss: 2.642864\n",
      "Epoch: 32593 \tTraining Loss: 1.316514 \tValidation Loss: 2.642896\n",
      "Epoch: 32594 \tTraining Loss: 1.260881 \tValidation Loss: 2.642544\n",
      "Epoch: 32595 \tTraining Loss: 1.254693 \tValidation Loss: 2.642182\n",
      "Epoch: 32596 \tTraining Loss: 1.305382 \tValidation Loss: 2.642838\n",
      "Epoch: 32597 \tTraining Loss: 1.282773 \tValidation Loss: 2.640930\n",
      "Epoch: 32598 \tTraining Loss: 1.273378 \tValidation Loss: 2.643534\n",
      "Epoch: 32599 \tTraining Loss: 1.249967 \tValidation Loss: 2.642565\n",
      "Epoch: 32600 \tTraining Loss: 1.230320 \tValidation Loss: 2.643555\n",
      "Epoch: 32601 \tTraining Loss: 1.261253 \tValidation Loss: 2.642558\n",
      "Epoch: 32602 \tTraining Loss: 1.283036 \tValidation Loss: 2.642915\n",
      "Epoch: 32603 \tTraining Loss: 1.291923 \tValidation Loss: 2.642258\n",
      "Epoch: 32604 \tTraining Loss: 1.288857 \tValidation Loss: 2.642749\n",
      "Epoch: 32605 \tTraining Loss: 1.293367 \tValidation Loss: 2.644161\n",
      "Epoch: 32606 \tTraining Loss: 1.257520 \tValidation Loss: 2.644177\n",
      "Epoch: 32607 \tTraining Loss: 1.258140 \tValidation Loss: 2.643549\n",
      "Epoch: 32608 \tTraining Loss: 1.259617 \tValidation Loss: 2.644431\n",
      "Epoch: 32609 \tTraining Loss: 1.287662 \tValidation Loss: 2.642729\n",
      "Epoch: 32610 \tTraining Loss: 1.266500 \tValidation Loss: 2.644736\n",
      "Epoch: 32611 \tTraining Loss: 1.260814 \tValidation Loss: 2.644266\n",
      "Epoch: 32612 \tTraining Loss: 1.301840 \tValidation Loss: 2.643139\n",
      "Epoch: 32613 \tTraining Loss: 1.255220 \tValidation Loss: 2.643142\n",
      "Epoch: 32614 \tTraining Loss: 1.246554 \tValidation Loss: 2.643349\n",
      "Epoch: 32615 \tTraining Loss: 1.149222 \tValidation Loss: 2.644729\n",
      "Epoch: 32616 \tTraining Loss: 1.277693 \tValidation Loss: 2.643295\n",
      "Epoch: 32617 \tTraining Loss: 1.240387 \tValidation Loss: 2.643464\n",
      "Epoch: 32618 \tTraining Loss: 1.258390 \tValidation Loss: 2.643783\n",
      "Epoch: 32619 \tTraining Loss: 1.238968 \tValidation Loss: 2.645221\n",
      "Epoch: 32620 \tTraining Loss: 1.248749 \tValidation Loss: 2.643916\n",
      "Epoch: 32621 \tTraining Loss: 1.283252 \tValidation Loss: 2.644168\n",
      "Epoch: 32622 \tTraining Loss: 1.271581 \tValidation Loss: 2.642414\n",
      "Epoch: 32623 \tTraining Loss: 1.241254 \tValidation Loss: 2.642634\n",
      "Epoch: 32624 \tTraining Loss: 1.285592 \tValidation Loss: 2.643408\n",
      "Epoch: 32625 \tTraining Loss: 1.238065 \tValidation Loss: 2.645062\n",
      "Epoch: 32626 \tTraining Loss: 1.253929 \tValidation Loss: 2.644896\n",
      "Epoch: 32627 \tTraining Loss: 1.287953 \tValidation Loss: 2.643277\n",
      "Epoch: 32628 \tTraining Loss: 1.225091 \tValidation Loss: 2.644103\n",
      "Epoch: 32629 \tTraining Loss: 1.267583 \tValidation Loss: 2.643438\n",
      "Epoch: 32630 \tTraining Loss: 1.295525 \tValidation Loss: 2.643577\n",
      "Epoch: 32631 \tTraining Loss: 1.304962 \tValidation Loss: 2.642750\n",
      "Epoch: 32632 \tTraining Loss: 1.257819 \tValidation Loss: 2.644205\n",
      "Epoch: 32633 \tTraining Loss: 1.314121 \tValidation Loss: 2.644328\n",
      "Epoch: 32634 \tTraining Loss: 1.244612 \tValidation Loss: 2.643311\n",
      "Epoch: 32635 \tTraining Loss: 1.281256 \tValidation Loss: 2.642928\n",
      "Epoch: 32636 \tTraining Loss: 1.210681 \tValidation Loss: 2.643381\n",
      "Epoch: 32637 \tTraining Loss: 1.262015 \tValidation Loss: 2.644547\n",
      "Epoch: 32638 \tTraining Loss: 1.269700 \tValidation Loss: 2.644890\n",
      "Epoch: 32639 \tTraining Loss: 1.278514 \tValidation Loss: 2.643423\n",
      "Epoch: 32640 \tTraining Loss: 1.271959 \tValidation Loss: 2.643740\n",
      "Epoch: 32641 \tTraining Loss: 1.255349 \tValidation Loss: 2.644172\n",
      "Epoch: 32642 \tTraining Loss: 1.265769 \tValidation Loss: 2.644405\n",
      "Epoch: 32643 \tTraining Loss: 1.245881 \tValidation Loss: 2.645657\n",
      "Epoch: 32644 \tTraining Loss: 1.241470 \tValidation Loss: 2.643948\n",
      "Epoch: 32645 \tTraining Loss: 1.230095 \tValidation Loss: 2.645564\n",
      "Epoch: 32646 \tTraining Loss: 1.261098 \tValidation Loss: 2.645084\n",
      "Epoch: 32647 \tTraining Loss: 1.278842 \tValidation Loss: 2.644885\n",
      "Epoch: 32648 \tTraining Loss: 1.288529 \tValidation Loss: 2.643510\n",
      "Epoch: 32649 \tTraining Loss: 1.257571 \tValidation Loss: 2.643885\n",
      "Epoch: 32650 \tTraining Loss: 1.257072 \tValidation Loss: 2.644521\n",
      "Epoch: 32651 \tTraining Loss: 1.237594 \tValidation Loss: 2.643601\n",
      "Epoch: 32652 \tTraining Loss: 1.281243 \tValidation Loss: 2.643864\n",
      "Epoch: 32653 \tTraining Loss: 1.272788 \tValidation Loss: 2.643945\n",
      "Epoch: 32654 \tTraining Loss: 1.282884 \tValidation Loss: 2.643908\n",
      "Epoch: 32655 \tTraining Loss: 1.297837 \tValidation Loss: 2.645070\n",
      "Epoch: 32656 \tTraining Loss: 1.260167 \tValidation Loss: 2.643791\n",
      "Epoch: 32657 \tTraining Loss: 1.282062 \tValidation Loss: 2.644012\n",
      "Epoch: 32658 \tTraining Loss: 1.273563 \tValidation Loss: 2.643543\n",
      "Epoch: 32659 \tTraining Loss: 1.230782 \tValidation Loss: 2.645070\n",
      "Epoch: 32660 \tTraining Loss: 1.276906 \tValidation Loss: 2.644622\n",
      "Epoch: 32661 \tTraining Loss: 1.294184 \tValidation Loss: 2.645877\n",
      "Epoch: 32662 \tTraining Loss: 1.263235 \tValidation Loss: 2.644874\n",
      "Epoch: 32663 \tTraining Loss: 1.286909 \tValidation Loss: 2.644749\n",
      "Epoch: 32664 \tTraining Loss: 1.279370 \tValidation Loss: 2.643506\n",
      "Epoch: 32665 \tTraining Loss: 1.228523 \tValidation Loss: 2.644546\n",
      "Epoch: 32666 \tTraining Loss: 1.159899 \tValidation Loss: 2.646934\n",
      "Epoch: 32667 \tTraining Loss: 1.234730 \tValidation Loss: 2.645586\n",
      "Epoch: 32668 \tTraining Loss: 1.250145 \tValidation Loss: 2.643852\n",
      "Epoch: 32669 \tTraining Loss: 1.220091 \tValidation Loss: 2.646042\n",
      "Epoch: 32670 \tTraining Loss: 1.295253 \tValidation Loss: 2.644890\n",
      "Epoch: 32671 \tTraining Loss: 1.270682 \tValidation Loss: 2.646833\n",
      "Epoch: 32672 \tTraining Loss: 1.246451 \tValidation Loss: 2.645284\n",
      "Epoch: 32673 \tTraining Loss: 1.293957 \tValidation Loss: 2.646115\n",
      "Epoch: 32674 \tTraining Loss: 1.253715 \tValidation Loss: 2.645518\n",
      "Epoch: 32675 \tTraining Loss: 1.320572 \tValidation Loss: 2.643445\n",
      "Epoch: 32676 \tTraining Loss: 1.255706 \tValidation Loss: 2.645838\n",
      "Epoch: 32677 \tTraining Loss: 1.251218 \tValidation Loss: 2.644655\n",
      "Epoch: 32678 \tTraining Loss: 1.254596 \tValidation Loss: 2.645567\n",
      "Epoch: 32679 \tTraining Loss: 1.291018 \tValidation Loss: 2.643730\n",
      "Epoch: 32680 \tTraining Loss: 1.280857 \tValidation Loss: 2.643333\n",
      "Epoch: 32681 \tTraining Loss: 1.241114 \tValidation Loss: 2.645411\n",
      "Epoch: 32682 \tTraining Loss: 1.242938 \tValidation Loss: 2.646091\n",
      "Epoch: 32683 \tTraining Loss: 1.249659 \tValidation Loss: 2.646755\n",
      "Epoch: 32684 \tTraining Loss: 1.248649 \tValidation Loss: 2.647082\n",
      "Epoch: 32685 \tTraining Loss: 1.273927 \tValidation Loss: 2.644751\n",
      "Epoch: 32686 \tTraining Loss: 1.288038 \tValidation Loss: 2.645084\n",
      "Epoch: 32687 \tTraining Loss: 1.248986 \tValidation Loss: 2.645447\n",
      "Epoch: 32688 \tTraining Loss: 1.264766 \tValidation Loss: 2.646377\n",
      "Epoch: 32689 \tTraining Loss: 1.207101 \tValidation Loss: 2.644901\n",
      "Epoch: 32690 \tTraining Loss: 1.243821 \tValidation Loss: 2.645559\n",
      "Epoch: 32691 \tTraining Loss: 1.248016 \tValidation Loss: 2.645911\n",
      "Epoch: 32692 \tTraining Loss: 1.268193 \tValidation Loss: 2.644997\n",
      "Epoch: 32693 \tTraining Loss: 1.241514 \tValidation Loss: 2.645353\n",
      "Epoch: 32694 \tTraining Loss: 1.302080 \tValidation Loss: 2.645992\n",
      "Epoch: 32695 \tTraining Loss: 1.243507 \tValidation Loss: 2.646628\n",
      "Epoch: 32696 \tTraining Loss: 1.280853 \tValidation Loss: 2.646309\n",
      "Epoch: 32697 \tTraining Loss: 1.309281 \tValidation Loss: 2.644753\n",
      "Epoch: 32698 \tTraining Loss: 1.316895 \tValidation Loss: 2.643642\n",
      "Epoch: 32699 \tTraining Loss: 1.245834 \tValidation Loss: 2.646342\n",
      "Epoch: 32700 \tTraining Loss: 1.261918 \tValidation Loss: 2.645972\n",
      "Epoch: 32701 \tTraining Loss: 1.222719 \tValidation Loss: 2.646398\n",
      "Epoch: 32702 \tTraining Loss: 1.292022 \tValidation Loss: 2.646129\n",
      "Epoch: 32703 \tTraining Loss: 1.240503 \tValidation Loss: 2.645886\n",
      "Epoch: 32704 \tTraining Loss: 1.281666 \tValidation Loss: 2.646520\n",
      "Epoch: 32705 \tTraining Loss: 1.316071 \tValidation Loss: 2.644533\n",
      "Epoch: 32706 \tTraining Loss: 1.279008 \tValidation Loss: 2.646119\n",
      "Epoch: 32707 \tTraining Loss: 1.266417 \tValidation Loss: 2.646299\n",
      "Epoch: 32708 \tTraining Loss: 1.265110 \tValidation Loss: 2.645534\n",
      "Epoch: 32709 \tTraining Loss: 1.284230 \tValidation Loss: 2.644511\n",
      "Epoch: 32710 \tTraining Loss: 1.214234 \tValidation Loss: 2.646189\n",
      "Epoch: 32711 \tTraining Loss: 1.307617 \tValidation Loss: 2.645103\n",
      "Epoch: 32712 \tTraining Loss: 1.287563 \tValidation Loss: 2.645335\n",
      "Epoch: 32713 \tTraining Loss: 1.240644 \tValidation Loss: 2.645419\n",
      "Epoch: 32714 \tTraining Loss: 1.255126 \tValidation Loss: 2.645011\n",
      "Epoch: 32715 \tTraining Loss: 1.282425 \tValidation Loss: 2.646099\n",
      "Epoch: 32716 \tTraining Loss: 1.236541 \tValidation Loss: 2.645420\n",
      "Epoch: 32717 \tTraining Loss: 1.256616 \tValidation Loss: 2.645340\n",
      "Epoch: 32718 \tTraining Loss: 1.308268 \tValidation Loss: 2.646432\n",
      "Epoch: 32719 \tTraining Loss: 1.255163 \tValidation Loss: 2.645815\n",
      "Epoch: 32720 \tTraining Loss: 1.209684 \tValidation Loss: 2.647078\n",
      "Epoch: 32721 \tTraining Loss: 1.268949 \tValidation Loss: 2.647086\n",
      "Epoch: 32722 \tTraining Loss: 1.207470 \tValidation Loss: 2.647680\n",
      "Epoch: 32723 \tTraining Loss: 1.226568 \tValidation Loss: 2.645437\n",
      "Epoch: 32724 \tTraining Loss: 1.269543 \tValidation Loss: 2.645698\n",
      "Epoch: 32725 \tTraining Loss: 1.248858 \tValidation Loss: 2.646132\n",
      "Epoch: 32726 \tTraining Loss: 1.244449 \tValidation Loss: 2.646013\n",
      "Epoch: 32727 \tTraining Loss: 1.266188 \tValidation Loss: 2.646097\n",
      "Epoch: 32728 \tTraining Loss: 1.275789 \tValidation Loss: 2.646070\n",
      "Epoch: 32729 \tTraining Loss: 1.271556 \tValidation Loss: 2.645715\n",
      "Epoch: 32730 \tTraining Loss: 1.248671 \tValidation Loss: 2.646475\n",
      "Epoch: 32731 \tTraining Loss: 1.204779 \tValidation Loss: 2.647933\n",
      "Epoch: 32732 \tTraining Loss: 1.243731 \tValidation Loss: 2.646914\n",
      "Epoch: 32733 \tTraining Loss: 1.272531 \tValidation Loss: 2.647907\n",
      "Epoch: 32734 \tTraining Loss: 1.258121 \tValidation Loss: 2.647745\n",
      "Epoch: 32735 \tTraining Loss: 1.297459 \tValidation Loss: 2.645978\n",
      "Epoch: 32736 \tTraining Loss: 1.272335 \tValidation Loss: 2.647150\n",
      "Epoch: 32737 \tTraining Loss: 1.283195 \tValidation Loss: 2.648366\n",
      "Epoch: 32738 \tTraining Loss: 1.183471 \tValidation Loss: 2.645309\n",
      "Epoch: 32739 \tTraining Loss: 1.300041 \tValidation Loss: 2.645474\n",
      "Epoch: 32740 \tTraining Loss: 1.278008 \tValidation Loss: 2.646061\n",
      "Epoch: 32741 \tTraining Loss: 1.200243 \tValidation Loss: 2.647126\n",
      "Epoch: 32742 \tTraining Loss: 1.270884 \tValidation Loss: 2.647326\n",
      "Epoch: 32743 \tTraining Loss: 1.255933 \tValidation Loss: 2.645563\n",
      "Epoch: 32744 \tTraining Loss: 1.291156 \tValidation Loss: 2.646648\n",
      "Epoch: 32745 \tTraining Loss: 1.314137 \tValidation Loss: 2.646883\n",
      "Epoch: 32746 \tTraining Loss: 1.200739 \tValidation Loss: 2.647492\n",
      "Epoch: 32747 \tTraining Loss: 1.301632 \tValidation Loss: 2.646366\n",
      "Epoch: 32748 \tTraining Loss: 1.281725 \tValidation Loss: 2.646965\n",
      "Epoch: 32749 \tTraining Loss: 1.327480 \tValidation Loss: 2.646068\n",
      "Epoch: 32750 \tTraining Loss: 1.251630 \tValidation Loss: 2.646184\n",
      "Epoch: 32751 \tTraining Loss: 1.275912 \tValidation Loss: 2.646277\n",
      "Epoch: 32752 \tTraining Loss: 1.275262 \tValidation Loss: 2.646038\n",
      "Epoch: 32753 \tTraining Loss: 1.247915 \tValidation Loss: 2.647644\n",
      "Epoch: 32754 \tTraining Loss: 1.327965 \tValidation Loss: 2.645860\n",
      "Epoch: 32755 \tTraining Loss: 1.264835 \tValidation Loss: 2.646008\n",
      "Epoch: 32756 \tTraining Loss: 1.253571 \tValidation Loss: 2.646196\n",
      "Epoch: 32757 \tTraining Loss: 1.246080 \tValidation Loss: 2.647561\n",
      "Epoch: 32758 \tTraining Loss: 1.253382 \tValidation Loss: 2.648852\n",
      "Epoch: 32759 \tTraining Loss: 1.244028 \tValidation Loss: 2.646217\n",
      "Epoch: 32760 \tTraining Loss: 1.281985 \tValidation Loss: 2.647684\n",
      "Epoch: 32761 \tTraining Loss: 1.268288 \tValidation Loss: 2.647453\n",
      "Epoch: 32762 \tTraining Loss: 1.267458 \tValidation Loss: 2.647792\n",
      "Epoch: 32763 \tTraining Loss: 1.285641 \tValidation Loss: 2.644900\n",
      "Epoch: 32764 \tTraining Loss: 1.283135 \tValidation Loss: 2.647133\n",
      "Epoch: 32765 \tTraining Loss: 1.292939 \tValidation Loss: 2.646860\n",
      "Epoch: 32766 \tTraining Loss: 1.310203 \tValidation Loss: 2.647270\n",
      "Epoch: 32767 \tTraining Loss: 1.282401 \tValidation Loss: 2.646220\n",
      "Epoch: 32768 \tTraining Loss: 1.218775 \tValidation Loss: 2.649166\n",
      "Epoch: 32769 \tTraining Loss: 1.224728 \tValidation Loss: 2.647558\n",
      "Epoch: 32770 \tTraining Loss: 1.305967 \tValidation Loss: 2.646920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32771 \tTraining Loss: 1.221027 \tValidation Loss: 2.648050\n",
      "Epoch: 32772 \tTraining Loss: 1.256008 \tValidation Loss: 2.649036\n",
      "Epoch: 32773 \tTraining Loss: 1.223612 \tValidation Loss: 2.647902\n",
      "Epoch: 32774 \tTraining Loss: 1.264966 \tValidation Loss: 2.647603\n",
      "Epoch: 32775 \tTraining Loss: 1.301211 \tValidation Loss: 2.647583\n",
      "Epoch: 32776 \tTraining Loss: 1.310387 \tValidation Loss: 2.647344\n",
      "Epoch: 32777 \tTraining Loss: 1.290956 \tValidation Loss: 2.647728\n",
      "Epoch: 32778 \tTraining Loss: 1.267251 \tValidation Loss: 2.647759\n",
      "Epoch: 32779 \tTraining Loss: 1.266784 \tValidation Loss: 2.646896\n",
      "Epoch: 32780 \tTraining Loss: 1.254189 \tValidation Loss: 2.648584\n",
      "Epoch: 32781 \tTraining Loss: 1.283980 \tValidation Loss: 2.647797\n",
      "Epoch: 32782 \tTraining Loss: 1.230811 \tValidation Loss: 2.648700\n",
      "Epoch: 32783 \tTraining Loss: 1.248125 \tValidation Loss: 2.646967\n",
      "Epoch: 32784 \tTraining Loss: 1.236998 \tValidation Loss: 2.648261\n",
      "Epoch: 32785 \tTraining Loss: 1.279309 \tValidation Loss: 2.647799\n",
      "Epoch: 32786 \tTraining Loss: 1.313080 \tValidation Loss: 2.648269\n",
      "Epoch: 32787 \tTraining Loss: 1.261829 \tValidation Loss: 2.648989\n",
      "Epoch: 32788 \tTraining Loss: 1.272914 \tValidation Loss: 2.648926\n",
      "Epoch: 32789 \tTraining Loss: 1.267819 \tValidation Loss: 2.648314\n",
      "Epoch: 32790 \tTraining Loss: 1.239962 \tValidation Loss: 2.647931\n",
      "Epoch: 32791 \tTraining Loss: 1.251891 \tValidation Loss: 2.647594\n",
      "Epoch: 32792 \tTraining Loss: 1.251455 \tValidation Loss: 2.648692\n",
      "Epoch: 32793 \tTraining Loss: 1.244209 \tValidation Loss: 2.648667\n",
      "Epoch: 32794 \tTraining Loss: 1.259116 \tValidation Loss: 2.649342\n",
      "Epoch: 32795 \tTraining Loss: 1.231785 \tValidation Loss: 2.649431\n",
      "Epoch: 32796 \tTraining Loss: 1.250444 \tValidation Loss: 2.648507\n",
      "Epoch: 32797 \tTraining Loss: 1.280121 \tValidation Loss: 2.648549\n",
      "Epoch: 32798 \tTraining Loss: 1.251173 \tValidation Loss: 2.648263\n",
      "Epoch: 32799 \tTraining Loss: 1.278422 \tValidation Loss: 2.647170\n",
      "Epoch: 32800 \tTraining Loss: 1.250801 \tValidation Loss: 2.648045\n",
      "Epoch: 32801 \tTraining Loss: 1.291425 \tValidation Loss: 2.647613\n",
      "Epoch: 32802 \tTraining Loss: 1.244737 \tValidation Loss: 2.648095\n",
      "Epoch: 32803 \tTraining Loss: 1.287662 \tValidation Loss: 2.649356\n",
      "Epoch: 32804 \tTraining Loss: 1.231301 \tValidation Loss: 2.649224\n",
      "Epoch: 32805 \tTraining Loss: 1.277479 \tValidation Loss: 2.648413\n",
      "Epoch: 32806 \tTraining Loss: 1.257072 \tValidation Loss: 2.647970\n",
      "Epoch: 32807 \tTraining Loss: 1.312468 \tValidation Loss: 2.649340\n",
      "Epoch: 32808 \tTraining Loss: 1.209448 \tValidation Loss: 2.648969\n",
      "Epoch: 32809 \tTraining Loss: 1.240530 \tValidation Loss: 2.648215\n",
      "Epoch: 32810 \tTraining Loss: 1.259188 \tValidation Loss: 2.647953\n",
      "Epoch: 32811 \tTraining Loss: 1.281305 \tValidation Loss: 2.648052\n",
      "Epoch: 32812 \tTraining Loss: 1.314164 \tValidation Loss: 2.646676\n",
      "Epoch: 32813 \tTraining Loss: 1.291043 \tValidation Loss: 2.648041\n",
      "Epoch: 32814 \tTraining Loss: 1.273579 \tValidation Loss: 2.649112\n",
      "Epoch: 32815 \tTraining Loss: 1.264295 \tValidation Loss: 2.649162\n",
      "Epoch: 32816 \tTraining Loss: 1.227146 \tValidation Loss: 2.649659\n",
      "Epoch: 32817 \tTraining Loss: 1.283075 \tValidation Loss: 2.647787\n",
      "Epoch: 32818 \tTraining Loss: 1.267203 \tValidation Loss: 2.647645\n",
      "Epoch: 32819 \tTraining Loss: 1.252724 \tValidation Loss: 2.648611\n",
      "Epoch: 32820 \tTraining Loss: 1.236712 \tValidation Loss: 2.649124\n",
      "Epoch: 32821 \tTraining Loss: 1.219728 \tValidation Loss: 2.647519\n",
      "Epoch: 32822 \tTraining Loss: 1.280077 \tValidation Loss: 2.648367\n",
      "Epoch: 32823 \tTraining Loss: 1.289119 \tValidation Loss: 2.646959\n",
      "Epoch: 32824 \tTraining Loss: 1.287288 \tValidation Loss: 2.648262\n",
      "Epoch: 32825 \tTraining Loss: 1.239925 \tValidation Loss: 2.648438\n",
      "Epoch: 32826 \tTraining Loss: 1.289039 \tValidation Loss: 2.649252\n",
      "Epoch: 32827 \tTraining Loss: 1.192577 \tValidation Loss: 2.649782\n",
      "Epoch: 32828 \tTraining Loss: 1.273118 \tValidation Loss: 2.648047\n",
      "Epoch: 32829 \tTraining Loss: 1.255744 \tValidation Loss: 2.647962\n",
      "Epoch: 32830 \tTraining Loss: 1.274532 \tValidation Loss: 2.649322\n",
      "Epoch: 32831 \tTraining Loss: 1.245780 \tValidation Loss: 2.649905\n",
      "Epoch: 32832 \tTraining Loss: 1.267783 \tValidation Loss: 2.647549\n",
      "Epoch: 32833 \tTraining Loss: 1.327821 \tValidation Loss: 2.647850\n",
      "Epoch: 32834 \tTraining Loss: 1.278937 \tValidation Loss: 2.650298\n",
      "Epoch: 32835 \tTraining Loss: 1.272374 \tValidation Loss: 2.650209\n",
      "Epoch: 32836 \tTraining Loss: 1.238968 \tValidation Loss: 2.649892\n",
      "Epoch: 32837 \tTraining Loss: 1.284288 \tValidation Loss: 2.649574\n",
      "Epoch: 32838 \tTraining Loss: 1.223161 \tValidation Loss: 2.651205\n",
      "Epoch: 32839 \tTraining Loss: 1.250866 \tValidation Loss: 2.648603\n",
      "Epoch: 32840 \tTraining Loss: 1.233578 \tValidation Loss: 2.649508\n",
      "Epoch: 32841 \tTraining Loss: 1.221839 \tValidation Loss: 2.649364\n",
      "Epoch: 32842 \tTraining Loss: 1.268668 \tValidation Loss: 2.649100\n",
      "Epoch: 32843 \tTraining Loss: 1.284484 \tValidation Loss: 2.649340\n",
      "Epoch: 32844 \tTraining Loss: 1.287041 \tValidation Loss: 2.648912\n",
      "Epoch: 32845 \tTraining Loss: 1.280892 \tValidation Loss: 2.649199\n",
      "Epoch: 32846 \tTraining Loss: 1.269368 \tValidation Loss: 2.649757\n",
      "Epoch: 32847 \tTraining Loss: 1.240101 \tValidation Loss: 2.650506\n",
      "Epoch: 32848 \tTraining Loss: 1.277923 \tValidation Loss: 2.648727\n",
      "Epoch: 32849 \tTraining Loss: 1.242947 \tValidation Loss: 2.649922\n",
      "Epoch: 32850 \tTraining Loss: 1.297505 \tValidation Loss: 2.650396\n",
      "Epoch: 32851 \tTraining Loss: 1.239908 \tValidation Loss: 2.649014\n",
      "Epoch: 32852 \tTraining Loss: 1.229554 \tValidation Loss: 2.649071\n",
      "Epoch: 32853 \tTraining Loss: 1.270537 \tValidation Loss: 2.650144\n",
      "Epoch: 32854 \tTraining Loss: 1.305514 \tValidation Loss: 2.650115\n",
      "Epoch: 32855 \tTraining Loss: 1.270083 \tValidation Loss: 2.648565\n",
      "Epoch: 32856 \tTraining Loss: 1.318999 \tValidation Loss: 2.647866\n",
      "Epoch: 32857 \tTraining Loss: 1.268858 \tValidation Loss: 2.650617\n",
      "Epoch: 32858 \tTraining Loss: 1.298569 \tValidation Loss: 2.648122\n",
      "Epoch: 32859 \tTraining Loss: 1.271906 \tValidation Loss: 2.650392\n",
      "Epoch: 32860 \tTraining Loss: 1.275386 \tValidation Loss: 2.648512\n",
      "Epoch: 32861 \tTraining Loss: 1.241212 \tValidation Loss: 2.649667\n",
      "Epoch: 32862 \tTraining Loss: 1.296770 \tValidation Loss: 2.648665\n",
      "Epoch: 32863 \tTraining Loss: 1.265778 \tValidation Loss: 2.650477\n",
      "Epoch: 32864 \tTraining Loss: 1.305903 \tValidation Loss: 2.649703\n",
      "Epoch: 32865 \tTraining Loss: 1.272068 \tValidation Loss: 2.649184\n",
      "Epoch: 32866 \tTraining Loss: 1.258089 \tValidation Loss: 2.650910\n",
      "Epoch: 32867 \tTraining Loss: 1.246226 \tValidation Loss: 2.651030\n",
      "Epoch: 32868 \tTraining Loss: 1.291977 \tValidation Loss: 2.650387\n",
      "Epoch: 32869 \tTraining Loss: 1.206762 \tValidation Loss: 2.650660\n",
      "Epoch: 32870 \tTraining Loss: 1.253818 \tValidation Loss: 2.651014\n",
      "Epoch: 32871 \tTraining Loss: 1.251559 \tValidation Loss: 2.649480\n",
      "Epoch: 32872 \tTraining Loss: 1.242146 \tValidation Loss: 2.649938\n",
      "Epoch: 32873 \tTraining Loss: 1.212623 \tValidation Loss: 2.651016\n",
      "Epoch: 32874 \tTraining Loss: 1.310837 \tValidation Loss: 2.649065\n",
      "Epoch: 32875 \tTraining Loss: 1.267565 \tValidation Loss: 2.650307\n",
      "Epoch: 32876 \tTraining Loss: 1.269991 \tValidation Loss: 2.650278\n",
      "Epoch: 32877 \tTraining Loss: 1.219301 \tValidation Loss: 2.649494\n",
      "Epoch: 32878 \tTraining Loss: 1.248664 \tValidation Loss: 2.650006\n",
      "Epoch: 32879 \tTraining Loss: 1.230964 \tValidation Loss: 2.650543\n",
      "Epoch: 32880 \tTraining Loss: 1.263088 \tValidation Loss: 2.650682\n",
      "Epoch: 32881 \tTraining Loss: 1.268021 \tValidation Loss: 2.648674\n",
      "Epoch: 32882 \tTraining Loss: 1.259848 \tValidation Loss: 2.649410\n",
      "Epoch: 32883 \tTraining Loss: 1.276894 \tValidation Loss: 2.648949\n",
      "Epoch: 32884 \tTraining Loss: 1.222483 \tValidation Loss: 2.650513\n",
      "Epoch: 32885 \tTraining Loss: 1.264282 \tValidation Loss: 2.648674\n",
      "Epoch: 32886 \tTraining Loss: 1.277285 \tValidation Loss: 2.649907\n",
      "Epoch: 32887 \tTraining Loss: 1.259567 \tValidation Loss: 2.651501\n",
      "Epoch: 32888 \tTraining Loss: 1.285061 \tValidation Loss: 2.650747\n",
      "Epoch: 32889 \tTraining Loss: 1.274608 \tValidation Loss: 2.651397\n",
      "Epoch: 32890 \tTraining Loss: 1.264277 \tValidation Loss: 2.651685\n",
      "Epoch: 32891 \tTraining Loss: 1.262957 \tValidation Loss: 2.650664\n",
      "Epoch: 32892 \tTraining Loss: 1.226632 \tValidation Loss: 2.651555\n",
      "Epoch: 32893 \tTraining Loss: 1.254714 \tValidation Loss: 2.649318\n",
      "Epoch: 32894 \tTraining Loss: 1.267057 \tValidation Loss: 2.651283\n",
      "Epoch: 32895 \tTraining Loss: 1.202117 \tValidation Loss: 2.651454\n",
      "Epoch: 32896 \tTraining Loss: 1.251476 \tValidation Loss: 2.650127\n",
      "Epoch: 32897 \tTraining Loss: 1.193346 \tValidation Loss: 2.651336\n",
      "Epoch: 32898 \tTraining Loss: 1.265931 \tValidation Loss: 2.649244\n",
      "Epoch: 32899 \tTraining Loss: 1.264973 \tValidation Loss: 2.650657\n",
      "Epoch: 32900 \tTraining Loss: 1.301347 \tValidation Loss: 2.648987\n",
      "Epoch: 32901 \tTraining Loss: 1.262289 \tValidation Loss: 2.650303\n",
      "Epoch: 32902 \tTraining Loss: 1.234713 \tValidation Loss: 2.651544\n",
      "Epoch: 32903 \tTraining Loss: 1.303114 \tValidation Loss: 2.649093\n",
      "Epoch: 32904 \tTraining Loss: 1.276886 \tValidation Loss: 2.651294\n",
      "Epoch: 32905 \tTraining Loss: 1.252293 \tValidation Loss: 2.649753\n",
      "Epoch: 32906 \tTraining Loss: 1.312590 \tValidation Loss: 2.650915\n",
      "Epoch: 32907 \tTraining Loss: 1.226065 \tValidation Loss: 2.652406\n",
      "Epoch: 32908 \tTraining Loss: 1.268090 \tValidation Loss: 2.649639\n",
      "Epoch: 32909 \tTraining Loss: 1.227128 \tValidation Loss: 2.650869\n",
      "Epoch: 32910 \tTraining Loss: 1.269958 \tValidation Loss: 2.651138\n",
      "Epoch: 32911 \tTraining Loss: 1.270410 \tValidation Loss: 2.650368\n",
      "Epoch: 32912 \tTraining Loss: 1.279536 \tValidation Loss: 2.651300\n",
      "Epoch: 32913 \tTraining Loss: 1.245398 \tValidation Loss: 2.652649\n",
      "Epoch: 32914 \tTraining Loss: 1.303719 \tValidation Loss: 2.649809\n",
      "Epoch: 32915 \tTraining Loss: 1.222599 \tValidation Loss: 2.652946\n",
      "Epoch: 32916 \tTraining Loss: 1.260247 \tValidation Loss: 2.652075\n",
      "Epoch: 32917 \tTraining Loss: 1.234925 \tValidation Loss: 2.651845\n",
      "Epoch: 32918 \tTraining Loss: 1.272832 \tValidation Loss: 2.650838\n",
      "Epoch: 32919 \tTraining Loss: 1.225268 \tValidation Loss: 2.651864\n",
      "Epoch: 32920 \tTraining Loss: 1.272214 \tValidation Loss: 2.651563\n",
      "Epoch: 32921 \tTraining Loss: 1.244142 \tValidation Loss: 2.651376\n",
      "Epoch: 32922 \tTraining Loss: 1.245537 \tValidation Loss: 2.650354\n",
      "Epoch: 32923 \tTraining Loss: 1.286032 \tValidation Loss: 2.650586\n",
      "Epoch: 32924 \tTraining Loss: 1.313500 \tValidation Loss: 2.649817\n",
      "Epoch: 32925 \tTraining Loss: 1.222997 \tValidation Loss: 2.651238\n",
      "Epoch: 32926 \tTraining Loss: 1.263646 \tValidation Loss: 2.650834\n",
      "Epoch: 32927 \tTraining Loss: 1.279537 \tValidation Loss: 2.651752\n",
      "Epoch: 32928 \tTraining Loss: 1.269773 \tValidation Loss: 2.651210\n",
      "Epoch: 32929 \tTraining Loss: 1.261385 \tValidation Loss: 2.651111\n",
      "Epoch: 32930 \tTraining Loss: 1.279193 \tValidation Loss: 2.649409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32931 \tTraining Loss: 1.235082 \tValidation Loss: 2.651961\n",
      "Epoch: 32932 \tTraining Loss: 1.240657 \tValidation Loss: 2.650310\n",
      "Epoch: 32933 \tTraining Loss: 1.235245 \tValidation Loss: 2.651500\n",
      "Epoch: 32934 \tTraining Loss: 1.267438 \tValidation Loss: 2.651874\n",
      "Epoch: 32935 \tTraining Loss: 1.270848 \tValidation Loss: 2.650631\n",
      "Epoch: 32936 \tTraining Loss: 1.211748 \tValidation Loss: 2.653888\n",
      "Epoch: 32937 \tTraining Loss: 1.249667 \tValidation Loss: 2.650146\n",
      "Epoch: 32938 \tTraining Loss: 1.279302 \tValidation Loss: 2.651447\n",
      "Epoch: 32939 \tTraining Loss: 1.277802 \tValidation Loss: 2.649916\n",
      "Epoch: 32940 \tTraining Loss: 1.288529 \tValidation Loss: 2.651227\n",
      "Epoch: 32941 \tTraining Loss: 1.315654 \tValidation Loss: 2.650653\n",
      "Epoch: 32942 \tTraining Loss: 1.264341 \tValidation Loss: 2.652476\n",
      "Epoch: 32943 \tTraining Loss: 1.240097 \tValidation Loss: 2.653543\n",
      "Epoch: 32944 \tTraining Loss: 1.265147 \tValidation Loss: 2.652505\n",
      "Epoch: 32945 \tTraining Loss: 1.243216 \tValidation Loss: 2.653712\n",
      "Epoch: 32946 \tTraining Loss: 1.272820 \tValidation Loss: 2.650988\n",
      "Epoch: 32947 \tTraining Loss: 1.246717 \tValidation Loss: 2.652254\n",
      "Epoch: 32948 \tTraining Loss: 1.251208 \tValidation Loss: 2.651228\n",
      "Epoch: 32949 \tTraining Loss: 1.281242 \tValidation Loss: 2.651848\n",
      "Epoch: 32950 \tTraining Loss: 1.262957 \tValidation Loss: 2.652412\n",
      "Epoch: 32951 \tTraining Loss: 1.222311 \tValidation Loss: 2.651575\n",
      "Epoch: 32952 \tTraining Loss: 1.275939 \tValidation Loss: 2.652694\n",
      "Epoch: 32953 \tTraining Loss: 1.301946 \tValidation Loss: 2.651412\n",
      "Epoch: 32954 \tTraining Loss: 1.287253 \tValidation Loss: 2.651927\n",
      "Epoch: 32955 \tTraining Loss: 1.289890 \tValidation Loss: 2.650695\n",
      "Epoch: 32956 \tTraining Loss: 1.253199 \tValidation Loss: 2.651864\n",
      "Epoch: 32957 \tTraining Loss: 1.242510 \tValidation Loss: 2.653097\n",
      "Epoch: 32958 \tTraining Loss: 1.208992 \tValidation Loss: 2.653275\n",
      "Epoch: 32959 \tTraining Loss: 1.316347 \tValidation Loss: 2.651502\n",
      "Epoch: 32960 \tTraining Loss: 1.219974 \tValidation Loss: 2.653118\n",
      "Epoch: 32961 \tTraining Loss: 1.276720 \tValidation Loss: 2.651410\n",
      "Epoch: 32962 \tTraining Loss: 1.248881 \tValidation Loss: 2.653437\n",
      "Epoch: 32963 \tTraining Loss: 1.259793 \tValidation Loss: 2.651450\n",
      "Epoch: 32964 \tTraining Loss: 1.286437 \tValidation Loss: 2.651901\n",
      "Epoch: 32965 \tTraining Loss: 1.240396 \tValidation Loss: 2.651341\n",
      "Epoch: 32966 \tTraining Loss: 1.287798 \tValidation Loss: 2.652700\n",
      "Epoch: 32967 \tTraining Loss: 1.224413 \tValidation Loss: 2.652803\n",
      "Epoch: 32968 \tTraining Loss: 1.294442 \tValidation Loss: 2.653192\n",
      "Epoch: 32969 \tTraining Loss: 1.283761 \tValidation Loss: 2.650338\n",
      "Epoch: 32970 \tTraining Loss: 1.271853 \tValidation Loss: 2.651037\n",
      "Epoch: 32971 \tTraining Loss: 1.258040 \tValidation Loss: 2.651871\n",
      "Epoch: 32972 \tTraining Loss: 1.250960 \tValidation Loss: 2.652349\n",
      "Epoch: 32973 \tTraining Loss: 1.265660 \tValidation Loss: 2.651771\n",
      "Epoch: 32974 \tTraining Loss: 1.205048 \tValidation Loss: 2.652580\n",
      "Epoch: 32975 \tTraining Loss: 1.268959 \tValidation Loss: 2.651169\n",
      "Epoch: 32976 \tTraining Loss: 1.256201 \tValidation Loss: 2.652271\n",
      "Epoch: 32977 \tTraining Loss: 1.277109 \tValidation Loss: 2.650761\n",
      "Epoch: 32978 \tTraining Loss: 1.243104 \tValidation Loss: 2.653232\n",
      "Epoch: 32979 \tTraining Loss: 1.247465 \tValidation Loss: 2.651863\n",
      "Epoch: 32980 \tTraining Loss: 1.210930 \tValidation Loss: 2.654326\n",
      "Epoch: 32981 \tTraining Loss: 1.255435 \tValidation Loss: 2.652853\n",
      "Epoch: 32982 \tTraining Loss: 1.253542 \tValidation Loss: 2.651236\n",
      "Epoch: 32983 \tTraining Loss: 1.247404 \tValidation Loss: 2.652222\n",
      "Epoch: 32984 \tTraining Loss: 1.319536 \tValidation Loss: 2.650813\n",
      "Epoch: 32985 \tTraining Loss: 1.218827 \tValidation Loss: 2.653386\n",
      "Epoch: 32986 \tTraining Loss: 1.273314 \tValidation Loss: 2.653197\n",
      "Epoch: 32987 \tTraining Loss: 1.222653 \tValidation Loss: 2.654119\n",
      "Epoch: 32988 \tTraining Loss: 1.242645 \tValidation Loss: 2.652721\n",
      "Epoch: 32989 \tTraining Loss: 1.314115 \tValidation Loss: 2.651122\n",
      "Epoch: 32990 \tTraining Loss: 1.298484 \tValidation Loss: 2.651305\n",
      "Epoch: 32991 \tTraining Loss: 1.269682 \tValidation Loss: 2.652317\n",
      "Epoch: 32992 \tTraining Loss: 1.283389 \tValidation Loss: 2.651532\n",
      "Epoch: 32993 \tTraining Loss: 1.254987 \tValidation Loss: 2.652572\n",
      "Epoch: 32994 \tTraining Loss: 1.240675 \tValidation Loss: 2.652695\n",
      "Epoch: 32995 \tTraining Loss: 1.261373 \tValidation Loss: 2.653815\n",
      "Epoch: 32996 \tTraining Loss: 1.259214 \tValidation Loss: 2.652234\n",
      "Epoch: 32997 \tTraining Loss: 1.202628 \tValidation Loss: 2.653968\n",
      "Epoch: 32998 \tTraining Loss: 1.249382 \tValidation Loss: 2.652134\n",
      "Epoch: 32999 \tTraining Loss: 1.276085 \tValidation Loss: 2.652555\n",
      "Epoch: 33000 \tTraining Loss: 1.221934 \tValidation Loss: 2.652045\n",
      "Epoch: 33001 \tTraining Loss: 1.224042 \tValidation Loss: 2.654154\n",
      "Epoch: 33002 \tTraining Loss: 1.260646 \tValidation Loss: 2.654325\n",
      "Epoch: 33003 \tTraining Loss: 1.268804 \tValidation Loss: 2.653443\n",
      "Epoch: 33004 \tTraining Loss: 1.293824 \tValidation Loss: 2.653608\n",
      "Epoch: 33005 \tTraining Loss: 1.263686 \tValidation Loss: 2.652256\n",
      "Epoch: 33006 \tTraining Loss: 1.287504 \tValidation Loss: 2.652572\n",
      "Epoch: 33007 \tTraining Loss: 1.260534 \tValidation Loss: 2.652785\n",
      "Epoch: 33008 \tTraining Loss: 1.286465 \tValidation Loss: 2.652887\n",
      "Epoch: 33009 \tTraining Loss: 1.288457 \tValidation Loss: 2.652590\n",
      "Epoch: 33010 \tTraining Loss: 1.300507 \tValidation Loss: 2.652478\n",
      "Epoch: 33011 \tTraining Loss: 1.220198 \tValidation Loss: 2.655252\n",
      "Epoch: 33012 \tTraining Loss: 1.220494 \tValidation Loss: 2.653780\n",
      "Epoch: 33013 \tTraining Loss: 1.267493 \tValidation Loss: 2.652412\n",
      "Epoch: 33014 \tTraining Loss: 1.303037 \tValidation Loss: 2.654268\n",
      "Epoch: 33015 \tTraining Loss: 1.261182 \tValidation Loss: 2.651906\n",
      "Epoch: 33016 \tTraining Loss: 1.257666 \tValidation Loss: 2.652497\n",
      "Epoch: 33017 \tTraining Loss: 1.259853 \tValidation Loss: 2.653745\n",
      "Epoch: 33018 \tTraining Loss: 1.289621 \tValidation Loss: 2.653289\n",
      "Epoch: 33019 \tTraining Loss: 1.267767 \tValidation Loss: 2.653277\n",
      "Epoch: 33020 \tTraining Loss: 1.211774 \tValidation Loss: 2.653710\n",
      "Epoch: 33021 \tTraining Loss: 1.285181 \tValidation Loss: 2.653672\n",
      "Epoch: 33022 \tTraining Loss: 1.235124 \tValidation Loss: 2.653326\n",
      "Epoch: 33023 \tTraining Loss: 1.278876 \tValidation Loss: 2.653887\n",
      "Epoch: 33024 \tTraining Loss: 1.249887 \tValidation Loss: 2.653970\n",
      "Epoch: 33025 \tTraining Loss: 1.274846 \tValidation Loss: 2.653617\n",
      "Epoch: 33026 \tTraining Loss: 1.278855 \tValidation Loss: 2.653504\n",
      "Epoch: 33027 \tTraining Loss: 1.276567 \tValidation Loss: 2.652016\n",
      "Epoch: 33028 \tTraining Loss: 1.280417 \tValidation Loss: 2.653963\n",
      "Epoch: 33029 \tTraining Loss: 1.292777 \tValidation Loss: 2.653075\n",
      "Epoch: 33030 \tTraining Loss: 1.243512 \tValidation Loss: 2.653298\n",
      "Epoch: 33031 \tTraining Loss: 1.237109 \tValidation Loss: 2.654980\n",
      "Epoch: 33032 \tTraining Loss: 1.248023 \tValidation Loss: 2.653703\n",
      "Epoch: 33033 \tTraining Loss: 1.201897 \tValidation Loss: 2.654869\n",
      "Epoch: 33034 \tTraining Loss: 1.267025 \tValidation Loss: 2.654396\n",
      "Epoch: 33035 \tTraining Loss: 1.242407 \tValidation Loss: 2.653657\n",
      "Epoch: 33036 \tTraining Loss: 1.252908 \tValidation Loss: 2.654387\n",
      "Epoch: 33037 \tTraining Loss: 1.263631 \tValidation Loss: 2.653665\n",
      "Epoch: 33038 \tTraining Loss: 1.249466 \tValidation Loss: 2.654436\n",
      "Epoch: 33039 \tTraining Loss: 1.248425 \tValidation Loss: 2.652533\n",
      "Epoch: 33040 \tTraining Loss: 1.256590 \tValidation Loss: 2.653782\n",
      "Epoch: 33041 \tTraining Loss: 1.260748 \tValidation Loss: 2.654500\n",
      "Epoch: 33042 \tTraining Loss: 1.276352 \tValidation Loss: 2.653092\n",
      "Epoch: 33043 \tTraining Loss: 1.158822 \tValidation Loss: 2.654359\n",
      "Epoch: 33044 \tTraining Loss: 1.273373 \tValidation Loss: 2.653698\n",
      "Epoch: 33045 \tTraining Loss: 1.304200 \tValidation Loss: 2.653106\n",
      "Epoch: 33046 \tTraining Loss: 1.275777 \tValidation Loss: 2.655144\n",
      "Epoch: 33047 \tTraining Loss: 1.257216 \tValidation Loss: 2.652787\n",
      "Epoch: 33048 \tTraining Loss: 1.260824 \tValidation Loss: 2.653548\n",
      "Epoch: 33049 \tTraining Loss: 1.269531 \tValidation Loss: 2.652779\n",
      "Epoch: 33050 \tTraining Loss: 1.281393 \tValidation Loss: 2.654068\n",
      "Epoch: 33051 \tTraining Loss: 1.228529 \tValidation Loss: 2.654681\n",
      "Epoch: 33052 \tTraining Loss: 1.249559 \tValidation Loss: 2.653480\n",
      "Epoch: 33053 \tTraining Loss: 1.284805 \tValidation Loss: 2.652928\n",
      "Epoch: 33054 \tTraining Loss: 1.227307 \tValidation Loss: 2.653656\n",
      "Epoch: 33055 \tTraining Loss: 1.276454 \tValidation Loss: 2.654207\n",
      "Epoch: 33056 \tTraining Loss: 1.227067 \tValidation Loss: 2.654659\n",
      "Epoch: 33057 \tTraining Loss: 1.260633 \tValidation Loss: 2.654539\n",
      "Epoch: 33058 \tTraining Loss: 1.315292 \tValidation Loss: 2.653190\n",
      "Epoch: 33059 \tTraining Loss: 1.263319 \tValidation Loss: 2.656070\n",
      "Epoch: 33060 \tTraining Loss: 1.278103 \tValidation Loss: 2.652580\n",
      "Epoch: 33061 \tTraining Loss: 1.286770 \tValidation Loss: 2.653842\n",
      "Epoch: 33062 \tTraining Loss: 1.271179 \tValidation Loss: 2.653541\n",
      "Epoch: 33063 \tTraining Loss: 1.297525 \tValidation Loss: 2.653976\n",
      "Epoch: 33064 \tTraining Loss: 1.296083 \tValidation Loss: 2.654081\n",
      "Epoch: 33065 \tTraining Loss: 1.272050 \tValidation Loss: 2.653435\n",
      "Epoch: 33066 \tTraining Loss: 1.238147 \tValidation Loss: 2.655310\n",
      "Epoch: 33067 \tTraining Loss: 1.224994 \tValidation Loss: 2.655361\n",
      "Epoch: 33068 \tTraining Loss: 1.246383 \tValidation Loss: 2.654284\n",
      "Epoch: 33069 \tTraining Loss: 1.238094 \tValidation Loss: 2.654233\n",
      "Epoch: 33070 \tTraining Loss: 1.255377 \tValidation Loss: 2.654537\n",
      "Epoch: 33071 \tTraining Loss: 1.263588 \tValidation Loss: 2.653673\n",
      "Epoch: 33072 \tTraining Loss: 1.241670 \tValidation Loss: 2.654210\n",
      "Epoch: 33073 \tTraining Loss: 1.263385 \tValidation Loss: 2.654473\n",
      "Epoch: 33074 \tTraining Loss: 1.261124 \tValidation Loss: 2.654650\n",
      "Epoch: 33075 \tTraining Loss: 1.242535 \tValidation Loss: 2.654022\n",
      "Epoch: 33076 \tTraining Loss: 1.296430 \tValidation Loss: 2.653570\n",
      "Epoch: 33077 \tTraining Loss: 1.216523 \tValidation Loss: 2.653863\n",
      "Epoch: 33078 \tTraining Loss: 1.275126 \tValidation Loss: 2.653912\n",
      "Epoch: 33079 \tTraining Loss: 1.248186 \tValidation Loss: 2.655088\n",
      "Epoch: 33080 \tTraining Loss: 1.256531 \tValidation Loss: 2.655394\n",
      "Epoch: 33081 \tTraining Loss: 1.258874 \tValidation Loss: 2.655664\n",
      "Epoch: 33082 \tTraining Loss: 1.289945 \tValidation Loss: 2.653167\n",
      "Epoch: 33083 \tTraining Loss: 1.259828 \tValidation Loss: 2.655457\n",
      "Epoch: 33084 \tTraining Loss: 1.243053 \tValidation Loss: 2.655456\n",
      "Epoch: 33085 \tTraining Loss: 1.247177 \tValidation Loss: 2.655503\n",
      "Epoch: 33086 \tTraining Loss: 1.253507 \tValidation Loss: 2.654222\n",
      "Epoch: 33087 \tTraining Loss: 1.253763 \tValidation Loss: 2.654858\n",
      "Epoch: 33088 \tTraining Loss: 1.216330 \tValidation Loss: 2.655787\n",
      "Epoch: 33089 \tTraining Loss: 1.230141 \tValidation Loss: 2.656037\n",
      "Epoch: 33090 \tTraining Loss: 1.265728 \tValidation Loss: 2.655484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33091 \tTraining Loss: 1.283731 \tValidation Loss: 2.654807\n",
      "Epoch: 33092 \tTraining Loss: 1.281701 \tValidation Loss: 2.654812\n",
      "Epoch: 33093 \tTraining Loss: 1.248436 \tValidation Loss: 2.654064\n",
      "Epoch: 33094 \tTraining Loss: 1.256270 \tValidation Loss: 2.655105\n",
      "Epoch: 33095 \tTraining Loss: 1.273527 \tValidation Loss: 2.655501\n",
      "Epoch: 33096 \tTraining Loss: 1.253271 \tValidation Loss: 2.655011\n",
      "Epoch: 33097 \tTraining Loss: 1.214321 \tValidation Loss: 2.653977\n",
      "Epoch: 33098 \tTraining Loss: 1.269180 \tValidation Loss: 2.653893\n",
      "Epoch: 33099 \tTraining Loss: 1.293245 \tValidation Loss: 2.655047\n",
      "Epoch: 33100 \tTraining Loss: 1.279742 \tValidation Loss: 2.654391\n",
      "Epoch: 33101 \tTraining Loss: 1.299123 \tValidation Loss: 2.654188\n",
      "Epoch: 33102 \tTraining Loss: 1.262785 \tValidation Loss: 2.654331\n",
      "Epoch: 33103 \tTraining Loss: 1.233106 \tValidation Loss: 2.654302\n",
      "Epoch: 33104 \tTraining Loss: 1.226521 \tValidation Loss: 2.655001\n",
      "Epoch: 33105 \tTraining Loss: 1.251698 \tValidation Loss: 2.653880\n",
      "Epoch: 33106 \tTraining Loss: 1.239586 \tValidation Loss: 2.656128\n",
      "Epoch: 33107 \tTraining Loss: 1.232395 \tValidation Loss: 2.654723\n",
      "Epoch: 33108 \tTraining Loss: 1.277789 \tValidation Loss: 2.654451\n",
      "Epoch: 33109 \tTraining Loss: 1.215208 \tValidation Loss: 2.655936\n",
      "Epoch: 33110 \tTraining Loss: 1.230022 \tValidation Loss: 2.655271\n",
      "Epoch: 33111 \tTraining Loss: 1.284648 \tValidation Loss: 2.654545\n",
      "Epoch: 33112 \tTraining Loss: 1.257789 \tValidation Loss: 2.655041\n",
      "Epoch: 33113 \tTraining Loss: 1.259581 \tValidation Loss: 2.654770\n",
      "Epoch: 33114 \tTraining Loss: 1.252764 \tValidation Loss: 2.655139\n",
      "Epoch: 33115 \tTraining Loss: 1.268813 \tValidation Loss: 2.653768\n",
      "Epoch: 33116 \tTraining Loss: 1.287757 \tValidation Loss: 2.655205\n",
      "Epoch: 33117 \tTraining Loss: 1.239958 \tValidation Loss: 2.655074\n",
      "Epoch: 33118 \tTraining Loss: 1.270202 \tValidation Loss: 2.654438\n",
      "Epoch: 33119 \tTraining Loss: 1.227031 \tValidation Loss: 2.655879\n",
      "Epoch: 33120 \tTraining Loss: 1.203780 \tValidation Loss: 2.656194\n",
      "Epoch: 33121 \tTraining Loss: 1.271032 \tValidation Loss: 2.654637\n",
      "Epoch: 33122 \tTraining Loss: 1.251937 \tValidation Loss: 2.655681\n",
      "Epoch: 33123 \tTraining Loss: 1.254912 \tValidation Loss: 2.654730\n",
      "Epoch: 33124 \tTraining Loss: 1.282822 \tValidation Loss: 2.656016\n",
      "Epoch: 33125 \tTraining Loss: 1.265198 \tValidation Loss: 2.654751\n",
      "Epoch: 33126 \tTraining Loss: 1.263939 \tValidation Loss: 2.657504\n",
      "Epoch: 33127 \tTraining Loss: 1.236879 \tValidation Loss: 2.654638\n",
      "Epoch: 33128 \tTraining Loss: 1.274292 \tValidation Loss: 2.656139\n",
      "Epoch: 33129 \tTraining Loss: 1.256018 \tValidation Loss: 2.655751\n",
      "Epoch: 33130 \tTraining Loss: 1.239847 \tValidation Loss: 2.655937\n",
      "Epoch: 33131 \tTraining Loss: 1.257025 \tValidation Loss: 2.654562\n",
      "Epoch: 33132 \tTraining Loss: 1.271173 \tValidation Loss: 2.657213\n",
      "Epoch: 33133 \tTraining Loss: 1.228744 \tValidation Loss: 2.656793\n",
      "Epoch: 33134 \tTraining Loss: 1.228300 \tValidation Loss: 2.657146\n",
      "Epoch: 33135 \tTraining Loss: 1.286942 \tValidation Loss: 2.655737\n",
      "Epoch: 33136 \tTraining Loss: 1.218010 \tValidation Loss: 2.657154\n",
      "Epoch: 33137 \tTraining Loss: 1.237016 \tValidation Loss: 2.656419\n",
      "Epoch: 33138 \tTraining Loss: 1.299509 \tValidation Loss: 2.655385\n",
      "Epoch: 33139 \tTraining Loss: 1.271875 \tValidation Loss: 2.655262\n",
      "Epoch: 33140 \tTraining Loss: 1.271929 \tValidation Loss: 2.655712\n",
      "Epoch: 33141 \tTraining Loss: 1.295033 \tValidation Loss: 2.655234\n",
      "Epoch: 33142 \tTraining Loss: 1.188478 \tValidation Loss: 2.656431\n",
      "Epoch: 33143 \tTraining Loss: 1.253380 \tValidation Loss: 2.655649\n",
      "Epoch: 33144 \tTraining Loss: 1.267464 \tValidation Loss: 2.656096\n",
      "Epoch: 33145 \tTraining Loss: 1.224338 \tValidation Loss: 2.658262\n",
      "Epoch: 33146 \tTraining Loss: 1.251381 \tValidation Loss: 2.655560\n",
      "Epoch: 33147 \tTraining Loss: 1.216446 \tValidation Loss: 2.656589\n",
      "Epoch: 33148 \tTraining Loss: 1.193289 \tValidation Loss: 2.657461\n",
      "Epoch: 33149 \tTraining Loss: 1.218770 \tValidation Loss: 2.656813\n",
      "Epoch: 33150 \tTraining Loss: 1.277691 \tValidation Loss: 2.654876\n",
      "Epoch: 33151 \tTraining Loss: 1.208138 \tValidation Loss: 2.656877\n",
      "Epoch: 33152 \tTraining Loss: 1.261004 \tValidation Loss: 2.657713\n",
      "Epoch: 33153 \tTraining Loss: 1.226328 \tValidation Loss: 2.656020\n",
      "Epoch: 33154 \tTraining Loss: 1.265583 \tValidation Loss: 2.655874\n",
      "Epoch: 33155 \tTraining Loss: 1.243237 \tValidation Loss: 2.656831\n",
      "Epoch: 33156 \tTraining Loss: 1.285128 \tValidation Loss: 2.653915\n",
      "Epoch: 33157 \tTraining Loss: 1.298845 \tValidation Loss: 2.655312\n",
      "Epoch: 33158 \tTraining Loss: 1.234444 \tValidation Loss: 2.657465\n",
      "Epoch: 33159 \tTraining Loss: 1.247045 \tValidation Loss: 2.658106\n",
      "Epoch: 33160 \tTraining Loss: 1.283333 \tValidation Loss: 2.656713\n",
      "Epoch: 33161 \tTraining Loss: 1.237525 \tValidation Loss: 2.657305\n",
      "Epoch: 33162 \tTraining Loss: 1.278140 \tValidation Loss: 2.657150\n",
      "Epoch: 33163 \tTraining Loss: 1.258144 \tValidation Loss: 2.657252\n",
      "Epoch: 33164 \tTraining Loss: 1.255403 \tValidation Loss: 2.657097\n",
      "Epoch: 33165 \tTraining Loss: 1.210253 \tValidation Loss: 2.657472\n",
      "Epoch: 33166 \tTraining Loss: 1.268999 \tValidation Loss: 2.656268\n",
      "Epoch: 33167 \tTraining Loss: 1.229347 \tValidation Loss: 2.657126\n",
      "Epoch: 33168 \tTraining Loss: 1.275244 \tValidation Loss: 2.656154\n",
      "Epoch: 33169 \tTraining Loss: 1.244254 \tValidation Loss: 2.657125\n",
      "Epoch: 33170 \tTraining Loss: 1.223568 \tValidation Loss: 2.656422\n",
      "Epoch: 33171 \tTraining Loss: 1.237617 \tValidation Loss: 2.657740\n",
      "Epoch: 33172 \tTraining Loss: 1.247080 \tValidation Loss: 2.658183\n",
      "Epoch: 33173 \tTraining Loss: 1.239601 \tValidation Loss: 2.658970\n",
      "Epoch: 33174 \tTraining Loss: 1.209560 \tValidation Loss: 2.659240\n",
      "Epoch: 33175 \tTraining Loss: 1.262682 \tValidation Loss: 2.656253\n",
      "Epoch: 33176 \tTraining Loss: 1.242430 \tValidation Loss: 2.657985\n",
      "Epoch: 33177 \tTraining Loss: 1.242676 \tValidation Loss: 2.657928\n",
      "Epoch: 33178 \tTraining Loss: 1.288892 \tValidation Loss: 2.655532\n",
      "Epoch: 33179 \tTraining Loss: 1.289565 \tValidation Loss: 2.655943\n",
      "Epoch: 33180 \tTraining Loss: 1.223993 \tValidation Loss: 2.655888\n",
      "Epoch: 33181 \tTraining Loss: 1.282719 \tValidation Loss: 2.656205\n",
      "Epoch: 33182 \tTraining Loss: 1.294904 \tValidation Loss: 2.656158\n",
      "Epoch: 33183 \tTraining Loss: 1.234547 \tValidation Loss: 2.656515\n",
      "Epoch: 33184 \tTraining Loss: 1.239711 \tValidation Loss: 2.657464\n",
      "Epoch: 33185 \tTraining Loss: 1.250531 \tValidation Loss: 2.655065\n",
      "Epoch: 33186 \tTraining Loss: 1.278323 \tValidation Loss: 2.655144\n",
      "Epoch: 33187 \tTraining Loss: 1.266426 \tValidation Loss: 2.654766\n",
      "Epoch: 33188 \tTraining Loss: 1.260954 \tValidation Loss: 2.656960\n",
      "Epoch: 33189 \tTraining Loss: 1.276517 \tValidation Loss: 2.656023\n",
      "Epoch: 33190 \tTraining Loss: 1.245423 \tValidation Loss: 2.657549\n",
      "Epoch: 33191 \tTraining Loss: 1.256799 \tValidation Loss: 2.656731\n",
      "Epoch: 33192 \tTraining Loss: 1.286988 \tValidation Loss: 2.656991\n",
      "Epoch: 33193 \tTraining Loss: 1.245908 \tValidation Loss: 2.656126\n",
      "Epoch: 33194 \tTraining Loss: 1.231469 \tValidation Loss: 2.657888\n",
      "Epoch: 33195 \tTraining Loss: 1.231886 \tValidation Loss: 2.657800\n",
      "Epoch: 33196 \tTraining Loss: 1.222625 \tValidation Loss: 2.657921\n",
      "Epoch: 33197 \tTraining Loss: 1.250488 \tValidation Loss: 2.657388\n",
      "Epoch: 33198 \tTraining Loss: 1.264326 \tValidation Loss: 2.656938\n",
      "Epoch: 33199 \tTraining Loss: 1.230188 \tValidation Loss: 2.657366\n",
      "Epoch: 33200 \tTraining Loss: 1.227161 \tValidation Loss: 2.656989\n",
      "Epoch: 33201 \tTraining Loss: 1.252236 \tValidation Loss: 2.655771\n",
      "Epoch: 33202 \tTraining Loss: 1.233700 \tValidation Loss: 2.656634\n",
      "Epoch: 33203 \tTraining Loss: 1.252726 \tValidation Loss: 2.657739\n",
      "Epoch: 33204 \tTraining Loss: 1.243791 \tValidation Loss: 2.657718\n",
      "Epoch: 33205 \tTraining Loss: 1.213386 \tValidation Loss: 2.658235\n",
      "Epoch: 33206 \tTraining Loss: 1.213158 \tValidation Loss: 2.656941\n",
      "Epoch: 33207 \tTraining Loss: 1.280539 \tValidation Loss: 2.657711\n",
      "Epoch: 33208 \tTraining Loss: 1.255179 \tValidation Loss: 2.656693\n",
      "Epoch: 33209 \tTraining Loss: 1.198745 \tValidation Loss: 2.656878\n",
      "Epoch: 33210 \tTraining Loss: 1.287928 \tValidation Loss: 2.656436\n",
      "Epoch: 33211 \tTraining Loss: 1.268599 \tValidation Loss: 2.657228\n",
      "Epoch: 33212 \tTraining Loss: 1.233636 \tValidation Loss: 2.657534\n",
      "Epoch: 33213 \tTraining Loss: 1.246041 \tValidation Loss: 2.657191\n",
      "Epoch: 33214 \tTraining Loss: 1.268560 \tValidation Loss: 2.656084\n",
      "Epoch: 33215 \tTraining Loss: 1.240513 \tValidation Loss: 2.657609\n",
      "Epoch: 33216 \tTraining Loss: 1.259220 \tValidation Loss: 2.658098\n",
      "Epoch: 33217 \tTraining Loss: 1.236673 \tValidation Loss: 2.658462\n",
      "Epoch: 33218 \tTraining Loss: 1.247563 \tValidation Loss: 2.657573\n",
      "Epoch: 33219 \tTraining Loss: 1.263430 \tValidation Loss: 2.657658\n",
      "Epoch: 33220 \tTraining Loss: 1.266499 \tValidation Loss: 2.658252\n",
      "Epoch: 33221 \tTraining Loss: 1.255446 \tValidation Loss: 2.657862\n",
      "Epoch: 33222 \tTraining Loss: 1.246071 \tValidation Loss: 2.657549\n",
      "Epoch: 33223 \tTraining Loss: 1.249731 \tValidation Loss: 2.657371\n",
      "Epoch: 33224 \tTraining Loss: 1.259718 \tValidation Loss: 2.657325\n",
      "Epoch: 33225 \tTraining Loss: 1.223842 \tValidation Loss: 2.658017\n",
      "Epoch: 33226 \tTraining Loss: 1.276772 \tValidation Loss: 2.657464\n",
      "Epoch: 33227 \tTraining Loss: 1.297151 \tValidation Loss: 2.656742\n",
      "Epoch: 33228 \tTraining Loss: 1.274607 \tValidation Loss: 2.657870\n",
      "Epoch: 33229 \tTraining Loss: 1.245509 \tValidation Loss: 2.657625\n",
      "Epoch: 33230 \tTraining Loss: 1.228964 \tValidation Loss: 2.657307\n",
      "Epoch: 33231 \tTraining Loss: 1.224265 \tValidation Loss: 2.658154\n",
      "Epoch: 33232 \tTraining Loss: 1.256388 \tValidation Loss: 2.657430\n",
      "Epoch: 33233 \tTraining Loss: 1.207970 \tValidation Loss: 2.659055\n",
      "Epoch: 33234 \tTraining Loss: 1.242337 \tValidation Loss: 2.658132\n",
      "Epoch: 33235 \tTraining Loss: 1.263191 \tValidation Loss: 2.657505\n",
      "Epoch: 33236 \tTraining Loss: 1.270591 \tValidation Loss: 2.658593\n",
      "Epoch: 33237 \tTraining Loss: 1.213988 \tValidation Loss: 2.657995\n",
      "Epoch: 33238 \tTraining Loss: 1.245608 \tValidation Loss: 2.658527\n",
      "Epoch: 33239 \tTraining Loss: 1.209418 \tValidation Loss: 2.658041\n",
      "Epoch: 33240 \tTraining Loss: 1.266845 \tValidation Loss: 2.657814\n",
      "Epoch: 33241 \tTraining Loss: 1.282077 \tValidation Loss: 2.658607\n",
      "Epoch: 33242 \tTraining Loss: 1.264268 \tValidation Loss: 2.658841\n",
      "Epoch: 33243 \tTraining Loss: 1.251286 \tValidation Loss: 2.658350\n",
      "Epoch: 33244 \tTraining Loss: 1.268324 \tValidation Loss: 2.660213\n",
      "Epoch: 33245 \tTraining Loss: 1.296117 \tValidation Loss: 2.658527\n",
      "Epoch: 33246 \tTraining Loss: 1.249895 \tValidation Loss: 2.658944\n",
      "Epoch: 33247 \tTraining Loss: 1.261152 \tValidation Loss: 2.657938\n",
      "Epoch: 33248 \tTraining Loss: 1.211718 \tValidation Loss: 2.659723\n",
      "Epoch: 33249 \tTraining Loss: 1.290673 \tValidation Loss: 2.658691\n",
      "Epoch: 33250 \tTraining Loss: 1.211861 \tValidation Loss: 2.660178\n",
      "Epoch: 33251 \tTraining Loss: 1.238048 \tValidation Loss: 2.659572\n",
      "Epoch: 33252 \tTraining Loss: 1.263983 \tValidation Loss: 2.657707\n",
      "Epoch: 33253 \tTraining Loss: 1.270082 \tValidation Loss: 2.658710\n",
      "Epoch: 33254 \tTraining Loss: 1.250624 \tValidation Loss: 2.659707\n",
      "Epoch: 33255 \tTraining Loss: 1.279247 \tValidation Loss: 2.658427\n",
      "Epoch: 33256 \tTraining Loss: 1.243349 \tValidation Loss: 2.658404\n",
      "Epoch: 33257 \tTraining Loss: 1.295586 \tValidation Loss: 2.657847\n",
      "Epoch: 33258 \tTraining Loss: 1.227721 \tValidation Loss: 2.659258\n",
      "Epoch: 33259 \tTraining Loss: 1.247869 \tValidation Loss: 2.660190\n",
      "Epoch: 33260 \tTraining Loss: 1.310452 \tValidation Loss: 2.658084\n",
      "Epoch: 33261 \tTraining Loss: 1.250024 \tValidation Loss: 2.659851\n",
      "Epoch: 33262 \tTraining Loss: 1.284632 \tValidation Loss: 2.658670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33263 \tTraining Loss: 1.243088 \tValidation Loss: 2.658269\n",
      "Epoch: 33264 \tTraining Loss: 1.207593 \tValidation Loss: 2.658569\n",
      "Epoch: 33265 \tTraining Loss: 1.260790 \tValidation Loss: 2.659250\n",
      "Epoch: 33266 \tTraining Loss: 1.244461 \tValidation Loss: 2.660193\n",
      "Epoch: 33267 \tTraining Loss: 1.294596 \tValidation Loss: 2.660047\n",
      "Epoch: 33268 \tTraining Loss: 1.263510 \tValidation Loss: 2.659255\n",
      "Epoch: 33269 \tTraining Loss: 1.232850 \tValidation Loss: 2.658986\n",
      "Epoch: 33270 \tTraining Loss: 1.220444 \tValidation Loss: 2.661155\n",
      "Epoch: 33271 \tTraining Loss: 1.241175 \tValidation Loss: 2.659736\n",
      "Epoch: 33272 \tTraining Loss: 1.252714 \tValidation Loss: 2.661658\n",
      "Epoch: 33273 \tTraining Loss: 1.271496 \tValidation Loss: 2.659002\n",
      "Epoch: 33274 \tTraining Loss: 1.200217 \tValidation Loss: 2.659820\n",
      "Epoch: 33275 \tTraining Loss: 1.262191 \tValidation Loss: 2.658911\n",
      "Epoch: 33276 \tTraining Loss: 1.276577 \tValidation Loss: 2.658787\n",
      "Epoch: 33277 \tTraining Loss: 1.266846 \tValidation Loss: 2.657530\n",
      "Epoch: 33278 \tTraining Loss: 1.258630 \tValidation Loss: 2.657827\n",
      "Epoch: 33279 \tTraining Loss: 1.306920 \tValidation Loss: 2.658770\n",
      "Epoch: 33280 \tTraining Loss: 1.268407 \tValidation Loss: 2.658904\n",
      "Epoch: 33281 \tTraining Loss: 1.235402 \tValidation Loss: 2.660732\n",
      "Epoch: 33282 \tTraining Loss: 1.227250 \tValidation Loss: 2.658692\n",
      "Epoch: 33283 \tTraining Loss: 1.194070 \tValidation Loss: 2.660483\n",
      "Epoch: 33284 \tTraining Loss: 1.260936 \tValidation Loss: 2.658844\n",
      "Epoch: 33285 \tTraining Loss: 1.259232 \tValidation Loss: 2.658574\n",
      "Epoch: 33286 \tTraining Loss: 1.220752 \tValidation Loss: 2.659178\n",
      "Epoch: 33287 \tTraining Loss: 1.241654 \tValidation Loss: 2.658101\n",
      "Epoch: 33288 \tTraining Loss: 1.217302 \tValidation Loss: 2.659501\n",
      "Epoch: 33289 \tTraining Loss: 1.264814 \tValidation Loss: 2.657526\n",
      "Epoch: 33290 \tTraining Loss: 1.229663 \tValidation Loss: 2.659515\n",
      "Epoch: 33291 \tTraining Loss: 1.276749 \tValidation Loss: 2.659399\n",
      "Epoch: 33292 \tTraining Loss: 1.231138 \tValidation Loss: 2.660781\n",
      "Epoch: 33293 \tTraining Loss: 1.246109 \tValidation Loss: 2.660055\n",
      "Epoch: 33294 \tTraining Loss: 1.250587 \tValidation Loss: 2.658239\n",
      "Epoch: 33295 \tTraining Loss: 1.317672 \tValidation Loss: 2.659779\n",
      "Epoch: 33296 \tTraining Loss: 1.296765 \tValidation Loss: 2.657926\n",
      "Epoch: 33297 \tTraining Loss: 1.268803 \tValidation Loss: 2.658301\n",
      "Epoch: 33298 \tTraining Loss: 1.238755 \tValidation Loss: 2.659000\n",
      "Epoch: 33299 \tTraining Loss: 1.260828 \tValidation Loss: 2.658552\n",
      "Epoch: 33300 \tTraining Loss: 1.269351 \tValidation Loss: 2.658581\n",
      "Epoch: 33301 \tTraining Loss: 1.268875 \tValidation Loss: 2.658366\n",
      "Epoch: 33302 \tTraining Loss: 1.238717 \tValidation Loss: 2.657655\n",
      "Epoch: 33303 \tTraining Loss: 1.215269 \tValidation Loss: 2.659629\n",
      "Epoch: 33304 \tTraining Loss: 1.263092 \tValidation Loss: 2.659657\n",
      "Epoch: 33305 \tTraining Loss: 1.258530 \tValidation Loss: 2.657865\n",
      "Epoch: 33306 \tTraining Loss: 1.245819 \tValidation Loss: 2.659372\n",
      "Epoch: 33307 \tTraining Loss: 1.275102 \tValidation Loss: 2.660049\n",
      "Epoch: 33308 \tTraining Loss: 1.282173 \tValidation Loss: 2.660035\n",
      "Epoch: 33309 \tTraining Loss: 1.247580 \tValidation Loss: 2.659757\n",
      "Epoch: 33310 \tTraining Loss: 1.239818 \tValidation Loss: 2.661129\n",
      "Epoch: 33311 \tTraining Loss: 1.248412 \tValidation Loss: 2.659867\n",
      "Epoch: 33312 \tTraining Loss: 1.265073 \tValidation Loss: 2.661278\n",
      "Epoch: 33313 \tTraining Loss: 1.204107 \tValidation Loss: 2.658903\n",
      "Epoch: 33314 \tTraining Loss: 1.256450 \tValidation Loss: 2.659532\n",
      "Epoch: 33315 \tTraining Loss: 1.254936 \tValidation Loss: 2.658859\n",
      "Epoch: 33316 \tTraining Loss: 1.298477 \tValidation Loss: 2.658919\n",
      "Epoch: 33317 \tTraining Loss: 1.228658 \tValidation Loss: 2.659321\n",
      "Epoch: 33318 \tTraining Loss: 1.241180 \tValidation Loss: 2.659013\n",
      "Epoch: 33319 \tTraining Loss: 1.268414 \tValidation Loss: 2.658416\n",
      "Epoch: 33320 \tTraining Loss: 1.237177 \tValidation Loss: 2.659644\n",
      "Epoch: 33321 \tTraining Loss: 1.272789 \tValidation Loss: 2.659787\n",
      "Epoch: 33322 \tTraining Loss: 1.270933 \tValidation Loss: 2.659271\n",
      "Epoch: 33323 \tTraining Loss: 1.245910 \tValidation Loss: 2.660904\n",
      "Epoch: 33324 \tTraining Loss: 1.274561 \tValidation Loss: 2.659737\n",
      "Epoch: 33325 \tTraining Loss: 1.292755 \tValidation Loss: 2.658552\n",
      "Epoch: 33326 \tTraining Loss: 1.286080 \tValidation Loss: 2.659002\n",
      "Epoch: 33327 \tTraining Loss: 1.257280 \tValidation Loss: 2.661317\n",
      "Epoch: 33328 \tTraining Loss: 1.271673 \tValidation Loss: 2.660843\n",
      "Epoch: 33329 \tTraining Loss: 1.207264 \tValidation Loss: 2.660408\n",
      "Epoch: 33330 \tTraining Loss: 1.257475 \tValidation Loss: 2.660349\n",
      "Epoch: 33331 \tTraining Loss: 1.254871 \tValidation Loss: 2.661628\n",
      "Epoch: 33332 \tTraining Loss: 1.227285 \tValidation Loss: 2.659726\n",
      "Epoch: 33333 \tTraining Loss: 1.272717 \tValidation Loss: 2.659043\n",
      "Epoch: 33334 \tTraining Loss: 1.213905 \tValidation Loss: 2.659516\n",
      "Epoch: 33335 \tTraining Loss: 1.267608 \tValidation Loss: 2.660151\n",
      "Epoch: 33336 \tTraining Loss: 1.200055 \tValidation Loss: 2.660608\n",
      "Epoch: 33337 \tTraining Loss: 1.211793 \tValidation Loss: 2.661080\n",
      "Epoch: 33338 \tTraining Loss: 1.246513 \tValidation Loss: 2.659749\n",
      "Epoch: 33339 \tTraining Loss: 1.263668 \tValidation Loss: 2.658739\n",
      "Epoch: 33340 \tTraining Loss: 1.253217 \tValidation Loss: 2.660164\n",
      "Epoch: 33341 \tTraining Loss: 1.266478 \tValidation Loss: 2.659928\n",
      "Epoch: 33342 \tTraining Loss: 1.246409 \tValidation Loss: 2.659392\n",
      "Epoch: 33343 \tTraining Loss: 1.226998 \tValidation Loss: 2.662102\n",
      "Epoch: 33344 \tTraining Loss: 1.252008 \tValidation Loss: 2.661091\n",
      "Epoch: 33345 \tTraining Loss: 1.195763 \tValidation Loss: 2.662007\n",
      "Epoch: 33346 \tTraining Loss: 1.284445 \tValidation Loss: 2.660085\n",
      "Epoch: 33347 \tTraining Loss: 1.248738 \tValidation Loss: 2.660993\n",
      "Epoch: 33348 \tTraining Loss: 1.255899 \tValidation Loss: 2.662582\n",
      "Epoch: 33349 \tTraining Loss: 1.215484 \tValidation Loss: 2.661484\n",
      "Epoch: 33350 \tTraining Loss: 1.254576 \tValidation Loss: 2.659980\n",
      "Epoch: 33351 \tTraining Loss: 1.217454 \tValidation Loss: 2.660689\n",
      "Epoch: 33352 \tTraining Loss: 1.231218 \tValidation Loss: 2.661139\n",
      "Epoch: 33353 \tTraining Loss: 1.263869 \tValidation Loss: 2.660340\n",
      "Epoch: 33354 \tTraining Loss: 1.234425 \tValidation Loss: 2.660194\n",
      "Epoch: 33355 \tTraining Loss: 1.239430 \tValidation Loss: 2.661476\n",
      "Epoch: 33356 \tTraining Loss: 1.224034 \tValidation Loss: 2.660582\n",
      "Epoch: 33357 \tTraining Loss: 1.195180 \tValidation Loss: 2.661733\n",
      "Epoch: 33358 \tTraining Loss: 1.209460 \tValidation Loss: 2.661709\n",
      "Epoch: 33359 \tTraining Loss: 1.285212 \tValidation Loss: 2.661846\n",
      "Epoch: 33360 \tTraining Loss: 1.246498 \tValidation Loss: 2.661061\n",
      "Epoch: 33361 \tTraining Loss: 1.303723 \tValidation Loss: 2.659105\n",
      "Epoch: 33362 \tTraining Loss: 1.241444 \tValidation Loss: 2.662191\n",
      "Epoch: 33363 \tTraining Loss: 1.241227 \tValidation Loss: 2.662809\n",
      "Epoch: 33364 \tTraining Loss: 1.266284 \tValidation Loss: 2.659946\n",
      "Epoch: 33365 \tTraining Loss: 1.269349 \tValidation Loss: 2.661173\n",
      "Epoch: 33366 \tTraining Loss: 1.267176 \tValidation Loss: 2.659885\n",
      "Epoch: 33367 \tTraining Loss: 1.286493 \tValidation Loss: 2.660164\n",
      "Epoch: 33368 \tTraining Loss: 1.230709 \tValidation Loss: 2.660509\n",
      "Epoch: 33369 \tTraining Loss: 1.268933 \tValidation Loss: 2.661177\n",
      "Epoch: 33370 \tTraining Loss: 1.296026 \tValidation Loss: 2.660965\n",
      "Epoch: 33371 \tTraining Loss: 1.193969 \tValidation Loss: 2.661860\n",
      "Epoch: 33372 \tTraining Loss: 1.228264 \tValidation Loss: 2.661897\n",
      "Epoch: 33373 \tTraining Loss: 1.235145 \tValidation Loss: 2.661317\n",
      "Epoch: 33374 \tTraining Loss: 1.229079 \tValidation Loss: 2.660675\n",
      "Epoch: 33375 \tTraining Loss: 1.234788 \tValidation Loss: 2.660294\n",
      "Epoch: 33376 \tTraining Loss: 1.224793 \tValidation Loss: 2.660745\n",
      "Epoch: 33377 \tTraining Loss: 1.280599 \tValidation Loss: 2.662117\n",
      "Epoch: 33378 \tTraining Loss: 1.226691 \tValidation Loss: 2.660026\n",
      "Epoch: 33379 \tTraining Loss: 1.230955 \tValidation Loss: 2.662354\n",
      "Epoch: 33380 \tTraining Loss: 1.249612 \tValidation Loss: 2.661437\n",
      "Epoch: 33381 \tTraining Loss: 1.254922 \tValidation Loss: 2.661632\n",
      "Epoch: 33382 \tTraining Loss: 1.258695 \tValidation Loss: 2.663071\n",
      "Epoch: 33383 \tTraining Loss: 1.194676 \tValidation Loss: 2.662973\n",
      "Epoch: 33384 \tTraining Loss: 1.278368 \tValidation Loss: 2.663242\n",
      "Epoch: 33385 \tTraining Loss: 1.293346 \tValidation Loss: 2.661245\n",
      "Epoch: 33386 \tTraining Loss: 1.279020 \tValidation Loss: 2.661471\n",
      "Epoch: 33387 \tTraining Loss: 1.281839 \tValidation Loss: 2.662067\n",
      "Epoch: 33388 \tTraining Loss: 1.271824 \tValidation Loss: 2.662407\n",
      "Epoch: 33389 \tTraining Loss: 1.252116 \tValidation Loss: 2.661862\n",
      "Epoch: 33390 \tTraining Loss: 1.251302 \tValidation Loss: 2.661422\n",
      "Epoch: 33391 \tTraining Loss: 1.255270 \tValidation Loss: 2.661735\n",
      "Epoch: 33392 \tTraining Loss: 1.245242 \tValidation Loss: 2.660849\n",
      "Epoch: 33393 \tTraining Loss: 1.292583 \tValidation Loss: 2.661532\n",
      "Epoch: 33394 \tTraining Loss: 1.215371 \tValidation Loss: 2.661473\n",
      "Epoch: 33395 \tTraining Loss: 1.206939 \tValidation Loss: 2.662787\n",
      "Epoch: 33396 \tTraining Loss: 1.280490 \tValidation Loss: 2.661965\n",
      "Epoch: 33397 \tTraining Loss: 1.214242 \tValidation Loss: 2.661804\n",
      "Epoch: 33398 \tTraining Loss: 1.246992 \tValidation Loss: 2.662227\n",
      "Epoch: 33399 \tTraining Loss: 1.208069 \tValidation Loss: 2.663414\n",
      "Epoch: 33400 \tTraining Loss: 1.228047 \tValidation Loss: 2.661490\n",
      "Epoch: 33401 \tTraining Loss: 1.241685 \tValidation Loss: 2.662795\n",
      "Epoch: 33402 \tTraining Loss: 1.265913 \tValidation Loss: 2.662728\n",
      "Epoch: 33403 \tTraining Loss: 1.269533 \tValidation Loss: 2.662175\n",
      "Epoch: 33404 \tTraining Loss: 1.253510 \tValidation Loss: 2.661691\n",
      "Epoch: 33405 \tTraining Loss: 1.251202 \tValidation Loss: 2.663303\n",
      "Epoch: 33406 \tTraining Loss: 1.314602 \tValidation Loss: 2.660379\n",
      "Epoch: 33407 \tTraining Loss: 1.234123 \tValidation Loss: 2.661716\n",
      "Epoch: 33408 \tTraining Loss: 1.228720 \tValidation Loss: 2.662392\n",
      "Epoch: 33409 \tTraining Loss: 1.252392 \tValidation Loss: 2.660962\n",
      "Epoch: 33410 \tTraining Loss: 1.318518 \tValidation Loss: 2.660446\n",
      "Epoch: 33411 \tTraining Loss: 1.292727 \tValidation Loss: 2.662867\n",
      "Epoch: 33412 \tTraining Loss: 1.249448 \tValidation Loss: 2.660909\n",
      "Epoch: 33413 \tTraining Loss: 1.245809 \tValidation Loss: 2.661783\n",
      "Epoch: 33414 \tTraining Loss: 1.239049 \tValidation Loss: 2.662348\n",
      "Epoch: 33415 \tTraining Loss: 1.278861 \tValidation Loss: 2.661681\n",
      "Epoch: 33416 \tTraining Loss: 1.272031 \tValidation Loss: 2.661453\n",
      "Epoch: 33417 \tTraining Loss: 1.201192 \tValidation Loss: 2.662154\n",
      "Epoch: 33418 \tTraining Loss: 1.254140 \tValidation Loss: 2.662032\n",
      "Epoch: 33419 \tTraining Loss: 1.242015 \tValidation Loss: 2.661922\n",
      "Epoch: 33420 \tTraining Loss: 1.282873 \tValidation Loss: 2.662401\n",
      "Epoch: 33421 \tTraining Loss: 1.236123 \tValidation Loss: 2.662935\n",
      "Epoch: 33422 \tTraining Loss: 1.217945 \tValidation Loss: 2.663034\n",
      "Epoch: 33423 \tTraining Loss: 1.262415 \tValidation Loss: 2.663096\n",
      "Epoch: 33424 \tTraining Loss: 1.245542 \tValidation Loss: 2.663172\n",
      "Epoch: 33425 \tTraining Loss: 1.276246 \tValidation Loss: 2.662549\n",
      "Epoch: 33426 \tTraining Loss: 1.250507 \tValidation Loss: 2.663680\n",
      "Epoch: 33427 \tTraining Loss: 1.222626 \tValidation Loss: 2.661847\n",
      "Epoch: 33428 \tTraining Loss: 1.278887 \tValidation Loss: 2.663019\n",
      "Epoch: 33429 \tTraining Loss: 1.222649 \tValidation Loss: 2.662245\n",
      "Epoch: 33430 \tTraining Loss: 1.291586 \tValidation Loss: 2.662359\n",
      "Epoch: 33431 \tTraining Loss: 1.212838 \tValidation Loss: 2.661947\n",
      "Epoch: 33432 \tTraining Loss: 1.248980 \tValidation Loss: 2.662148\n",
      "Epoch: 33433 \tTraining Loss: 1.280192 \tValidation Loss: 2.661460\n",
      "Epoch: 33434 \tTraining Loss: 1.236995 \tValidation Loss: 2.662986\n",
      "Epoch: 33435 \tTraining Loss: 1.228853 \tValidation Loss: 2.663138\n",
      "Epoch: 33436 \tTraining Loss: 1.272613 \tValidation Loss: 2.663290\n",
      "Epoch: 33437 \tTraining Loss: 1.293181 \tValidation Loss: 2.662993\n",
      "Epoch: 33438 \tTraining Loss: 1.263362 \tValidation Loss: 2.662780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33439 \tTraining Loss: 1.231656 \tValidation Loss: 2.663355\n",
      "Epoch: 33440 \tTraining Loss: 1.275982 \tValidation Loss: 2.661793\n",
      "Epoch: 33441 \tTraining Loss: 1.239299 \tValidation Loss: 2.663391\n",
      "Epoch: 33442 \tTraining Loss: 1.250697 \tValidation Loss: 2.661661\n",
      "Epoch: 33443 \tTraining Loss: 1.222360 \tValidation Loss: 2.662152\n",
      "Epoch: 33444 \tTraining Loss: 1.242736 \tValidation Loss: 2.662417\n",
      "Epoch: 33445 \tTraining Loss: 1.281077 \tValidation Loss: 2.662205\n",
      "Epoch: 33446 \tTraining Loss: 1.276777 \tValidation Loss: 2.664050\n",
      "Epoch: 33447 \tTraining Loss: 1.203328 \tValidation Loss: 2.663505\n",
      "Epoch: 33448 \tTraining Loss: 1.271662 \tValidation Loss: 2.661798\n",
      "Epoch: 33449 \tTraining Loss: 1.272753 \tValidation Loss: 2.663918\n",
      "Epoch: 33450 \tTraining Loss: 1.208284 \tValidation Loss: 2.664540\n",
      "Epoch: 33451 \tTraining Loss: 1.179186 \tValidation Loss: 2.664151\n",
      "Epoch: 33452 \tTraining Loss: 1.238966 \tValidation Loss: 2.662212\n",
      "Epoch: 33453 \tTraining Loss: 1.255648 \tValidation Loss: 2.662903\n",
      "Epoch: 33454 \tTraining Loss: 1.239466 \tValidation Loss: 2.662169\n",
      "Epoch: 33455 \tTraining Loss: 1.270231 \tValidation Loss: 2.663202\n",
      "Epoch: 33456 \tTraining Loss: 1.263335 \tValidation Loss: 2.661608\n",
      "Epoch: 33457 \tTraining Loss: 1.238997 \tValidation Loss: 2.664039\n",
      "Epoch: 33458 \tTraining Loss: 1.232244 \tValidation Loss: 2.662769\n",
      "Epoch: 33459 \tTraining Loss: 1.218930 \tValidation Loss: 2.662930\n",
      "Epoch: 33460 \tTraining Loss: 1.212607 \tValidation Loss: 2.663657\n",
      "Epoch: 33461 \tTraining Loss: 1.271186 \tValidation Loss: 2.662508\n",
      "Epoch: 33462 \tTraining Loss: 1.243943 \tValidation Loss: 2.661876\n",
      "Epoch: 33463 \tTraining Loss: 1.218340 \tValidation Loss: 2.663481\n",
      "Epoch: 33464 \tTraining Loss: 1.254302 \tValidation Loss: 2.662954\n",
      "Epoch: 33465 \tTraining Loss: 1.223909 \tValidation Loss: 2.663123\n",
      "Epoch: 33466 \tTraining Loss: 1.216161 \tValidation Loss: 2.663707\n",
      "Epoch: 33467 \tTraining Loss: 1.254857 \tValidation Loss: 2.663145\n",
      "Epoch: 33468 \tTraining Loss: 1.257875 \tValidation Loss: 2.662679\n",
      "Epoch: 33469 \tTraining Loss: 1.275932 \tValidation Loss: 2.662158\n",
      "Epoch: 33470 \tTraining Loss: 1.254742 \tValidation Loss: 2.663672\n",
      "Epoch: 33471 \tTraining Loss: 1.272034 \tValidation Loss: 2.662328\n",
      "Epoch: 33472 \tTraining Loss: 1.206072 \tValidation Loss: 2.663187\n",
      "Epoch: 33473 \tTraining Loss: 1.260969 \tValidation Loss: 2.662830\n",
      "Epoch: 33474 \tTraining Loss: 1.258479 \tValidation Loss: 2.663589\n",
      "Epoch: 33475 \tTraining Loss: 1.257966 \tValidation Loss: 2.663718\n",
      "Epoch: 33476 \tTraining Loss: 1.253772 \tValidation Loss: 2.663900\n",
      "Epoch: 33477 \tTraining Loss: 1.267228 \tValidation Loss: 2.663546\n",
      "Epoch: 33478 \tTraining Loss: 1.211768 \tValidation Loss: 2.665361\n",
      "Epoch: 33479 \tTraining Loss: 1.247817 \tValidation Loss: 2.664725\n",
      "Epoch: 33480 \tTraining Loss: 1.257591 \tValidation Loss: 2.663359\n",
      "Epoch: 33481 \tTraining Loss: 1.234519 \tValidation Loss: 2.663705\n",
      "Epoch: 33482 \tTraining Loss: 1.280305 \tValidation Loss: 2.663315\n",
      "Epoch: 33483 \tTraining Loss: 1.321577 \tValidation Loss: 2.662547\n",
      "Epoch: 33484 \tTraining Loss: 1.284302 \tValidation Loss: 2.664846\n",
      "Epoch: 33485 \tTraining Loss: 1.250731 \tValidation Loss: 2.663639\n",
      "Epoch: 33486 \tTraining Loss: 1.263812 \tValidation Loss: 2.664187\n",
      "Epoch: 33487 \tTraining Loss: 1.250427 \tValidation Loss: 2.663317\n",
      "Epoch: 33488 \tTraining Loss: 1.236977 \tValidation Loss: 2.664028\n",
      "Epoch: 33489 \tTraining Loss: 1.259031 \tValidation Loss: 2.662943\n",
      "Epoch: 33490 \tTraining Loss: 1.261001 \tValidation Loss: 2.663011\n",
      "Epoch: 33491 \tTraining Loss: 1.262983 \tValidation Loss: 2.663572\n",
      "Epoch: 33492 \tTraining Loss: 1.218416 \tValidation Loss: 2.664084\n",
      "Epoch: 33493 \tTraining Loss: 1.279310 \tValidation Loss: 2.663968\n",
      "Epoch: 33494 \tTraining Loss: 1.232069 \tValidation Loss: 2.664963\n",
      "Epoch: 33495 \tTraining Loss: 1.245414 \tValidation Loss: 2.664678\n",
      "Epoch: 33496 \tTraining Loss: 1.226384 \tValidation Loss: 2.664625\n",
      "Epoch: 33497 \tTraining Loss: 1.207151 \tValidation Loss: 2.664768\n",
      "Epoch: 33498 \tTraining Loss: 1.237328 \tValidation Loss: 2.664474\n",
      "Epoch: 33499 \tTraining Loss: 1.232641 \tValidation Loss: 2.664995\n",
      "Epoch: 33500 \tTraining Loss: 1.253661 \tValidation Loss: 2.666305\n",
      "Epoch: 33501 \tTraining Loss: 1.199950 \tValidation Loss: 2.663456\n",
      "Epoch: 33502 \tTraining Loss: 1.261275 \tValidation Loss: 2.663317\n",
      "Epoch: 33503 \tTraining Loss: 1.263100 \tValidation Loss: 2.664172\n",
      "Epoch: 33504 \tTraining Loss: 1.269381 \tValidation Loss: 2.662959\n",
      "Epoch: 33505 \tTraining Loss: 1.262636 \tValidation Loss: 2.664701\n",
      "Epoch: 33506 \tTraining Loss: 1.286206 \tValidation Loss: 2.663375\n",
      "Epoch: 33507 \tTraining Loss: 1.236849 \tValidation Loss: 2.665302\n",
      "Epoch: 33508 \tTraining Loss: 1.278802 \tValidation Loss: 2.663869\n",
      "Epoch: 33509 \tTraining Loss: 1.267200 \tValidation Loss: 2.662720\n",
      "Epoch: 33510 \tTraining Loss: 1.236732 \tValidation Loss: 2.663613\n",
      "Epoch: 33511 \tTraining Loss: 1.231395 \tValidation Loss: 2.664344\n",
      "Epoch: 33512 \tTraining Loss: 1.271735 \tValidation Loss: 2.664560\n",
      "Epoch: 33513 \tTraining Loss: 1.231273 \tValidation Loss: 2.664845\n",
      "Epoch: 33514 \tTraining Loss: 1.302058 \tValidation Loss: 2.663316\n",
      "Epoch: 33515 \tTraining Loss: 1.249940 \tValidation Loss: 2.665644\n",
      "Epoch: 33516 \tTraining Loss: 1.268391 \tValidation Loss: 2.664808\n",
      "Epoch: 33517 \tTraining Loss: 1.236866 \tValidation Loss: 2.663035\n",
      "Epoch: 33518 \tTraining Loss: 1.275710 \tValidation Loss: 2.664335\n",
      "Epoch: 33519 \tTraining Loss: 1.199032 \tValidation Loss: 2.664931\n",
      "Epoch: 33520 \tTraining Loss: 1.268413 \tValidation Loss: 2.663144\n",
      "Epoch: 33521 \tTraining Loss: 1.211179 \tValidation Loss: 2.663543\n",
      "Epoch: 33522 \tTraining Loss: 1.264076 \tValidation Loss: 2.663216\n",
      "Epoch: 33523 \tTraining Loss: 1.259277 \tValidation Loss: 2.664181\n",
      "Epoch: 33524 \tTraining Loss: 1.281679 \tValidation Loss: 2.664618\n",
      "Epoch: 33525 \tTraining Loss: 1.250182 \tValidation Loss: 2.662575\n",
      "Epoch: 33526 \tTraining Loss: 1.276157 \tValidation Loss: 2.665012\n",
      "Epoch: 33527 \tTraining Loss: 1.226220 \tValidation Loss: 2.663691\n",
      "Epoch: 33528 \tTraining Loss: 1.254919 \tValidation Loss: 2.665076\n",
      "Epoch: 33529 \tTraining Loss: 1.250753 \tValidation Loss: 2.663142\n",
      "Epoch: 33530 \tTraining Loss: 1.217103 \tValidation Loss: 2.662885\n",
      "Epoch: 33531 \tTraining Loss: 1.225699 \tValidation Loss: 2.664232\n",
      "Epoch: 33532 \tTraining Loss: 1.272475 \tValidation Loss: 2.662436\n",
      "Epoch: 33533 \tTraining Loss: 1.238798 \tValidation Loss: 2.663763\n",
      "Epoch: 33534 \tTraining Loss: 1.209863 \tValidation Loss: 2.664263\n",
      "Epoch: 33535 \tTraining Loss: 1.254731 \tValidation Loss: 2.663849\n",
      "Epoch: 33536 \tTraining Loss: 1.185417 \tValidation Loss: 2.665627\n",
      "Epoch: 33537 \tTraining Loss: 1.267083 \tValidation Loss: 2.663160\n",
      "Epoch: 33538 \tTraining Loss: 1.232828 \tValidation Loss: 2.664492\n",
      "Epoch: 33539 \tTraining Loss: 1.262370 \tValidation Loss: 2.665124\n",
      "Epoch: 33540 \tTraining Loss: 1.263873 \tValidation Loss: 2.663721\n",
      "Epoch: 33541 \tTraining Loss: 1.248530 \tValidation Loss: 2.664208\n",
      "Epoch: 33542 \tTraining Loss: 1.265124 \tValidation Loss: 2.663311\n",
      "Epoch: 33543 \tTraining Loss: 1.225060 \tValidation Loss: 2.665637\n",
      "Epoch: 33544 \tTraining Loss: 1.248130 \tValidation Loss: 2.666384\n",
      "Epoch: 33545 \tTraining Loss: 1.239498 \tValidation Loss: 2.665245\n",
      "Epoch: 33546 \tTraining Loss: 1.231609 \tValidation Loss: 2.664707\n",
      "Epoch: 33547 \tTraining Loss: 1.245217 \tValidation Loss: 2.664440\n",
      "Epoch: 33548 \tTraining Loss: 1.202620 \tValidation Loss: 2.666357\n",
      "Epoch: 33549 \tTraining Loss: 1.206182 \tValidation Loss: 2.666043\n",
      "Epoch: 33550 \tTraining Loss: 1.244143 \tValidation Loss: 2.665712\n",
      "Epoch: 33551 \tTraining Loss: 1.226929 \tValidation Loss: 2.665616\n",
      "Epoch: 33552 \tTraining Loss: 1.241086 \tValidation Loss: 2.665610\n",
      "Epoch: 33553 \tTraining Loss: 1.246295 \tValidation Loss: 2.664696\n",
      "Epoch: 33554 \tTraining Loss: 1.250618 \tValidation Loss: 2.664948\n",
      "Epoch: 33555 \tTraining Loss: 1.225023 \tValidation Loss: 2.665250\n",
      "Epoch: 33556 \tTraining Loss: 1.257512 \tValidation Loss: 2.666046\n",
      "Epoch: 33557 \tTraining Loss: 1.234594 \tValidation Loss: 2.666388\n",
      "Epoch: 33558 \tTraining Loss: 1.276684 \tValidation Loss: 2.665283\n",
      "Epoch: 33559 \tTraining Loss: 1.305147 \tValidation Loss: 2.664800\n",
      "Epoch: 33560 \tTraining Loss: 1.273913 \tValidation Loss: 2.665124\n",
      "Epoch: 33561 \tTraining Loss: 1.235572 \tValidation Loss: 2.665570\n",
      "Epoch: 33562 \tTraining Loss: 1.282634 \tValidation Loss: 2.664788\n",
      "Epoch: 33563 \tTraining Loss: 1.214653 \tValidation Loss: 2.664964\n",
      "Epoch: 33564 \tTraining Loss: 1.233298 \tValidation Loss: 2.665495\n",
      "Epoch: 33565 \tTraining Loss: 1.251216 \tValidation Loss: 2.664745\n",
      "Epoch: 33566 \tTraining Loss: 1.238961 \tValidation Loss: 2.665815\n",
      "Epoch: 33567 \tTraining Loss: 1.235347 \tValidation Loss: 2.666722\n",
      "Epoch: 33568 \tTraining Loss: 1.282412 \tValidation Loss: 2.664810\n",
      "Epoch: 33569 \tTraining Loss: 1.248827 \tValidation Loss: 2.665132\n",
      "Epoch: 33570 \tTraining Loss: 1.253883 \tValidation Loss: 2.666533\n",
      "Epoch: 33571 \tTraining Loss: 1.247563 \tValidation Loss: 2.667149\n",
      "Epoch: 33572 \tTraining Loss: 1.228967 \tValidation Loss: 2.666216\n",
      "Epoch: 33573 \tTraining Loss: 1.269304 \tValidation Loss: 2.663010\n",
      "Epoch: 33574 \tTraining Loss: 1.258996 \tValidation Loss: 2.666163\n",
      "Epoch: 33575 \tTraining Loss: 1.261474 \tValidation Loss: 2.666386\n",
      "Epoch: 33576 \tTraining Loss: 1.223645 \tValidation Loss: 2.666445\n",
      "Epoch: 33577 \tTraining Loss: 1.259578 \tValidation Loss: 2.664097\n",
      "Epoch: 33578 \tTraining Loss: 1.277773 \tValidation Loss: 2.665899\n",
      "Epoch: 33579 \tTraining Loss: 1.220357 \tValidation Loss: 2.666722\n",
      "Epoch: 33580 \tTraining Loss: 1.300065 \tValidation Loss: 2.665905\n",
      "Epoch: 33581 \tTraining Loss: 1.255743 \tValidation Loss: 2.666516\n",
      "Epoch: 33582 \tTraining Loss: 1.204316 \tValidation Loss: 2.665986\n",
      "Epoch: 33583 \tTraining Loss: 1.232034 \tValidation Loss: 2.665421\n",
      "Epoch: 33584 \tTraining Loss: 1.215017 \tValidation Loss: 2.667671\n",
      "Epoch: 33585 \tTraining Loss: 1.228538 \tValidation Loss: 2.668509\n",
      "Epoch: 33586 \tTraining Loss: 1.267515 \tValidation Loss: 2.666244\n",
      "Epoch: 33587 \tTraining Loss: 1.215217 \tValidation Loss: 2.666427\n",
      "Epoch: 33588 \tTraining Loss: 1.204976 \tValidation Loss: 2.666533\n",
      "Epoch: 33589 \tTraining Loss: 1.252385 \tValidation Loss: 2.667160\n",
      "Epoch: 33590 \tTraining Loss: 1.205430 \tValidation Loss: 2.667269\n",
      "Epoch: 33591 \tTraining Loss: 1.244761 \tValidation Loss: 2.666796\n",
      "Epoch: 33592 \tTraining Loss: 1.249385 \tValidation Loss: 2.667757\n",
      "Epoch: 33593 \tTraining Loss: 1.273323 \tValidation Loss: 2.666749\n",
      "Epoch: 33594 \tTraining Loss: 1.249493 \tValidation Loss: 2.665905\n",
      "Epoch: 33595 \tTraining Loss: 1.279696 \tValidation Loss: 2.665197\n",
      "Epoch: 33596 \tTraining Loss: 1.206771 \tValidation Loss: 2.666936\n",
      "Epoch: 33597 \tTraining Loss: 1.271337 \tValidation Loss: 2.665387\n",
      "Epoch: 33598 \tTraining Loss: 1.220041 \tValidation Loss: 2.666866\n",
      "Epoch: 33599 \tTraining Loss: 1.249989 \tValidation Loss: 2.665891\n",
      "Epoch: 33600 \tTraining Loss: 1.266824 \tValidation Loss: 2.666940\n",
      "Epoch: 33601 \tTraining Loss: 1.211192 \tValidation Loss: 2.666533\n",
      "Epoch: 33602 \tTraining Loss: 1.289893 \tValidation Loss: 2.667056\n",
      "Epoch: 33603 \tTraining Loss: 1.217710 \tValidation Loss: 2.666682\n",
      "Epoch: 33604 \tTraining Loss: 1.258605 \tValidation Loss: 2.665598\n",
      "Epoch: 33605 \tTraining Loss: 1.226044 \tValidation Loss: 2.666309\n",
      "Epoch: 33606 \tTraining Loss: 1.289160 \tValidation Loss: 2.666123\n",
      "Epoch: 33607 \tTraining Loss: 1.274835 \tValidation Loss: 2.666708\n",
      "Epoch: 33608 \tTraining Loss: 1.263287 \tValidation Loss: 2.666971\n",
      "Epoch: 33609 \tTraining Loss: 1.229520 \tValidation Loss: 2.666136\n",
      "Epoch: 33610 \tTraining Loss: 1.248661 \tValidation Loss: 2.666035\n",
      "Epoch: 33611 \tTraining Loss: 1.232416 \tValidation Loss: 2.666755\n",
      "Epoch: 33612 \tTraining Loss: 1.236157 \tValidation Loss: 2.666080\n",
      "Epoch: 33613 \tTraining Loss: 1.236838 \tValidation Loss: 2.666436\n",
      "Epoch: 33614 \tTraining Loss: 1.231537 \tValidation Loss: 2.666987\n",
      "Epoch: 33615 \tTraining Loss: 1.239788 \tValidation Loss: 2.666290\n",
      "Epoch: 33616 \tTraining Loss: 1.248903 \tValidation Loss: 2.664346\n",
      "Epoch: 33617 \tTraining Loss: 1.247444 \tValidation Loss: 2.667086\n",
      "Epoch: 33618 \tTraining Loss: 1.317818 \tValidation Loss: 2.665909\n",
      "Epoch: 33619 \tTraining Loss: 1.204521 \tValidation Loss: 2.666524\n",
      "Epoch: 33620 \tTraining Loss: 1.249305 \tValidation Loss: 2.666740\n",
      "Epoch: 33621 \tTraining Loss: 1.227094 \tValidation Loss: 2.668468\n",
      "Epoch: 33622 \tTraining Loss: 1.248471 \tValidation Loss: 2.666151\n",
      "Epoch: 33623 \tTraining Loss: 1.273172 \tValidation Loss: 2.665562\n",
      "Epoch: 33624 \tTraining Loss: 1.288151 \tValidation Loss: 2.666978\n",
      "Epoch: 33625 \tTraining Loss: 1.237020 \tValidation Loss: 2.666018\n",
      "Epoch: 33626 \tTraining Loss: 1.270184 \tValidation Loss: 2.667119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33627 \tTraining Loss: 1.228168 \tValidation Loss: 2.668236\n",
      "Epoch: 33628 \tTraining Loss: 1.252516 \tValidation Loss: 2.666752\n",
      "Epoch: 33629 \tTraining Loss: 1.238370 \tValidation Loss: 2.669029\n",
      "Epoch: 33630 \tTraining Loss: 1.240469 \tValidation Loss: 2.667317\n",
      "Epoch: 33631 \tTraining Loss: 1.235862 \tValidation Loss: 2.666697\n",
      "Epoch: 33632 \tTraining Loss: 1.265371 \tValidation Loss: 2.667806\n",
      "Epoch: 33633 \tTraining Loss: 1.264185 \tValidation Loss: 2.665680\n",
      "Epoch: 33634 \tTraining Loss: 1.245488 \tValidation Loss: 2.666645\n",
      "Epoch: 33635 \tTraining Loss: 1.265418 \tValidation Loss: 2.667442\n",
      "Epoch: 33636 \tTraining Loss: 1.267171 \tValidation Loss: 2.666632\n",
      "Epoch: 33637 \tTraining Loss: 1.220997 \tValidation Loss: 2.667498\n",
      "Epoch: 33638 \tTraining Loss: 1.216313 \tValidation Loss: 2.667947\n",
      "Epoch: 33639 \tTraining Loss: 1.234791 \tValidation Loss: 2.666476\n",
      "Epoch: 33640 \tTraining Loss: 1.200041 \tValidation Loss: 2.667605\n",
      "Epoch: 33641 \tTraining Loss: 1.275204 \tValidation Loss: 2.666643\n",
      "Epoch: 33642 \tTraining Loss: 1.271913 \tValidation Loss: 2.667110\n",
      "Epoch: 33643 \tTraining Loss: 1.218961 \tValidation Loss: 2.667567\n",
      "Epoch: 33644 \tTraining Loss: 1.261057 \tValidation Loss: 2.666283\n",
      "Epoch: 33645 \tTraining Loss: 1.239864 \tValidation Loss: 2.667495\n",
      "Epoch: 33646 \tTraining Loss: 1.224091 \tValidation Loss: 2.668022\n",
      "Epoch: 33647 \tTraining Loss: 1.274955 \tValidation Loss: 2.667894\n",
      "Epoch: 33648 \tTraining Loss: 1.239952 \tValidation Loss: 2.666949\n",
      "Epoch: 33649 \tTraining Loss: 1.240082 \tValidation Loss: 2.668349\n",
      "Epoch: 33650 \tTraining Loss: 1.238260 \tValidation Loss: 2.666413\n",
      "Epoch: 33651 \tTraining Loss: 1.226734 \tValidation Loss: 2.667156\n",
      "Epoch: 33652 \tTraining Loss: 1.256965 \tValidation Loss: 2.666602\n",
      "Epoch: 33653 \tTraining Loss: 1.241563 \tValidation Loss: 2.667017\n",
      "Epoch: 33654 \tTraining Loss: 1.242100 \tValidation Loss: 2.666975\n",
      "Epoch: 33655 \tTraining Loss: 1.292995 \tValidation Loss: 2.666190\n",
      "Epoch: 33656 \tTraining Loss: 1.254541 \tValidation Loss: 2.667460\n",
      "Epoch: 33657 \tTraining Loss: 1.229739 \tValidation Loss: 2.668251\n",
      "Epoch: 33658 \tTraining Loss: 1.307549 \tValidation Loss: 2.666115\n",
      "Epoch: 33659 \tTraining Loss: 1.252106 \tValidation Loss: 2.667654\n",
      "Epoch: 33660 \tTraining Loss: 1.258878 \tValidation Loss: 2.665156\n",
      "Epoch: 33661 \tTraining Loss: 1.214966 \tValidation Loss: 2.667594\n",
      "Epoch: 33662 \tTraining Loss: 1.253028 \tValidation Loss: 2.666708\n",
      "Epoch: 33663 \tTraining Loss: 1.259445 \tValidation Loss: 2.668274\n",
      "Epoch: 33664 \tTraining Loss: 1.237967 \tValidation Loss: 2.667570\n",
      "Epoch: 33665 \tTraining Loss: 1.260842 \tValidation Loss: 2.667589\n",
      "Epoch: 33666 \tTraining Loss: 1.202512 \tValidation Loss: 2.667191\n",
      "Epoch: 33667 \tTraining Loss: 1.255310 \tValidation Loss: 2.668947\n",
      "Epoch: 33668 \tTraining Loss: 1.302244 \tValidation Loss: 2.666044\n",
      "Epoch: 33669 \tTraining Loss: 1.244737 \tValidation Loss: 2.667491\n",
      "Epoch: 33670 \tTraining Loss: 1.264636 \tValidation Loss: 2.667207\n",
      "Epoch: 33671 \tTraining Loss: 1.196913 \tValidation Loss: 2.667188\n",
      "Epoch: 33672 \tTraining Loss: 1.241351 \tValidation Loss: 2.668887\n",
      "Epoch: 33673 \tTraining Loss: 1.271426 \tValidation Loss: 2.668144\n",
      "Epoch: 33674 \tTraining Loss: 1.245672 \tValidation Loss: 2.668735\n",
      "Epoch: 33675 \tTraining Loss: 1.236759 \tValidation Loss: 2.668803\n",
      "Epoch: 33676 \tTraining Loss: 1.233538 \tValidation Loss: 2.668349\n",
      "Epoch: 33677 \tTraining Loss: 1.223912 \tValidation Loss: 2.668515\n",
      "Epoch: 33678 \tTraining Loss: 1.251499 \tValidation Loss: 2.668616\n",
      "Epoch: 33679 \tTraining Loss: 1.273590 \tValidation Loss: 2.668506\n",
      "Epoch: 33680 \tTraining Loss: 1.277494 \tValidation Loss: 2.667721\n",
      "Epoch: 33681 \tTraining Loss: 1.226958 \tValidation Loss: 2.667824\n",
      "Epoch: 33682 \tTraining Loss: 1.276851 \tValidation Loss: 2.668034\n",
      "Epoch: 33683 \tTraining Loss: 1.219601 \tValidation Loss: 2.668561\n",
      "Epoch: 33684 \tTraining Loss: 1.249869 \tValidation Loss: 2.668869\n",
      "Epoch: 33685 \tTraining Loss: 1.189794 \tValidation Loss: 2.668958\n",
      "Epoch: 33686 \tTraining Loss: 1.244173 \tValidation Loss: 2.669073\n",
      "Epoch: 33687 \tTraining Loss: 1.238967 \tValidation Loss: 2.668598\n",
      "Epoch: 33688 \tTraining Loss: 1.268171 \tValidation Loss: 2.668285\n",
      "Epoch: 33689 \tTraining Loss: 1.237613 \tValidation Loss: 2.667567\n",
      "Epoch: 33690 \tTraining Loss: 1.218537 \tValidation Loss: 2.669589\n",
      "Epoch: 33691 \tTraining Loss: 1.267623 \tValidation Loss: 2.668112\n",
      "Epoch: 33692 \tTraining Loss: 1.203396 \tValidation Loss: 2.668183\n",
      "Epoch: 33693 \tTraining Loss: 1.231969 \tValidation Loss: 2.667292\n",
      "Epoch: 33694 \tTraining Loss: 1.278734 \tValidation Loss: 2.669439\n",
      "Epoch: 33695 \tTraining Loss: 1.298040 \tValidation Loss: 2.669541\n",
      "Epoch: 33696 \tTraining Loss: 1.228436 \tValidation Loss: 2.668645\n",
      "Epoch: 33697 \tTraining Loss: 1.219003 \tValidation Loss: 2.670001\n",
      "Epoch: 33698 \tTraining Loss: 1.259885 \tValidation Loss: 2.667735\n",
      "Epoch: 33699 \tTraining Loss: 1.307973 \tValidation Loss: 2.667347\n",
      "Epoch: 33700 \tTraining Loss: 1.238869 \tValidation Loss: 2.670595\n",
      "Epoch: 33701 \tTraining Loss: 1.226528 \tValidation Loss: 2.667159\n",
      "Epoch: 33702 \tTraining Loss: 1.231970 \tValidation Loss: 2.668395\n",
      "Epoch: 33703 \tTraining Loss: 1.284034 \tValidation Loss: 2.668902\n",
      "Epoch: 33704 \tTraining Loss: 1.204630 \tValidation Loss: 2.668805\n",
      "Epoch: 33705 \tTraining Loss: 1.192529 \tValidation Loss: 2.668600\n",
      "Epoch: 33706 \tTraining Loss: 1.226461 \tValidation Loss: 2.669096\n",
      "Epoch: 33707 \tTraining Loss: 1.237768 \tValidation Loss: 2.668752\n",
      "Epoch: 33708 \tTraining Loss: 1.262213 \tValidation Loss: 2.669570\n",
      "Epoch: 33709 \tTraining Loss: 1.262200 \tValidation Loss: 2.668034\n",
      "Epoch: 33710 \tTraining Loss: 1.283514 \tValidation Loss: 2.668534\n",
      "Epoch: 33711 \tTraining Loss: 1.243566 \tValidation Loss: 2.668682\n",
      "Epoch: 33712 \tTraining Loss: 1.272554 \tValidation Loss: 2.667070\n",
      "Epoch: 33713 \tTraining Loss: 1.190534 \tValidation Loss: 2.669253\n",
      "Epoch: 33714 \tTraining Loss: 1.253240 \tValidation Loss: 2.668833\n",
      "Epoch: 33715 \tTraining Loss: 1.276196 \tValidation Loss: 2.666708\n",
      "Epoch: 33716 \tTraining Loss: 1.259146 \tValidation Loss: 2.668842\n",
      "Epoch: 33717 \tTraining Loss: 1.248470 \tValidation Loss: 2.669211\n",
      "Epoch: 33718 \tTraining Loss: 1.205874 \tValidation Loss: 2.669176\n",
      "Epoch: 33719 \tTraining Loss: 1.216495 \tValidation Loss: 2.668104\n",
      "Epoch: 33720 \tTraining Loss: 1.238883 \tValidation Loss: 2.670758\n",
      "Epoch: 33721 \tTraining Loss: 1.271397 \tValidation Loss: 2.668601\n",
      "Epoch: 33722 \tTraining Loss: 1.257958 \tValidation Loss: 2.670154\n",
      "Epoch: 33723 \tTraining Loss: 1.231043 \tValidation Loss: 2.669498\n",
      "Epoch: 33724 \tTraining Loss: 1.259452 \tValidation Loss: 2.669394\n",
      "Epoch: 33725 \tTraining Loss: 1.279117 \tValidation Loss: 2.669612\n",
      "Epoch: 33726 \tTraining Loss: 1.240521 \tValidation Loss: 2.669784\n",
      "Epoch: 33727 \tTraining Loss: 1.256450 \tValidation Loss: 2.668537\n",
      "Epoch: 33728 \tTraining Loss: 1.242511 \tValidation Loss: 2.666914\n",
      "Epoch: 33729 \tTraining Loss: 1.227144 \tValidation Loss: 2.669122\n",
      "Epoch: 33730 \tTraining Loss: 1.250280 \tValidation Loss: 2.669381\n",
      "Epoch: 33731 \tTraining Loss: 1.242284 \tValidation Loss: 2.670078\n",
      "Epoch: 33732 \tTraining Loss: 1.246718 \tValidation Loss: 2.668938\n",
      "Epoch: 33733 \tTraining Loss: 1.252181 \tValidation Loss: 2.668599\n",
      "Epoch: 33734 \tTraining Loss: 1.218245 \tValidation Loss: 2.669493\n",
      "Epoch: 33735 \tTraining Loss: 1.249451 \tValidation Loss: 2.670929\n",
      "Epoch: 33736 \tTraining Loss: 1.240120 \tValidation Loss: 2.667964\n",
      "Epoch: 33737 \tTraining Loss: 1.271576 \tValidation Loss: 2.669005\n",
      "Epoch: 33738 \tTraining Loss: 1.228827 \tValidation Loss: 2.669531\n",
      "Epoch: 33739 \tTraining Loss: 1.219151 \tValidation Loss: 2.670783\n",
      "Epoch: 33740 \tTraining Loss: 1.259808 \tValidation Loss: 2.669609\n",
      "Epoch: 33741 \tTraining Loss: 1.257881 \tValidation Loss: 2.669909\n",
      "Epoch: 33742 \tTraining Loss: 1.232931 \tValidation Loss: 2.670737\n",
      "Epoch: 33743 \tTraining Loss: 1.281676 \tValidation Loss: 2.669213\n",
      "Epoch: 33744 \tTraining Loss: 1.264864 \tValidation Loss: 2.670294\n",
      "Epoch: 33745 \tTraining Loss: 1.235675 \tValidation Loss: 2.670203\n",
      "Epoch: 33746 \tTraining Loss: 1.200755 \tValidation Loss: 2.669918\n",
      "Epoch: 33747 \tTraining Loss: 1.257114 \tValidation Loss: 2.670118\n",
      "Epoch: 33748 \tTraining Loss: 1.299902 \tValidation Loss: 2.669738\n",
      "Epoch: 33749 \tTraining Loss: 1.253143 \tValidation Loss: 2.669955\n",
      "Epoch: 33750 \tTraining Loss: 1.228493 \tValidation Loss: 2.670845\n",
      "Epoch: 33751 \tTraining Loss: 1.233518 \tValidation Loss: 2.671057\n",
      "Epoch: 33752 \tTraining Loss: 1.278045 \tValidation Loss: 2.670150\n",
      "Epoch: 33753 \tTraining Loss: 1.270975 \tValidation Loss: 2.669445\n",
      "Epoch: 33754 \tTraining Loss: 1.221354 \tValidation Loss: 2.670389\n",
      "Epoch: 33755 \tTraining Loss: 1.223249 \tValidation Loss: 2.670268\n",
      "Epoch: 33756 \tTraining Loss: 1.261343 \tValidation Loss: 2.670392\n",
      "Epoch: 33757 \tTraining Loss: 1.254358 \tValidation Loss: 2.671017\n",
      "Epoch: 33758 \tTraining Loss: 1.256690 \tValidation Loss: 2.671292\n",
      "Epoch: 33759 \tTraining Loss: 1.246789 \tValidation Loss: 2.669522\n",
      "Epoch: 33760 \tTraining Loss: 1.227768 \tValidation Loss: 2.671247\n",
      "Epoch: 33761 \tTraining Loss: 1.193706 \tValidation Loss: 2.669783\n",
      "Epoch: 33762 \tTraining Loss: 1.260539 \tValidation Loss: 2.670172\n",
      "Epoch: 33763 \tTraining Loss: 1.201578 \tValidation Loss: 2.669497\n",
      "Epoch: 33764 \tTraining Loss: 1.235551 \tValidation Loss: 2.671306\n",
      "Epoch: 33765 \tTraining Loss: 1.206670 \tValidation Loss: 2.671687\n",
      "Epoch: 33766 \tTraining Loss: 1.255286 \tValidation Loss: 2.669611\n",
      "Epoch: 33767 \tTraining Loss: 1.229694 \tValidation Loss: 2.670578\n",
      "Epoch: 33768 \tTraining Loss: 1.262435 \tValidation Loss: 2.670221\n",
      "Epoch: 33769 \tTraining Loss: 1.246719 \tValidation Loss: 2.670622\n",
      "Epoch: 33770 \tTraining Loss: 1.259244 \tValidation Loss: 2.670554\n",
      "Epoch: 33771 \tTraining Loss: 1.198677 \tValidation Loss: 2.670791\n",
      "Epoch: 33772 \tTraining Loss: 1.229470 \tValidation Loss: 2.670796\n",
      "Epoch: 33773 \tTraining Loss: 1.275014 \tValidation Loss: 2.669706\n",
      "Epoch: 33774 \tTraining Loss: 1.229426 \tValidation Loss: 2.670928\n",
      "Epoch: 33775 \tTraining Loss: 1.249060 \tValidation Loss: 2.671883\n",
      "Epoch: 33776 \tTraining Loss: 1.225810 \tValidation Loss: 2.670992\n",
      "Epoch: 33777 \tTraining Loss: 1.196169 \tValidation Loss: 2.670682\n",
      "Epoch: 33778 \tTraining Loss: 1.213675 \tValidation Loss: 2.672382\n",
      "Epoch: 33779 \tTraining Loss: 1.281402 \tValidation Loss: 2.670062\n",
      "Epoch: 33780 \tTraining Loss: 1.185279 \tValidation Loss: 2.672030\n",
      "Epoch: 33781 \tTraining Loss: 1.229957 \tValidation Loss: 2.671399\n",
      "Epoch: 33782 \tTraining Loss: 1.266605 \tValidation Loss: 2.672723\n",
      "Epoch: 33783 \tTraining Loss: 1.250691 \tValidation Loss: 2.671395\n",
      "Epoch: 33784 \tTraining Loss: 1.258189 \tValidation Loss: 2.670373\n",
      "Epoch: 33785 \tTraining Loss: 1.177818 \tValidation Loss: 2.671074\n",
      "Epoch: 33786 \tTraining Loss: 1.263429 \tValidation Loss: 2.668644\n",
      "Epoch: 33787 \tTraining Loss: 1.173770 \tValidation Loss: 2.670427\n",
      "Epoch: 33788 \tTraining Loss: 1.234794 \tValidation Loss: 2.671828\n",
      "Epoch: 33789 \tTraining Loss: 1.260918 \tValidation Loss: 2.669708\n",
      "Epoch: 33790 \tTraining Loss: 1.257537 \tValidation Loss: 2.670717\n",
      "Epoch: 33791 \tTraining Loss: 1.188355 \tValidation Loss: 2.671721\n",
      "Epoch: 33792 \tTraining Loss: 1.237441 \tValidation Loss: 2.671717\n",
      "Epoch: 33793 \tTraining Loss: 1.266631 \tValidation Loss: 2.670691\n",
      "Epoch: 33794 \tTraining Loss: 1.242450 \tValidation Loss: 2.669978\n",
      "Epoch: 33795 \tTraining Loss: 1.241496 \tValidation Loss: 2.670315\n",
      "Epoch: 33796 \tTraining Loss: 1.245998 \tValidation Loss: 2.670580\n",
      "Epoch: 33797 \tTraining Loss: 1.248740 \tValidation Loss: 2.670599\n",
      "Epoch: 33798 \tTraining Loss: 1.262719 \tValidation Loss: 2.671061\n",
      "Epoch: 33799 \tTraining Loss: 1.257141 \tValidation Loss: 2.671203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33800 \tTraining Loss: 1.256860 \tValidation Loss: 2.671406\n",
      "Epoch: 33801 \tTraining Loss: 1.213980 \tValidation Loss: 2.671046\n",
      "Epoch: 33802 \tTraining Loss: 1.260276 \tValidation Loss: 2.670502\n",
      "Epoch: 33803 \tTraining Loss: 1.178581 \tValidation Loss: 2.672539\n",
      "Epoch: 33804 \tTraining Loss: 1.235591 \tValidation Loss: 2.670130\n",
      "Epoch: 33805 \tTraining Loss: 1.258299 \tValidation Loss: 2.672072\n",
      "Epoch: 33806 \tTraining Loss: 1.273519 \tValidation Loss: 2.670834\n",
      "Epoch: 33807 \tTraining Loss: 1.228457 \tValidation Loss: 2.670652\n",
      "Epoch: 33808 \tTraining Loss: 1.240069 \tValidation Loss: 2.671310\n",
      "Epoch: 33809 \tTraining Loss: 1.188439 \tValidation Loss: 2.671924\n",
      "Epoch: 33810 \tTraining Loss: 1.244070 \tValidation Loss: 2.670138\n",
      "Epoch: 33811 \tTraining Loss: 1.263186 \tValidation Loss: 2.672050\n",
      "Epoch: 33812 \tTraining Loss: 1.186462 \tValidation Loss: 2.672647\n",
      "Epoch: 33813 \tTraining Loss: 1.215824 \tValidation Loss: 2.673023\n",
      "Epoch: 33814 \tTraining Loss: 1.257585 \tValidation Loss: 2.672629\n",
      "Epoch: 33815 \tTraining Loss: 1.257866 \tValidation Loss: 2.670994\n",
      "Epoch: 33816 \tTraining Loss: 1.232357 \tValidation Loss: 2.670950\n",
      "Epoch: 33817 \tTraining Loss: 1.236848 \tValidation Loss: 2.671037\n",
      "Epoch: 33818 \tTraining Loss: 1.244938 \tValidation Loss: 2.671757\n",
      "Epoch: 33819 \tTraining Loss: 1.268980 \tValidation Loss: 2.671992\n",
      "Epoch: 33820 \tTraining Loss: 1.211456 \tValidation Loss: 2.671753\n",
      "Epoch: 33821 \tTraining Loss: 1.280098 \tValidation Loss: 2.670491\n",
      "Epoch: 33822 \tTraining Loss: 1.184667 \tValidation Loss: 2.671580\n",
      "Epoch: 33823 \tTraining Loss: 1.262716 \tValidation Loss: 2.670318\n",
      "Epoch: 33824 \tTraining Loss: 1.276285 \tValidation Loss: 2.672558\n",
      "Epoch: 33825 \tTraining Loss: 1.186057 \tValidation Loss: 2.673616\n",
      "Epoch: 33826 \tTraining Loss: 1.248692 \tValidation Loss: 2.672013\n",
      "Epoch: 33827 \tTraining Loss: 1.210261 \tValidation Loss: 2.673180\n",
      "Epoch: 33828 \tTraining Loss: 1.216997 \tValidation Loss: 2.673041\n",
      "Epoch: 33829 \tTraining Loss: 1.250393 \tValidation Loss: 2.669965\n",
      "Epoch: 33830 \tTraining Loss: 1.238315 \tValidation Loss: 2.671076\n",
      "Epoch: 33831 \tTraining Loss: 1.233139 \tValidation Loss: 2.670746\n",
      "Epoch: 33832 \tTraining Loss: 1.226850 \tValidation Loss: 2.671835\n",
      "Epoch: 33833 \tTraining Loss: 1.246679 \tValidation Loss: 2.670443\n",
      "Epoch: 33834 \tTraining Loss: 1.269660 \tValidation Loss: 2.670775\n",
      "Epoch: 33835 \tTraining Loss: 1.245853 \tValidation Loss: 2.672610\n",
      "Epoch: 33836 \tTraining Loss: 1.277640 \tValidation Loss: 2.671285\n",
      "Epoch: 33837 \tTraining Loss: 1.307093 \tValidation Loss: 2.671075\n",
      "Epoch: 33838 \tTraining Loss: 1.294830 \tValidation Loss: 2.671695\n",
      "Epoch: 33839 \tTraining Loss: 1.250217 \tValidation Loss: 2.671584\n",
      "Epoch: 33840 \tTraining Loss: 1.284965 \tValidation Loss: 2.672398\n",
      "Epoch: 33841 \tTraining Loss: 1.292996 \tValidation Loss: 2.669503\n",
      "Epoch: 33842 \tTraining Loss: 1.247541 \tValidation Loss: 2.670715\n",
      "Epoch: 33843 \tTraining Loss: 1.235405 \tValidation Loss: 2.671817\n",
      "Epoch: 33844 \tTraining Loss: 1.240514 \tValidation Loss: 2.670625\n",
      "Epoch: 33845 \tTraining Loss: 1.278132 \tValidation Loss: 2.671535\n",
      "Epoch: 33846 \tTraining Loss: 1.244475 \tValidation Loss: 2.670886\n",
      "Epoch: 33847 \tTraining Loss: 1.199153 \tValidation Loss: 2.671993\n",
      "Epoch: 33848 \tTraining Loss: 1.236278 \tValidation Loss: 2.673242\n",
      "Epoch: 33849 \tTraining Loss: 1.230057 \tValidation Loss: 2.672268\n",
      "Epoch: 33850 \tTraining Loss: 1.274366 \tValidation Loss: 2.670273\n",
      "Epoch: 33851 \tTraining Loss: 1.239055 \tValidation Loss: 2.671468\n",
      "Epoch: 33852 \tTraining Loss: 1.244246 \tValidation Loss: 2.672405\n",
      "Epoch: 33853 \tTraining Loss: 1.293986 \tValidation Loss: 2.672387\n",
      "Epoch: 33854 \tTraining Loss: 1.241958 \tValidation Loss: 2.671579\n",
      "Epoch: 33855 \tTraining Loss: 1.260056 \tValidation Loss: 2.671791\n",
      "Epoch: 33856 \tTraining Loss: 1.192710 \tValidation Loss: 2.672588\n",
      "Epoch: 33857 \tTraining Loss: 1.237571 \tValidation Loss: 2.672199\n",
      "Epoch: 33858 \tTraining Loss: 1.243065 \tValidation Loss: 2.671556\n",
      "Epoch: 33859 \tTraining Loss: 1.238741 \tValidation Loss: 2.671385\n",
      "Epoch: 33860 \tTraining Loss: 1.237573 \tValidation Loss: 2.672207\n",
      "Epoch: 33861 \tTraining Loss: 1.258031 \tValidation Loss: 2.671724\n",
      "Epoch: 33862 \tTraining Loss: 1.257384 \tValidation Loss: 2.671661\n",
      "Epoch: 33863 \tTraining Loss: 1.204021 \tValidation Loss: 2.672308\n",
      "Epoch: 33864 \tTraining Loss: 1.207892 \tValidation Loss: 2.673434\n",
      "Epoch: 33865 \tTraining Loss: 1.291214 \tValidation Loss: 2.672622\n",
      "Epoch: 33866 \tTraining Loss: 1.237171 \tValidation Loss: 2.671840\n",
      "Epoch: 33867 \tTraining Loss: 1.238647 \tValidation Loss: 2.673963\n",
      "Epoch: 33868 \tTraining Loss: 1.244763 \tValidation Loss: 2.673481\n",
      "Epoch: 33869 \tTraining Loss: 1.220528 \tValidation Loss: 2.672863\n",
      "Epoch: 33870 \tTraining Loss: 1.265265 \tValidation Loss: 2.672547\n",
      "Epoch: 33871 \tTraining Loss: 1.219249 \tValidation Loss: 2.673315\n",
      "Epoch: 33872 \tTraining Loss: 1.266423 \tValidation Loss: 2.673469\n",
      "Epoch: 33873 \tTraining Loss: 1.254593 \tValidation Loss: 2.671076\n",
      "Epoch: 33874 \tTraining Loss: 1.216879 \tValidation Loss: 2.673129\n",
      "Epoch: 33875 \tTraining Loss: 1.305317 \tValidation Loss: 2.671976\n",
      "Epoch: 33876 \tTraining Loss: 1.237438 \tValidation Loss: 2.671517\n",
      "Epoch: 33877 \tTraining Loss: 1.274106 \tValidation Loss: 2.671893\n",
      "Epoch: 33878 \tTraining Loss: 1.236039 \tValidation Loss: 2.672723\n",
      "Epoch: 33879 \tTraining Loss: 1.276971 \tValidation Loss: 2.671762\n",
      "Epoch: 33880 \tTraining Loss: 1.280950 \tValidation Loss: 2.672923\n",
      "Epoch: 33881 \tTraining Loss: 1.237698 \tValidation Loss: 2.671782\n",
      "Epoch: 33882 \tTraining Loss: 1.254598 \tValidation Loss: 2.672728\n",
      "Epoch: 33883 \tTraining Loss: 1.264212 \tValidation Loss: 2.671860\n",
      "Epoch: 33884 \tTraining Loss: 1.218002 \tValidation Loss: 2.672518\n",
      "Epoch: 33885 \tTraining Loss: 1.229651 \tValidation Loss: 2.672982\n",
      "Epoch: 33886 \tTraining Loss: 1.281662 \tValidation Loss: 2.673161\n",
      "Epoch: 33887 \tTraining Loss: 1.261594 \tValidation Loss: 2.673236\n",
      "Epoch: 33888 \tTraining Loss: 1.221785 \tValidation Loss: 2.674097\n",
      "Epoch: 33889 \tTraining Loss: 1.296327 \tValidation Loss: 2.672385\n",
      "Epoch: 33890 \tTraining Loss: 1.199515 \tValidation Loss: 2.674657\n",
      "Epoch: 33891 \tTraining Loss: 1.240894 \tValidation Loss: 2.673437\n",
      "Epoch: 33892 \tTraining Loss: 1.239535 \tValidation Loss: 2.672362\n",
      "Epoch: 33893 \tTraining Loss: 1.247119 \tValidation Loss: 2.673317\n",
      "Epoch: 33894 \tTraining Loss: 1.235459 \tValidation Loss: 2.673195\n",
      "Epoch: 33895 \tTraining Loss: 1.243174 \tValidation Loss: 2.672223\n",
      "Epoch: 33896 \tTraining Loss: 1.240459 \tValidation Loss: 2.675051\n",
      "Epoch: 33897 \tTraining Loss: 1.251248 \tValidation Loss: 2.674229\n",
      "Epoch: 33898 \tTraining Loss: 1.277173 \tValidation Loss: 2.672430\n",
      "Epoch: 33899 \tTraining Loss: 1.293832 \tValidation Loss: 2.674002\n",
      "Epoch: 33900 \tTraining Loss: 1.279459 \tValidation Loss: 2.674644\n",
      "Epoch: 33901 \tTraining Loss: 1.262623 \tValidation Loss: 2.671714\n",
      "Epoch: 33902 \tTraining Loss: 1.211469 \tValidation Loss: 2.672437\n",
      "Epoch: 33903 \tTraining Loss: 1.215966 \tValidation Loss: 2.674333\n",
      "Epoch: 33904 \tTraining Loss: 1.254817 \tValidation Loss: 2.673430\n",
      "Epoch: 33905 \tTraining Loss: 1.264406 \tValidation Loss: 2.674594\n",
      "Epoch: 33906 \tTraining Loss: 1.263855 \tValidation Loss: 2.673452\n",
      "Epoch: 33907 \tTraining Loss: 1.281966 \tValidation Loss: 2.673725\n",
      "Epoch: 33908 \tTraining Loss: 1.234258 \tValidation Loss: 2.673306\n",
      "Epoch: 33909 \tTraining Loss: 1.297216 \tValidation Loss: 2.672043\n",
      "Epoch: 33910 \tTraining Loss: 1.218730 \tValidation Loss: 2.675001\n",
      "Epoch: 33911 \tTraining Loss: 1.269750 \tValidation Loss: 2.672070\n",
      "Epoch: 33912 \tTraining Loss: 1.229313 \tValidation Loss: 2.673188\n",
      "Epoch: 33913 \tTraining Loss: 1.210240 \tValidation Loss: 2.674731\n",
      "Epoch: 33914 \tTraining Loss: 1.228308 \tValidation Loss: 2.673020\n",
      "Epoch: 33915 \tTraining Loss: 1.258079 \tValidation Loss: 2.672361\n",
      "Epoch: 33916 \tTraining Loss: 1.224486 \tValidation Loss: 2.674217\n",
      "Epoch: 33917 \tTraining Loss: 1.214881 \tValidation Loss: 2.673745\n",
      "Epoch: 33918 \tTraining Loss: 1.212434 \tValidation Loss: 2.673316\n",
      "Epoch: 33919 \tTraining Loss: 1.248148 \tValidation Loss: 2.673429\n",
      "Epoch: 33920 \tTraining Loss: 1.222129 \tValidation Loss: 2.674142\n",
      "Epoch: 33921 \tTraining Loss: 1.226020 \tValidation Loss: 2.674711\n",
      "Epoch: 33922 \tTraining Loss: 1.235175 \tValidation Loss: 2.672093\n",
      "Epoch: 33923 \tTraining Loss: 1.198008 \tValidation Loss: 2.673970\n",
      "Epoch: 33924 \tTraining Loss: 1.246364 \tValidation Loss: 2.674249\n",
      "Epoch: 33925 \tTraining Loss: 1.233992 \tValidation Loss: 2.674919\n",
      "Epoch: 33926 \tTraining Loss: 1.244711 \tValidation Loss: 2.674268\n",
      "Epoch: 33927 \tTraining Loss: 1.247535 \tValidation Loss: 2.673248\n",
      "Epoch: 33928 \tTraining Loss: 1.219020 \tValidation Loss: 2.673498\n",
      "Epoch: 33929 \tTraining Loss: 1.253431 \tValidation Loss: 2.674156\n",
      "Epoch: 33930 \tTraining Loss: 1.223311 \tValidation Loss: 2.673544\n",
      "Epoch: 33931 \tTraining Loss: 1.242624 \tValidation Loss: 2.673122\n",
      "Epoch: 33932 \tTraining Loss: 1.222685 \tValidation Loss: 2.675200\n",
      "Epoch: 33933 \tTraining Loss: 1.231454 \tValidation Loss: 2.673746\n",
      "Epoch: 33934 \tTraining Loss: 1.237480 \tValidation Loss: 2.674400\n",
      "Epoch: 33935 \tTraining Loss: 1.210116 \tValidation Loss: 2.675441\n",
      "Epoch: 33936 \tTraining Loss: 1.233649 \tValidation Loss: 2.673281\n",
      "Epoch: 33937 \tTraining Loss: 1.234919 \tValidation Loss: 2.673652\n",
      "Epoch: 33938 \tTraining Loss: 1.250612 \tValidation Loss: 2.672758\n",
      "Epoch: 33939 \tTraining Loss: 1.238736 \tValidation Loss: 2.673980\n",
      "Epoch: 33940 \tTraining Loss: 1.233696 \tValidation Loss: 2.673628\n",
      "Epoch: 33941 \tTraining Loss: 1.215225 \tValidation Loss: 2.674970\n",
      "Epoch: 33942 \tTraining Loss: 1.223051 \tValidation Loss: 2.673078\n",
      "Epoch: 33943 \tTraining Loss: 1.233240 \tValidation Loss: 2.674124\n",
      "Epoch: 33944 \tTraining Loss: 1.235193 \tValidation Loss: 2.674903\n",
      "Epoch: 33945 \tTraining Loss: 1.243726 \tValidation Loss: 2.675127\n",
      "Epoch: 33946 \tTraining Loss: 1.264420 \tValidation Loss: 2.673943\n",
      "Epoch: 33947 \tTraining Loss: 1.257543 \tValidation Loss: 2.674268\n",
      "Epoch: 33948 \tTraining Loss: 1.251092 \tValidation Loss: 2.673482\n",
      "Epoch: 33949 \tTraining Loss: 1.251860 \tValidation Loss: 2.674043\n",
      "Epoch: 33950 \tTraining Loss: 1.194518 \tValidation Loss: 2.674852\n",
      "Epoch: 33951 \tTraining Loss: 1.226898 \tValidation Loss: 2.675082\n",
      "Epoch: 33952 \tTraining Loss: 1.241542 \tValidation Loss: 2.673979\n",
      "Epoch: 33953 \tTraining Loss: 1.248931 \tValidation Loss: 2.674196\n",
      "Epoch: 33954 \tTraining Loss: 1.273643 \tValidation Loss: 2.674016\n",
      "Epoch: 33955 \tTraining Loss: 1.221290 \tValidation Loss: 2.675824\n",
      "Epoch: 33956 \tTraining Loss: 1.284444 \tValidation Loss: 2.672237\n",
      "Epoch: 33957 \tTraining Loss: 1.276637 \tValidation Loss: 2.674077\n",
      "Epoch: 33958 \tTraining Loss: 1.228365 \tValidation Loss: 2.675071\n",
      "Epoch: 33959 \tTraining Loss: 1.257164 \tValidation Loss: 2.674801\n",
      "Epoch: 33960 \tTraining Loss: 1.223680 \tValidation Loss: 2.676797\n",
      "Epoch: 33961 \tTraining Loss: 1.278023 \tValidation Loss: 2.674065\n",
      "Epoch: 33962 \tTraining Loss: 1.242028 \tValidation Loss: 2.674466\n",
      "Epoch: 33963 \tTraining Loss: 1.237544 \tValidation Loss: 2.674050\n",
      "Epoch: 33964 \tTraining Loss: 1.240301 \tValidation Loss: 2.674780\n",
      "Epoch: 33965 \tTraining Loss: 1.184365 \tValidation Loss: 2.675736\n",
      "Epoch: 33966 \tTraining Loss: 1.242416 \tValidation Loss: 2.674669\n",
      "Epoch: 33967 \tTraining Loss: 1.229580 \tValidation Loss: 2.674491\n",
      "Epoch: 33968 \tTraining Loss: 1.274099 \tValidation Loss: 2.674176\n",
      "Epoch: 33969 \tTraining Loss: 1.245864 \tValidation Loss: 2.674766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33970 \tTraining Loss: 1.233725 \tValidation Loss: 2.674712\n",
      "Epoch: 33971 \tTraining Loss: 1.245919 \tValidation Loss: 2.675480\n",
      "Epoch: 33972 \tTraining Loss: 1.225034 \tValidation Loss: 2.675584\n",
      "Epoch: 33973 \tTraining Loss: 1.233394 \tValidation Loss: 2.675843\n",
      "Epoch: 33974 \tTraining Loss: 1.235391 \tValidation Loss: 2.676535\n",
      "Epoch: 33975 \tTraining Loss: 1.264912 \tValidation Loss: 2.673533\n",
      "Epoch: 33976 \tTraining Loss: 1.225410 \tValidation Loss: 2.674948\n",
      "Epoch: 33977 \tTraining Loss: 1.238654 \tValidation Loss: 2.674428\n",
      "Epoch: 33978 \tTraining Loss: 1.226135 \tValidation Loss: 2.674486\n",
      "Epoch: 33979 \tTraining Loss: 1.201074 \tValidation Loss: 2.674752\n",
      "Epoch: 33980 \tTraining Loss: 1.215984 \tValidation Loss: 2.674900\n",
      "Epoch: 33981 \tTraining Loss: 1.236264 \tValidation Loss: 2.675440\n",
      "Epoch: 33982 \tTraining Loss: 1.210673 \tValidation Loss: 2.674316\n",
      "Epoch: 33983 \tTraining Loss: 1.254139 \tValidation Loss: 2.673291\n",
      "Epoch: 33984 \tTraining Loss: 1.258417 \tValidation Loss: 2.676505\n",
      "Epoch: 33985 \tTraining Loss: 1.195970 \tValidation Loss: 2.675632\n",
      "Epoch: 33986 \tTraining Loss: 1.212972 \tValidation Loss: 2.676032\n",
      "Epoch: 33987 \tTraining Loss: 1.277054 \tValidation Loss: 2.673975\n",
      "Epoch: 33988 \tTraining Loss: 1.268427 \tValidation Loss: 2.674536\n",
      "Epoch: 33989 \tTraining Loss: 1.248468 \tValidation Loss: 2.676368\n",
      "Epoch: 33990 \tTraining Loss: 1.255414 \tValidation Loss: 2.673067\n",
      "Epoch: 33991 \tTraining Loss: 1.221336 \tValidation Loss: 2.675147\n",
      "Epoch: 33992 \tTraining Loss: 1.212274 \tValidation Loss: 2.675117\n",
      "Epoch: 33993 \tTraining Loss: 1.238249 \tValidation Loss: 2.675358\n",
      "Epoch: 33994 \tTraining Loss: 1.192236 \tValidation Loss: 2.674782\n",
      "Epoch: 33995 \tTraining Loss: 1.211825 \tValidation Loss: 2.676037\n",
      "Epoch: 33996 \tTraining Loss: 1.267146 \tValidation Loss: 2.674955\n",
      "Epoch: 33997 \tTraining Loss: 1.280212 \tValidation Loss: 2.675810\n",
      "Epoch: 33998 \tTraining Loss: 1.254154 \tValidation Loss: 2.674546\n",
      "Epoch: 33999 \tTraining Loss: 1.299511 \tValidation Loss: 2.674522\n",
      "Epoch: 34000 \tTraining Loss: 1.257753 \tValidation Loss: 2.675202\n",
      "Epoch: 34001 \tTraining Loss: 1.222034 \tValidation Loss: 2.675694\n",
      "Epoch: 34002 \tTraining Loss: 1.259807 \tValidation Loss: 2.675023\n",
      "Epoch: 34003 \tTraining Loss: 1.235711 \tValidation Loss: 2.675798\n",
      "Epoch: 34004 \tTraining Loss: 1.221946 \tValidation Loss: 2.677644\n",
      "Epoch: 34005 \tTraining Loss: 1.204655 \tValidation Loss: 2.675531\n",
      "Epoch: 34006 \tTraining Loss: 1.235622 \tValidation Loss: 2.674658\n",
      "Epoch: 34007 \tTraining Loss: 1.265525 \tValidation Loss: 2.673817\n",
      "Epoch: 34008 \tTraining Loss: 1.229408 \tValidation Loss: 2.675428\n",
      "Epoch: 34009 \tTraining Loss: 1.246133 \tValidation Loss: 2.676954\n",
      "Epoch: 34010 \tTraining Loss: 1.273517 \tValidation Loss: 2.673685\n",
      "Epoch: 34011 \tTraining Loss: 1.242625 \tValidation Loss: 2.675379\n",
      "Epoch: 34012 \tTraining Loss: 1.213783 \tValidation Loss: 2.674319\n",
      "Epoch: 34013 \tTraining Loss: 1.244794 \tValidation Loss: 2.676110\n",
      "Epoch: 34014 \tTraining Loss: 1.224221 \tValidation Loss: 2.675714\n",
      "Epoch: 34015 \tTraining Loss: 1.238797 \tValidation Loss: 2.675601\n",
      "Epoch: 34016 \tTraining Loss: 1.290515 \tValidation Loss: 2.674763\n",
      "Epoch: 34017 \tTraining Loss: 1.215838 \tValidation Loss: 2.675929\n",
      "Epoch: 34018 \tTraining Loss: 1.248121 \tValidation Loss: 2.676628\n",
      "Epoch: 34019 \tTraining Loss: 1.240803 \tValidation Loss: 2.675943\n",
      "Epoch: 34020 \tTraining Loss: 1.228524 \tValidation Loss: 2.676798\n",
      "Epoch: 34021 \tTraining Loss: 1.251600 \tValidation Loss: 2.675240\n",
      "Epoch: 34022 \tTraining Loss: 1.241834 \tValidation Loss: 2.676179\n",
      "Epoch: 34023 \tTraining Loss: 1.232029 \tValidation Loss: 2.677119\n",
      "Epoch: 34024 \tTraining Loss: 1.211779 \tValidation Loss: 2.676376\n",
      "Epoch: 34025 \tTraining Loss: 1.201383 \tValidation Loss: 2.676107\n",
      "Epoch: 34026 \tTraining Loss: 1.256093 \tValidation Loss: 2.677868\n",
      "Epoch: 34027 \tTraining Loss: 1.281604 \tValidation Loss: 2.675832\n",
      "Epoch: 34028 \tTraining Loss: 1.181492 \tValidation Loss: 2.676981\n",
      "Epoch: 34029 \tTraining Loss: 1.268968 \tValidation Loss: 2.676580\n",
      "Epoch: 34030 \tTraining Loss: 1.233303 \tValidation Loss: 2.674519\n",
      "Epoch: 34031 \tTraining Loss: 1.235293 \tValidation Loss: 2.676411\n",
      "Epoch: 34032 \tTraining Loss: 1.234275 \tValidation Loss: 2.676393\n",
      "Epoch: 34033 \tTraining Loss: 1.272090 \tValidation Loss: 2.674933\n",
      "Epoch: 34034 \tTraining Loss: 1.240445 \tValidation Loss: 2.676318\n",
      "Epoch: 34035 \tTraining Loss: 1.227943 \tValidation Loss: 2.676404\n",
      "Epoch: 34036 \tTraining Loss: 1.197899 \tValidation Loss: 2.678108\n",
      "Epoch: 34037 \tTraining Loss: 1.229954 \tValidation Loss: 2.676235\n",
      "Epoch: 34038 \tTraining Loss: 1.262701 \tValidation Loss: 2.677235\n",
      "Epoch: 34039 \tTraining Loss: 1.282634 \tValidation Loss: 2.677172\n",
      "Epoch: 34040 \tTraining Loss: 1.249574 \tValidation Loss: 2.676877\n",
      "Epoch: 34041 \tTraining Loss: 1.255936 \tValidation Loss: 2.677852\n",
      "Epoch: 34042 \tTraining Loss: 1.229867 \tValidation Loss: 2.675809\n",
      "Epoch: 34043 \tTraining Loss: 1.267519 \tValidation Loss: 2.676949\n",
      "Epoch: 34044 \tTraining Loss: 1.251774 \tValidation Loss: 2.675932\n",
      "Epoch: 34045 \tTraining Loss: 1.231749 \tValidation Loss: 2.677237\n",
      "Epoch: 34046 \tTraining Loss: 1.254426 \tValidation Loss: 2.675491\n",
      "Epoch: 34047 \tTraining Loss: 1.236760 \tValidation Loss: 2.676378\n",
      "Epoch: 34048 \tTraining Loss: 1.248290 \tValidation Loss: 2.676025\n",
      "Epoch: 34049 \tTraining Loss: 1.246274 \tValidation Loss: 2.675359\n",
      "Epoch: 34050 \tTraining Loss: 1.248493 \tValidation Loss: 2.675583\n",
      "Epoch: 34051 \tTraining Loss: 1.205066 \tValidation Loss: 2.676414\n",
      "Epoch: 34052 \tTraining Loss: 1.256346 \tValidation Loss: 2.675726\n",
      "Epoch: 34053 \tTraining Loss: 1.230256 \tValidation Loss: 2.676731\n",
      "Epoch: 34054 \tTraining Loss: 1.257604 \tValidation Loss: 2.675785\n",
      "Epoch: 34055 \tTraining Loss: 1.217764 \tValidation Loss: 2.675920\n",
      "Epoch: 34056 \tTraining Loss: 1.278627 \tValidation Loss: 2.676222\n",
      "Epoch: 34057 \tTraining Loss: 1.215845 \tValidation Loss: 2.676736\n",
      "Epoch: 34058 \tTraining Loss: 1.190277 \tValidation Loss: 2.678227\n",
      "Epoch: 34059 \tTraining Loss: 1.295108 \tValidation Loss: 2.676336\n",
      "Epoch: 34060 \tTraining Loss: 1.237679 \tValidation Loss: 2.675422\n",
      "Epoch: 34061 \tTraining Loss: 1.286321 \tValidation Loss: 2.676738\n",
      "Epoch: 34062 \tTraining Loss: 1.240781 \tValidation Loss: 2.676096\n",
      "Epoch: 34063 \tTraining Loss: 1.165469 \tValidation Loss: 2.678480\n",
      "Epoch: 34064 \tTraining Loss: 1.276552 \tValidation Loss: 2.677329\n",
      "Epoch: 34065 \tTraining Loss: 1.231903 \tValidation Loss: 2.678266\n",
      "Epoch: 34066 \tTraining Loss: 1.231296 \tValidation Loss: 2.678122\n",
      "Epoch: 34067 \tTraining Loss: 1.232808 \tValidation Loss: 2.677591\n",
      "Epoch: 34068 \tTraining Loss: 1.265262 \tValidation Loss: 2.677705\n",
      "Epoch: 34069 \tTraining Loss: 1.282912 \tValidation Loss: 2.675852\n",
      "Epoch: 34070 \tTraining Loss: 1.239130 \tValidation Loss: 2.675390\n",
      "Epoch: 34071 \tTraining Loss: 1.229152 \tValidation Loss: 2.676357\n",
      "Epoch: 34072 \tTraining Loss: 1.283149 \tValidation Loss: 2.677170\n",
      "Epoch: 34073 \tTraining Loss: 1.260103 \tValidation Loss: 2.675037\n",
      "Epoch: 34074 \tTraining Loss: 1.214615 \tValidation Loss: 2.676420\n",
      "Epoch: 34075 \tTraining Loss: 1.239886 \tValidation Loss: 2.677989\n",
      "Epoch: 34076 \tTraining Loss: 1.242597 \tValidation Loss: 2.678607\n",
      "Epoch: 34077 \tTraining Loss: 1.251583 \tValidation Loss: 2.677007\n",
      "Epoch: 34078 \tTraining Loss: 1.236925 \tValidation Loss: 2.676970\n",
      "Epoch: 34079 \tTraining Loss: 1.277018 \tValidation Loss: 2.676270\n",
      "Epoch: 34080 \tTraining Loss: 1.239353 \tValidation Loss: 2.677488\n",
      "Epoch: 34081 \tTraining Loss: 1.246462 \tValidation Loss: 2.676125\n",
      "Epoch: 34082 \tTraining Loss: 1.234999 \tValidation Loss: 2.677300\n",
      "Epoch: 34083 \tTraining Loss: 1.238074 \tValidation Loss: 2.678404\n",
      "Epoch: 34084 \tTraining Loss: 1.230163 \tValidation Loss: 2.676706\n",
      "Epoch: 34085 \tTraining Loss: 1.198175 \tValidation Loss: 2.677998\n",
      "Epoch: 34086 \tTraining Loss: 1.189439 \tValidation Loss: 2.678742\n",
      "Epoch: 34087 \tTraining Loss: 1.181669 \tValidation Loss: 2.678689\n",
      "Epoch: 34088 \tTraining Loss: 1.227453 \tValidation Loss: 2.677296\n",
      "Epoch: 34089 \tTraining Loss: 1.230563 \tValidation Loss: 2.678237\n",
      "Epoch: 34090 \tTraining Loss: 1.255586 \tValidation Loss: 2.677699\n",
      "Epoch: 34091 \tTraining Loss: 1.203053 \tValidation Loss: 2.678024\n",
      "Epoch: 34092 \tTraining Loss: 1.215602 \tValidation Loss: 2.679120\n",
      "Epoch: 34093 \tTraining Loss: 1.241562 \tValidation Loss: 2.678320\n",
      "Epoch: 34094 \tTraining Loss: 1.219435 \tValidation Loss: 2.678326\n",
      "Epoch: 34095 \tTraining Loss: 1.234131 \tValidation Loss: 2.676834\n",
      "Epoch: 34096 \tTraining Loss: 1.256062 \tValidation Loss: 2.677573\n",
      "Epoch: 34097 \tTraining Loss: 1.259712 \tValidation Loss: 2.678394\n",
      "Epoch: 34098 \tTraining Loss: 1.191693 \tValidation Loss: 2.677929\n",
      "Epoch: 34099 \tTraining Loss: 1.295245 \tValidation Loss: 2.678030\n",
      "Epoch: 34100 \tTraining Loss: 1.289778 \tValidation Loss: 2.678098\n",
      "Epoch: 34101 \tTraining Loss: 1.227248 \tValidation Loss: 2.677468\n",
      "Epoch: 34102 \tTraining Loss: 1.259937 \tValidation Loss: 2.677807\n",
      "Epoch: 34103 \tTraining Loss: 1.174800 \tValidation Loss: 2.679492\n",
      "Epoch: 34104 \tTraining Loss: 1.271739 \tValidation Loss: 2.678116\n",
      "Epoch: 34105 \tTraining Loss: 1.244370 \tValidation Loss: 2.677201\n",
      "Epoch: 34106 \tTraining Loss: 1.227589 \tValidation Loss: 2.678014\n",
      "Epoch: 34107 \tTraining Loss: 1.251325 \tValidation Loss: 2.677554\n",
      "Epoch: 34108 \tTraining Loss: 1.242311 \tValidation Loss: 2.677872\n",
      "Epoch: 34109 \tTraining Loss: 1.197078 \tValidation Loss: 2.677957\n",
      "Epoch: 34110 \tTraining Loss: 1.241275 \tValidation Loss: 2.677239\n",
      "Epoch: 34111 \tTraining Loss: 1.251714 \tValidation Loss: 2.677510\n",
      "Epoch: 34112 \tTraining Loss: 1.258637 \tValidation Loss: 2.678694\n",
      "Epoch: 34113 \tTraining Loss: 1.269468 \tValidation Loss: 2.676929\n",
      "Epoch: 34114 \tTraining Loss: 1.263888 \tValidation Loss: 2.678341\n",
      "Epoch: 34115 \tTraining Loss: 1.235649 \tValidation Loss: 2.680145\n",
      "Epoch: 34116 \tTraining Loss: 1.238238 \tValidation Loss: 2.678455\n",
      "Epoch: 34117 \tTraining Loss: 1.208463 \tValidation Loss: 2.678649\n",
      "Epoch: 34118 \tTraining Loss: 1.186844 \tValidation Loss: 2.678752\n",
      "Epoch: 34119 \tTraining Loss: 1.223920 \tValidation Loss: 2.678002\n",
      "Epoch: 34120 \tTraining Loss: 1.215539 \tValidation Loss: 2.677617\n",
      "Epoch: 34121 \tTraining Loss: 1.233376 \tValidation Loss: 2.678187\n",
      "Epoch: 34122 \tTraining Loss: 1.210309 \tValidation Loss: 2.678339\n",
      "Epoch: 34123 \tTraining Loss: 1.250274 \tValidation Loss: 2.679277\n",
      "Epoch: 34124 \tTraining Loss: 1.259324 \tValidation Loss: 2.677482\n",
      "Epoch: 34125 \tTraining Loss: 1.260311 \tValidation Loss: 2.678283\n",
      "Epoch: 34126 \tTraining Loss: 1.249106 \tValidation Loss: 2.677838\n",
      "Epoch: 34127 \tTraining Loss: 1.229366 \tValidation Loss: 2.680042\n",
      "Epoch: 34128 \tTraining Loss: 1.227353 \tValidation Loss: 2.678969\n",
      "Epoch: 34129 \tTraining Loss: 1.251131 \tValidation Loss: 2.679767\n",
      "Epoch: 34130 \tTraining Loss: 1.187841 \tValidation Loss: 2.677270\n",
      "Epoch: 34131 \tTraining Loss: 1.286963 \tValidation Loss: 2.678496\n",
      "Epoch: 34132 \tTraining Loss: 1.243418 \tValidation Loss: 2.679495\n",
      "Epoch: 34133 \tTraining Loss: 1.208114 \tValidation Loss: 2.679558\n",
      "Epoch: 34134 \tTraining Loss: 1.247182 \tValidation Loss: 2.679328\n",
      "Epoch: 34135 \tTraining Loss: 1.229612 \tValidation Loss: 2.679039\n",
      "Epoch: 34136 \tTraining Loss: 1.232402 \tValidation Loss: 2.678618\n",
      "Epoch: 34137 \tTraining Loss: 1.244846 \tValidation Loss: 2.678630\n",
      "Epoch: 34138 \tTraining Loss: 1.198709 \tValidation Loss: 2.679351\n",
      "Epoch: 34139 \tTraining Loss: 1.273026 \tValidation Loss: 2.677737\n",
      "Epoch: 34140 \tTraining Loss: 1.222582 \tValidation Loss: 2.678288\n",
      "Epoch: 34141 \tTraining Loss: 1.249175 \tValidation Loss: 2.678569\n",
      "Epoch: 34142 \tTraining Loss: 1.227349 \tValidation Loss: 2.678793\n",
      "Epoch: 34143 \tTraining Loss: 1.230008 \tValidation Loss: 2.678721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34144 \tTraining Loss: 1.251261 \tValidation Loss: 2.678825\n",
      "Epoch: 34145 \tTraining Loss: 1.257589 \tValidation Loss: 2.678058\n",
      "Epoch: 34146 \tTraining Loss: 1.265162 \tValidation Loss: 2.677532\n",
      "Epoch: 34147 \tTraining Loss: 1.216443 \tValidation Loss: 2.680333\n",
      "Epoch: 34148 \tTraining Loss: 1.243901 \tValidation Loss: 2.679969\n",
      "Epoch: 34149 \tTraining Loss: 1.211779 \tValidation Loss: 2.679354\n",
      "Epoch: 34150 \tTraining Loss: 1.245183 \tValidation Loss: 2.680088\n",
      "Epoch: 34151 \tTraining Loss: 1.180535 \tValidation Loss: 2.679032\n",
      "Epoch: 34152 \tTraining Loss: 1.195107 \tValidation Loss: 2.679407\n",
      "Epoch: 34153 \tTraining Loss: 1.184619 \tValidation Loss: 2.680497\n",
      "Epoch: 34154 \tTraining Loss: 1.246883 \tValidation Loss: 2.680008\n",
      "Epoch: 34155 \tTraining Loss: 1.242017 \tValidation Loss: 2.678462\n",
      "Epoch: 34156 \tTraining Loss: 1.216114 \tValidation Loss: 2.679542\n",
      "Epoch: 34157 \tTraining Loss: 1.195302 \tValidation Loss: 2.679918\n",
      "Epoch: 34158 \tTraining Loss: 1.252763 \tValidation Loss: 2.679518\n",
      "Epoch: 34159 \tTraining Loss: 1.221352 \tValidation Loss: 2.679394\n",
      "Epoch: 34160 \tTraining Loss: 1.211453 \tValidation Loss: 2.679311\n",
      "Epoch: 34161 \tTraining Loss: 1.254532 \tValidation Loss: 2.680930\n",
      "Epoch: 34162 \tTraining Loss: 1.252800 \tValidation Loss: 2.679667\n",
      "Epoch: 34163 \tTraining Loss: 1.254446 \tValidation Loss: 2.678797\n",
      "Epoch: 34164 \tTraining Loss: 1.226540 \tValidation Loss: 2.679376\n",
      "Epoch: 34165 \tTraining Loss: 1.266530 \tValidation Loss: 2.680533\n",
      "Epoch: 34166 \tTraining Loss: 1.240008 \tValidation Loss: 2.679594\n",
      "Epoch: 34167 \tTraining Loss: 1.226313 \tValidation Loss: 2.680393\n",
      "Epoch: 34168 \tTraining Loss: 1.229732 \tValidation Loss: 2.680016\n",
      "Epoch: 34169 \tTraining Loss: 1.266815 \tValidation Loss: 2.679116\n",
      "Epoch: 34170 \tTraining Loss: 1.277446 \tValidation Loss: 2.679238\n",
      "Epoch: 34171 \tTraining Loss: 1.282147 \tValidation Loss: 2.678096\n",
      "Epoch: 34172 \tTraining Loss: 1.197772 \tValidation Loss: 2.680510\n",
      "Epoch: 34173 \tTraining Loss: 1.299246 \tValidation Loss: 2.678829\n",
      "Epoch: 34174 \tTraining Loss: 1.243410 \tValidation Loss: 2.680278\n",
      "Epoch: 34175 \tTraining Loss: 1.257077 \tValidation Loss: 2.679816\n",
      "Epoch: 34176 \tTraining Loss: 1.187628 \tValidation Loss: 2.679707\n",
      "Epoch: 34177 \tTraining Loss: 1.224064 \tValidation Loss: 2.680840\n",
      "Epoch: 34178 \tTraining Loss: 1.224184 \tValidation Loss: 2.678138\n",
      "Epoch: 34179 \tTraining Loss: 1.246978 \tValidation Loss: 2.679607\n",
      "Epoch: 34180 \tTraining Loss: 1.221543 \tValidation Loss: 2.679614\n",
      "Epoch: 34181 \tTraining Loss: 1.256067 \tValidation Loss: 2.677295\n",
      "Epoch: 34182 \tTraining Loss: 1.277635 \tValidation Loss: 2.679232\n",
      "Epoch: 34183 \tTraining Loss: 1.203088 \tValidation Loss: 2.679744\n",
      "Epoch: 34184 \tTraining Loss: 1.231601 \tValidation Loss: 2.680578\n",
      "Epoch: 34185 \tTraining Loss: 1.229578 \tValidation Loss: 2.678463\n",
      "Epoch: 34186 \tTraining Loss: 1.218493 \tValidation Loss: 2.681185\n",
      "Epoch: 34187 \tTraining Loss: 1.248218 \tValidation Loss: 2.679076\n",
      "Epoch: 34188 \tTraining Loss: 1.292260 \tValidation Loss: 2.679498\n",
      "Epoch: 34189 \tTraining Loss: 1.202750 \tValidation Loss: 2.680300\n",
      "Epoch: 34190 \tTraining Loss: 1.236162 \tValidation Loss: 2.679016\n",
      "Epoch: 34191 \tTraining Loss: 1.248520 \tValidation Loss: 2.679692\n",
      "Epoch: 34192 \tTraining Loss: 1.212829 \tValidation Loss: 2.680161\n",
      "Epoch: 34193 \tTraining Loss: 1.248445 \tValidation Loss: 2.680482\n",
      "Epoch: 34194 \tTraining Loss: 1.244168 \tValidation Loss: 2.680059\n",
      "Epoch: 34195 \tTraining Loss: 1.283051 \tValidation Loss: 2.679453\n",
      "Epoch: 34196 \tTraining Loss: 1.222243 \tValidation Loss: 2.679481\n",
      "Epoch: 34197 \tTraining Loss: 1.249203 \tValidation Loss: 2.680156\n",
      "Epoch: 34198 \tTraining Loss: 1.240515 \tValidation Loss: 2.680654\n",
      "Epoch: 34199 \tTraining Loss: 1.205101 \tValidation Loss: 2.681722\n",
      "Epoch: 34200 \tTraining Loss: 1.236442 \tValidation Loss: 2.679030\n",
      "Epoch: 34201 \tTraining Loss: 1.177765 \tValidation Loss: 2.680957\n",
      "Epoch: 34202 \tTraining Loss: 1.222026 \tValidation Loss: 2.680241\n",
      "Epoch: 34203 \tTraining Loss: 1.252911 \tValidation Loss: 2.679339\n",
      "Epoch: 34204 \tTraining Loss: 1.234229 \tValidation Loss: 2.680960\n",
      "Epoch: 34205 \tTraining Loss: 1.271506 \tValidation Loss: 2.679693\n",
      "Epoch: 34206 \tTraining Loss: 1.234449 \tValidation Loss: 2.679792\n",
      "Epoch: 34207 \tTraining Loss: 1.200606 \tValidation Loss: 2.681843\n",
      "Epoch: 34208 \tTraining Loss: 1.263543 \tValidation Loss: 2.678325\n",
      "Epoch: 34209 \tTraining Loss: 1.245407 \tValidation Loss: 2.679754\n",
      "Epoch: 34210 \tTraining Loss: 1.249544 \tValidation Loss: 2.680079\n",
      "Epoch: 34211 \tTraining Loss: 1.256853 \tValidation Loss: 2.679149\n",
      "Epoch: 34212 \tTraining Loss: 1.267222 \tValidation Loss: 2.681065\n",
      "Epoch: 34213 \tTraining Loss: 1.252288 \tValidation Loss: 2.679823\n",
      "Epoch: 34214 \tTraining Loss: 1.237573 \tValidation Loss: 2.680325\n",
      "Epoch: 34215 \tTraining Loss: 1.227769 \tValidation Loss: 2.680606\n",
      "Epoch: 34216 \tTraining Loss: 1.234609 \tValidation Loss: 2.680347\n",
      "Epoch: 34217 \tTraining Loss: 1.227340 \tValidation Loss: 2.679360\n",
      "Epoch: 34218 \tTraining Loss: 1.278630 \tValidation Loss: 2.681585\n",
      "Epoch: 34219 \tTraining Loss: 1.251332 \tValidation Loss: 2.681736\n",
      "Epoch: 34220 \tTraining Loss: 1.228067 \tValidation Loss: 2.680631\n",
      "Epoch: 34221 \tTraining Loss: 1.230925 \tValidation Loss: 2.681349\n",
      "Epoch: 34222 \tTraining Loss: 1.244094 \tValidation Loss: 2.679859\n",
      "Epoch: 34223 \tTraining Loss: 1.258288 \tValidation Loss: 2.680628\n",
      "Epoch: 34224 \tTraining Loss: 1.268903 \tValidation Loss: 2.681620\n",
      "Epoch: 34225 \tTraining Loss: 1.267797 \tValidation Loss: 2.679659\n",
      "Epoch: 34226 \tTraining Loss: 1.237812 \tValidation Loss: 2.681590\n",
      "Epoch: 34227 \tTraining Loss: 1.227876 \tValidation Loss: 2.680290\n",
      "Epoch: 34228 \tTraining Loss: 1.228972 \tValidation Loss: 2.680493\n",
      "Epoch: 34229 \tTraining Loss: 1.248007 \tValidation Loss: 2.682472\n",
      "Epoch: 34230 \tTraining Loss: 1.245425 \tValidation Loss: 2.681930\n",
      "Epoch: 34231 \tTraining Loss: 1.222000 \tValidation Loss: 2.679840\n",
      "Epoch: 34232 \tTraining Loss: 1.205305 \tValidation Loss: 2.681528\n",
      "Epoch: 34233 \tTraining Loss: 1.252477 \tValidation Loss: 2.681748\n",
      "Epoch: 34234 \tTraining Loss: 1.224502 \tValidation Loss: 2.680435\n",
      "Epoch: 34235 \tTraining Loss: 1.231834 \tValidation Loss: 2.681528\n",
      "Epoch: 34236 \tTraining Loss: 1.194981 \tValidation Loss: 2.682239\n",
      "Epoch: 34237 \tTraining Loss: 1.238241 \tValidation Loss: 2.680709\n",
      "Epoch: 34238 \tTraining Loss: 1.229279 \tValidation Loss: 2.681606\n",
      "Epoch: 34239 \tTraining Loss: 1.234506 \tValidation Loss: 2.681459\n",
      "Epoch: 34240 \tTraining Loss: 1.267813 \tValidation Loss: 2.679760\n",
      "Epoch: 34241 \tTraining Loss: 1.301160 \tValidation Loss: 2.679931\n",
      "Epoch: 34242 \tTraining Loss: 1.222260 \tValidation Loss: 2.680243\n",
      "Epoch: 34243 \tTraining Loss: 1.244611 \tValidation Loss: 2.681160\n",
      "Epoch: 34244 \tTraining Loss: 1.237722 \tValidation Loss: 2.681587\n",
      "Epoch: 34245 \tTraining Loss: 1.205446 \tValidation Loss: 2.683298\n",
      "Epoch: 34246 \tTraining Loss: 1.207334 \tValidation Loss: 2.681090\n",
      "Epoch: 34247 \tTraining Loss: 1.234095 \tValidation Loss: 2.682869\n",
      "Epoch: 34248 \tTraining Loss: 1.237623 \tValidation Loss: 2.680143\n",
      "Epoch: 34249 \tTraining Loss: 1.237077 \tValidation Loss: 2.680444\n",
      "Epoch: 34250 \tTraining Loss: 1.252120 \tValidation Loss: 2.680818\n",
      "Epoch: 34251 \tTraining Loss: 1.200913 \tValidation Loss: 2.682600\n",
      "Epoch: 34252 \tTraining Loss: 1.268158 \tValidation Loss: 2.682119\n",
      "Epoch: 34253 \tTraining Loss: 1.268491 \tValidation Loss: 2.680295\n",
      "Epoch: 34254 \tTraining Loss: 1.206033 \tValidation Loss: 2.681618\n",
      "Epoch: 34255 \tTraining Loss: 1.211753 \tValidation Loss: 2.681972\n",
      "Epoch: 34256 \tTraining Loss: 1.230002 \tValidation Loss: 2.682355\n",
      "Epoch: 34257 \tTraining Loss: 1.228755 \tValidation Loss: 2.681727\n",
      "Epoch: 34258 \tTraining Loss: 1.204424 \tValidation Loss: 2.682571\n",
      "Epoch: 34259 \tTraining Loss: 1.276391 \tValidation Loss: 2.680682\n",
      "Epoch: 34260 \tTraining Loss: 1.215989 \tValidation Loss: 2.682909\n",
      "Epoch: 34261 \tTraining Loss: 1.228751 \tValidation Loss: 2.681345\n",
      "Epoch: 34262 \tTraining Loss: 1.205926 \tValidation Loss: 2.683201\n",
      "Epoch: 34263 \tTraining Loss: 1.250155 \tValidation Loss: 2.682067\n",
      "Epoch: 34264 \tTraining Loss: 1.249087 \tValidation Loss: 2.680920\n",
      "Epoch: 34265 \tTraining Loss: 1.249874 \tValidation Loss: 2.681457\n",
      "Epoch: 34266 \tTraining Loss: 1.223179 \tValidation Loss: 2.683043\n",
      "Epoch: 34267 \tTraining Loss: 1.228867 \tValidation Loss: 2.682695\n",
      "Epoch: 34268 \tTraining Loss: 1.204510 \tValidation Loss: 2.684438\n",
      "Epoch: 34269 \tTraining Loss: 1.219188 \tValidation Loss: 2.681302\n",
      "Epoch: 34270 \tTraining Loss: 1.261925 \tValidation Loss: 2.682368\n",
      "Epoch: 34271 \tTraining Loss: 1.246811 \tValidation Loss: 2.681740\n",
      "Epoch: 34272 \tTraining Loss: 1.249072 \tValidation Loss: 2.682394\n",
      "Epoch: 34273 \tTraining Loss: 1.270730 \tValidation Loss: 2.683372\n",
      "Epoch: 34274 \tTraining Loss: 1.212174 \tValidation Loss: 2.682989\n",
      "Epoch: 34275 \tTraining Loss: 1.261500 \tValidation Loss: 2.680559\n",
      "Epoch: 34276 \tTraining Loss: 1.242606 \tValidation Loss: 2.681492\n",
      "Epoch: 34277 \tTraining Loss: 1.212520 \tValidation Loss: 2.681974\n",
      "Epoch: 34278 \tTraining Loss: 1.215204 \tValidation Loss: 2.682877\n",
      "Epoch: 34279 \tTraining Loss: 1.227115 \tValidation Loss: 2.684116\n",
      "Epoch: 34280 \tTraining Loss: 1.290047 \tValidation Loss: 2.682157\n",
      "Epoch: 34281 \tTraining Loss: 1.246091 \tValidation Loss: 2.681264\n",
      "Epoch: 34282 \tTraining Loss: 1.227618 \tValidation Loss: 2.683605\n",
      "Epoch: 34283 \tTraining Loss: 1.236741 \tValidation Loss: 2.682022\n",
      "Epoch: 34284 \tTraining Loss: 1.265105 \tValidation Loss: 2.683808\n",
      "Epoch: 34285 \tTraining Loss: 1.231506 \tValidation Loss: 2.682540\n",
      "Epoch: 34286 \tTraining Loss: 1.214994 \tValidation Loss: 2.683866\n",
      "Epoch: 34287 \tTraining Loss: 1.246857 \tValidation Loss: 2.680988\n",
      "Epoch: 34288 \tTraining Loss: 1.236565 \tValidation Loss: 2.682722\n",
      "Epoch: 34289 \tTraining Loss: 1.211580 \tValidation Loss: 2.681728\n",
      "Epoch: 34290 \tTraining Loss: 1.230965 \tValidation Loss: 2.684791\n",
      "Epoch: 34291 \tTraining Loss: 1.260235 \tValidation Loss: 2.680215\n",
      "Epoch: 34292 \tTraining Loss: 1.245601 \tValidation Loss: 2.682105\n",
      "Epoch: 34293 \tTraining Loss: 1.236622 \tValidation Loss: 2.683524\n",
      "Epoch: 34294 \tTraining Loss: 1.246847 \tValidation Loss: 2.682675\n",
      "Epoch: 34295 \tTraining Loss: 1.262647 \tValidation Loss: 2.681911\n",
      "Epoch: 34296 \tTraining Loss: 1.208101 \tValidation Loss: 2.682469\n",
      "Epoch: 34297 \tTraining Loss: 1.239184 \tValidation Loss: 2.684487\n",
      "Epoch: 34298 \tTraining Loss: 1.215637 \tValidation Loss: 2.683196\n",
      "Epoch: 34299 \tTraining Loss: 1.221207 \tValidation Loss: 2.681136\n",
      "Epoch: 34300 \tTraining Loss: 1.224939 \tValidation Loss: 2.682181\n",
      "Epoch: 34301 \tTraining Loss: 1.217191 \tValidation Loss: 2.681273\n",
      "Epoch: 34302 \tTraining Loss: 1.228417 \tValidation Loss: 2.683314\n",
      "Epoch: 34303 \tTraining Loss: 1.195043 \tValidation Loss: 2.684771\n",
      "Epoch: 34304 \tTraining Loss: 1.232686 \tValidation Loss: 2.682947\n",
      "Epoch: 34305 \tTraining Loss: 1.181727 \tValidation Loss: 2.682847\n",
      "Epoch: 34306 \tTraining Loss: 1.255658 \tValidation Loss: 2.680853\n",
      "Epoch: 34307 \tTraining Loss: 1.253209 \tValidation Loss: 2.681517\n",
      "Epoch: 34308 \tTraining Loss: 1.214717 \tValidation Loss: 2.681505\n",
      "Epoch: 34309 \tTraining Loss: 1.227807 \tValidation Loss: 2.683207\n",
      "Epoch: 34310 \tTraining Loss: 1.238356 \tValidation Loss: 2.683558\n",
      "Epoch: 34311 \tTraining Loss: 1.223535 \tValidation Loss: 2.683021\n",
      "Epoch: 34312 \tTraining Loss: 1.232497 \tValidation Loss: 2.681469\n",
      "Epoch: 34313 \tTraining Loss: 1.253191 \tValidation Loss: 2.683115\n",
      "Epoch: 34314 \tTraining Loss: 1.221683 \tValidation Loss: 2.683420\n",
      "Epoch: 34315 \tTraining Loss: 1.244224 \tValidation Loss: 2.682390\n",
      "Epoch: 34316 \tTraining Loss: 1.231476 \tValidation Loss: 2.683800\n",
      "Epoch: 34317 \tTraining Loss: 1.290036 \tValidation Loss: 2.683567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34318 \tTraining Loss: 1.242662 \tValidation Loss: 2.681387\n",
      "Epoch: 34319 \tTraining Loss: 1.211035 \tValidation Loss: 2.683378\n",
      "Epoch: 34320 \tTraining Loss: 1.225707 \tValidation Loss: 2.682871\n",
      "Epoch: 34321 \tTraining Loss: 1.194634 \tValidation Loss: 2.683849\n",
      "Epoch: 34322 \tTraining Loss: 1.233997 \tValidation Loss: 2.682321\n",
      "Epoch: 34323 \tTraining Loss: 1.274701 \tValidation Loss: 2.683186\n",
      "Epoch: 34324 \tTraining Loss: 1.224231 \tValidation Loss: 2.684748\n",
      "Epoch: 34325 \tTraining Loss: 1.237690 \tValidation Loss: 2.683388\n",
      "Epoch: 34326 \tTraining Loss: 1.235701 \tValidation Loss: 2.682988\n",
      "Epoch: 34327 \tTraining Loss: 1.176766 \tValidation Loss: 2.684582\n",
      "Epoch: 34328 \tTraining Loss: 1.235772 \tValidation Loss: 2.683727\n",
      "Epoch: 34329 \tTraining Loss: 1.236344 \tValidation Loss: 2.684032\n",
      "Epoch: 34330 \tTraining Loss: 1.213942 \tValidation Loss: 2.683908\n",
      "Epoch: 34331 \tTraining Loss: 1.209155 \tValidation Loss: 2.684147\n",
      "Epoch: 34332 \tTraining Loss: 1.289132 \tValidation Loss: 2.682338\n",
      "Epoch: 34333 \tTraining Loss: 1.253170 \tValidation Loss: 2.682525\n",
      "Epoch: 34334 \tTraining Loss: 1.254825 \tValidation Loss: 2.683465\n",
      "Epoch: 34335 \tTraining Loss: 1.175209 \tValidation Loss: 2.684873\n",
      "Epoch: 34336 \tTraining Loss: 1.221497 \tValidation Loss: 2.682515\n",
      "Epoch: 34337 \tTraining Loss: 1.277587 \tValidation Loss: 2.684949\n",
      "Epoch: 34338 \tTraining Loss: 1.230724 \tValidation Loss: 2.684357\n",
      "Epoch: 34339 \tTraining Loss: 1.240284 \tValidation Loss: 2.685012\n",
      "Epoch: 34340 \tTraining Loss: 1.259730 \tValidation Loss: 2.681932\n",
      "Epoch: 34341 \tTraining Loss: 1.210473 \tValidation Loss: 2.684662\n",
      "Epoch: 34342 \tTraining Loss: 1.230697 \tValidation Loss: 2.682582\n",
      "Epoch: 34343 \tTraining Loss: 1.256489 \tValidation Loss: 2.683222\n",
      "Epoch: 34344 \tTraining Loss: 1.245199 \tValidation Loss: 2.682630\n",
      "Epoch: 34345 \tTraining Loss: 1.238622 \tValidation Loss: 2.684273\n",
      "Epoch: 34346 \tTraining Loss: 1.276432 \tValidation Loss: 2.682497\n",
      "Epoch: 34347 \tTraining Loss: 1.219543 \tValidation Loss: 2.683434\n",
      "Epoch: 34348 \tTraining Loss: 1.260518 \tValidation Loss: 2.682943\n",
      "Epoch: 34349 \tTraining Loss: 1.224814 \tValidation Loss: 2.684094\n",
      "Epoch: 34350 \tTraining Loss: 1.227009 \tValidation Loss: 2.684762\n",
      "Epoch: 34351 \tTraining Loss: 1.231530 \tValidation Loss: 2.681776\n",
      "Epoch: 34352 \tTraining Loss: 1.226756 \tValidation Loss: 2.683390\n",
      "Epoch: 34353 \tTraining Loss: 1.150857 \tValidation Loss: 2.685439\n",
      "Epoch: 34354 \tTraining Loss: 1.259063 \tValidation Loss: 2.683533\n",
      "Epoch: 34355 \tTraining Loss: 1.231884 \tValidation Loss: 2.684530\n",
      "Epoch: 34356 \tTraining Loss: 1.231204 \tValidation Loss: 2.682909\n",
      "Epoch: 34357 \tTraining Loss: 1.226699 \tValidation Loss: 2.684095\n",
      "Epoch: 34358 \tTraining Loss: 1.209408 \tValidation Loss: 2.684760\n",
      "Epoch: 34359 \tTraining Loss: 1.247460 \tValidation Loss: 2.682515\n",
      "Epoch: 34360 \tTraining Loss: 1.247936 \tValidation Loss: 2.682787\n",
      "Epoch: 34361 \tTraining Loss: 1.257147 \tValidation Loss: 2.683984\n",
      "Epoch: 34362 \tTraining Loss: 1.228972 \tValidation Loss: 2.684198\n",
      "Epoch: 34363 \tTraining Loss: 1.276347 \tValidation Loss: 2.682413\n",
      "Epoch: 34364 \tTraining Loss: 1.231214 \tValidation Loss: 2.683401\n",
      "Epoch: 34365 \tTraining Loss: 1.251163 \tValidation Loss: 2.684251\n",
      "Epoch: 34366 \tTraining Loss: 1.251580 \tValidation Loss: 2.684756\n",
      "Epoch: 34367 \tTraining Loss: 1.215864 \tValidation Loss: 2.682364\n",
      "Epoch: 34368 \tTraining Loss: 1.249609 \tValidation Loss: 2.682549\n",
      "Epoch: 34369 \tTraining Loss: 1.235391 \tValidation Loss: 2.683690\n",
      "Epoch: 34370 \tTraining Loss: 1.215148 \tValidation Loss: 2.684279\n",
      "Epoch: 34371 \tTraining Loss: 1.190652 \tValidation Loss: 2.684114\n",
      "Epoch: 34372 \tTraining Loss: 1.214331 \tValidation Loss: 2.685438\n",
      "Epoch: 34373 \tTraining Loss: 1.228138 \tValidation Loss: 2.682688\n",
      "Epoch: 34374 \tTraining Loss: 1.227432 \tValidation Loss: 2.682194\n",
      "Epoch: 34375 \tTraining Loss: 1.256285 \tValidation Loss: 2.683797\n",
      "Epoch: 34376 \tTraining Loss: 1.256890 \tValidation Loss: 2.684327\n",
      "Epoch: 34377 \tTraining Loss: 1.221926 \tValidation Loss: 2.684840\n",
      "Epoch: 34378 \tTraining Loss: 1.235696 \tValidation Loss: 2.683018\n",
      "Epoch: 34379 \tTraining Loss: 1.230304 \tValidation Loss: 2.683599\n",
      "Epoch: 34380 \tTraining Loss: 1.228412 \tValidation Loss: 2.685374\n",
      "Epoch: 34381 \tTraining Loss: 1.199285 \tValidation Loss: 2.683493\n",
      "Epoch: 34382 \tTraining Loss: 1.211559 \tValidation Loss: 2.684077\n",
      "Epoch: 34383 \tTraining Loss: 1.217106 \tValidation Loss: 2.684492\n",
      "Epoch: 34384 \tTraining Loss: 1.279852 \tValidation Loss: 2.683936\n",
      "Epoch: 34385 \tTraining Loss: 1.212291 \tValidation Loss: 2.684502\n",
      "Epoch: 34386 \tTraining Loss: 1.227266 \tValidation Loss: 2.683894\n",
      "Epoch: 34387 \tTraining Loss: 1.253110 \tValidation Loss: 2.682814\n",
      "Epoch: 34388 \tTraining Loss: 1.236148 \tValidation Loss: 2.683534\n",
      "Epoch: 34389 \tTraining Loss: 1.240190 \tValidation Loss: 2.685453\n",
      "Epoch: 34390 \tTraining Loss: 1.245900 \tValidation Loss: 2.684086\n",
      "Epoch: 34391 \tTraining Loss: 1.238080 \tValidation Loss: 2.684628\n",
      "Epoch: 34392 \tTraining Loss: 1.236904 \tValidation Loss: 2.683494\n",
      "Epoch: 34393 \tTraining Loss: 1.242135 \tValidation Loss: 2.683734\n",
      "Epoch: 34394 \tTraining Loss: 1.239650 \tValidation Loss: 2.684688\n",
      "Epoch: 34395 \tTraining Loss: 1.245687 \tValidation Loss: 2.684207\n",
      "Epoch: 34396 \tTraining Loss: 1.226361 \tValidation Loss: 2.685434\n",
      "Epoch: 34397 \tTraining Loss: 1.196358 \tValidation Loss: 2.685417\n",
      "Epoch: 34398 \tTraining Loss: 1.201355 \tValidation Loss: 2.685052\n",
      "Epoch: 34399 \tTraining Loss: 1.269323 \tValidation Loss: 2.683329\n",
      "Epoch: 34400 \tTraining Loss: 1.191303 \tValidation Loss: 2.686233\n",
      "Epoch: 34401 \tTraining Loss: 1.215668 \tValidation Loss: 2.683208\n",
      "Epoch: 34402 \tTraining Loss: 1.247751 \tValidation Loss: 2.684144\n",
      "Epoch: 34403 \tTraining Loss: 1.244760 \tValidation Loss: 2.683919\n",
      "Epoch: 34404 \tTraining Loss: 1.218222 \tValidation Loss: 2.684245\n",
      "Epoch: 34405 \tTraining Loss: 1.273221 \tValidation Loss: 2.683248\n",
      "Epoch: 34406 \tTraining Loss: 1.225263 \tValidation Loss: 2.683487\n",
      "Epoch: 34407 \tTraining Loss: 1.183041 \tValidation Loss: 2.684484\n",
      "Epoch: 34408 \tTraining Loss: 1.232749 \tValidation Loss: 2.684536\n",
      "Epoch: 34409 \tTraining Loss: 1.220945 \tValidation Loss: 2.685152\n",
      "Epoch: 34410 \tTraining Loss: 1.255573 \tValidation Loss: 2.683956\n",
      "Epoch: 34411 \tTraining Loss: 1.267923 \tValidation Loss: 2.684996\n",
      "Epoch: 34412 \tTraining Loss: 1.217648 \tValidation Loss: 2.686426\n",
      "Epoch: 34413 \tTraining Loss: 1.205847 \tValidation Loss: 2.684501\n",
      "Epoch: 34414 \tTraining Loss: 1.243134 \tValidation Loss: 2.683895\n",
      "Epoch: 34415 \tTraining Loss: 1.214011 \tValidation Loss: 2.684506\n",
      "Epoch: 34416 \tTraining Loss: 1.282054 \tValidation Loss: 2.684750\n",
      "Epoch: 34417 \tTraining Loss: 1.268007 \tValidation Loss: 2.684457\n",
      "Epoch: 34418 \tTraining Loss: 1.210104 \tValidation Loss: 2.686283\n",
      "Epoch: 34419 \tTraining Loss: 1.291426 \tValidation Loss: 2.683830\n",
      "Epoch: 34420 \tTraining Loss: 1.257696 \tValidation Loss: 2.684498\n",
      "Epoch: 34421 \tTraining Loss: 1.237236 \tValidation Loss: 2.684719\n",
      "Epoch: 34422 \tTraining Loss: 1.231573 \tValidation Loss: 2.684107\n",
      "Epoch: 34423 \tTraining Loss: 1.233248 \tValidation Loss: 2.684591\n",
      "Epoch: 34424 \tTraining Loss: 1.207767 \tValidation Loss: 2.684278\n",
      "Epoch: 34425 \tTraining Loss: 1.213386 \tValidation Loss: 2.684829\n",
      "Epoch: 34426 \tTraining Loss: 1.259789 \tValidation Loss: 2.684317\n",
      "Epoch: 34427 \tTraining Loss: 1.216740 \tValidation Loss: 2.685809\n",
      "Epoch: 34428 \tTraining Loss: 1.247876 \tValidation Loss: 2.684898\n",
      "Epoch: 34429 \tTraining Loss: 1.287050 \tValidation Loss: 2.683742\n",
      "Epoch: 34430 \tTraining Loss: 1.217926 \tValidation Loss: 2.686236\n",
      "Epoch: 34431 \tTraining Loss: 1.198798 \tValidation Loss: 2.685816\n",
      "Epoch: 34432 \tTraining Loss: 1.260635 \tValidation Loss: 2.685024\n",
      "Epoch: 34433 \tTraining Loss: 1.219331 \tValidation Loss: 2.684405\n",
      "Epoch: 34434 \tTraining Loss: 1.241508 \tValidation Loss: 2.684679\n",
      "Epoch: 34435 \tTraining Loss: 1.221560 \tValidation Loss: 2.685135\n",
      "Epoch: 34436 \tTraining Loss: 1.236915 \tValidation Loss: 2.686133\n",
      "Epoch: 34437 \tTraining Loss: 1.234707 \tValidation Loss: 2.685857\n",
      "Epoch: 34438 \tTraining Loss: 1.206193 \tValidation Loss: 2.686358\n",
      "Epoch: 34439 \tTraining Loss: 1.234440 \tValidation Loss: 2.684393\n",
      "Epoch: 34440 \tTraining Loss: 1.276782 \tValidation Loss: 2.685427\n",
      "Epoch: 34441 \tTraining Loss: 1.232774 \tValidation Loss: 2.684920\n",
      "Epoch: 34442 \tTraining Loss: 1.258892 \tValidation Loss: 2.685422\n",
      "Epoch: 34443 \tTraining Loss: 1.218576 \tValidation Loss: 2.686448\n",
      "Epoch: 34444 \tTraining Loss: 1.201386 \tValidation Loss: 2.685415\n",
      "Epoch: 34445 \tTraining Loss: 1.234705 \tValidation Loss: 2.686828\n",
      "Epoch: 34446 \tTraining Loss: 1.230565 \tValidation Loss: 2.685164\n",
      "Epoch: 34447 \tTraining Loss: 1.198711 \tValidation Loss: 2.685260\n",
      "Epoch: 34448 \tTraining Loss: 1.200977 \tValidation Loss: 2.684649\n",
      "Epoch: 34449 \tTraining Loss: 1.239972 \tValidation Loss: 2.685987\n",
      "Epoch: 34450 \tTraining Loss: 1.235526 \tValidation Loss: 2.683938\n",
      "Epoch: 34451 \tTraining Loss: 1.221340 \tValidation Loss: 2.685918\n",
      "Epoch: 34452 \tTraining Loss: 1.229331 \tValidation Loss: 2.685481\n",
      "Epoch: 34453 \tTraining Loss: 1.221504 \tValidation Loss: 2.685700\n",
      "Epoch: 34454 \tTraining Loss: 1.260005 \tValidation Loss: 2.685172\n",
      "Epoch: 34455 \tTraining Loss: 1.226702 \tValidation Loss: 2.686376\n",
      "Epoch: 34456 \tTraining Loss: 1.227081 \tValidation Loss: 2.686085\n",
      "Epoch: 34457 \tTraining Loss: 1.267794 \tValidation Loss: 2.686091\n",
      "Epoch: 34458 \tTraining Loss: 1.198122 \tValidation Loss: 2.686422\n",
      "Epoch: 34459 \tTraining Loss: 1.211268 \tValidation Loss: 2.685655\n",
      "Epoch: 34460 \tTraining Loss: 1.281044 \tValidation Loss: 2.683383\n",
      "Epoch: 34461 \tTraining Loss: 1.237695 \tValidation Loss: 2.684895\n",
      "Epoch: 34462 \tTraining Loss: 1.237691 \tValidation Loss: 2.685746\n",
      "Epoch: 34463 \tTraining Loss: 1.207952 \tValidation Loss: 2.686665\n",
      "Epoch: 34464 \tTraining Loss: 1.215762 \tValidation Loss: 2.683985\n",
      "Epoch: 34465 \tTraining Loss: 1.228685 \tValidation Loss: 2.686197\n",
      "Epoch: 34466 \tTraining Loss: 1.211160 \tValidation Loss: 2.688004\n",
      "Epoch: 34467 \tTraining Loss: 1.220652 \tValidation Loss: 2.686622\n",
      "Epoch: 34468 \tTraining Loss: 1.239441 \tValidation Loss: 2.685586\n",
      "Epoch: 34469 \tTraining Loss: 1.204824 \tValidation Loss: 2.685523\n",
      "Epoch: 34470 \tTraining Loss: 1.230759 \tValidation Loss: 2.686926\n",
      "Epoch: 34471 \tTraining Loss: 1.201425 \tValidation Loss: 2.686446\n",
      "Epoch: 34472 \tTraining Loss: 1.202654 \tValidation Loss: 2.685607\n",
      "Epoch: 34473 \tTraining Loss: 1.255126 \tValidation Loss: 2.684451\n",
      "Epoch: 34474 \tTraining Loss: 1.193551 \tValidation Loss: 2.686514\n",
      "Epoch: 34475 \tTraining Loss: 1.228262 \tValidation Loss: 2.686792\n",
      "Epoch: 34476 \tTraining Loss: 1.257004 \tValidation Loss: 2.685898\n",
      "Epoch: 34477 \tTraining Loss: 1.255666 \tValidation Loss: 2.685511\n",
      "Epoch: 34478 \tTraining Loss: 1.226550 \tValidation Loss: 2.685560\n",
      "Epoch: 34479 \tTraining Loss: 1.240391 \tValidation Loss: 2.687310\n",
      "Epoch: 34480 \tTraining Loss: 1.277858 \tValidation Loss: 2.685448\n",
      "Epoch: 34481 \tTraining Loss: 1.221766 \tValidation Loss: 2.685762\n",
      "Epoch: 34482 \tTraining Loss: 1.223074 \tValidation Loss: 2.686204\n",
      "Epoch: 34483 \tTraining Loss: 1.209391 \tValidation Loss: 2.687107\n",
      "Epoch: 34484 \tTraining Loss: 1.285723 \tValidation Loss: 2.685889\n",
      "Epoch: 34485 \tTraining Loss: 1.242701 \tValidation Loss: 2.685350\n",
      "Epoch: 34486 \tTraining Loss: 1.212526 \tValidation Loss: 2.684538\n",
      "Epoch: 34487 \tTraining Loss: 1.238993 \tValidation Loss: 2.686029\n",
      "Epoch: 34488 \tTraining Loss: 1.228081 \tValidation Loss: 2.687241\n",
      "Epoch: 34489 \tTraining Loss: 1.227535 \tValidation Loss: 2.688172\n",
      "Epoch: 34490 \tTraining Loss: 1.210183 \tValidation Loss: 2.686834\n",
      "Epoch: 34491 \tTraining Loss: 1.242456 \tValidation Loss: 2.686010\n",
      "Epoch: 34492 \tTraining Loss: 1.236452 \tValidation Loss: 2.688029\n",
      "Epoch: 34493 \tTraining Loss: 1.201591 \tValidation Loss: 2.686215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34494 \tTraining Loss: 1.227860 \tValidation Loss: 2.687688\n",
      "Epoch: 34495 \tTraining Loss: 1.249272 \tValidation Loss: 2.685318\n",
      "Epoch: 34496 \tTraining Loss: 1.213279 \tValidation Loss: 2.686426\n",
      "Epoch: 34497 \tTraining Loss: 1.211268 \tValidation Loss: 2.685091\n",
      "Epoch: 34498 \tTraining Loss: 1.195637 \tValidation Loss: 2.686264\n",
      "Epoch: 34499 \tTraining Loss: 1.230938 \tValidation Loss: 2.686916\n",
      "Epoch: 34500 \tTraining Loss: 1.249477 \tValidation Loss: 2.685713\n",
      "Epoch: 34501 \tTraining Loss: 1.252022 \tValidation Loss: 2.687087\n",
      "Epoch: 34502 \tTraining Loss: 1.237697 \tValidation Loss: 2.686672\n",
      "Epoch: 34503 \tTraining Loss: 1.240119 \tValidation Loss: 2.685808\n",
      "Epoch: 34504 \tTraining Loss: 1.225951 \tValidation Loss: 2.686333\n",
      "Epoch: 34505 \tTraining Loss: 1.197370 \tValidation Loss: 2.687598\n",
      "Epoch: 34506 \tTraining Loss: 1.227520 \tValidation Loss: 2.688108\n",
      "Epoch: 34507 \tTraining Loss: 1.252403 \tValidation Loss: 2.686028\n",
      "Epoch: 34508 \tTraining Loss: 1.263652 \tValidation Loss: 2.686409\n",
      "Epoch: 34509 \tTraining Loss: 1.230703 \tValidation Loss: 2.688593\n",
      "Epoch: 34510 \tTraining Loss: 1.221092 \tValidation Loss: 2.686404\n",
      "Epoch: 34511 \tTraining Loss: 1.225178 \tValidation Loss: 2.685915\n",
      "Epoch: 34512 \tTraining Loss: 1.233698 \tValidation Loss: 2.686207\n",
      "Epoch: 34513 \tTraining Loss: 1.186054 \tValidation Loss: 2.688355\n",
      "Epoch: 34514 \tTraining Loss: 1.214148 \tValidation Loss: 2.687841\n",
      "Epoch: 34515 \tTraining Loss: 1.261411 \tValidation Loss: 2.686250\n",
      "Epoch: 34516 \tTraining Loss: 1.207468 \tValidation Loss: 2.687129\n",
      "Epoch: 34517 \tTraining Loss: 1.252347 \tValidation Loss: 2.687373\n",
      "Epoch: 34518 \tTraining Loss: 1.220430 \tValidation Loss: 2.686784\n",
      "Epoch: 34519 \tTraining Loss: 1.256579 \tValidation Loss: 2.686297\n",
      "Epoch: 34520 \tTraining Loss: 1.215890 \tValidation Loss: 2.686518\n",
      "Epoch: 34521 \tTraining Loss: 1.246335 \tValidation Loss: 2.685039\n",
      "Epoch: 34522 \tTraining Loss: 1.248638 \tValidation Loss: 2.686001\n",
      "Epoch: 34523 \tTraining Loss: 1.229027 \tValidation Loss: 2.687723\n",
      "Epoch: 34524 \tTraining Loss: 1.246324 \tValidation Loss: 2.686557\n",
      "Epoch: 34525 \tTraining Loss: 1.176959 \tValidation Loss: 2.688316\n",
      "Epoch: 34526 \tTraining Loss: 1.228713 \tValidation Loss: 2.687018\n",
      "Epoch: 34527 \tTraining Loss: 1.223652 \tValidation Loss: 2.688929\n",
      "Epoch: 34528 \tTraining Loss: 1.214254 \tValidation Loss: 2.689205\n",
      "Epoch: 34529 \tTraining Loss: 1.271519 \tValidation Loss: 2.687264\n",
      "Epoch: 34530 \tTraining Loss: 1.192597 \tValidation Loss: 2.687984\n",
      "Epoch: 34531 \tTraining Loss: 1.209511 \tValidation Loss: 2.687376\n",
      "Epoch: 34532 \tTraining Loss: 1.247870 \tValidation Loss: 2.685964\n",
      "Epoch: 34533 \tTraining Loss: 1.285936 \tValidation Loss: 2.686589\n",
      "Epoch: 34534 \tTraining Loss: 1.183399 \tValidation Loss: 2.690215\n",
      "Epoch: 34535 \tTraining Loss: 1.265675 \tValidation Loss: 2.688421\n",
      "Epoch: 34536 \tTraining Loss: 1.259239 \tValidation Loss: 2.686479\n",
      "Epoch: 34537 \tTraining Loss: 1.238992 \tValidation Loss: 2.687018\n",
      "Epoch: 34538 \tTraining Loss: 1.255253 \tValidation Loss: 2.687947\n",
      "Epoch: 34539 \tTraining Loss: 1.233268 \tValidation Loss: 2.687685\n",
      "Epoch: 34540 \tTraining Loss: 1.244247 \tValidation Loss: 2.687266\n",
      "Epoch: 34541 \tTraining Loss: 1.227520 \tValidation Loss: 2.688130\n",
      "Epoch: 34542 \tTraining Loss: 1.286333 \tValidation Loss: 2.687568\n",
      "Epoch: 34543 \tTraining Loss: 1.220006 \tValidation Loss: 2.688034\n",
      "Epoch: 34544 \tTraining Loss: 1.218481 \tValidation Loss: 2.688813\n",
      "Epoch: 34545 \tTraining Loss: 1.230970 \tValidation Loss: 2.687770\n",
      "Epoch: 34546 \tTraining Loss: 1.236275 \tValidation Loss: 2.685606\n",
      "Epoch: 34547 \tTraining Loss: 1.260257 \tValidation Loss: 2.687057\n",
      "Epoch: 34548 \tTraining Loss: 1.239946 \tValidation Loss: 2.688429\n",
      "Epoch: 34549 \tTraining Loss: 1.270238 \tValidation Loss: 2.686039\n",
      "Epoch: 34550 \tTraining Loss: 1.246122 \tValidation Loss: 2.686822\n",
      "Epoch: 34551 \tTraining Loss: 1.246424 \tValidation Loss: 2.687832\n",
      "Epoch: 34552 \tTraining Loss: 1.209809 \tValidation Loss: 2.687771\n",
      "Epoch: 34553 \tTraining Loss: 1.238138 \tValidation Loss: 2.687834\n",
      "Epoch: 34554 \tTraining Loss: 1.222022 \tValidation Loss: 2.689032\n",
      "Epoch: 34555 \tTraining Loss: 1.260124 \tValidation Loss: 2.688228\n",
      "Epoch: 34556 \tTraining Loss: 1.246063 \tValidation Loss: 2.688273\n",
      "Epoch: 34557 \tTraining Loss: 1.249106 \tValidation Loss: 2.688014\n",
      "Epoch: 34558 \tTraining Loss: 1.230914 \tValidation Loss: 2.688264\n",
      "Epoch: 34559 \tTraining Loss: 1.223406 \tValidation Loss: 2.688810\n",
      "Epoch: 34560 \tTraining Loss: 1.180300 \tValidation Loss: 2.689074\n",
      "Epoch: 34561 \tTraining Loss: 1.194425 \tValidation Loss: 2.688476\n",
      "Epoch: 34562 \tTraining Loss: 1.220284 \tValidation Loss: 2.687712\n",
      "Epoch: 34563 \tTraining Loss: 1.215634 \tValidation Loss: 2.687865\n",
      "Epoch: 34564 \tTraining Loss: 1.208271 \tValidation Loss: 2.688925\n",
      "Epoch: 34565 \tTraining Loss: 1.213042 \tValidation Loss: 2.687152\n",
      "Epoch: 34566 \tTraining Loss: 1.275522 \tValidation Loss: 2.688738\n",
      "Epoch: 34567 \tTraining Loss: 1.251874 \tValidation Loss: 2.686544\n",
      "Epoch: 34568 \tTraining Loss: 1.227181 \tValidation Loss: 2.689439\n",
      "Epoch: 34569 \tTraining Loss: 1.226559 \tValidation Loss: 2.687242\n",
      "Epoch: 34570 \tTraining Loss: 1.263022 \tValidation Loss: 2.688828\n",
      "Epoch: 34571 \tTraining Loss: 1.244796 \tValidation Loss: 2.688828\n",
      "Epoch: 34572 \tTraining Loss: 1.285923 \tValidation Loss: 2.687439\n",
      "Epoch: 34573 \tTraining Loss: 1.213843 \tValidation Loss: 2.688686\n",
      "Epoch: 34574 \tTraining Loss: 1.288282 \tValidation Loss: 2.687635\n",
      "Epoch: 34575 \tTraining Loss: 1.219692 \tValidation Loss: 2.689724\n",
      "Epoch: 34576 \tTraining Loss: 1.243048 \tValidation Loss: 2.688083\n",
      "Epoch: 34577 \tTraining Loss: 1.234082 \tValidation Loss: 2.688422\n",
      "Epoch: 34578 \tTraining Loss: 1.197461 \tValidation Loss: 2.688914\n",
      "Epoch: 34579 \tTraining Loss: 1.217964 \tValidation Loss: 2.689005\n",
      "Epoch: 34580 \tTraining Loss: 1.267243 \tValidation Loss: 2.688366\n",
      "Epoch: 34581 \tTraining Loss: 1.221274 \tValidation Loss: 2.687207\n",
      "Epoch: 34582 \tTraining Loss: 1.195622 \tValidation Loss: 2.688720\n",
      "Epoch: 34583 \tTraining Loss: 1.255345 \tValidation Loss: 2.687098\n",
      "Epoch: 34584 \tTraining Loss: 1.262028 \tValidation Loss: 2.687446\n",
      "Epoch: 34585 \tTraining Loss: 1.223821 \tValidation Loss: 2.689479\n",
      "Epoch: 34586 \tTraining Loss: 1.236338 \tValidation Loss: 2.688882\n",
      "Epoch: 34587 \tTraining Loss: 1.237503 \tValidation Loss: 2.688146\n",
      "Epoch: 34588 \tTraining Loss: 1.239536 \tValidation Loss: 2.689127\n",
      "Epoch: 34589 \tTraining Loss: 1.238510 \tValidation Loss: 2.689296\n",
      "Epoch: 34590 \tTraining Loss: 1.214860 \tValidation Loss: 2.688789\n",
      "Epoch: 34591 \tTraining Loss: 1.216520 \tValidation Loss: 2.688708\n",
      "Epoch: 34592 \tTraining Loss: 1.204566 \tValidation Loss: 2.688163\n",
      "Epoch: 34593 \tTraining Loss: 1.196536 \tValidation Loss: 2.689325\n",
      "Epoch: 34594 \tTraining Loss: 1.219349 \tValidation Loss: 2.687475\n",
      "Epoch: 34595 \tTraining Loss: 1.195548 \tValidation Loss: 2.689768\n",
      "Epoch: 34596 \tTraining Loss: 1.220079 \tValidation Loss: 2.688231\n",
      "Epoch: 34597 \tTraining Loss: 1.278768 \tValidation Loss: 2.685819\n",
      "Epoch: 34598 \tTraining Loss: 1.241771 \tValidation Loss: 2.687855\n",
      "Epoch: 34599 \tTraining Loss: 1.251662 \tValidation Loss: 2.688570\n",
      "Epoch: 34600 \tTraining Loss: 1.286229 \tValidation Loss: 2.687886\n",
      "Epoch: 34601 \tTraining Loss: 1.191742 \tValidation Loss: 2.686904\n",
      "Epoch: 34602 \tTraining Loss: 1.212290 \tValidation Loss: 2.686610\n",
      "Epoch: 34603 \tTraining Loss: 1.194626 \tValidation Loss: 2.688463\n",
      "Epoch: 34604 \tTraining Loss: 1.252728 \tValidation Loss: 2.689962\n",
      "Epoch: 34605 \tTraining Loss: 1.277660 \tValidation Loss: 2.687382\n",
      "Epoch: 34606 \tTraining Loss: 1.193438 \tValidation Loss: 2.689613\n",
      "Epoch: 34607 \tTraining Loss: 1.254560 \tValidation Loss: 2.687484\n",
      "Epoch: 34608 \tTraining Loss: 1.233216 \tValidation Loss: 2.687832\n",
      "Epoch: 34609 \tTraining Loss: 1.230042 \tValidation Loss: 2.688239\n",
      "Epoch: 34610 \tTraining Loss: 1.217044 \tValidation Loss: 2.689154\n",
      "Epoch: 34611 \tTraining Loss: 1.236576 \tValidation Loss: 2.687854\n",
      "Epoch: 34612 \tTraining Loss: 1.208470 \tValidation Loss: 2.688690\n",
      "Epoch: 34613 \tTraining Loss: 1.231545 \tValidation Loss: 2.688700\n",
      "Epoch: 34614 \tTraining Loss: 1.253452 \tValidation Loss: 2.689122\n",
      "Epoch: 34615 \tTraining Loss: 1.255828 \tValidation Loss: 2.688034\n",
      "Epoch: 34616 \tTraining Loss: 1.255342 \tValidation Loss: 2.689414\n",
      "Epoch: 34617 \tTraining Loss: 1.216601 \tValidation Loss: 2.690362\n",
      "Epoch: 34618 \tTraining Loss: 1.225565 \tValidation Loss: 2.687321\n",
      "Epoch: 34619 \tTraining Loss: 1.241641 \tValidation Loss: 2.688244\n",
      "Epoch: 34620 \tTraining Loss: 1.186781 \tValidation Loss: 2.690218\n",
      "Epoch: 34621 \tTraining Loss: 1.217389 \tValidation Loss: 2.689655\n",
      "Epoch: 34622 \tTraining Loss: 1.222084 \tValidation Loss: 2.689723\n",
      "Epoch: 34623 \tTraining Loss: 1.215713 \tValidation Loss: 2.689964\n",
      "Epoch: 34624 \tTraining Loss: 1.231224 \tValidation Loss: 2.690118\n",
      "Epoch: 34625 \tTraining Loss: 1.264250 \tValidation Loss: 2.688925\n",
      "Epoch: 34626 \tTraining Loss: 1.248623 \tValidation Loss: 2.687155\n",
      "Epoch: 34627 \tTraining Loss: 1.244600 \tValidation Loss: 2.689641\n",
      "Epoch: 34628 \tTraining Loss: 1.239579 \tValidation Loss: 2.687031\n",
      "Epoch: 34629 \tTraining Loss: 1.207845 \tValidation Loss: 2.688589\n",
      "Epoch: 34630 \tTraining Loss: 1.253342 \tValidation Loss: 2.690164\n",
      "Epoch: 34631 \tTraining Loss: 1.236240 \tValidation Loss: 2.689447\n",
      "Epoch: 34632 \tTraining Loss: 1.247946 \tValidation Loss: 2.688159\n",
      "Epoch: 34633 \tTraining Loss: 1.231947 \tValidation Loss: 2.690235\n",
      "Epoch: 34634 \tTraining Loss: 1.231664 \tValidation Loss: 2.689430\n",
      "Epoch: 34635 \tTraining Loss: 1.263559 \tValidation Loss: 2.689093\n",
      "Epoch: 34636 \tTraining Loss: 1.250252 \tValidation Loss: 2.687759\n",
      "Epoch: 34637 \tTraining Loss: 1.215174 \tValidation Loss: 2.689525\n",
      "Epoch: 34638 \tTraining Loss: 1.271443 \tValidation Loss: 2.690377\n",
      "Epoch: 34639 \tTraining Loss: 1.241945 \tValidation Loss: 2.689144\n",
      "Epoch: 34640 \tTraining Loss: 1.277569 \tValidation Loss: 2.689252\n",
      "Epoch: 34641 \tTraining Loss: 1.227273 \tValidation Loss: 2.689984\n",
      "Epoch: 34642 \tTraining Loss: 1.267517 \tValidation Loss: 2.688085\n",
      "Epoch: 34643 \tTraining Loss: 1.235781 \tValidation Loss: 2.690282\n",
      "Epoch: 34644 \tTraining Loss: 1.200286 \tValidation Loss: 2.689069\n",
      "Epoch: 34645 \tTraining Loss: 1.230605 \tValidation Loss: 2.689418\n",
      "Epoch: 34646 \tTraining Loss: 1.239390 \tValidation Loss: 2.690960\n",
      "Epoch: 34647 \tTraining Loss: 1.225725 \tValidation Loss: 2.689784\n",
      "Epoch: 34648 \tTraining Loss: 1.249972 \tValidation Loss: 2.690179\n",
      "Epoch: 34649 \tTraining Loss: 1.184393 \tValidation Loss: 2.689495\n",
      "Epoch: 34650 \tTraining Loss: 1.240229 \tValidation Loss: 2.689206\n",
      "Epoch: 34651 \tTraining Loss: 1.206328 \tValidation Loss: 2.689076\n",
      "Epoch: 34652 \tTraining Loss: 1.156139 \tValidation Loss: 2.690145\n",
      "Epoch: 34653 \tTraining Loss: 1.210310 \tValidation Loss: 2.690636\n",
      "Epoch: 34654 \tTraining Loss: 1.211600 \tValidation Loss: 2.690294\n",
      "Epoch: 34655 \tTraining Loss: 1.218846 \tValidation Loss: 2.690484\n",
      "Epoch: 34656 \tTraining Loss: 1.182612 \tValidation Loss: 2.690230\n",
      "Epoch: 34657 \tTraining Loss: 1.228747 \tValidation Loss: 2.690602\n",
      "Epoch: 34658 \tTraining Loss: 1.239331 \tValidation Loss: 2.690218\n",
      "Epoch: 34659 \tTraining Loss: 1.275446 \tValidation Loss: 2.689172\n",
      "Epoch: 34660 \tTraining Loss: 1.191102 \tValidation Loss: 2.690188\n",
      "Epoch: 34661 \tTraining Loss: 1.229529 \tValidation Loss: 2.691198\n",
      "Epoch: 34662 \tTraining Loss: 1.198992 \tValidation Loss: 2.690856\n",
      "Epoch: 34663 \tTraining Loss: 1.196510 \tValidation Loss: 2.690955\n",
      "Epoch: 34664 \tTraining Loss: 1.233621 \tValidation Loss: 2.688803\n",
      "Epoch: 34665 \tTraining Loss: 1.288008 \tValidation Loss: 2.688708\n",
      "Epoch: 34666 \tTraining Loss: 1.244940 \tValidation Loss: 2.689630\n",
      "Epoch: 34667 \tTraining Loss: 1.247854 \tValidation Loss: 2.689779\n",
      "Epoch: 34668 \tTraining Loss: 1.224139 \tValidation Loss: 2.690930\n",
      "Epoch: 34669 \tTraining Loss: 1.216665 \tValidation Loss: 2.690092\n",
      "Epoch: 34670 \tTraining Loss: 1.222194 \tValidation Loss: 2.689566\n",
      "Epoch: 34671 \tTraining Loss: 1.256812 \tValidation Loss: 2.689495\n",
      "Epoch: 34672 \tTraining Loss: 1.217596 \tValidation Loss: 2.690706\n",
      "Epoch: 34673 \tTraining Loss: 1.247059 \tValidation Loss: 2.690497\n",
      "Epoch: 34674 \tTraining Loss: 1.251170 \tValidation Loss: 2.691255\n",
      "Epoch: 34675 \tTraining Loss: 1.213205 \tValidation Loss: 2.688950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34676 \tTraining Loss: 1.188038 \tValidation Loss: 2.689651\n",
      "Epoch: 34677 \tTraining Loss: 1.228184 \tValidation Loss: 2.690455\n",
      "Epoch: 34678 \tTraining Loss: 1.213601 \tValidation Loss: 2.690254\n",
      "Epoch: 34679 \tTraining Loss: 1.268542 \tValidation Loss: 2.689590\n",
      "Epoch: 34680 \tTraining Loss: 1.220935 \tValidation Loss: 2.691369\n",
      "Epoch: 34681 \tTraining Loss: 1.234774 \tValidation Loss: 2.689599\n",
      "Epoch: 34682 \tTraining Loss: 1.233556 \tValidation Loss: 2.689911\n",
      "Epoch: 34683 \tTraining Loss: 1.226453 \tValidation Loss: 2.690948\n",
      "Epoch: 34684 \tTraining Loss: 1.244707 \tValidation Loss: 2.690063\n",
      "Epoch: 34685 \tTraining Loss: 1.236051 \tValidation Loss: 2.688826\n",
      "Epoch: 34686 \tTraining Loss: 1.238828 \tValidation Loss: 2.689652\n",
      "Epoch: 34687 \tTraining Loss: 1.231947 \tValidation Loss: 2.691554\n",
      "Epoch: 34688 \tTraining Loss: 1.227173 \tValidation Loss: 2.690922\n",
      "Epoch: 34689 \tTraining Loss: 1.168391 \tValidation Loss: 2.691354\n",
      "Epoch: 34690 \tTraining Loss: 1.285954 \tValidation Loss: 2.689870\n",
      "Epoch: 34691 \tTraining Loss: 1.231472 \tValidation Loss: 2.691506\n",
      "Epoch: 34692 \tTraining Loss: 1.205607 \tValidation Loss: 2.691610\n",
      "Epoch: 34693 \tTraining Loss: 1.215431 \tValidation Loss: 2.689978\n",
      "Epoch: 34694 \tTraining Loss: 1.251846 \tValidation Loss: 2.691118\n",
      "Epoch: 34695 \tTraining Loss: 1.223425 \tValidation Loss: 2.691647\n",
      "Epoch: 34696 \tTraining Loss: 1.249431 \tValidation Loss: 2.691839\n",
      "Epoch: 34697 \tTraining Loss: 1.232654 \tValidation Loss: 2.689519\n",
      "Epoch: 34698 \tTraining Loss: 1.245932 \tValidation Loss: 2.692909\n",
      "Epoch: 34699 \tTraining Loss: 1.250053 \tValidation Loss: 2.690686\n",
      "Epoch: 34700 \tTraining Loss: 1.216180 \tValidation Loss: 2.691152\n",
      "Epoch: 34701 \tTraining Loss: 1.194784 \tValidation Loss: 2.689405\n",
      "Epoch: 34702 \tTraining Loss: 1.252383 \tValidation Loss: 2.691053\n",
      "Epoch: 34703 \tTraining Loss: 1.222944 \tValidation Loss: 2.690403\n",
      "Epoch: 34704 \tTraining Loss: 1.235831 \tValidation Loss: 2.690530\n",
      "Epoch: 34705 \tTraining Loss: 1.233295 \tValidation Loss: 2.691677\n",
      "Epoch: 34706 \tTraining Loss: 1.222649 \tValidation Loss: 2.689543\n",
      "Epoch: 34707 \tTraining Loss: 1.232788 \tValidation Loss: 2.692159\n",
      "Epoch: 34708 \tTraining Loss: 1.210841 \tValidation Loss: 2.691202\n",
      "Epoch: 34709 \tTraining Loss: 1.175913 \tValidation Loss: 2.690689\n",
      "Epoch: 34710 \tTraining Loss: 1.204739 \tValidation Loss: 2.691303\n",
      "Epoch: 34711 \tTraining Loss: 1.190417 \tValidation Loss: 2.693358\n",
      "Epoch: 34712 \tTraining Loss: 1.265558 \tValidation Loss: 2.690554\n",
      "Epoch: 34713 \tTraining Loss: 1.218967 \tValidation Loss: 2.690248\n",
      "Epoch: 34714 \tTraining Loss: 1.232476 \tValidation Loss: 2.691283\n",
      "Epoch: 34715 \tTraining Loss: 1.243332 \tValidation Loss: 2.690685\n",
      "Epoch: 34716 \tTraining Loss: 1.201566 \tValidation Loss: 2.691808\n",
      "Epoch: 34717 \tTraining Loss: 1.205129 \tValidation Loss: 2.691266\n",
      "Epoch: 34718 \tTraining Loss: 1.252291 \tValidation Loss: 2.689433\n",
      "Epoch: 34719 \tTraining Loss: 1.229512 \tValidation Loss: 2.691866\n",
      "Epoch: 34720 \tTraining Loss: 1.246059 \tValidation Loss: 2.690982\n",
      "Epoch: 34721 \tTraining Loss: 1.200935 \tValidation Loss: 2.689862\n",
      "Epoch: 34722 \tTraining Loss: 1.219768 \tValidation Loss: 2.690578\n",
      "Epoch: 34723 \tTraining Loss: 1.272537 \tValidation Loss: 2.690238\n",
      "Epoch: 34724 \tTraining Loss: 1.224861 \tValidation Loss: 2.691384\n",
      "Epoch: 34725 \tTraining Loss: 1.230901 \tValidation Loss: 2.692648\n",
      "Epoch: 34726 \tTraining Loss: 1.234411 \tValidation Loss: 2.690833\n",
      "Epoch: 34727 \tTraining Loss: 1.191059 \tValidation Loss: 2.693101\n",
      "Epoch: 34728 \tTraining Loss: 1.218847 \tValidation Loss: 2.691946\n",
      "Epoch: 34729 \tTraining Loss: 1.188822 \tValidation Loss: 2.692248\n",
      "Epoch: 34730 \tTraining Loss: 1.259238 \tValidation Loss: 2.693246\n",
      "Epoch: 34731 \tTraining Loss: 1.191180 \tValidation Loss: 2.690004\n",
      "Epoch: 34732 \tTraining Loss: 1.206421 \tValidation Loss: 2.692522\n",
      "Epoch: 34733 \tTraining Loss: 1.234166 \tValidation Loss: 2.691803\n",
      "Epoch: 34734 \tTraining Loss: 1.226185 \tValidation Loss: 2.692121\n",
      "Epoch: 34735 \tTraining Loss: 1.211607 \tValidation Loss: 2.691683\n",
      "Epoch: 34736 \tTraining Loss: 1.214738 \tValidation Loss: 2.692229\n",
      "Epoch: 34737 \tTraining Loss: 1.198655 \tValidation Loss: 2.692535\n",
      "Epoch: 34738 \tTraining Loss: 1.242873 \tValidation Loss: 2.693111\n",
      "Epoch: 34739 \tTraining Loss: 1.228770 \tValidation Loss: 2.690843\n",
      "Epoch: 34740 \tTraining Loss: 1.232780 \tValidation Loss: 2.691995\n",
      "Epoch: 34741 \tTraining Loss: 1.234887 \tValidation Loss: 2.691337\n",
      "Epoch: 34742 \tTraining Loss: 1.231869 \tValidation Loss: 2.690348\n",
      "Epoch: 34743 \tTraining Loss: 1.210302 \tValidation Loss: 2.690786\n",
      "Epoch: 34744 \tTraining Loss: 1.226413 \tValidation Loss: 2.690498\n",
      "Epoch: 34745 \tTraining Loss: 1.247722 \tValidation Loss: 2.692481\n",
      "Epoch: 34746 \tTraining Loss: 1.224255 \tValidation Loss: 2.692234\n",
      "Epoch: 34747 \tTraining Loss: 1.274943 \tValidation Loss: 2.690000\n",
      "Epoch: 34748 \tTraining Loss: 1.254904 \tValidation Loss: 2.692273\n",
      "Epoch: 34749 \tTraining Loss: 1.223446 \tValidation Loss: 2.690676\n",
      "Epoch: 34750 \tTraining Loss: 1.272853 \tValidation Loss: 2.691826\n",
      "Epoch: 34751 \tTraining Loss: 1.201961 \tValidation Loss: 2.691942\n",
      "Epoch: 34752 \tTraining Loss: 1.211728 \tValidation Loss: 2.692319\n",
      "Epoch: 34753 \tTraining Loss: 1.205541 \tValidation Loss: 2.693060\n",
      "Epoch: 34754 \tTraining Loss: 1.222099 \tValidation Loss: 2.692297\n",
      "Epoch: 34755 \tTraining Loss: 1.214015 \tValidation Loss: 2.693707\n",
      "Epoch: 34756 \tTraining Loss: 1.266637 \tValidation Loss: 2.691462\n",
      "Epoch: 34757 \tTraining Loss: 1.219707 \tValidation Loss: 2.691964\n",
      "Epoch: 34758 \tTraining Loss: 1.231604 \tValidation Loss: 2.690521\n",
      "Epoch: 34759 \tTraining Loss: 1.256526 \tValidation Loss: 2.691321\n",
      "Epoch: 34760 \tTraining Loss: 1.184870 \tValidation Loss: 2.692563\n",
      "Epoch: 34761 \tTraining Loss: 1.228247 \tValidation Loss: 2.692090\n",
      "Epoch: 34762 \tTraining Loss: 1.213916 \tValidation Loss: 2.691300\n",
      "Epoch: 34763 \tTraining Loss: 1.243239 \tValidation Loss: 2.691200\n",
      "Epoch: 34764 \tTraining Loss: 1.248554 \tValidation Loss: 2.689805\n",
      "Epoch: 34765 \tTraining Loss: 1.198579 \tValidation Loss: 2.692236\n",
      "Epoch: 34766 \tTraining Loss: 1.243934 \tValidation Loss: 2.692947\n",
      "Epoch: 34767 \tTraining Loss: 1.243785 \tValidation Loss: 2.692743\n",
      "Epoch: 34768 \tTraining Loss: 1.238656 \tValidation Loss: 2.691401\n",
      "Epoch: 34769 \tTraining Loss: 1.216259 \tValidation Loss: 2.691645\n",
      "Epoch: 34770 \tTraining Loss: 1.210018 \tValidation Loss: 2.693979\n",
      "Epoch: 34771 \tTraining Loss: 1.223984 \tValidation Loss: 2.693638\n",
      "Epoch: 34772 \tTraining Loss: 1.244370 \tValidation Loss: 2.691862\n",
      "Epoch: 34773 \tTraining Loss: 1.227858 \tValidation Loss: 2.691420\n",
      "Epoch: 34774 \tTraining Loss: 1.225407 \tValidation Loss: 2.691997\n",
      "Epoch: 34775 \tTraining Loss: 1.243090 \tValidation Loss: 2.693326\n",
      "Epoch: 34776 \tTraining Loss: 1.242831 \tValidation Loss: 2.693095\n",
      "Epoch: 34777 \tTraining Loss: 1.217990 \tValidation Loss: 2.694530\n",
      "Epoch: 34778 \tTraining Loss: 1.238074 \tValidation Loss: 2.692415\n",
      "Epoch: 34779 \tTraining Loss: 1.235915 \tValidation Loss: 2.693659\n",
      "Epoch: 34780 \tTraining Loss: 1.226412 \tValidation Loss: 2.692780\n",
      "Epoch: 34781 \tTraining Loss: 1.219944 \tValidation Loss: 2.693882\n",
      "Epoch: 34782 \tTraining Loss: 1.244374 \tValidation Loss: 2.694494\n",
      "Epoch: 34783 \tTraining Loss: 1.199744 \tValidation Loss: 2.692410\n",
      "Epoch: 34784 \tTraining Loss: 1.249606 \tValidation Loss: 2.690957\n",
      "Epoch: 34785 \tTraining Loss: 1.227086 \tValidation Loss: 2.693478\n",
      "Epoch: 34786 \tTraining Loss: 1.231255 \tValidation Loss: 2.691569\n",
      "Epoch: 34787 \tTraining Loss: 1.226337 \tValidation Loss: 2.692880\n",
      "Epoch: 34788 \tTraining Loss: 1.221176 \tValidation Loss: 2.692365\n",
      "Epoch: 34789 \tTraining Loss: 1.233455 \tValidation Loss: 2.694782\n",
      "Epoch: 34790 \tTraining Loss: 1.221391 \tValidation Loss: 2.694230\n",
      "Epoch: 34791 \tTraining Loss: 1.207711 \tValidation Loss: 2.693171\n",
      "Epoch: 34792 \tTraining Loss: 1.234461 \tValidation Loss: 2.692446\n",
      "Epoch: 34793 \tTraining Loss: 1.221794 \tValidation Loss: 2.691341\n",
      "Epoch: 34794 \tTraining Loss: 1.249155 \tValidation Loss: 2.694431\n",
      "Epoch: 34795 \tTraining Loss: 1.233158 \tValidation Loss: 2.693589\n",
      "Epoch: 34796 \tTraining Loss: 1.208438 \tValidation Loss: 2.692802\n",
      "Epoch: 34797 \tTraining Loss: 1.234043 \tValidation Loss: 2.693404\n",
      "Epoch: 34798 \tTraining Loss: 1.237587 \tValidation Loss: 2.691636\n",
      "Epoch: 34799 \tTraining Loss: 1.275162 \tValidation Loss: 2.692286\n",
      "Epoch: 34800 \tTraining Loss: 1.267898 \tValidation Loss: 2.694264\n",
      "Epoch: 34801 \tTraining Loss: 1.221552 \tValidation Loss: 2.692214\n",
      "Epoch: 34802 \tTraining Loss: 1.234126 \tValidation Loss: 2.693093\n",
      "Epoch: 34803 \tTraining Loss: 1.206197 \tValidation Loss: 2.691983\n",
      "Epoch: 34804 \tTraining Loss: 1.241372 \tValidation Loss: 2.691496\n",
      "Epoch: 34805 \tTraining Loss: 1.227782 \tValidation Loss: 2.690621\n",
      "Epoch: 34806 \tTraining Loss: 1.223459 \tValidation Loss: 2.693853\n",
      "Epoch: 34807 \tTraining Loss: 1.202683 \tValidation Loss: 2.693600\n",
      "Epoch: 34808 \tTraining Loss: 1.212202 \tValidation Loss: 2.692946\n",
      "Epoch: 34809 \tTraining Loss: 1.230438 \tValidation Loss: 2.692375\n",
      "Epoch: 34810 \tTraining Loss: 1.229938 \tValidation Loss: 2.693432\n",
      "Epoch: 34811 \tTraining Loss: 1.237202 \tValidation Loss: 2.692034\n",
      "Epoch: 34812 \tTraining Loss: 1.248547 \tValidation Loss: 2.691538\n",
      "Epoch: 34813 \tTraining Loss: 1.201143 \tValidation Loss: 2.694109\n",
      "Epoch: 34814 \tTraining Loss: 1.196311 \tValidation Loss: 2.694779\n",
      "Epoch: 34815 \tTraining Loss: 1.233287 \tValidation Loss: 2.693742\n",
      "Epoch: 34816 \tTraining Loss: 1.209929 \tValidation Loss: 2.692852\n",
      "Epoch: 34817 \tTraining Loss: 1.206679 \tValidation Loss: 2.693615\n",
      "Epoch: 34818 \tTraining Loss: 1.229021 \tValidation Loss: 2.693664\n",
      "Epoch: 34819 \tTraining Loss: 1.237744 \tValidation Loss: 2.693401\n",
      "Epoch: 34820 \tTraining Loss: 1.202144 \tValidation Loss: 2.694889\n",
      "Epoch: 34821 \tTraining Loss: 1.235871 \tValidation Loss: 2.694086\n",
      "Epoch: 34822 \tTraining Loss: 1.241332 \tValidation Loss: 2.693497\n",
      "Epoch: 34823 \tTraining Loss: 1.266383 \tValidation Loss: 2.691522\n",
      "Epoch: 34824 \tTraining Loss: 1.242349 \tValidation Loss: 2.694415\n",
      "Epoch: 34825 \tTraining Loss: 1.196450 \tValidation Loss: 2.695039\n",
      "Epoch: 34826 \tTraining Loss: 1.210085 \tValidation Loss: 2.692791\n",
      "Epoch: 34827 \tTraining Loss: 1.227409 \tValidation Loss: 2.694082\n",
      "Epoch: 34828 \tTraining Loss: 1.222552 \tValidation Loss: 2.693965\n",
      "Epoch: 34829 \tTraining Loss: 1.242637 \tValidation Loss: 2.692911\n",
      "Epoch: 34830 \tTraining Loss: 1.187508 \tValidation Loss: 2.694361\n",
      "Epoch: 34831 \tTraining Loss: 1.243931 \tValidation Loss: 2.692547\n",
      "Epoch: 34832 \tTraining Loss: 1.225796 \tValidation Loss: 2.694239\n",
      "Epoch: 34833 \tTraining Loss: 1.226359 \tValidation Loss: 2.694083\n",
      "Epoch: 34834 \tTraining Loss: 1.228852 \tValidation Loss: 2.693007\n",
      "Epoch: 34835 \tTraining Loss: 1.229988 \tValidation Loss: 2.693015\n",
      "Epoch: 34836 \tTraining Loss: 1.203818 \tValidation Loss: 2.694158\n",
      "Epoch: 34837 \tTraining Loss: 1.211253 \tValidation Loss: 2.694573\n",
      "Epoch: 34838 \tTraining Loss: 1.262501 \tValidation Loss: 2.693110\n",
      "Epoch: 34839 \tTraining Loss: 1.195246 \tValidation Loss: 2.695689\n",
      "Epoch: 34840 \tTraining Loss: 1.210713 \tValidation Loss: 2.694118\n",
      "Epoch: 34841 \tTraining Loss: 1.220497 \tValidation Loss: 2.694595\n",
      "Epoch: 34842 \tTraining Loss: 1.234687 \tValidation Loss: 2.694751\n",
      "Epoch: 34843 \tTraining Loss: 1.228061 \tValidation Loss: 2.693745\n",
      "Epoch: 34844 \tTraining Loss: 1.191957 \tValidation Loss: 2.694221\n",
      "Epoch: 34845 \tTraining Loss: 1.208924 \tValidation Loss: 2.694035\n",
      "Epoch: 34846 \tTraining Loss: 1.207704 \tValidation Loss: 2.694402\n",
      "Epoch: 34847 \tTraining Loss: 1.224199 \tValidation Loss: 2.693567\n",
      "Epoch: 34848 \tTraining Loss: 1.234380 \tValidation Loss: 2.694225\n",
      "Epoch: 34849 \tTraining Loss: 1.207147 \tValidation Loss: 2.696250\n",
      "Epoch: 34850 \tTraining Loss: 1.219914 \tValidation Loss: 2.693486\n",
      "Epoch: 34851 \tTraining Loss: 1.248427 \tValidation Loss: 2.693445\n",
      "Epoch: 34852 \tTraining Loss: 1.225270 \tValidation Loss: 2.693921\n",
      "Epoch: 34853 \tTraining Loss: 1.246663 \tValidation Loss: 2.693366\n",
      "Epoch: 34854 \tTraining Loss: 1.203879 \tValidation Loss: 2.692541\n",
      "Epoch: 34855 \tTraining Loss: 1.221713 \tValidation Loss: 2.694161\n",
      "Epoch: 34856 \tTraining Loss: 1.247492 \tValidation Loss: 2.693811\n",
      "Epoch: 34857 \tTraining Loss: 1.252473 \tValidation Loss: 2.692250\n",
      "Epoch: 34858 \tTraining Loss: 1.210136 \tValidation Loss: 2.694193\n",
      "Epoch: 34859 \tTraining Loss: 1.219370 \tValidation Loss: 2.694713\n",
      "Epoch: 34860 \tTraining Loss: 1.270508 \tValidation Loss: 2.692795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34861 \tTraining Loss: 1.246735 \tValidation Loss: 2.694411\n",
      "Epoch: 34862 \tTraining Loss: 1.290303 \tValidation Loss: 2.693053\n",
      "Epoch: 34863 \tTraining Loss: 1.208837 \tValidation Loss: 2.695204\n",
      "Epoch: 34864 \tTraining Loss: 1.221890 \tValidation Loss: 2.692918\n",
      "Epoch: 34865 \tTraining Loss: 1.213701 \tValidation Loss: 2.694732\n",
      "Epoch: 34866 \tTraining Loss: 1.193166 \tValidation Loss: 2.695758\n",
      "Epoch: 34867 \tTraining Loss: 1.230127 \tValidation Loss: 2.693913\n",
      "Epoch: 34868 \tTraining Loss: 1.185911 \tValidation Loss: 2.694480\n",
      "Epoch: 34869 \tTraining Loss: 1.253120 \tValidation Loss: 2.697358\n",
      "Epoch: 34870 \tTraining Loss: 1.235641 \tValidation Loss: 2.695057\n",
      "Epoch: 34871 \tTraining Loss: 1.225157 \tValidation Loss: 2.694047\n",
      "Epoch: 34872 \tTraining Loss: 1.255194 \tValidation Loss: 2.695410\n",
      "Epoch: 34873 \tTraining Loss: 1.222607 \tValidation Loss: 2.696239\n",
      "Epoch: 34874 \tTraining Loss: 1.120344 \tValidation Loss: 2.695416\n",
      "Epoch: 34875 \tTraining Loss: 1.249053 \tValidation Loss: 2.693836\n",
      "Epoch: 34876 \tTraining Loss: 1.243745 \tValidation Loss: 2.695571\n",
      "Epoch: 34877 \tTraining Loss: 1.181412 \tValidation Loss: 2.696757\n",
      "Epoch: 34878 \tTraining Loss: 1.242287 \tValidation Loss: 2.694972\n",
      "Epoch: 34879 \tTraining Loss: 1.248125 \tValidation Loss: 2.694592\n",
      "Epoch: 34880 \tTraining Loss: 1.252113 \tValidation Loss: 2.692784\n",
      "Epoch: 34881 \tTraining Loss: 1.254960 \tValidation Loss: 2.694294\n",
      "Epoch: 34882 \tTraining Loss: 1.184981 \tValidation Loss: 2.695356\n",
      "Epoch: 34883 \tTraining Loss: 1.202904 \tValidation Loss: 2.696600\n",
      "Epoch: 34884 \tTraining Loss: 1.186652 \tValidation Loss: 2.694023\n",
      "Epoch: 34885 \tTraining Loss: 1.181809 \tValidation Loss: 2.695481\n",
      "Epoch: 34886 \tTraining Loss: 1.194632 \tValidation Loss: 2.695103\n",
      "Epoch: 34887 \tTraining Loss: 1.234667 \tValidation Loss: 2.695027\n",
      "Epoch: 34888 \tTraining Loss: 1.242742 \tValidation Loss: 2.696444\n",
      "Epoch: 34889 \tTraining Loss: 1.207707 \tValidation Loss: 2.694715\n",
      "Epoch: 34890 \tTraining Loss: 1.200040 \tValidation Loss: 2.696518\n",
      "Epoch: 34891 \tTraining Loss: 1.200096 \tValidation Loss: 2.694952\n",
      "Epoch: 34892 \tTraining Loss: 1.226641 \tValidation Loss: 2.694880\n",
      "Epoch: 34893 \tTraining Loss: 1.221575 \tValidation Loss: 2.696353\n",
      "Epoch: 34894 \tTraining Loss: 1.257179 \tValidation Loss: 2.694042\n",
      "Epoch: 34895 \tTraining Loss: 1.210692 \tValidation Loss: 2.696062\n",
      "Epoch: 34896 \tTraining Loss: 1.251076 \tValidation Loss: 2.695243\n",
      "Epoch: 34897 \tTraining Loss: 1.206885 \tValidation Loss: 2.695805\n",
      "Epoch: 34898 \tTraining Loss: 1.254607 \tValidation Loss: 2.694250\n",
      "Epoch: 34899 \tTraining Loss: 1.196113 \tValidation Loss: 2.695716\n",
      "Epoch: 34900 \tTraining Loss: 1.250129 \tValidation Loss: 2.693968\n",
      "Epoch: 34901 \tTraining Loss: 1.235377 \tValidation Loss: 2.695192\n",
      "Epoch: 34902 \tTraining Loss: 1.245643 \tValidation Loss: 2.694520\n",
      "Epoch: 34903 \tTraining Loss: 1.225576 \tValidation Loss: 2.695488\n",
      "Epoch: 34904 \tTraining Loss: 1.243636 \tValidation Loss: 2.693410\n",
      "Epoch: 34905 \tTraining Loss: 1.201810 \tValidation Loss: 2.695593\n",
      "Epoch: 34906 \tTraining Loss: 1.224257 \tValidation Loss: 2.695367\n",
      "Epoch: 34907 \tTraining Loss: 1.277382 \tValidation Loss: 2.694691\n",
      "Epoch: 34908 \tTraining Loss: 1.247861 \tValidation Loss: 2.695765\n",
      "Epoch: 34909 \tTraining Loss: 1.227244 \tValidation Loss: 2.695249\n",
      "Epoch: 34910 \tTraining Loss: 1.208608 \tValidation Loss: 2.695605\n",
      "Epoch: 34911 \tTraining Loss: 1.252846 \tValidation Loss: 2.696427\n",
      "Epoch: 34912 \tTraining Loss: 1.216335 \tValidation Loss: 2.695340\n",
      "Epoch: 34913 \tTraining Loss: 1.195270 \tValidation Loss: 2.696380\n",
      "Epoch: 34914 \tTraining Loss: 1.210830 \tValidation Loss: 2.696749\n",
      "Epoch: 34915 \tTraining Loss: 1.244480 \tValidation Loss: 2.695543\n",
      "Epoch: 34916 \tTraining Loss: 1.216777 \tValidation Loss: 2.696471\n",
      "Epoch: 34917 \tTraining Loss: 1.220764 \tValidation Loss: 2.695127\n",
      "Epoch: 34918 \tTraining Loss: 1.225564 \tValidation Loss: 2.696639\n",
      "Epoch: 34919 \tTraining Loss: 1.260855 \tValidation Loss: 2.693274\n",
      "Epoch: 34920 \tTraining Loss: 1.213059 \tValidation Loss: 2.694611\n",
      "Epoch: 34921 \tTraining Loss: 1.196964 \tValidation Loss: 2.696664\n",
      "Epoch: 34922 \tTraining Loss: 1.177515 \tValidation Loss: 2.697186\n",
      "Epoch: 34923 \tTraining Loss: 1.228149 \tValidation Loss: 2.697416\n",
      "Epoch: 34924 \tTraining Loss: 1.208201 \tValidation Loss: 2.694364\n",
      "Epoch: 34925 \tTraining Loss: 1.237994 \tValidation Loss: 2.697167\n",
      "Epoch: 34926 \tTraining Loss: 1.216543 \tValidation Loss: 2.694934\n",
      "Epoch: 34927 \tTraining Loss: 1.191327 \tValidation Loss: 2.695250\n",
      "Epoch: 34928 \tTraining Loss: 1.212207 \tValidation Loss: 2.696934\n",
      "Epoch: 34929 \tTraining Loss: 1.235291 \tValidation Loss: 2.696635\n",
      "Epoch: 34930 \tTraining Loss: 1.285970 \tValidation Loss: 2.694375\n",
      "Epoch: 34931 \tTraining Loss: 1.234149 \tValidation Loss: 2.695117\n",
      "Epoch: 34932 \tTraining Loss: 1.219658 \tValidation Loss: 2.695849\n",
      "Epoch: 34933 \tTraining Loss: 1.241337 \tValidation Loss: 2.694813\n",
      "Epoch: 34934 \tTraining Loss: 1.218244 \tValidation Loss: 2.696536\n",
      "Epoch: 34935 \tTraining Loss: 1.243954 \tValidation Loss: 2.697075\n",
      "Epoch: 34936 \tTraining Loss: 1.212599 \tValidation Loss: 2.694722\n",
      "Epoch: 34937 \tTraining Loss: 1.240592 \tValidation Loss: 2.696407\n",
      "Epoch: 34938 \tTraining Loss: 1.215931 \tValidation Loss: 2.695849\n",
      "Epoch: 34939 \tTraining Loss: 1.248096 \tValidation Loss: 2.695856\n",
      "Epoch: 34940 \tTraining Loss: 1.158860 \tValidation Loss: 2.697160\n",
      "Epoch: 34941 \tTraining Loss: 1.253341 \tValidation Loss: 2.695543\n",
      "Epoch: 34942 \tTraining Loss: 1.218630 \tValidation Loss: 2.694488\n",
      "Epoch: 34943 \tTraining Loss: 1.239759 \tValidation Loss: 2.694438\n",
      "Epoch: 34944 \tTraining Loss: 1.199668 \tValidation Loss: 2.696009\n",
      "Epoch: 34945 \tTraining Loss: 1.233700 \tValidation Loss: 2.695615\n",
      "Epoch: 34946 \tTraining Loss: 1.238453 \tValidation Loss: 2.697097\n",
      "Epoch: 34947 \tTraining Loss: 1.204374 \tValidation Loss: 2.697829\n",
      "Epoch: 34948 \tTraining Loss: 1.232344 \tValidation Loss: 2.694280\n",
      "Epoch: 34949 \tTraining Loss: 1.208669 \tValidation Loss: 2.695839\n",
      "Epoch: 34950 \tTraining Loss: 1.211585 \tValidation Loss: 2.694907\n",
      "Epoch: 34951 \tTraining Loss: 1.203461 \tValidation Loss: 2.696608\n",
      "Epoch: 34952 \tTraining Loss: 1.230212 \tValidation Loss: 2.695724\n",
      "Epoch: 34953 \tTraining Loss: 1.231560 \tValidation Loss: 2.695491\n",
      "Epoch: 34954 \tTraining Loss: 1.211622 \tValidation Loss: 2.696011\n",
      "Epoch: 34955 \tTraining Loss: 1.269269 \tValidation Loss: 2.696125\n",
      "Epoch: 34956 \tTraining Loss: 1.222787 \tValidation Loss: 2.696705\n",
      "Epoch: 34957 \tTraining Loss: 1.229509 \tValidation Loss: 2.694767\n",
      "Epoch: 34958 \tTraining Loss: 1.232878 \tValidation Loss: 2.696573\n",
      "Epoch: 34959 \tTraining Loss: 1.233275 \tValidation Loss: 2.695695\n",
      "Epoch: 34960 \tTraining Loss: 1.224159 \tValidation Loss: 2.697216\n",
      "Epoch: 34961 \tTraining Loss: 1.235146 \tValidation Loss: 2.697395\n",
      "Epoch: 34962 \tTraining Loss: 1.196942 \tValidation Loss: 2.697765\n",
      "Epoch: 34963 \tTraining Loss: 1.213154 \tValidation Loss: 2.695967\n",
      "Epoch: 34964 \tTraining Loss: 1.204937 \tValidation Loss: 2.696791\n",
      "Epoch: 34965 \tTraining Loss: 1.205735 \tValidation Loss: 2.697897\n",
      "Epoch: 34966 \tTraining Loss: 1.208756 \tValidation Loss: 2.695389\n",
      "Epoch: 34967 \tTraining Loss: 1.273755 \tValidation Loss: 2.695872\n",
      "Epoch: 34968 \tTraining Loss: 1.203800 \tValidation Loss: 2.696068\n",
      "Epoch: 34969 \tTraining Loss: 1.181258 \tValidation Loss: 2.696971\n",
      "Epoch: 34970 \tTraining Loss: 1.176837 \tValidation Loss: 2.698218\n",
      "Epoch: 34971 \tTraining Loss: 1.212580 \tValidation Loss: 2.698807\n",
      "Epoch: 34972 \tTraining Loss: 1.258985 \tValidation Loss: 2.697699\n",
      "Epoch: 34973 \tTraining Loss: 1.235151 \tValidation Loss: 2.696609\n",
      "Epoch: 34974 \tTraining Loss: 1.189304 \tValidation Loss: 2.698765\n",
      "Epoch: 34975 \tTraining Loss: 1.222750 \tValidation Loss: 2.698250\n",
      "Epoch: 34976 \tTraining Loss: 1.260836 \tValidation Loss: 2.695848\n",
      "Epoch: 34977 \tTraining Loss: 1.203186 \tValidation Loss: 2.698122\n",
      "Epoch: 34978 \tTraining Loss: 1.172317 \tValidation Loss: 2.695972\n",
      "Epoch: 34979 \tTraining Loss: 1.237874 \tValidation Loss: 2.697312\n",
      "Epoch: 34980 \tTraining Loss: 1.234171 \tValidation Loss: 2.695877\n",
      "Epoch: 34981 \tTraining Loss: 1.262201 \tValidation Loss: 2.697192\n",
      "Epoch: 34982 \tTraining Loss: 1.204681 \tValidation Loss: 2.699222\n",
      "Epoch: 34983 \tTraining Loss: 1.244452 \tValidation Loss: 2.695927\n",
      "Epoch: 34984 \tTraining Loss: 1.226823 \tValidation Loss: 2.696482\n",
      "Epoch: 34985 \tTraining Loss: 1.222679 \tValidation Loss: 2.697534\n",
      "Epoch: 34986 \tTraining Loss: 1.210957 \tValidation Loss: 2.695827\n",
      "Epoch: 34987 \tTraining Loss: 1.168532 \tValidation Loss: 2.696762\n",
      "Epoch: 34988 \tTraining Loss: 1.200337 \tValidation Loss: 2.696225\n",
      "Epoch: 34989 \tTraining Loss: 1.260127 \tValidation Loss: 2.697304\n",
      "Epoch: 34990 \tTraining Loss: 1.265347 \tValidation Loss: 2.696057\n",
      "Epoch: 34991 \tTraining Loss: 1.198781 \tValidation Loss: 2.697583\n",
      "Epoch: 34992 \tTraining Loss: 1.204292 \tValidation Loss: 2.697912\n",
      "Epoch: 34993 \tTraining Loss: 1.220979 \tValidation Loss: 2.696469\n",
      "Epoch: 34994 \tTraining Loss: 1.251802 \tValidation Loss: 2.695548\n",
      "Epoch: 34995 \tTraining Loss: 1.192452 \tValidation Loss: 2.696366\n",
      "Epoch: 34996 \tTraining Loss: 1.250167 \tValidation Loss: 2.696002\n",
      "Epoch: 34997 \tTraining Loss: 1.256335 \tValidation Loss: 2.696013\n",
      "Epoch: 34998 \tTraining Loss: 1.201559 \tValidation Loss: 2.694964\n",
      "Epoch: 34999 \tTraining Loss: 1.245851 \tValidation Loss: 2.695751\n",
      "Epoch: 35000 \tTraining Loss: 1.222951 \tValidation Loss: 2.695569\n",
      "Epoch: 35001 \tTraining Loss: 1.255772 \tValidation Loss: 2.696808\n",
      "Epoch: 35002 \tTraining Loss: 1.225265 \tValidation Loss: 2.696750\n",
      "Epoch: 35003 \tTraining Loss: 1.209455 \tValidation Loss: 2.698261\n",
      "Epoch: 35004 \tTraining Loss: 1.205531 \tValidation Loss: 2.698884\n",
      "Epoch: 35005 \tTraining Loss: 1.217051 \tValidation Loss: 2.696642\n",
      "Epoch: 35006 \tTraining Loss: 1.233679 \tValidation Loss: 2.696249\n",
      "Epoch: 35007 \tTraining Loss: 1.243372 \tValidation Loss: 2.697597\n",
      "Epoch: 35008 \tTraining Loss: 1.226397 \tValidation Loss: 2.697794\n",
      "Epoch: 35009 \tTraining Loss: 1.243079 \tValidation Loss: 2.696414\n",
      "Epoch: 35010 \tTraining Loss: 1.241646 \tValidation Loss: 2.697696\n",
      "Epoch: 35011 \tTraining Loss: 1.230031 \tValidation Loss: 2.696198\n",
      "Epoch: 35012 \tTraining Loss: 1.227541 \tValidation Loss: 2.697519\n",
      "Epoch: 35013 \tTraining Loss: 1.260266 \tValidation Loss: 2.697456\n",
      "Epoch: 35014 \tTraining Loss: 1.220844 \tValidation Loss: 2.697288\n",
      "Epoch: 35015 \tTraining Loss: 1.211071 \tValidation Loss: 2.696933\n",
      "Epoch: 35016 \tTraining Loss: 1.230123 \tValidation Loss: 2.698022\n",
      "Epoch: 35017 \tTraining Loss: 1.222875 \tValidation Loss: 2.696761\n",
      "Epoch: 35018 \tTraining Loss: 1.219820 \tValidation Loss: 2.696784\n",
      "Epoch: 35019 \tTraining Loss: 1.217873 \tValidation Loss: 2.699209\n",
      "Epoch: 35020 \tTraining Loss: 1.270096 \tValidation Loss: 2.696103\n",
      "Epoch: 35021 \tTraining Loss: 1.238530 \tValidation Loss: 2.698355\n",
      "Epoch: 35022 \tTraining Loss: 1.246907 \tValidation Loss: 2.696569\n",
      "Epoch: 35023 \tTraining Loss: 1.217784 \tValidation Loss: 2.698178\n",
      "Epoch: 35024 \tTraining Loss: 1.226357 \tValidation Loss: 2.696538\n",
      "Epoch: 35025 \tTraining Loss: 1.216159 \tValidation Loss: 2.698435\n",
      "Epoch: 35026 \tTraining Loss: 1.227799 \tValidation Loss: 2.697291\n",
      "Epoch: 35027 \tTraining Loss: 1.232645 \tValidation Loss: 2.698330\n",
      "Epoch: 35028 \tTraining Loss: 1.223441 \tValidation Loss: 2.697459\n",
      "Epoch: 35029 \tTraining Loss: 1.226886 \tValidation Loss: 2.698179\n",
      "Epoch: 35030 \tTraining Loss: 1.213910 \tValidation Loss: 2.696722\n",
      "Epoch: 35031 \tTraining Loss: 1.231356 \tValidation Loss: 2.696764\n",
      "Epoch: 35032 \tTraining Loss: 1.221582 \tValidation Loss: 2.699299\n",
      "Epoch: 35033 \tTraining Loss: 1.234040 \tValidation Loss: 2.696731\n",
      "Epoch: 35034 \tTraining Loss: 1.240213 \tValidation Loss: 2.698527\n",
      "Epoch: 35035 \tTraining Loss: 1.196532 \tValidation Loss: 2.698253\n",
      "Epoch: 35036 \tTraining Loss: 1.249942 \tValidation Loss: 2.698768\n",
      "Epoch: 35037 \tTraining Loss: 1.247993 \tValidation Loss: 2.697027\n",
      "Epoch: 35038 \tTraining Loss: 1.211730 \tValidation Loss: 2.697188\n",
      "Epoch: 35039 \tTraining Loss: 1.251979 \tValidation Loss: 2.697226\n",
      "Epoch: 35040 \tTraining Loss: 1.226898 \tValidation Loss: 2.696526\n",
      "Epoch: 35041 \tTraining Loss: 1.248069 \tValidation Loss: 2.696793\n",
      "Epoch: 35042 \tTraining Loss: 1.191229 \tValidation Loss: 2.700068\n",
      "Epoch: 35043 \tTraining Loss: 1.199337 \tValidation Loss: 2.696822\n",
      "Epoch: 35044 \tTraining Loss: 1.254071 \tValidation Loss: 2.696759\n",
      "Epoch: 35045 \tTraining Loss: 1.214609 \tValidation Loss: 2.698679\n",
      "Epoch: 35046 \tTraining Loss: 1.182658 \tValidation Loss: 2.697893\n",
      "Epoch: 35047 \tTraining Loss: 1.238989 \tValidation Loss: 2.697624\n",
      "Epoch: 35048 \tTraining Loss: 1.233090 \tValidation Loss: 2.697554\n",
      "Epoch: 35049 \tTraining Loss: 1.252048 \tValidation Loss: 2.696432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35050 \tTraining Loss: 1.235380 \tValidation Loss: 2.696967\n",
      "Epoch: 35051 \tTraining Loss: 1.240862 \tValidation Loss: 2.698495\n",
      "Epoch: 35052 \tTraining Loss: 1.209517 \tValidation Loss: 2.697101\n",
      "Epoch: 35053 \tTraining Loss: 1.195770 \tValidation Loss: 2.698603\n",
      "Epoch: 35054 \tTraining Loss: 1.215892 \tValidation Loss: 2.696556\n",
      "Epoch: 35055 \tTraining Loss: 1.198964 \tValidation Loss: 2.698368\n",
      "Epoch: 35056 \tTraining Loss: 1.173443 \tValidation Loss: 2.699080\n",
      "Epoch: 35057 \tTraining Loss: 1.175589 \tValidation Loss: 2.697755\n",
      "Epoch: 35058 \tTraining Loss: 1.211312 \tValidation Loss: 2.698273\n",
      "Epoch: 35059 \tTraining Loss: 1.222405 \tValidation Loss: 2.698929\n",
      "Epoch: 35060 \tTraining Loss: 1.240612 \tValidation Loss: 2.699061\n",
      "Epoch: 35061 \tTraining Loss: 1.189358 \tValidation Loss: 2.698678\n",
      "Epoch: 35062 \tTraining Loss: 1.223231 \tValidation Loss: 2.697701\n",
      "Epoch: 35063 \tTraining Loss: 1.243321 \tValidation Loss: 2.698009\n",
      "Epoch: 35064 \tTraining Loss: 1.198796 \tValidation Loss: 2.699821\n",
      "Epoch: 35065 \tTraining Loss: 1.198837 \tValidation Loss: 2.697673\n",
      "Epoch: 35066 \tTraining Loss: 1.227588 \tValidation Loss: 2.696598\n",
      "Epoch: 35067 \tTraining Loss: 1.223631 \tValidation Loss: 2.697549\n",
      "Epoch: 35068 \tTraining Loss: 1.237303 \tValidation Loss: 2.697534\n",
      "Epoch: 35069 \tTraining Loss: 1.197639 \tValidation Loss: 2.699193\n",
      "Epoch: 35070 \tTraining Loss: 1.256962 \tValidation Loss: 2.699349\n",
      "Epoch: 35071 \tTraining Loss: 1.206457 \tValidation Loss: 2.699827\n",
      "Epoch: 35072 \tTraining Loss: 1.214735 \tValidation Loss: 2.698634\n",
      "Epoch: 35073 \tTraining Loss: 1.236296 \tValidation Loss: 2.698060\n",
      "Epoch: 35074 \tTraining Loss: 1.203534 \tValidation Loss: 2.698733\n",
      "Epoch: 35075 \tTraining Loss: 1.233748 \tValidation Loss: 2.698688\n",
      "Epoch: 35076 \tTraining Loss: 1.244596 \tValidation Loss: 2.698452\n",
      "Epoch: 35077 \tTraining Loss: 1.222019 \tValidation Loss: 2.699422\n",
      "Epoch: 35078 \tTraining Loss: 1.212313 \tValidation Loss: 2.698817\n",
      "Epoch: 35079 \tTraining Loss: 1.217461 \tValidation Loss: 2.698160\n",
      "Epoch: 35080 \tTraining Loss: 1.241653 \tValidation Loss: 2.697940\n",
      "Epoch: 35081 \tTraining Loss: 1.219263 \tValidation Loss: 2.698388\n",
      "Epoch: 35082 \tTraining Loss: 1.236675 \tValidation Loss: 2.699984\n",
      "Epoch: 35083 \tTraining Loss: 1.200883 \tValidation Loss: 2.699288\n",
      "Epoch: 35084 \tTraining Loss: 1.253492 \tValidation Loss: 2.698197\n",
      "Epoch: 35085 \tTraining Loss: 1.246235 \tValidation Loss: 2.699983\n",
      "Epoch: 35086 \tTraining Loss: 1.233051 \tValidation Loss: 2.700614\n",
      "Epoch: 35087 \tTraining Loss: 1.212827 \tValidation Loss: 2.698659\n",
      "Epoch: 35088 \tTraining Loss: 1.217994 \tValidation Loss: 2.698917\n",
      "Epoch: 35089 \tTraining Loss: 1.234510 \tValidation Loss: 2.698949\n",
      "Epoch: 35090 \tTraining Loss: 1.230938 \tValidation Loss: 2.698066\n",
      "Epoch: 35091 \tTraining Loss: 1.230732 \tValidation Loss: 2.697990\n",
      "Epoch: 35092 \tTraining Loss: 1.245191 \tValidation Loss: 2.697445\n",
      "Epoch: 35093 \tTraining Loss: 1.197825 \tValidation Loss: 2.697946\n",
      "Epoch: 35094 \tTraining Loss: 1.189827 \tValidation Loss: 2.699748\n",
      "Epoch: 35095 \tTraining Loss: 1.211748 \tValidation Loss: 2.699240\n",
      "Epoch: 35096 \tTraining Loss: 1.228119 \tValidation Loss: 2.698776\n",
      "Epoch: 35097 \tTraining Loss: 1.162810 \tValidation Loss: 2.698759\n",
      "Epoch: 35098 \tTraining Loss: 1.177217 \tValidation Loss: 2.699371\n",
      "Epoch: 35099 \tTraining Loss: 1.238039 \tValidation Loss: 2.697940\n",
      "Epoch: 35100 \tTraining Loss: 1.211948 \tValidation Loss: 2.699334\n",
      "Epoch: 35101 \tTraining Loss: 1.229901 \tValidation Loss: 2.698986\n",
      "Epoch: 35102 \tTraining Loss: 1.209275 \tValidation Loss: 2.698977\n",
      "Epoch: 35103 \tTraining Loss: 1.237609 \tValidation Loss: 2.700358\n",
      "Epoch: 35104 \tTraining Loss: 1.226365 \tValidation Loss: 2.698988\n",
      "Epoch: 35105 \tTraining Loss: 1.190673 \tValidation Loss: 2.699342\n",
      "Epoch: 35106 \tTraining Loss: 1.185952 \tValidation Loss: 2.699548\n",
      "Epoch: 35107 \tTraining Loss: 1.199573 \tValidation Loss: 2.700097\n",
      "Epoch: 35108 \tTraining Loss: 1.203482 \tValidation Loss: 2.700408\n",
      "Epoch: 35109 \tTraining Loss: 1.217001 \tValidation Loss: 2.701214\n",
      "Epoch: 35110 \tTraining Loss: 1.290875 \tValidation Loss: 2.698800\n",
      "Epoch: 35111 \tTraining Loss: 1.238395 \tValidation Loss: 2.700527\n",
      "Epoch: 35112 \tTraining Loss: 1.219231 \tValidation Loss: 2.699294\n",
      "Epoch: 35113 \tTraining Loss: 1.192976 \tValidation Loss: 2.700032\n",
      "Epoch: 35114 \tTraining Loss: 1.186517 \tValidation Loss: 2.699131\n",
      "Epoch: 35115 \tTraining Loss: 1.199726 \tValidation Loss: 2.700254\n",
      "Epoch: 35116 \tTraining Loss: 1.246526 \tValidation Loss: 2.698823\n",
      "Epoch: 35117 \tTraining Loss: 1.167711 \tValidation Loss: 2.699706\n",
      "Epoch: 35118 \tTraining Loss: 1.222322 \tValidation Loss: 2.699337\n",
      "Epoch: 35119 \tTraining Loss: 1.207624 \tValidation Loss: 2.700721\n",
      "Epoch: 35120 \tTraining Loss: 1.211087 \tValidation Loss: 2.699234\n",
      "Epoch: 35121 \tTraining Loss: 1.174920 \tValidation Loss: 2.700672\n",
      "Epoch: 35122 \tTraining Loss: 1.200755 \tValidation Loss: 2.701216\n",
      "Epoch: 35123 \tTraining Loss: 1.208054 \tValidation Loss: 2.701059\n",
      "Epoch: 35124 \tTraining Loss: 1.268041 \tValidation Loss: 2.698937\n",
      "Epoch: 35125 \tTraining Loss: 1.203753 \tValidation Loss: 2.700543\n",
      "Epoch: 35126 \tTraining Loss: 1.199494 \tValidation Loss: 2.700240\n",
      "Epoch: 35127 \tTraining Loss: 1.167708 \tValidation Loss: 2.700116\n",
      "Epoch: 35128 \tTraining Loss: 1.231503 \tValidation Loss: 2.699678\n",
      "Epoch: 35129 \tTraining Loss: 1.235326 \tValidation Loss: 2.700222\n",
      "Epoch: 35130 \tTraining Loss: 1.226041 \tValidation Loss: 2.698322\n",
      "Epoch: 35131 \tTraining Loss: 1.227206 \tValidation Loss: 2.699322\n",
      "Epoch: 35132 \tTraining Loss: 1.226635 \tValidation Loss: 2.700093\n",
      "Epoch: 35133 \tTraining Loss: 1.211913 \tValidation Loss: 2.699809\n",
      "Epoch: 35134 \tTraining Loss: 1.228554 \tValidation Loss: 2.699233\n",
      "Epoch: 35135 \tTraining Loss: 1.239930 \tValidation Loss: 2.699512\n",
      "Epoch: 35136 \tTraining Loss: 1.218900 \tValidation Loss: 2.701501\n",
      "Epoch: 35137 \tTraining Loss: 1.199891 \tValidation Loss: 2.698225\n",
      "Epoch: 35138 \tTraining Loss: 1.210246 \tValidation Loss: 2.701096\n",
      "Epoch: 35139 \tTraining Loss: 1.231439 \tValidation Loss: 2.698720\n",
      "Epoch: 35140 \tTraining Loss: 1.234473 \tValidation Loss: 2.699710\n",
      "Epoch: 35141 \tTraining Loss: 1.230769 \tValidation Loss: 2.700913\n",
      "Epoch: 35142 \tTraining Loss: 1.208187 \tValidation Loss: 2.699419\n",
      "Epoch: 35143 \tTraining Loss: 1.227022 \tValidation Loss: 2.700428\n",
      "Epoch: 35144 \tTraining Loss: 1.193309 \tValidation Loss: 2.701051\n",
      "Epoch: 35145 \tTraining Loss: 1.173162 \tValidation Loss: 2.702065\n",
      "Epoch: 35146 \tTraining Loss: 1.195993 \tValidation Loss: 2.698760\n",
      "Epoch: 35147 \tTraining Loss: 1.208617 \tValidation Loss: 2.700954\n",
      "Epoch: 35148 \tTraining Loss: 1.183177 \tValidation Loss: 2.701548\n",
      "Epoch: 35149 \tTraining Loss: 1.243252 \tValidation Loss: 2.699746\n",
      "Epoch: 35150 \tTraining Loss: 1.209258 \tValidation Loss: 2.700205\n",
      "Epoch: 35151 \tTraining Loss: 1.267080 \tValidation Loss: 2.700257\n",
      "Epoch: 35152 \tTraining Loss: 1.246927 \tValidation Loss: 2.700035\n",
      "Epoch: 35153 \tTraining Loss: 1.175433 \tValidation Loss: 2.701978\n",
      "Epoch: 35154 \tTraining Loss: 1.230963 \tValidation Loss: 2.701420\n",
      "Epoch: 35155 \tTraining Loss: 1.227870 \tValidation Loss: 2.700398\n",
      "Epoch: 35156 \tTraining Loss: 1.259828 \tValidation Loss: 2.701077\n",
      "Epoch: 35157 \tTraining Loss: 1.233510 \tValidation Loss: 2.700533\n",
      "Epoch: 35158 \tTraining Loss: 1.270299 \tValidation Loss: 2.700013\n",
      "Epoch: 35159 \tTraining Loss: 1.239862 \tValidation Loss: 2.699642\n",
      "Epoch: 35160 \tTraining Loss: 1.266597 \tValidation Loss: 2.699897\n",
      "Epoch: 35161 \tTraining Loss: 1.236245 \tValidation Loss: 2.699391\n",
      "Epoch: 35162 \tTraining Loss: 1.195531 \tValidation Loss: 2.701414\n",
      "Epoch: 35163 \tTraining Loss: 1.214602 \tValidation Loss: 2.700750\n",
      "Epoch: 35164 \tTraining Loss: 1.231059 \tValidation Loss: 2.701615\n",
      "Epoch: 35165 \tTraining Loss: 1.226796 \tValidation Loss: 2.700170\n",
      "Epoch: 35166 \tTraining Loss: 1.214559 \tValidation Loss: 2.702361\n",
      "Epoch: 35167 \tTraining Loss: 1.225879 \tValidation Loss: 2.701244\n",
      "Epoch: 35168 \tTraining Loss: 1.182722 \tValidation Loss: 2.700550\n",
      "Epoch: 35169 \tTraining Loss: 1.213053 \tValidation Loss: 2.701711\n",
      "Epoch: 35170 \tTraining Loss: 1.247015 \tValidation Loss: 2.700982\n",
      "Epoch: 35171 \tTraining Loss: 1.212490 \tValidation Loss: 2.701923\n",
      "Epoch: 35172 \tTraining Loss: 1.200577 \tValidation Loss: 2.700644\n",
      "Epoch: 35173 \tTraining Loss: 1.194856 \tValidation Loss: 2.701863\n",
      "Epoch: 35174 \tTraining Loss: 1.261552 \tValidation Loss: 2.701232\n",
      "Epoch: 35175 \tTraining Loss: 1.231862 \tValidation Loss: 2.698434\n",
      "Epoch: 35176 \tTraining Loss: 1.210705 \tValidation Loss: 2.701438\n",
      "Epoch: 35177 \tTraining Loss: 1.251571 \tValidation Loss: 2.700264\n",
      "Epoch: 35178 \tTraining Loss: 1.238551 \tValidation Loss: 2.700269\n",
      "Epoch: 35179 \tTraining Loss: 1.173265 \tValidation Loss: 2.701993\n",
      "Epoch: 35180 \tTraining Loss: 1.193488 \tValidation Loss: 2.700292\n",
      "Epoch: 35181 \tTraining Loss: 1.245999 \tValidation Loss: 2.701015\n",
      "Epoch: 35182 \tTraining Loss: 1.226620 \tValidation Loss: 2.701185\n",
      "Epoch: 35183 \tTraining Loss: 1.256646 \tValidation Loss: 2.700991\n",
      "Epoch: 35184 \tTraining Loss: 1.186881 \tValidation Loss: 2.701029\n",
      "Epoch: 35185 \tTraining Loss: 1.267229 \tValidation Loss: 2.700980\n",
      "Epoch: 35186 \tTraining Loss: 1.182038 \tValidation Loss: 2.702465\n",
      "Epoch: 35187 \tTraining Loss: 1.197889 \tValidation Loss: 2.701555\n",
      "Epoch: 35188 \tTraining Loss: 1.206117 \tValidation Loss: 2.701540\n",
      "Epoch: 35189 \tTraining Loss: 1.238222 \tValidation Loss: 2.701203\n",
      "Epoch: 35190 \tTraining Loss: 1.218751 \tValidation Loss: 2.703476\n",
      "Epoch: 35191 \tTraining Loss: 1.241152 \tValidation Loss: 2.702034\n",
      "Epoch: 35192 \tTraining Loss: 1.256072 \tValidation Loss: 2.700960\n",
      "Epoch: 35193 \tTraining Loss: 1.216070 \tValidation Loss: 2.700833\n",
      "Epoch: 35194 \tTraining Loss: 1.229997 \tValidation Loss: 2.702229\n",
      "Epoch: 35195 \tTraining Loss: 1.278938 \tValidation Loss: 2.700108\n",
      "Epoch: 35196 \tTraining Loss: 1.196138 \tValidation Loss: 2.699709\n",
      "Epoch: 35197 \tTraining Loss: 1.231246 \tValidation Loss: 2.701797\n",
      "Epoch: 35198 \tTraining Loss: 1.224209 \tValidation Loss: 2.700734\n",
      "Epoch: 35199 \tTraining Loss: 1.181742 \tValidation Loss: 2.702770\n",
      "Epoch: 35200 \tTraining Loss: 1.200439 \tValidation Loss: 2.701163\n",
      "Epoch: 35201 \tTraining Loss: 1.240357 \tValidation Loss: 2.701473\n",
      "Epoch: 35202 \tTraining Loss: 1.206920 \tValidation Loss: 2.701944\n",
      "Epoch: 35203 \tTraining Loss: 1.203362 \tValidation Loss: 2.701978\n",
      "Epoch: 35204 \tTraining Loss: 1.183264 \tValidation Loss: 2.701962\n",
      "Epoch: 35205 \tTraining Loss: 1.172759 \tValidation Loss: 2.700808\n",
      "Epoch: 35206 \tTraining Loss: 1.210186 \tValidation Loss: 2.701823\n",
      "Epoch: 35207 \tTraining Loss: 1.234841 \tValidation Loss: 2.702603\n",
      "Epoch: 35208 \tTraining Loss: 1.222112 \tValidation Loss: 2.701783\n",
      "Epoch: 35209 \tTraining Loss: 1.246565 \tValidation Loss: 2.702539\n",
      "Epoch: 35210 \tTraining Loss: 1.204945 \tValidation Loss: 2.701949\n",
      "Epoch: 35211 \tTraining Loss: 1.261916 \tValidation Loss: 2.699298\n",
      "Epoch: 35212 \tTraining Loss: 1.207074 \tValidation Loss: 2.702066\n",
      "Epoch: 35213 \tTraining Loss: 1.200652 \tValidation Loss: 2.703082\n",
      "Epoch: 35214 \tTraining Loss: 1.252527 \tValidation Loss: 2.701576\n",
      "Epoch: 35215 \tTraining Loss: 1.195065 \tValidation Loss: 2.702017\n",
      "Epoch: 35216 \tTraining Loss: 1.254666 \tValidation Loss: 2.700653\n",
      "Epoch: 35217 \tTraining Loss: 1.248557 \tValidation Loss: 2.702330\n",
      "Epoch: 35218 \tTraining Loss: 1.244463 \tValidation Loss: 2.700412\n",
      "Epoch: 35219 \tTraining Loss: 1.188414 \tValidation Loss: 2.703361\n",
      "Epoch: 35220 \tTraining Loss: 1.202617 \tValidation Loss: 2.701699\n",
      "Epoch: 35221 \tTraining Loss: 1.224518 \tValidation Loss: 2.700480\n",
      "Epoch: 35222 \tTraining Loss: 1.203131 \tValidation Loss: 2.702379\n",
      "Epoch: 35223 \tTraining Loss: 1.185907 \tValidation Loss: 2.702393\n",
      "Epoch: 35224 \tTraining Loss: 1.233377 \tValidation Loss: 2.702179\n",
      "Epoch: 35225 \tTraining Loss: 1.172276 \tValidation Loss: 2.703610\n",
      "Epoch: 35226 \tTraining Loss: 1.214629 \tValidation Loss: 2.701550\n",
      "Epoch: 35227 \tTraining Loss: 1.232532 \tValidation Loss: 2.702909\n",
      "Epoch: 35228 \tTraining Loss: 1.190654 \tValidation Loss: 2.703585\n",
      "Epoch: 35229 \tTraining Loss: 1.219754 \tValidation Loss: 2.702399\n",
      "Epoch: 35230 \tTraining Loss: 1.198423 \tValidation Loss: 2.703002\n",
      "Epoch: 35231 \tTraining Loss: 1.192147 \tValidation Loss: 2.702669\n",
      "Epoch: 35232 \tTraining Loss: 1.260913 \tValidation Loss: 2.700092\n",
      "Epoch: 35233 \tTraining Loss: 1.221800 \tValidation Loss: 2.700939\n",
      "Epoch: 35234 \tTraining Loss: 1.194117 \tValidation Loss: 2.703892\n",
      "Epoch: 35235 \tTraining Loss: 1.185013 \tValidation Loss: 2.702711\n",
      "Epoch: 35236 \tTraining Loss: 1.205634 \tValidation Loss: 2.702712\n",
      "Epoch: 35237 \tTraining Loss: 1.191975 \tValidation Loss: 2.704107\n",
      "Epoch: 35238 \tTraining Loss: 1.203597 \tValidation Loss: 2.703085\n",
      "Epoch: 35239 \tTraining Loss: 1.244023 \tValidation Loss: 2.702276\n",
      "Epoch: 35240 \tTraining Loss: 1.217018 \tValidation Loss: 2.701282\n",
      "Epoch: 35241 \tTraining Loss: 1.233299 \tValidation Loss: 2.701633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35242 \tTraining Loss: 1.190341 \tValidation Loss: 2.703336\n",
      "Epoch: 35243 \tTraining Loss: 1.276210 \tValidation Loss: 2.701657\n",
      "Epoch: 35244 \tTraining Loss: 1.234809 \tValidation Loss: 2.703323\n",
      "Epoch: 35245 \tTraining Loss: 1.256042 \tValidation Loss: 2.704676\n",
      "Epoch: 35246 \tTraining Loss: 1.221366 \tValidation Loss: 2.702704\n",
      "Epoch: 35247 \tTraining Loss: 1.185013 \tValidation Loss: 2.704054\n",
      "Epoch: 35248 \tTraining Loss: 1.226679 \tValidation Loss: 2.703561\n",
      "Epoch: 35249 \tTraining Loss: 1.234866 \tValidation Loss: 2.701557\n",
      "Epoch: 35250 \tTraining Loss: 1.189766 \tValidation Loss: 2.702219\n",
      "Epoch: 35251 \tTraining Loss: 1.238376 \tValidation Loss: 2.702700\n",
      "Epoch: 35252 \tTraining Loss: 1.251897 \tValidation Loss: 2.702832\n",
      "Epoch: 35253 \tTraining Loss: 1.192213 \tValidation Loss: 2.701981\n",
      "Epoch: 35254 \tTraining Loss: 1.221958 \tValidation Loss: 2.704663\n",
      "Epoch: 35255 \tTraining Loss: 1.195652 \tValidation Loss: 2.703009\n",
      "Epoch: 35256 \tTraining Loss: 1.234716 \tValidation Loss: 2.701842\n",
      "Epoch: 35257 \tTraining Loss: 1.192112 \tValidation Loss: 2.703117\n",
      "Epoch: 35258 \tTraining Loss: 1.201413 \tValidation Loss: 2.704034\n",
      "Epoch: 35259 \tTraining Loss: 1.191056 \tValidation Loss: 2.701954\n",
      "Epoch: 35260 \tTraining Loss: 1.242771 \tValidation Loss: 2.703243\n",
      "Epoch: 35261 \tTraining Loss: 1.270863 \tValidation Loss: 2.702975\n",
      "Epoch: 35262 \tTraining Loss: 1.205176 \tValidation Loss: 2.703717\n",
      "Epoch: 35263 \tTraining Loss: 1.194759 \tValidation Loss: 2.702680\n",
      "Epoch: 35264 \tTraining Loss: 1.238804 \tValidation Loss: 2.702585\n",
      "Epoch: 35265 \tTraining Loss: 1.209366 \tValidation Loss: 2.703897\n",
      "Epoch: 35266 \tTraining Loss: 1.220045 \tValidation Loss: 2.701289\n",
      "Epoch: 35267 \tTraining Loss: 1.236200 \tValidation Loss: 2.701486\n",
      "Epoch: 35268 \tTraining Loss: 1.221638 \tValidation Loss: 2.702637\n",
      "Epoch: 35269 \tTraining Loss: 1.200784 \tValidation Loss: 2.703502\n",
      "Epoch: 35270 \tTraining Loss: 1.184781 \tValidation Loss: 2.704000\n",
      "Epoch: 35271 \tTraining Loss: 1.227038 \tValidation Loss: 2.701910\n",
      "Epoch: 35272 \tTraining Loss: 1.233644 \tValidation Loss: 2.701121\n",
      "Epoch: 35273 \tTraining Loss: 1.232042 \tValidation Loss: 2.702795\n",
      "Epoch: 35274 \tTraining Loss: 1.194125 \tValidation Loss: 2.701970\n",
      "Epoch: 35275 \tTraining Loss: 1.245855 \tValidation Loss: 2.703425\n",
      "Epoch: 35276 \tTraining Loss: 1.197036 \tValidation Loss: 2.704117\n",
      "Epoch: 35277 \tTraining Loss: 1.205794 \tValidation Loss: 2.702465\n",
      "Epoch: 35278 \tTraining Loss: 1.267598 \tValidation Loss: 2.701014\n",
      "Epoch: 35279 \tTraining Loss: 1.207823 \tValidation Loss: 2.702598\n",
      "Epoch: 35280 \tTraining Loss: 1.202314 \tValidation Loss: 2.703148\n",
      "Epoch: 35281 \tTraining Loss: 1.197476 \tValidation Loss: 2.704243\n",
      "Epoch: 35282 \tTraining Loss: 1.272702 \tValidation Loss: 2.702253\n",
      "Epoch: 35283 \tTraining Loss: 1.230865 \tValidation Loss: 2.703342\n",
      "Epoch: 35284 \tTraining Loss: 1.257343 \tValidation Loss: 2.703744\n",
      "Epoch: 35285 \tTraining Loss: 1.206651 \tValidation Loss: 2.705171\n",
      "Epoch: 35286 \tTraining Loss: 1.222107 \tValidation Loss: 2.702226\n",
      "Epoch: 35287 \tTraining Loss: 1.256936 \tValidation Loss: 2.703273\n",
      "Epoch: 35288 \tTraining Loss: 1.198530 \tValidation Loss: 2.706037\n",
      "Epoch: 35289 \tTraining Loss: 1.237391 \tValidation Loss: 2.703052\n",
      "Epoch: 35290 \tTraining Loss: 1.196251 \tValidation Loss: 2.703968\n",
      "Epoch: 35291 \tTraining Loss: 1.223187 \tValidation Loss: 2.703992\n",
      "Epoch: 35292 \tTraining Loss: 1.200641 \tValidation Loss: 2.704426\n",
      "Epoch: 35293 \tTraining Loss: 1.205644 \tValidation Loss: 2.703044\n",
      "Epoch: 35294 \tTraining Loss: 1.209543 \tValidation Loss: 2.703520\n",
      "Epoch: 35295 \tTraining Loss: 1.253986 \tValidation Loss: 2.702372\n",
      "Epoch: 35296 \tTraining Loss: 1.183492 \tValidation Loss: 2.704173\n",
      "Epoch: 35297 \tTraining Loss: 1.268184 \tValidation Loss: 2.703416\n",
      "Epoch: 35298 \tTraining Loss: 1.265465 \tValidation Loss: 2.702896\n",
      "Epoch: 35299 \tTraining Loss: 1.212812 \tValidation Loss: 2.703976\n",
      "Epoch: 35300 \tTraining Loss: 1.223810 \tValidation Loss: 2.701633\n",
      "Epoch: 35301 \tTraining Loss: 1.267407 \tValidation Loss: 2.702813\n",
      "Epoch: 35302 \tTraining Loss: 1.225191 \tValidation Loss: 2.703175\n",
      "Epoch: 35303 \tTraining Loss: 1.232797 \tValidation Loss: 2.703269\n",
      "Epoch: 35304 \tTraining Loss: 1.207271 \tValidation Loss: 2.701913\n",
      "Epoch: 35305 \tTraining Loss: 1.214039 \tValidation Loss: 2.702745\n",
      "Epoch: 35306 \tTraining Loss: 1.254333 \tValidation Loss: 2.704618\n",
      "Epoch: 35307 \tTraining Loss: 1.199180 \tValidation Loss: 2.704318\n",
      "Epoch: 35308 \tTraining Loss: 1.194512 \tValidation Loss: 2.705201\n",
      "Epoch: 35309 \tTraining Loss: 1.248982 \tValidation Loss: 2.705021\n",
      "Epoch: 35310 \tTraining Loss: 1.211045 \tValidation Loss: 2.703316\n",
      "Epoch: 35311 \tTraining Loss: 1.180475 \tValidation Loss: 2.705395\n",
      "Epoch: 35312 \tTraining Loss: 1.226162 \tValidation Loss: 2.703844\n",
      "Epoch: 35313 \tTraining Loss: 1.223669 \tValidation Loss: 2.705357\n",
      "Epoch: 35314 \tTraining Loss: 1.194537 \tValidation Loss: 2.704443\n",
      "Epoch: 35315 \tTraining Loss: 1.229126 \tValidation Loss: 2.703617\n",
      "Epoch: 35316 \tTraining Loss: 1.192478 \tValidation Loss: 2.705083\n",
      "Epoch: 35317 \tTraining Loss: 1.203317 \tValidation Loss: 2.703463\n",
      "Epoch: 35318 \tTraining Loss: 1.172440 \tValidation Loss: 2.705313\n",
      "Epoch: 35319 \tTraining Loss: 1.230343 \tValidation Loss: 2.702944\n",
      "Epoch: 35320 \tTraining Loss: 1.246652 \tValidation Loss: 2.703637\n",
      "Epoch: 35321 \tTraining Loss: 1.244971 \tValidation Loss: 2.702407\n",
      "Epoch: 35322 \tTraining Loss: 1.259277 \tValidation Loss: 2.703590\n",
      "Epoch: 35323 \tTraining Loss: 1.138455 \tValidation Loss: 2.704481\n",
      "Epoch: 35324 \tTraining Loss: 1.227877 \tValidation Loss: 2.704525\n",
      "Epoch: 35325 \tTraining Loss: 1.189576 \tValidation Loss: 2.705434\n",
      "Epoch: 35326 \tTraining Loss: 1.247219 \tValidation Loss: 2.704183\n",
      "Epoch: 35327 \tTraining Loss: 1.231109 \tValidation Loss: 2.704957\n",
      "Epoch: 35328 \tTraining Loss: 1.197479 \tValidation Loss: 2.703948\n",
      "Epoch: 35329 \tTraining Loss: 1.210737 \tValidation Loss: 2.704812\n",
      "Epoch: 35330 \tTraining Loss: 1.218793 \tValidation Loss: 2.705295\n",
      "Epoch: 35331 \tTraining Loss: 1.214551 \tValidation Loss: 2.704546\n",
      "Epoch: 35332 \tTraining Loss: 1.217042 \tValidation Loss: 2.704790\n",
      "Epoch: 35333 \tTraining Loss: 1.256313 \tValidation Loss: 2.705521\n",
      "Epoch: 35334 \tTraining Loss: 1.209367 \tValidation Loss: 2.705038\n",
      "Epoch: 35335 \tTraining Loss: 1.242138 \tValidation Loss: 2.703940\n",
      "Epoch: 35336 \tTraining Loss: 1.232178 \tValidation Loss: 2.704957\n",
      "Epoch: 35337 \tTraining Loss: 1.205266 \tValidation Loss: 2.704712\n",
      "Epoch: 35338 \tTraining Loss: 1.234445 \tValidation Loss: 2.703501\n",
      "Epoch: 35339 \tTraining Loss: 1.182684 \tValidation Loss: 2.705769\n",
      "Epoch: 35340 \tTraining Loss: 1.197513 \tValidation Loss: 2.705543\n",
      "Epoch: 35341 \tTraining Loss: 1.239114 \tValidation Loss: 2.706181\n",
      "Epoch: 35342 \tTraining Loss: 1.189368 \tValidation Loss: 2.704180\n",
      "Epoch: 35343 \tTraining Loss: 1.253919 \tValidation Loss: 2.704836\n",
      "Epoch: 35344 \tTraining Loss: 1.205467 \tValidation Loss: 2.704004\n",
      "Epoch: 35345 \tTraining Loss: 1.206343 \tValidation Loss: 2.705315\n",
      "Epoch: 35346 \tTraining Loss: 1.227048 \tValidation Loss: 2.705041\n",
      "Epoch: 35347 \tTraining Loss: 1.190629 \tValidation Loss: 2.704471\n",
      "Epoch: 35348 \tTraining Loss: 1.193762 \tValidation Loss: 2.704868\n",
      "Epoch: 35349 \tTraining Loss: 1.205770 \tValidation Loss: 2.705225\n",
      "Epoch: 35350 \tTraining Loss: 1.226218 \tValidation Loss: 2.703581\n",
      "Epoch: 35351 \tTraining Loss: 1.216223 \tValidation Loss: 2.704608\n",
      "Epoch: 35352 \tTraining Loss: 1.225972 \tValidation Loss: 2.705561\n",
      "Epoch: 35353 \tTraining Loss: 1.180174 \tValidation Loss: 2.705504\n",
      "Epoch: 35354 \tTraining Loss: 1.189552 \tValidation Loss: 2.707100\n",
      "Epoch: 35355 \tTraining Loss: 1.240258 \tValidation Loss: 2.704214\n",
      "Epoch: 35356 \tTraining Loss: 1.165768 \tValidation Loss: 2.706556\n",
      "Epoch: 35357 \tTraining Loss: 1.190317 \tValidation Loss: 2.704783\n",
      "Epoch: 35358 \tTraining Loss: 1.238216 \tValidation Loss: 2.703379\n",
      "Epoch: 35359 \tTraining Loss: 1.235825 \tValidation Loss: 2.704556\n",
      "Epoch: 35360 \tTraining Loss: 1.220824 \tValidation Loss: 2.706000\n",
      "Epoch: 35361 \tTraining Loss: 1.195304 \tValidation Loss: 2.705112\n",
      "Epoch: 35362 \tTraining Loss: 1.231653 \tValidation Loss: 2.704310\n",
      "Epoch: 35363 \tTraining Loss: 1.262318 \tValidation Loss: 2.704319\n",
      "Epoch: 35364 \tTraining Loss: 1.206332 \tValidation Loss: 2.705142\n",
      "Epoch: 35365 \tTraining Loss: 1.225282 \tValidation Loss: 2.705298\n",
      "Epoch: 35366 \tTraining Loss: 1.221688 \tValidation Loss: 2.705718\n",
      "Epoch: 35367 \tTraining Loss: 1.173350 \tValidation Loss: 2.706658\n",
      "Epoch: 35368 \tTraining Loss: 1.197650 \tValidation Loss: 2.706703\n",
      "Epoch: 35369 \tTraining Loss: 1.249404 \tValidation Loss: 2.704511\n",
      "Epoch: 35370 \tTraining Loss: 1.202373 \tValidation Loss: 2.705257\n",
      "Epoch: 35371 \tTraining Loss: 1.247297 \tValidation Loss: 2.706620\n",
      "Epoch: 35372 \tTraining Loss: 1.231515 \tValidation Loss: 2.705366\n",
      "Epoch: 35373 \tTraining Loss: 1.234604 \tValidation Loss: 2.706994\n",
      "Epoch: 35374 \tTraining Loss: 1.216443 \tValidation Loss: 2.706369\n",
      "Epoch: 35375 \tTraining Loss: 1.218040 \tValidation Loss: 2.704341\n",
      "Epoch: 35376 \tTraining Loss: 1.207082 \tValidation Loss: 2.706697\n",
      "Epoch: 35377 \tTraining Loss: 1.234790 \tValidation Loss: 2.704887\n",
      "Epoch: 35378 \tTraining Loss: 1.260112 \tValidation Loss: 2.704294\n",
      "Epoch: 35379 \tTraining Loss: 1.168513 \tValidation Loss: 2.707178\n",
      "Epoch: 35380 \tTraining Loss: 1.194735 \tValidation Loss: 2.706704\n",
      "Epoch: 35381 \tTraining Loss: 1.251926 \tValidation Loss: 2.704976\n",
      "Epoch: 35382 \tTraining Loss: 1.196380 \tValidation Loss: 2.707073\n",
      "Epoch: 35383 \tTraining Loss: 1.181720 \tValidation Loss: 2.706135\n",
      "Epoch: 35384 \tTraining Loss: 1.274557 \tValidation Loss: 2.707063\n",
      "Epoch: 35385 \tTraining Loss: 1.230899 \tValidation Loss: 2.705178\n",
      "Epoch: 35386 \tTraining Loss: 1.220597 \tValidation Loss: 2.706052\n",
      "Epoch: 35387 \tTraining Loss: 1.258293 \tValidation Loss: 2.706395\n",
      "Epoch: 35388 \tTraining Loss: 1.203183 \tValidation Loss: 2.703769\n",
      "Epoch: 35389 \tTraining Loss: 1.217777 \tValidation Loss: 2.705949\n",
      "Epoch: 35390 \tTraining Loss: 1.211624 \tValidation Loss: 2.706435\n",
      "Epoch: 35391 \tTraining Loss: 1.239831 \tValidation Loss: 2.704675\n",
      "Epoch: 35392 \tTraining Loss: 1.173438 \tValidation Loss: 2.706962\n",
      "Epoch: 35393 \tTraining Loss: 1.254157 \tValidation Loss: 2.706073\n",
      "Epoch: 35394 \tTraining Loss: 1.197732 \tValidation Loss: 2.705062\n",
      "Epoch: 35395 \tTraining Loss: 1.251249 \tValidation Loss: 2.705029\n",
      "Epoch: 35396 \tTraining Loss: 1.188709 \tValidation Loss: 2.704688\n",
      "Epoch: 35397 \tTraining Loss: 1.210004 \tValidation Loss: 2.705820\n",
      "Epoch: 35398 \tTraining Loss: 1.256059 \tValidation Loss: 2.707342\n",
      "Epoch: 35399 \tTraining Loss: 1.254737 \tValidation Loss: 2.707578\n",
      "Epoch: 35400 \tTraining Loss: 1.213317 \tValidation Loss: 2.705741\n",
      "Epoch: 35401 \tTraining Loss: 1.242127 \tValidation Loss: 2.707610\n",
      "Epoch: 35402 \tTraining Loss: 1.235417 \tValidation Loss: 2.706290\n",
      "Epoch: 35403 \tTraining Loss: 1.230707 \tValidation Loss: 2.706538\n",
      "Epoch: 35404 \tTraining Loss: 1.218777 \tValidation Loss: 2.704799\n",
      "Epoch: 35405 \tTraining Loss: 1.219477 \tValidation Loss: 2.705160\n",
      "Epoch: 35406 \tTraining Loss: 1.227777 \tValidation Loss: 2.706425\n",
      "Epoch: 35407 \tTraining Loss: 1.198367 \tValidation Loss: 2.706347\n",
      "Epoch: 35408 \tTraining Loss: 1.202070 \tValidation Loss: 2.706617\n",
      "Epoch: 35409 \tTraining Loss: 1.237858 \tValidation Loss: 2.706852\n",
      "Epoch: 35410 \tTraining Loss: 1.202747 \tValidation Loss: 2.707613\n",
      "Epoch: 35411 \tTraining Loss: 1.213951 \tValidation Loss: 2.707399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35412 \tTraining Loss: 1.262162 \tValidation Loss: 2.705763\n",
      "Epoch: 35413 \tTraining Loss: 1.247235 \tValidation Loss: 2.706372\n",
      "Epoch: 35414 \tTraining Loss: 1.250094 \tValidation Loss: 2.707425\n",
      "Epoch: 35415 \tTraining Loss: 1.207215 \tValidation Loss: 2.705106\n",
      "Epoch: 35416 \tTraining Loss: 1.225239 \tValidation Loss: 2.706414\n",
      "Epoch: 35417 \tTraining Loss: 1.234979 \tValidation Loss: 2.707190\n",
      "Epoch: 35418 \tTraining Loss: 1.240616 \tValidation Loss: 2.705965\n",
      "Epoch: 35419 \tTraining Loss: 1.182363 \tValidation Loss: 2.706665\n",
      "Epoch: 35420 \tTraining Loss: 1.237250 \tValidation Loss: 2.706635\n",
      "Epoch: 35421 \tTraining Loss: 1.205342 \tValidation Loss: 2.708095\n",
      "Epoch: 35422 \tTraining Loss: 1.206161 \tValidation Loss: 2.707647\n",
      "Epoch: 35423 \tTraining Loss: 1.221876 \tValidation Loss: 2.706824\n",
      "Epoch: 35424 \tTraining Loss: 1.193994 \tValidation Loss: 2.708773\n",
      "Epoch: 35425 \tTraining Loss: 1.200025 \tValidation Loss: 2.707425\n",
      "Epoch: 35426 \tTraining Loss: 1.200180 \tValidation Loss: 2.707204\n",
      "Epoch: 35427 \tTraining Loss: 1.236684 \tValidation Loss: 2.707044\n",
      "Epoch: 35428 \tTraining Loss: 1.188482 \tValidation Loss: 2.705978\n",
      "Epoch: 35429 \tTraining Loss: 1.244812 \tValidation Loss: 2.706938\n",
      "Epoch: 35430 \tTraining Loss: 1.223597 \tValidation Loss: 2.706613\n",
      "Epoch: 35431 \tTraining Loss: 1.233814 \tValidation Loss: 2.705567\n",
      "Epoch: 35432 \tTraining Loss: 1.224450 \tValidation Loss: 2.706738\n",
      "Epoch: 35433 \tTraining Loss: 1.220927 \tValidation Loss: 2.706055\n",
      "Epoch: 35434 \tTraining Loss: 1.275194 \tValidation Loss: 2.707588\n",
      "Epoch: 35435 \tTraining Loss: 1.248359 \tValidation Loss: 2.709436\n",
      "Epoch: 35436 \tTraining Loss: 1.175714 \tValidation Loss: 2.707344\n",
      "Epoch: 35437 \tTraining Loss: 1.187547 \tValidation Loss: 2.709064\n",
      "Epoch: 35438 \tTraining Loss: 1.184061 \tValidation Loss: 2.706496\n",
      "Epoch: 35439 \tTraining Loss: 1.265665 \tValidation Loss: 2.706359\n",
      "Epoch: 35440 \tTraining Loss: 1.192682 \tValidation Loss: 2.706698\n",
      "Epoch: 35441 \tTraining Loss: 1.206923 \tValidation Loss: 2.707798\n",
      "Epoch: 35442 \tTraining Loss: 1.231196 \tValidation Loss: 2.707279\n",
      "Epoch: 35443 \tTraining Loss: 1.212613 \tValidation Loss: 2.707328\n",
      "Epoch: 35444 \tTraining Loss: 1.222198 \tValidation Loss: 2.706164\n",
      "Epoch: 35445 \tTraining Loss: 1.246272 \tValidation Loss: 2.707871\n",
      "Epoch: 35446 \tTraining Loss: 1.223938 \tValidation Loss: 2.707353\n",
      "Epoch: 35447 \tTraining Loss: 1.194741 \tValidation Loss: 2.708271\n",
      "Epoch: 35448 \tTraining Loss: 1.218238 \tValidation Loss: 2.708187\n",
      "Epoch: 35449 \tTraining Loss: 1.187693 \tValidation Loss: 2.708217\n",
      "Epoch: 35450 \tTraining Loss: 1.208333 \tValidation Loss: 2.708025\n",
      "Epoch: 35451 \tTraining Loss: 1.219563 \tValidation Loss: 2.708135\n",
      "Epoch: 35452 \tTraining Loss: 1.219814 \tValidation Loss: 2.707469\n",
      "Epoch: 35453 \tTraining Loss: 1.205241 \tValidation Loss: 2.709469\n",
      "Epoch: 35454 \tTraining Loss: 1.213425 \tValidation Loss: 2.706796\n",
      "Epoch: 35455 \tTraining Loss: 1.229259 \tValidation Loss: 2.707928\n",
      "Epoch: 35456 \tTraining Loss: 1.216472 \tValidation Loss: 2.707767\n",
      "Epoch: 35457 \tTraining Loss: 1.174172 \tValidation Loss: 2.707540\n",
      "Epoch: 35458 \tTraining Loss: 1.200974 \tValidation Loss: 2.706685\n",
      "Epoch: 35459 \tTraining Loss: 1.156531 \tValidation Loss: 2.708411\n",
      "Epoch: 35460 \tTraining Loss: 1.213708 \tValidation Loss: 2.708340\n",
      "Epoch: 35461 \tTraining Loss: 1.219810 \tValidation Loss: 2.708047\n",
      "Epoch: 35462 \tTraining Loss: 1.210372 \tValidation Loss: 2.708485\n",
      "Epoch: 35463 \tTraining Loss: 1.255908 \tValidation Loss: 2.707182\n",
      "Epoch: 35464 \tTraining Loss: 1.220352 \tValidation Loss: 2.708156\n",
      "Epoch: 35465 \tTraining Loss: 1.222588 \tValidation Loss: 2.706845\n",
      "Epoch: 35466 \tTraining Loss: 1.269420 \tValidation Loss: 2.707057\n",
      "Epoch: 35467 \tTraining Loss: 1.198601 \tValidation Loss: 2.708116\n",
      "Epoch: 35468 \tTraining Loss: 1.193250 \tValidation Loss: 2.708655\n",
      "Epoch: 35469 \tTraining Loss: 1.224436 \tValidation Loss: 2.708220\n",
      "Epoch: 35470 \tTraining Loss: 1.216309 \tValidation Loss: 2.707693\n",
      "Epoch: 35471 \tTraining Loss: 1.228506 \tValidation Loss: 2.708025\n",
      "Epoch: 35472 \tTraining Loss: 1.219362 \tValidation Loss: 2.707687\n",
      "Epoch: 35473 \tTraining Loss: 1.262108 \tValidation Loss: 2.709477\n",
      "Epoch: 35474 \tTraining Loss: 1.201167 \tValidation Loss: 2.707516\n",
      "Epoch: 35475 \tTraining Loss: 1.195679 \tValidation Loss: 2.708250\n",
      "Epoch: 35476 \tTraining Loss: 1.259422 \tValidation Loss: 2.706549\n",
      "Epoch: 35477 \tTraining Loss: 1.219054 \tValidation Loss: 2.707623\n",
      "Epoch: 35478 \tTraining Loss: 1.233987 \tValidation Loss: 2.707596\n",
      "Epoch: 35479 \tTraining Loss: 1.200932 \tValidation Loss: 2.708964\n",
      "Epoch: 35480 \tTraining Loss: 1.269414 \tValidation Loss: 2.707120\n",
      "Epoch: 35481 \tTraining Loss: 1.257585 \tValidation Loss: 2.707154\n",
      "Epoch: 35482 \tTraining Loss: 1.240903 \tValidation Loss: 2.710079\n",
      "Epoch: 35483 \tTraining Loss: 1.197624 \tValidation Loss: 2.707585\n",
      "Epoch: 35484 \tTraining Loss: 1.190948 \tValidation Loss: 2.708485\n",
      "Epoch: 35485 \tTraining Loss: 1.213398 \tValidation Loss: 2.708807\n",
      "Epoch: 35486 \tTraining Loss: 1.207637 \tValidation Loss: 2.707951\n",
      "Epoch: 35487 \tTraining Loss: 1.209077 \tValidation Loss: 2.708642\n",
      "Epoch: 35488 \tTraining Loss: 1.233701 \tValidation Loss: 2.708856\n",
      "Epoch: 35489 \tTraining Loss: 1.233195 \tValidation Loss: 2.709041\n",
      "Epoch: 35490 \tTraining Loss: 1.217530 \tValidation Loss: 2.708101\n",
      "Epoch: 35491 \tTraining Loss: 1.184146 \tValidation Loss: 2.709508\n",
      "Epoch: 35492 \tTraining Loss: 1.217368 \tValidation Loss: 2.708763\n",
      "Epoch: 35493 \tTraining Loss: 1.193449 \tValidation Loss: 2.707053\n",
      "Epoch: 35494 \tTraining Loss: 1.260854 \tValidation Loss: 2.708619\n",
      "Epoch: 35495 \tTraining Loss: 1.232334 \tValidation Loss: 2.709330\n",
      "Epoch: 35496 \tTraining Loss: 1.197014 \tValidation Loss: 2.707678\n",
      "Epoch: 35497 \tTraining Loss: 1.254755 \tValidation Loss: 2.706635\n",
      "Epoch: 35498 \tTraining Loss: 1.207512 \tValidation Loss: 2.708802\n",
      "Epoch: 35499 \tTraining Loss: 1.155457 \tValidation Loss: 2.709674\n",
      "Epoch: 35500 \tTraining Loss: 1.245389 \tValidation Loss: 2.707249\n",
      "Epoch: 35501 \tTraining Loss: 1.221603 \tValidation Loss: 2.708625\n",
      "Epoch: 35502 \tTraining Loss: 1.183929 \tValidation Loss: 2.707340\n",
      "Epoch: 35503 \tTraining Loss: 1.213189 \tValidation Loss: 2.708901\n",
      "Epoch: 35504 \tTraining Loss: 1.225833 \tValidation Loss: 2.708460\n",
      "Epoch: 35505 \tTraining Loss: 1.241221 \tValidation Loss: 2.708119\n",
      "Epoch: 35506 \tTraining Loss: 1.191308 \tValidation Loss: 2.708993\n",
      "Epoch: 35507 \tTraining Loss: 1.223265 \tValidation Loss: 2.708690\n",
      "Epoch: 35508 \tTraining Loss: 1.214978 \tValidation Loss: 2.707624\n",
      "Epoch: 35509 \tTraining Loss: 1.210448 \tValidation Loss: 2.709497\n",
      "Epoch: 35510 \tTraining Loss: 1.272763 \tValidation Loss: 2.708259\n",
      "Epoch: 35511 \tTraining Loss: 1.180614 \tValidation Loss: 2.710069\n",
      "Epoch: 35512 \tTraining Loss: 1.210813 \tValidation Loss: 2.707820\n",
      "Epoch: 35513 \tTraining Loss: 1.190507 \tValidation Loss: 2.708976\n",
      "Epoch: 35514 \tTraining Loss: 1.229519 \tValidation Loss: 2.708271\n",
      "Epoch: 35515 \tTraining Loss: 1.173815 \tValidation Loss: 2.709173\n",
      "Epoch: 35516 \tTraining Loss: 1.204705 \tValidation Loss: 2.709694\n",
      "Epoch: 35517 \tTraining Loss: 1.233870 \tValidation Loss: 2.707830\n",
      "Epoch: 35518 \tTraining Loss: 1.231673 \tValidation Loss: 2.708131\n",
      "Epoch: 35519 \tTraining Loss: 1.230216 \tValidation Loss: 2.709128\n",
      "Epoch: 35520 \tTraining Loss: 1.224242 \tValidation Loss: 2.707273\n",
      "Epoch: 35521 \tTraining Loss: 1.208703 \tValidation Loss: 2.708749\n",
      "Epoch: 35522 \tTraining Loss: 1.201592 \tValidation Loss: 2.709295\n",
      "Epoch: 35523 \tTraining Loss: 1.205433 \tValidation Loss: 2.709231\n",
      "Epoch: 35524 \tTraining Loss: 1.223920 \tValidation Loss: 2.709080\n",
      "Epoch: 35525 \tTraining Loss: 1.198805 \tValidation Loss: 2.709725\n",
      "Epoch: 35526 \tTraining Loss: 1.233878 \tValidation Loss: 2.708323\n",
      "Epoch: 35527 \tTraining Loss: 1.214212 \tValidation Loss: 2.709214\n",
      "Epoch: 35528 \tTraining Loss: 1.174737 \tValidation Loss: 2.709580\n",
      "Epoch: 35529 \tTraining Loss: 1.196475 \tValidation Loss: 2.709778\n",
      "Epoch: 35530 \tTraining Loss: 1.243243 \tValidation Loss: 2.708917\n",
      "Epoch: 35531 \tTraining Loss: 1.230142 \tValidation Loss: 2.709835\n",
      "Epoch: 35532 \tTraining Loss: 1.250145 \tValidation Loss: 2.709860\n",
      "Epoch: 35533 \tTraining Loss: 1.273427 \tValidation Loss: 2.708328\n",
      "Epoch: 35534 \tTraining Loss: 1.213226 \tValidation Loss: 2.710417\n",
      "Epoch: 35535 \tTraining Loss: 1.200163 \tValidation Loss: 2.710456\n",
      "Epoch: 35536 \tTraining Loss: 1.225420 \tValidation Loss: 2.709319\n",
      "Epoch: 35537 \tTraining Loss: 1.232006 \tValidation Loss: 2.710832\n",
      "Epoch: 35538 \tTraining Loss: 1.161614 \tValidation Loss: 2.710144\n",
      "Epoch: 35539 \tTraining Loss: 1.176123 \tValidation Loss: 2.709429\n",
      "Epoch: 35540 \tTraining Loss: 1.214738 \tValidation Loss: 2.708662\n",
      "Epoch: 35541 \tTraining Loss: 1.178728 \tValidation Loss: 2.710938\n",
      "Epoch: 35542 \tTraining Loss: 1.207365 \tValidation Loss: 2.710217\n",
      "Epoch: 35543 \tTraining Loss: 1.177737 \tValidation Loss: 2.711159\n",
      "Epoch: 35544 \tTraining Loss: 1.237208 \tValidation Loss: 2.708561\n",
      "Epoch: 35545 \tTraining Loss: 1.199275 \tValidation Loss: 2.710459\n",
      "Epoch: 35546 \tTraining Loss: 1.184417 \tValidation Loss: 2.710107\n",
      "Epoch: 35547 \tTraining Loss: 1.230159 \tValidation Loss: 2.708641\n",
      "Epoch: 35548 \tTraining Loss: 1.248594 \tValidation Loss: 2.709669\n",
      "Epoch: 35549 \tTraining Loss: 1.226809 \tValidation Loss: 2.709360\n",
      "Epoch: 35550 \tTraining Loss: 1.207781 \tValidation Loss: 2.710050\n",
      "Epoch: 35551 \tTraining Loss: 1.197854 \tValidation Loss: 2.710184\n",
      "Epoch: 35552 \tTraining Loss: 1.198114 \tValidation Loss: 2.710198\n",
      "Epoch: 35553 \tTraining Loss: 1.224637 \tValidation Loss: 2.710320\n",
      "Epoch: 35554 \tTraining Loss: 1.208740 \tValidation Loss: 2.709568\n",
      "Epoch: 35555 \tTraining Loss: 1.228607 \tValidation Loss: 2.710535\n",
      "Epoch: 35556 \tTraining Loss: 1.259622 \tValidation Loss: 2.708348\n",
      "Epoch: 35557 \tTraining Loss: 1.174662 \tValidation Loss: 2.710478\n",
      "Epoch: 35558 \tTraining Loss: 1.172521 \tValidation Loss: 2.710228\n",
      "Epoch: 35559 \tTraining Loss: 1.226820 \tValidation Loss: 2.708275\n",
      "Epoch: 35560 \tTraining Loss: 1.190731 \tValidation Loss: 2.710288\n",
      "Epoch: 35561 \tTraining Loss: 1.208123 \tValidation Loss: 2.710632\n",
      "Epoch: 35562 \tTraining Loss: 1.256608 \tValidation Loss: 2.710383\n",
      "Epoch: 35563 \tTraining Loss: 1.224955 \tValidation Loss: 2.710420\n",
      "Epoch: 35564 \tTraining Loss: 1.217830 \tValidation Loss: 2.709024\n",
      "Epoch: 35565 \tTraining Loss: 1.198168 \tValidation Loss: 2.711257\n",
      "Epoch: 35566 \tTraining Loss: 1.216306 \tValidation Loss: 2.709652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35567 \tTraining Loss: 1.269602 \tValidation Loss: 2.709276\n",
      "Epoch: 35568 \tTraining Loss: 1.202143 \tValidation Loss: 2.711033\n",
      "Epoch: 35569 \tTraining Loss: 1.196023 \tValidation Loss: 2.710321\n",
      "Epoch: 35570 \tTraining Loss: 1.251630 \tValidation Loss: 2.710256\n",
      "Epoch: 35571 \tTraining Loss: 1.166208 \tValidation Loss: 2.711794\n",
      "Epoch: 35572 \tTraining Loss: 1.234826 \tValidation Loss: 2.711166\n",
      "Epoch: 35573 \tTraining Loss: 1.187258 \tValidation Loss: 2.709930\n",
      "Epoch: 35574 \tTraining Loss: 1.257494 \tValidation Loss: 2.710251\n",
      "Epoch: 35575 \tTraining Loss: 1.231663 \tValidation Loss: 2.710222\n",
      "Epoch: 35576 \tTraining Loss: 1.254757 \tValidation Loss: 2.709577\n",
      "Epoch: 35577 \tTraining Loss: 1.244255 \tValidation Loss: 2.710042\n",
      "Epoch: 35578 \tTraining Loss: 1.240094 \tValidation Loss: 2.709874\n",
      "Epoch: 35579 \tTraining Loss: 1.231977 \tValidation Loss: 2.709207\n",
      "Epoch: 35580 \tTraining Loss: 1.197297 \tValidation Loss: 2.712196\n",
      "Epoch: 35581 \tTraining Loss: 1.210394 \tValidation Loss: 2.709523\n",
      "Epoch: 35582 \tTraining Loss: 1.198630 \tValidation Loss: 2.711334\n",
      "Epoch: 35583 \tTraining Loss: 1.166728 \tValidation Loss: 2.711246\n",
      "Epoch: 35584 \tTraining Loss: 1.257211 \tValidation Loss: 2.710385\n",
      "Epoch: 35585 \tTraining Loss: 1.197838 \tValidation Loss: 2.711052\n",
      "Epoch: 35586 \tTraining Loss: 1.190604 \tValidation Loss: 2.710120\n",
      "Epoch: 35587 \tTraining Loss: 1.220865 \tValidation Loss: 2.709492\n",
      "Epoch: 35588 \tTraining Loss: 1.170781 \tValidation Loss: 2.710928\n",
      "Epoch: 35589 \tTraining Loss: 1.211464 \tValidation Loss: 2.709197\n",
      "Epoch: 35590 \tTraining Loss: 1.221237 \tValidation Loss: 2.710960\n",
      "Epoch: 35591 \tTraining Loss: 1.199132 \tValidation Loss: 2.710631\n",
      "Epoch: 35592 \tTraining Loss: 1.227570 \tValidation Loss: 2.712175\n",
      "Epoch: 35593 \tTraining Loss: 1.227608 \tValidation Loss: 2.709975\n",
      "Epoch: 35594 \tTraining Loss: 1.236215 \tValidation Loss: 2.710451\n",
      "Epoch: 35595 \tTraining Loss: 1.179087 \tValidation Loss: 2.710131\n",
      "Epoch: 35596 \tTraining Loss: 1.175025 \tValidation Loss: 2.710000\n",
      "Epoch: 35597 \tTraining Loss: 1.188833 \tValidation Loss: 2.710457\n",
      "Epoch: 35598 \tTraining Loss: 1.186919 \tValidation Loss: 2.712044\n",
      "Epoch: 35599 \tTraining Loss: 1.188734 \tValidation Loss: 2.709049\n",
      "Epoch: 35600 \tTraining Loss: 1.242341 \tValidation Loss: 2.708853\n",
      "Epoch: 35601 \tTraining Loss: 1.181979 \tValidation Loss: 2.709009\n",
      "Epoch: 35602 \tTraining Loss: 1.219350 \tValidation Loss: 2.711033\n",
      "Epoch: 35603 \tTraining Loss: 1.226332 \tValidation Loss: 2.709554\n",
      "Epoch: 35604 \tTraining Loss: 1.227758 \tValidation Loss: 2.710559\n",
      "Epoch: 35605 \tTraining Loss: 1.246493 \tValidation Loss: 2.711346\n",
      "Epoch: 35606 \tTraining Loss: 1.269614 \tValidation Loss: 2.710650\n",
      "Epoch: 35607 \tTraining Loss: 1.247355 \tValidation Loss: 2.710849\n",
      "Epoch: 35608 \tTraining Loss: 1.188758 \tValidation Loss: 2.710697\n",
      "Epoch: 35609 \tTraining Loss: 1.243665 \tValidation Loss: 2.711309\n",
      "Epoch: 35610 \tTraining Loss: 1.224238 \tValidation Loss: 2.711612\n",
      "Epoch: 35611 \tTraining Loss: 1.254074 \tValidation Loss: 2.711106\n",
      "Epoch: 35612 \tTraining Loss: 1.199711 \tValidation Loss: 2.712072\n",
      "Epoch: 35613 \tTraining Loss: 1.232473 \tValidation Loss: 2.710456\n",
      "Epoch: 35614 \tTraining Loss: 1.223380 \tValidation Loss: 2.710879\n",
      "Epoch: 35615 \tTraining Loss: 1.209621 \tValidation Loss: 2.710212\n",
      "Epoch: 35616 \tTraining Loss: 1.240749 \tValidation Loss: 2.709092\n",
      "Epoch: 35617 \tTraining Loss: 1.245376 \tValidation Loss: 2.710669\n",
      "Epoch: 35618 \tTraining Loss: 1.203120 \tValidation Loss: 2.711788\n",
      "Epoch: 35619 \tTraining Loss: 1.231557 \tValidation Loss: 2.712069\n",
      "Epoch: 35620 \tTraining Loss: 1.202947 \tValidation Loss: 2.710803\n",
      "Epoch: 35621 \tTraining Loss: 1.226920 \tValidation Loss: 2.710508\n",
      "Epoch: 35622 \tTraining Loss: 1.197369 \tValidation Loss: 2.711844\n",
      "Epoch: 35623 \tTraining Loss: 1.223891 \tValidation Loss: 2.711382\n",
      "Epoch: 35624 \tTraining Loss: 1.219272 \tValidation Loss: 2.711726\n",
      "Epoch: 35625 \tTraining Loss: 1.200717 \tValidation Loss: 2.711514\n",
      "Epoch: 35626 \tTraining Loss: 1.252685 \tValidation Loss: 2.711540\n",
      "Epoch: 35627 \tTraining Loss: 1.228027 \tValidation Loss: 2.710711\n",
      "Epoch: 35628 \tTraining Loss: 1.228784 \tValidation Loss: 2.710644\n",
      "Epoch: 35629 \tTraining Loss: 1.265744 \tValidation Loss: 2.711415\n",
      "Epoch: 35630 \tTraining Loss: 1.205361 \tValidation Loss: 2.712954\n",
      "Epoch: 35631 \tTraining Loss: 1.225384 \tValidation Loss: 2.712041\n",
      "Epoch: 35632 \tTraining Loss: 1.196680 \tValidation Loss: 2.711444\n",
      "Epoch: 35633 \tTraining Loss: 1.216853 \tValidation Loss: 2.710861\n",
      "Epoch: 35634 \tTraining Loss: 1.216183 \tValidation Loss: 2.711138\n",
      "Epoch: 35635 \tTraining Loss: 1.245075 \tValidation Loss: 2.712106\n",
      "Epoch: 35636 \tTraining Loss: 1.236112 \tValidation Loss: 2.711363\n",
      "Epoch: 35637 \tTraining Loss: 1.300178 \tValidation Loss: 2.710649\n",
      "Epoch: 35638 \tTraining Loss: 1.224079 \tValidation Loss: 2.710533\n",
      "Epoch: 35639 \tTraining Loss: 1.218169 \tValidation Loss: 2.710432\n",
      "Epoch: 35640 \tTraining Loss: 1.240892 \tValidation Loss: 2.710807\n",
      "Epoch: 35641 \tTraining Loss: 1.226159 \tValidation Loss: 2.711307\n",
      "Epoch: 35642 \tTraining Loss: 1.184159 \tValidation Loss: 2.710710\n",
      "Epoch: 35643 \tTraining Loss: 1.216897 \tValidation Loss: 2.710751\n",
      "Epoch: 35644 \tTraining Loss: 1.269795 \tValidation Loss: 2.710622\n",
      "Epoch: 35645 \tTraining Loss: 1.212618 \tValidation Loss: 2.710924\n",
      "Epoch: 35646 \tTraining Loss: 1.249807 \tValidation Loss: 2.711343\n",
      "Epoch: 35647 \tTraining Loss: 1.227239 \tValidation Loss: 2.711460\n",
      "Epoch: 35648 \tTraining Loss: 1.218968 \tValidation Loss: 2.712723\n",
      "Epoch: 35649 \tTraining Loss: 1.187966 \tValidation Loss: 2.711884\n",
      "Epoch: 35650 \tTraining Loss: 1.273882 \tValidation Loss: 2.712215\n",
      "Epoch: 35651 \tTraining Loss: 1.195730 \tValidation Loss: 2.713461\n",
      "Epoch: 35652 \tTraining Loss: 1.213708 \tValidation Loss: 2.709720\n",
      "Epoch: 35653 \tTraining Loss: 1.204362 \tValidation Loss: 2.713091\n",
      "Epoch: 35654 \tTraining Loss: 1.199034 \tValidation Loss: 2.713285\n",
      "Epoch: 35655 \tTraining Loss: 1.197887 \tValidation Loss: 2.711957\n",
      "Epoch: 35656 \tTraining Loss: 1.210737 \tValidation Loss: 2.712408\n",
      "Epoch: 35657 \tTraining Loss: 1.221622 \tValidation Loss: 2.712543\n",
      "Epoch: 35658 \tTraining Loss: 1.175419 \tValidation Loss: 2.712409\n",
      "Epoch: 35659 \tTraining Loss: 1.229582 \tValidation Loss: 2.712636\n",
      "Epoch: 35660 \tTraining Loss: 1.234522 \tValidation Loss: 2.711995\n",
      "Epoch: 35661 \tTraining Loss: 1.210044 \tValidation Loss: 2.713704\n",
      "Epoch: 35662 \tTraining Loss: 1.195791 \tValidation Loss: 2.712255\n",
      "Epoch: 35663 \tTraining Loss: 1.208790 \tValidation Loss: 2.712160\n",
      "Epoch: 35664 \tTraining Loss: 1.201427 \tValidation Loss: 2.714142\n",
      "Epoch: 35665 \tTraining Loss: 1.224203 \tValidation Loss: 2.712587\n",
      "Epoch: 35666 \tTraining Loss: 1.248790 \tValidation Loss: 2.712472\n",
      "Epoch: 35667 \tTraining Loss: 1.224709 \tValidation Loss: 2.713267\n",
      "Epoch: 35668 \tTraining Loss: 1.202784 \tValidation Loss: 2.712999\n",
      "Epoch: 35669 \tTraining Loss: 1.183573 \tValidation Loss: 2.712535\n",
      "Epoch: 35670 \tTraining Loss: 1.191084 \tValidation Loss: 2.712112\n",
      "Epoch: 35671 \tTraining Loss: 1.197839 \tValidation Loss: 2.713131\n",
      "Epoch: 35672 \tTraining Loss: 1.209752 \tValidation Loss: 2.711714\n",
      "Epoch: 35673 \tTraining Loss: 1.258372 \tValidation Loss: 2.711241\n",
      "Epoch: 35674 \tTraining Loss: 1.252807 \tValidation Loss: 2.711078\n",
      "Epoch: 35675 \tTraining Loss: 1.174969 \tValidation Loss: 2.712857\n",
      "Epoch: 35676 \tTraining Loss: 1.203557 \tValidation Loss: 2.712656\n",
      "Epoch: 35677 \tTraining Loss: 1.203923 \tValidation Loss: 2.711172\n",
      "Epoch: 35678 \tTraining Loss: 1.183316 \tValidation Loss: 2.713874\n",
      "Epoch: 35679 \tTraining Loss: 1.172086 \tValidation Loss: 2.712667\n",
      "Epoch: 35680 \tTraining Loss: 1.216262 \tValidation Loss: 2.711845\n",
      "Epoch: 35681 \tTraining Loss: 1.156506 \tValidation Loss: 2.714791\n",
      "Epoch: 35682 \tTraining Loss: 1.241826 \tValidation Loss: 2.711978\n",
      "Epoch: 35683 \tTraining Loss: 1.208831 \tValidation Loss: 2.712590\n",
      "Epoch: 35684 \tTraining Loss: 1.238456 \tValidation Loss: 2.713088\n",
      "Epoch: 35685 \tTraining Loss: 1.218215 \tValidation Loss: 2.711698\n",
      "Epoch: 35686 \tTraining Loss: 1.256117 \tValidation Loss: 2.712776\n",
      "Epoch: 35687 \tTraining Loss: 1.188940 \tValidation Loss: 2.710823\n",
      "Epoch: 35688 \tTraining Loss: 1.244457 \tValidation Loss: 2.713183\n",
      "Epoch: 35689 \tTraining Loss: 1.186211 \tValidation Loss: 2.713500\n",
      "Epoch: 35690 \tTraining Loss: 1.218156 \tValidation Loss: 2.713682\n",
      "Epoch: 35691 \tTraining Loss: 1.196791 \tValidation Loss: 2.711971\n",
      "Epoch: 35692 \tTraining Loss: 1.207187 \tValidation Loss: 2.712897\n",
      "Epoch: 35693 \tTraining Loss: 1.191402 \tValidation Loss: 2.712574\n",
      "Epoch: 35694 \tTraining Loss: 1.166574 \tValidation Loss: 2.713096\n",
      "Epoch: 35695 \tTraining Loss: 1.241434 \tValidation Loss: 2.712876\n",
      "Epoch: 35696 \tTraining Loss: 1.204125 \tValidation Loss: 2.712402\n",
      "Epoch: 35697 \tTraining Loss: 1.158657 \tValidation Loss: 2.713604\n",
      "Epoch: 35698 \tTraining Loss: 1.276166 \tValidation Loss: 2.711661\n",
      "Epoch: 35699 \tTraining Loss: 1.219472 \tValidation Loss: 2.712422\n",
      "Epoch: 35700 \tTraining Loss: 1.221791 \tValidation Loss: 2.713818\n",
      "Epoch: 35701 \tTraining Loss: 1.215338 \tValidation Loss: 2.712040\n",
      "Epoch: 35702 \tTraining Loss: 1.184544 \tValidation Loss: 2.715403\n",
      "Epoch: 35703 \tTraining Loss: 1.226726 \tValidation Loss: 2.713086\n",
      "Epoch: 35704 \tTraining Loss: 1.219371 \tValidation Loss: 2.712961\n",
      "Epoch: 35705 \tTraining Loss: 1.247294 \tValidation Loss: 2.712955\n",
      "Epoch: 35706 \tTraining Loss: 1.242939 \tValidation Loss: 2.712611\n",
      "Epoch: 35707 \tTraining Loss: 1.210660 \tValidation Loss: 2.711297\n",
      "Epoch: 35708 \tTraining Loss: 1.217368 \tValidation Loss: 2.713286\n",
      "Epoch: 35709 \tTraining Loss: 1.189546 \tValidation Loss: 2.713442\n",
      "Epoch: 35710 \tTraining Loss: 1.237813 \tValidation Loss: 2.713890\n",
      "Epoch: 35711 \tTraining Loss: 1.223434 \tValidation Loss: 2.712685\n",
      "Epoch: 35712 \tTraining Loss: 1.222872 \tValidation Loss: 2.712638\n",
      "Epoch: 35713 \tTraining Loss: 1.220376 \tValidation Loss: 2.713622\n",
      "Epoch: 35714 \tTraining Loss: 1.192889 \tValidation Loss: 2.712282\n",
      "Epoch: 35715 \tTraining Loss: 1.264450 \tValidation Loss: 2.711412\n",
      "Epoch: 35716 \tTraining Loss: 1.207406 \tValidation Loss: 2.713675\n",
      "Epoch: 35717 \tTraining Loss: 1.186526 \tValidation Loss: 2.712724\n",
      "Epoch: 35718 \tTraining Loss: 1.158755 \tValidation Loss: 2.715143\n",
      "Epoch: 35719 \tTraining Loss: 1.246395 \tValidation Loss: 2.711790\n",
      "Epoch: 35720 \tTraining Loss: 1.200372 \tValidation Loss: 2.713769\n",
      "Epoch: 35721 \tTraining Loss: 1.175013 \tValidation Loss: 2.715885\n",
      "Epoch: 35722 \tTraining Loss: 1.240724 \tValidation Loss: 2.712302\n",
      "Epoch: 35723 \tTraining Loss: 1.209930 \tValidation Loss: 2.713314\n",
      "Epoch: 35724 \tTraining Loss: 1.182564 \tValidation Loss: 2.712852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35725 \tTraining Loss: 1.212392 \tValidation Loss: 2.712776\n",
      "Epoch: 35726 \tTraining Loss: 1.184978 \tValidation Loss: 2.714845\n",
      "Epoch: 35727 \tTraining Loss: 1.235792 \tValidation Loss: 2.715397\n",
      "Epoch: 35728 \tTraining Loss: 1.179456 \tValidation Loss: 2.714350\n",
      "Epoch: 35729 \tTraining Loss: 1.190901 \tValidation Loss: 2.713401\n",
      "Epoch: 35730 \tTraining Loss: 1.177789 \tValidation Loss: 2.712650\n",
      "Epoch: 35731 \tTraining Loss: 1.173000 \tValidation Loss: 2.715015\n",
      "Epoch: 35732 \tTraining Loss: 1.258489 \tValidation Loss: 2.714227\n",
      "Epoch: 35733 \tTraining Loss: 1.206176 \tValidation Loss: 2.712967\n",
      "Epoch: 35734 \tTraining Loss: 1.248754 \tValidation Loss: 2.713417\n",
      "Epoch: 35735 \tTraining Loss: 1.193513 \tValidation Loss: 2.714403\n",
      "Epoch: 35736 \tTraining Loss: 1.220752 \tValidation Loss: 2.712943\n",
      "Epoch: 35737 \tTraining Loss: 1.191759 \tValidation Loss: 2.712851\n",
      "Epoch: 35738 \tTraining Loss: 1.179470 \tValidation Loss: 2.714690\n",
      "Epoch: 35739 \tTraining Loss: 1.221442 \tValidation Loss: 2.713809\n",
      "Epoch: 35740 \tTraining Loss: 1.214387 \tValidation Loss: 2.713906\n",
      "Epoch: 35741 \tTraining Loss: 1.224963 \tValidation Loss: 2.715842\n",
      "Epoch: 35742 \tTraining Loss: 1.245792 \tValidation Loss: 2.713811\n",
      "Epoch: 35743 \tTraining Loss: 1.232569 \tValidation Loss: 2.714500\n",
      "Epoch: 35744 \tTraining Loss: 1.233144 \tValidation Loss: 2.713640\n",
      "Epoch: 35745 \tTraining Loss: 1.207248 \tValidation Loss: 2.712311\n",
      "Epoch: 35746 \tTraining Loss: 1.297349 \tValidation Loss: 2.713482\n",
      "Epoch: 35747 \tTraining Loss: 1.233314 \tValidation Loss: 2.713231\n",
      "Epoch: 35748 \tTraining Loss: 1.219555 \tValidation Loss: 2.713368\n",
      "Epoch: 35749 \tTraining Loss: 1.192213 \tValidation Loss: 2.712725\n",
      "Epoch: 35750 \tTraining Loss: 1.200411 \tValidation Loss: 2.714049\n",
      "Epoch: 35751 \tTraining Loss: 1.202240 \tValidation Loss: 2.713537\n",
      "Epoch: 35752 \tTraining Loss: 1.224373 \tValidation Loss: 2.712752\n",
      "Epoch: 35753 \tTraining Loss: 1.236868 \tValidation Loss: 2.714453\n",
      "Epoch: 35754 \tTraining Loss: 1.267707 \tValidation Loss: 2.714471\n",
      "Epoch: 35755 \tTraining Loss: 1.216256 \tValidation Loss: 2.713547\n",
      "Epoch: 35756 \tTraining Loss: 1.200211 \tValidation Loss: 2.714313\n",
      "Epoch: 35757 \tTraining Loss: 1.228665 \tValidation Loss: 2.714271\n",
      "Epoch: 35758 \tTraining Loss: 1.169682 \tValidation Loss: 2.716294\n",
      "Epoch: 35759 \tTraining Loss: 1.181144 \tValidation Loss: 2.715913\n",
      "Epoch: 35760 \tTraining Loss: 1.142526 \tValidation Loss: 2.715673\n",
      "Epoch: 35761 \tTraining Loss: 1.207943 \tValidation Loss: 2.713027\n",
      "Epoch: 35762 \tTraining Loss: 1.188920 \tValidation Loss: 2.714616\n",
      "Epoch: 35763 \tTraining Loss: 1.240767 \tValidation Loss: 2.713546\n",
      "Epoch: 35764 \tTraining Loss: 1.159397 \tValidation Loss: 2.715522\n",
      "Epoch: 35765 \tTraining Loss: 1.220698 \tValidation Loss: 2.714145\n",
      "Epoch: 35766 \tTraining Loss: 1.200295 \tValidation Loss: 2.714203\n",
      "Epoch: 35767 \tTraining Loss: 1.206632 \tValidation Loss: 2.714805\n",
      "Epoch: 35768 \tTraining Loss: 1.195311 \tValidation Loss: 2.713478\n",
      "Epoch: 35769 \tTraining Loss: 1.187585 \tValidation Loss: 2.716338\n",
      "Epoch: 35770 \tTraining Loss: 1.144302 \tValidation Loss: 2.717330\n",
      "Epoch: 35771 \tTraining Loss: 1.229142 \tValidation Loss: 2.715660\n",
      "Epoch: 35772 \tTraining Loss: 1.169955 \tValidation Loss: 2.714865\n",
      "Epoch: 35773 \tTraining Loss: 1.291328 \tValidation Loss: 2.714628\n",
      "Epoch: 35774 \tTraining Loss: 1.286837 \tValidation Loss: 2.712222\n",
      "Epoch: 35775 \tTraining Loss: 1.237213 \tValidation Loss: 2.714787\n",
      "Epoch: 35776 \tTraining Loss: 1.221089 \tValidation Loss: 2.712645\n",
      "Epoch: 35777 \tTraining Loss: 1.233663 \tValidation Loss: 2.713784\n",
      "Epoch: 35778 \tTraining Loss: 1.172925 \tValidation Loss: 2.714108\n",
      "Epoch: 35779 \tTraining Loss: 1.196087 \tValidation Loss: 2.714403\n",
      "Epoch: 35780 \tTraining Loss: 1.232936 \tValidation Loss: 2.715686\n",
      "Epoch: 35781 \tTraining Loss: 1.229977 \tValidation Loss: 2.715955\n",
      "Epoch: 35782 \tTraining Loss: 1.195638 \tValidation Loss: 2.714887\n",
      "Epoch: 35783 \tTraining Loss: 1.210117 \tValidation Loss: 2.714181\n",
      "Epoch: 35784 \tTraining Loss: 1.188132 \tValidation Loss: 2.715902\n",
      "Epoch: 35785 \tTraining Loss: 1.226403 \tValidation Loss: 2.713541\n",
      "Epoch: 35786 \tTraining Loss: 1.195903 \tValidation Loss: 2.715944\n",
      "Epoch: 35787 \tTraining Loss: 1.206839 \tValidation Loss: 2.714208\n",
      "Epoch: 35788 \tTraining Loss: 1.192927 \tValidation Loss: 2.713738\n",
      "Epoch: 35789 \tTraining Loss: 1.207711 \tValidation Loss: 2.715544\n",
      "Epoch: 35790 \tTraining Loss: 1.249515 \tValidation Loss: 2.715285\n",
      "Epoch: 35791 \tTraining Loss: 1.186472 \tValidation Loss: 2.716604\n",
      "Epoch: 35792 \tTraining Loss: 1.154993 \tValidation Loss: 2.717622\n",
      "Epoch: 35793 \tTraining Loss: 1.216194 \tValidation Loss: 2.714492\n",
      "Epoch: 35794 \tTraining Loss: 1.179958 \tValidation Loss: 2.715200\n",
      "Epoch: 35795 \tTraining Loss: 1.250239 \tValidation Loss: 2.716468\n",
      "Epoch: 35796 \tTraining Loss: 1.228484 \tValidation Loss: 2.715035\n",
      "Epoch: 35797 \tTraining Loss: 1.249180 \tValidation Loss: 2.714983\n",
      "Epoch: 35798 \tTraining Loss: 1.216588 \tValidation Loss: 2.713900\n",
      "Epoch: 35799 \tTraining Loss: 1.222475 \tValidation Loss: 2.716658\n",
      "Epoch: 35800 \tTraining Loss: 1.174609 \tValidation Loss: 2.716084\n",
      "Epoch: 35801 \tTraining Loss: 1.265586 \tValidation Loss: 2.714695\n",
      "Epoch: 35802 \tTraining Loss: 1.209960 \tValidation Loss: 2.715333\n",
      "Epoch: 35803 \tTraining Loss: 1.209179 \tValidation Loss: 2.713444\n",
      "Epoch: 35804 \tTraining Loss: 1.165934 \tValidation Loss: 2.714723\n",
      "Epoch: 35805 \tTraining Loss: 1.174381 \tValidation Loss: 2.715041\n",
      "Epoch: 35806 \tTraining Loss: 1.217699 \tValidation Loss: 2.714925\n",
      "Epoch: 35807 \tTraining Loss: 1.238541 \tValidation Loss: 2.716196\n",
      "Epoch: 35808 \tTraining Loss: 1.190934 \tValidation Loss: 2.716629\n",
      "Epoch: 35809 \tTraining Loss: 1.247836 \tValidation Loss: 2.713951\n",
      "Epoch: 35810 \tTraining Loss: 1.209615 \tValidation Loss: 2.714125\n",
      "Epoch: 35811 \tTraining Loss: 1.214705 \tValidation Loss: 2.715439\n",
      "Epoch: 35812 \tTraining Loss: 1.152758 \tValidation Loss: 2.716828\n",
      "Epoch: 35813 \tTraining Loss: 1.238506 \tValidation Loss: 2.716937\n",
      "Epoch: 35814 \tTraining Loss: 1.189060 \tValidation Loss: 2.714466\n",
      "Epoch: 35815 \tTraining Loss: 1.205185 \tValidation Loss: 2.716574\n",
      "Epoch: 35816 \tTraining Loss: 1.197068 \tValidation Loss: 2.716601\n",
      "Epoch: 35817 \tTraining Loss: 1.135121 \tValidation Loss: 2.716661\n",
      "Epoch: 35818 \tTraining Loss: 1.235420 \tValidation Loss: 2.715487\n",
      "Epoch: 35819 \tTraining Loss: 1.221540 \tValidation Loss: 2.716315\n",
      "Epoch: 35820 \tTraining Loss: 1.201841 \tValidation Loss: 2.716765\n",
      "Epoch: 35821 \tTraining Loss: 1.220045 \tValidation Loss: 2.714725\n",
      "Epoch: 35822 \tTraining Loss: 1.164524 \tValidation Loss: 2.715552\n",
      "Epoch: 35823 \tTraining Loss: 1.229992 \tValidation Loss: 2.716262\n",
      "Epoch: 35824 \tTraining Loss: 1.248390 \tValidation Loss: 2.715235\n",
      "Epoch: 35825 \tTraining Loss: 1.244767 \tValidation Loss: 2.716083\n",
      "Epoch: 35826 \tTraining Loss: 1.188887 \tValidation Loss: 2.715081\n",
      "Epoch: 35827 \tTraining Loss: 1.227216 \tValidation Loss: 2.716833\n",
      "Epoch: 35828 \tTraining Loss: 1.154304 \tValidation Loss: 2.716571\n",
      "Epoch: 35829 \tTraining Loss: 1.194103 \tValidation Loss: 2.717011\n",
      "Epoch: 35830 \tTraining Loss: 1.235664 \tValidation Loss: 2.716770\n",
      "Epoch: 35831 \tTraining Loss: 1.229885 \tValidation Loss: 2.716265\n",
      "Epoch: 35832 \tTraining Loss: 1.226468 \tValidation Loss: 2.714997\n",
      "Epoch: 35833 \tTraining Loss: 1.183884 \tValidation Loss: 2.716996\n",
      "Epoch: 35834 \tTraining Loss: 1.234748 \tValidation Loss: 2.716774\n",
      "Epoch: 35835 \tTraining Loss: 1.139679 \tValidation Loss: 2.717362\n",
      "Epoch: 35836 \tTraining Loss: 1.229996 \tValidation Loss: 2.717044\n",
      "Epoch: 35837 \tTraining Loss: 1.242472 \tValidation Loss: 2.716704\n",
      "Epoch: 35838 \tTraining Loss: 1.213457 \tValidation Loss: 2.716162\n",
      "Epoch: 35839 \tTraining Loss: 1.246947 \tValidation Loss: 2.714790\n",
      "Epoch: 35840 \tTraining Loss: 1.175359 \tValidation Loss: 2.715182\n",
      "Epoch: 35841 \tTraining Loss: 1.216053 \tValidation Loss: 2.714919\n",
      "Epoch: 35842 \tTraining Loss: 1.289597 \tValidation Loss: 2.715852\n",
      "Epoch: 35843 \tTraining Loss: 1.227244 \tValidation Loss: 2.718156\n",
      "Epoch: 35844 \tTraining Loss: 1.177283 \tValidation Loss: 2.716684\n",
      "Epoch: 35845 \tTraining Loss: 1.207832 \tValidation Loss: 2.716582\n",
      "Epoch: 35846 \tTraining Loss: 1.229834 \tValidation Loss: 2.716716\n",
      "Epoch: 35847 \tTraining Loss: 1.191135 \tValidation Loss: 2.717151\n",
      "Epoch: 35848 \tTraining Loss: 1.176936 \tValidation Loss: 2.718493\n",
      "Epoch: 35849 \tTraining Loss: 1.210718 \tValidation Loss: 2.716889\n",
      "Epoch: 35850 \tTraining Loss: 1.171131 \tValidation Loss: 2.717358\n",
      "Epoch: 35851 \tTraining Loss: 1.196120 \tValidation Loss: 2.716307\n",
      "Epoch: 35852 \tTraining Loss: 1.163447 \tValidation Loss: 2.717117\n",
      "Epoch: 35853 \tTraining Loss: 1.215721 \tValidation Loss: 2.716758\n",
      "Epoch: 35854 \tTraining Loss: 1.226968 \tValidation Loss: 2.715819\n",
      "Epoch: 35855 \tTraining Loss: 1.217325 \tValidation Loss: 2.716113\n",
      "Epoch: 35856 \tTraining Loss: 1.217300 \tValidation Loss: 2.716981\n",
      "Epoch: 35857 \tTraining Loss: 1.201068 \tValidation Loss: 2.716284\n",
      "Epoch: 35858 \tTraining Loss: 1.214956 \tValidation Loss: 2.718228\n",
      "Epoch: 35859 \tTraining Loss: 1.204240 \tValidation Loss: 2.715665\n",
      "Epoch: 35860 \tTraining Loss: 1.243059 \tValidation Loss: 2.716332\n",
      "Epoch: 35861 \tTraining Loss: 1.175928 \tValidation Loss: 2.717363\n",
      "Epoch: 35862 \tTraining Loss: 1.264545 \tValidation Loss: 2.715973\n",
      "Epoch: 35863 \tTraining Loss: 1.199459 \tValidation Loss: 2.716592\n",
      "Epoch: 35864 \tTraining Loss: 1.203901 \tValidation Loss: 2.717129\n",
      "Epoch: 35865 \tTraining Loss: 1.210992 \tValidation Loss: 2.717753\n",
      "Epoch: 35866 \tTraining Loss: 1.193292 \tValidation Loss: 2.715025\n",
      "Epoch: 35867 \tTraining Loss: 1.191824 \tValidation Loss: 2.716719\n",
      "Epoch: 35868 \tTraining Loss: 1.219912 \tValidation Loss: 2.715529\n",
      "Epoch: 35869 \tTraining Loss: 1.194239 \tValidation Loss: 2.717138\n",
      "Epoch: 35870 \tTraining Loss: 1.249560 \tValidation Loss: 2.715392\n",
      "Epoch: 35871 \tTraining Loss: 1.225939 \tValidation Loss: 2.715520\n",
      "Epoch: 35872 \tTraining Loss: 1.225268 \tValidation Loss: 2.715831\n",
      "Epoch: 35873 \tTraining Loss: 1.225188 \tValidation Loss: 2.718729\n",
      "Epoch: 35874 \tTraining Loss: 1.212871 \tValidation Loss: 2.717094\n",
      "Epoch: 35875 \tTraining Loss: 1.205320 \tValidation Loss: 2.717452\n",
      "Epoch: 35876 \tTraining Loss: 1.196116 \tValidation Loss: 2.719681\n",
      "Epoch: 35877 \tTraining Loss: 1.221447 \tValidation Loss: 2.714561\n",
      "Epoch: 35878 \tTraining Loss: 1.235308 \tValidation Loss: 2.715896\n",
      "Epoch: 35879 \tTraining Loss: 1.220442 \tValidation Loss: 2.715733\n",
      "Epoch: 35880 \tTraining Loss: 1.231661 \tValidation Loss: 2.716536\n",
      "Epoch: 35881 \tTraining Loss: 1.191767 \tValidation Loss: 2.718636\n",
      "Epoch: 35882 \tTraining Loss: 1.188494 \tValidation Loss: 2.717187\n",
      "Epoch: 35883 \tTraining Loss: 1.191837 \tValidation Loss: 2.716805\n",
      "Epoch: 35884 \tTraining Loss: 1.236171 \tValidation Loss: 2.717354\n",
      "Epoch: 35885 \tTraining Loss: 1.170068 \tValidation Loss: 2.718880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35886 \tTraining Loss: 1.215154 \tValidation Loss: 2.718746\n",
      "Epoch: 35887 \tTraining Loss: 1.194783 \tValidation Loss: 2.719290\n",
      "Epoch: 35888 \tTraining Loss: 1.237099 \tValidation Loss: 2.719322\n",
      "Epoch: 35889 \tTraining Loss: 1.176874 \tValidation Loss: 2.717397\n",
      "Epoch: 35890 \tTraining Loss: 1.224158 \tValidation Loss: 2.717004\n",
      "Epoch: 35891 \tTraining Loss: 1.188764 \tValidation Loss: 2.717917\n",
      "Epoch: 35892 \tTraining Loss: 1.191713 \tValidation Loss: 2.718607\n",
      "Epoch: 35893 \tTraining Loss: 1.254524 \tValidation Loss: 2.715756\n",
      "Epoch: 35894 \tTraining Loss: 1.251218 \tValidation Loss: 2.717034\n",
      "Epoch: 35895 \tTraining Loss: 1.250245 \tValidation Loss: 2.716459\n",
      "Epoch: 35896 \tTraining Loss: 1.213643 \tValidation Loss: 2.716585\n",
      "Epoch: 35897 \tTraining Loss: 1.205508 \tValidation Loss: 2.718362\n",
      "Epoch: 35898 \tTraining Loss: 1.212562 \tValidation Loss: 2.716914\n",
      "Epoch: 35899 \tTraining Loss: 1.272083 \tValidation Loss: 2.717401\n",
      "Epoch: 35900 \tTraining Loss: 1.169959 \tValidation Loss: 2.720108\n",
      "Epoch: 35901 \tTraining Loss: 1.186472 \tValidation Loss: 2.716614\n",
      "Epoch: 35902 \tTraining Loss: 1.199447 \tValidation Loss: 2.716873\n",
      "Epoch: 35903 \tTraining Loss: 1.199276 \tValidation Loss: 2.717182\n",
      "Epoch: 35904 \tTraining Loss: 1.189856 \tValidation Loss: 2.718532\n",
      "Epoch: 35905 \tTraining Loss: 1.226469 \tValidation Loss: 2.717635\n",
      "Epoch: 35906 \tTraining Loss: 1.209513 \tValidation Loss: 2.717014\n",
      "Epoch: 35907 \tTraining Loss: 1.210048 \tValidation Loss: 2.717288\n",
      "Epoch: 35908 \tTraining Loss: 1.205120 \tValidation Loss: 2.717356\n",
      "Epoch: 35909 \tTraining Loss: 1.221088 \tValidation Loss: 2.716978\n",
      "Epoch: 35910 \tTraining Loss: 1.202420 \tValidation Loss: 2.715652\n",
      "Epoch: 35911 \tTraining Loss: 1.206674 \tValidation Loss: 2.717801\n",
      "Epoch: 35912 \tTraining Loss: 1.169670 \tValidation Loss: 2.719429\n",
      "Epoch: 35913 \tTraining Loss: 1.209271 \tValidation Loss: 2.718539\n",
      "Epoch: 35914 \tTraining Loss: 1.226685 \tValidation Loss: 2.717656\n",
      "Epoch: 35915 \tTraining Loss: 1.213440 \tValidation Loss: 2.719066\n",
      "Epoch: 35916 \tTraining Loss: 1.183339 \tValidation Loss: 2.717485\n",
      "Epoch: 35917 \tTraining Loss: 1.200497 \tValidation Loss: 2.718966\n",
      "Epoch: 35918 \tTraining Loss: 1.219759 \tValidation Loss: 2.717830\n",
      "Epoch: 35919 \tTraining Loss: 1.212437 \tValidation Loss: 2.717172\n",
      "Epoch: 35920 \tTraining Loss: 1.200481 \tValidation Loss: 2.717782\n",
      "Epoch: 35921 \tTraining Loss: 1.218508 \tValidation Loss: 2.718066\n",
      "Epoch: 35922 \tTraining Loss: 1.155089 \tValidation Loss: 2.718987\n",
      "Epoch: 35923 \tTraining Loss: 1.211781 \tValidation Loss: 2.718682\n",
      "Epoch: 35924 \tTraining Loss: 1.214686 \tValidation Loss: 2.719155\n",
      "Epoch: 35925 \tTraining Loss: 1.207050 \tValidation Loss: 2.717600\n",
      "Epoch: 35926 \tTraining Loss: 1.204631 \tValidation Loss: 2.719085\n",
      "Epoch: 35927 \tTraining Loss: 1.208940 \tValidation Loss: 2.718627\n",
      "Epoch: 35928 \tTraining Loss: 1.176085 \tValidation Loss: 2.718395\n",
      "Epoch: 35929 \tTraining Loss: 1.229257 \tValidation Loss: 2.717465\n",
      "Epoch: 35930 \tTraining Loss: 1.208567 \tValidation Loss: 2.718333\n",
      "Epoch: 35931 \tTraining Loss: 1.192624 \tValidation Loss: 2.719872\n",
      "Epoch: 35932 \tTraining Loss: 1.168935 \tValidation Loss: 2.718375\n",
      "Epoch: 35933 \tTraining Loss: 1.242517 \tValidation Loss: 2.716312\n",
      "Epoch: 35934 \tTraining Loss: 1.195376 \tValidation Loss: 2.718903\n",
      "Epoch: 35935 \tTraining Loss: 1.221501 \tValidation Loss: 2.718835\n",
      "Epoch: 35936 \tTraining Loss: 1.215248 \tValidation Loss: 2.719689\n",
      "Epoch: 35937 \tTraining Loss: 1.228807 \tValidation Loss: 2.718038\n",
      "Epoch: 35938 \tTraining Loss: 1.159615 \tValidation Loss: 2.718644\n",
      "Epoch: 35939 \tTraining Loss: 1.201381 \tValidation Loss: 2.719175\n",
      "Epoch: 35940 \tTraining Loss: 1.193249 \tValidation Loss: 2.717520\n",
      "Epoch: 35941 \tTraining Loss: 1.194083 \tValidation Loss: 2.719722\n",
      "Epoch: 35942 \tTraining Loss: 1.207470 \tValidation Loss: 2.717697\n",
      "Epoch: 35943 \tTraining Loss: 1.191498 \tValidation Loss: 2.718659\n",
      "Epoch: 35944 \tTraining Loss: 1.213293 \tValidation Loss: 2.719946\n",
      "Epoch: 35945 \tTraining Loss: 1.169713 \tValidation Loss: 2.718479\n",
      "Epoch: 35946 \tTraining Loss: 1.209590 \tValidation Loss: 2.719889\n",
      "Epoch: 35947 \tTraining Loss: 1.233421 \tValidation Loss: 2.718969\n",
      "Epoch: 35948 \tTraining Loss: 1.194841 \tValidation Loss: 2.719106\n",
      "Epoch: 35949 \tTraining Loss: 1.232335 \tValidation Loss: 2.718501\n",
      "Epoch: 35950 \tTraining Loss: 1.208864 \tValidation Loss: 2.719222\n",
      "Epoch: 35951 \tTraining Loss: 1.201239 \tValidation Loss: 2.719654\n",
      "Epoch: 35952 \tTraining Loss: 1.226373 \tValidation Loss: 2.718997\n",
      "Epoch: 35953 \tTraining Loss: 1.231711 \tValidation Loss: 2.717947\n",
      "Epoch: 35954 \tTraining Loss: 1.206736 \tValidation Loss: 2.717817\n",
      "Epoch: 35955 \tTraining Loss: 1.198727 \tValidation Loss: 2.718716\n",
      "Epoch: 35956 \tTraining Loss: 1.186224 \tValidation Loss: 2.719123\n",
      "Epoch: 35957 \tTraining Loss: 1.242049 \tValidation Loss: 2.719897\n",
      "Epoch: 35958 \tTraining Loss: 1.252387 \tValidation Loss: 2.719318\n",
      "Epoch: 35959 \tTraining Loss: 1.193795 \tValidation Loss: 2.719813\n",
      "Epoch: 35960 \tTraining Loss: 1.241603 \tValidation Loss: 2.719788\n",
      "Epoch: 35961 \tTraining Loss: 1.185902 \tValidation Loss: 2.719916\n",
      "Epoch: 35962 \tTraining Loss: 1.206926 \tValidation Loss: 2.719078\n",
      "Epoch: 35963 \tTraining Loss: 1.176787 \tValidation Loss: 2.719233\n",
      "Epoch: 35964 \tTraining Loss: 1.234401 \tValidation Loss: 2.719426\n",
      "Epoch: 35965 \tTraining Loss: 1.245354 \tValidation Loss: 2.719161\n",
      "Epoch: 35966 \tTraining Loss: 1.203087 \tValidation Loss: 2.720268\n",
      "Epoch: 35967 \tTraining Loss: 1.173371 \tValidation Loss: 2.719423\n",
      "Epoch: 35968 \tTraining Loss: 1.216474 \tValidation Loss: 2.718670\n",
      "Epoch: 35969 \tTraining Loss: 1.177878 \tValidation Loss: 2.720598\n",
      "Epoch: 35970 \tTraining Loss: 1.148657 \tValidation Loss: 2.720900\n",
      "Epoch: 35971 \tTraining Loss: 1.165461 \tValidation Loss: 2.719811\n",
      "Epoch: 35972 \tTraining Loss: 1.242529 \tValidation Loss: 2.719117\n",
      "Epoch: 35973 \tTraining Loss: 1.234335 \tValidation Loss: 2.719021\n",
      "Epoch: 35974 \tTraining Loss: 1.167604 \tValidation Loss: 2.721404\n",
      "Epoch: 35975 \tTraining Loss: 1.197240 \tValidation Loss: 2.718334\n",
      "Epoch: 35976 \tTraining Loss: 1.215997 \tValidation Loss: 2.718807\n",
      "Epoch: 35977 \tTraining Loss: 1.189053 \tValidation Loss: 2.720805\n",
      "Epoch: 35978 \tTraining Loss: 1.245110 \tValidation Loss: 2.719226\n",
      "Epoch: 35979 \tTraining Loss: 1.155051 \tValidation Loss: 2.720996\n",
      "Epoch: 35980 \tTraining Loss: 1.199204 \tValidation Loss: 2.719609\n",
      "Epoch: 35981 \tTraining Loss: 1.242890 \tValidation Loss: 2.718578\n",
      "Epoch: 35982 \tTraining Loss: 1.225977 \tValidation Loss: 2.719019\n",
      "Epoch: 35983 \tTraining Loss: 1.295031 \tValidation Loss: 2.718631\n",
      "Epoch: 35984 \tTraining Loss: 1.237779 \tValidation Loss: 2.719204\n",
      "Epoch: 35985 \tTraining Loss: 1.201268 \tValidation Loss: 2.719918\n",
      "Epoch: 35986 \tTraining Loss: 1.193456 \tValidation Loss: 2.719172\n",
      "Epoch: 35987 \tTraining Loss: 1.238893 \tValidation Loss: 2.720124\n",
      "Epoch: 35988 \tTraining Loss: 1.244459 \tValidation Loss: 2.718533\n",
      "Epoch: 35989 \tTraining Loss: 1.243171 \tValidation Loss: 2.719489\n",
      "Epoch: 35990 \tTraining Loss: 1.198351 \tValidation Loss: 2.720834\n",
      "Epoch: 35991 \tTraining Loss: 1.194776 \tValidation Loss: 2.720324\n",
      "Epoch: 35992 \tTraining Loss: 1.198578 \tValidation Loss: 2.719944\n",
      "Epoch: 35993 \tTraining Loss: 1.237557 \tValidation Loss: 2.718844\n",
      "Epoch: 35994 \tTraining Loss: 1.191008 \tValidation Loss: 2.721808\n",
      "Epoch: 35995 \tTraining Loss: 1.199602 \tValidation Loss: 2.718356\n",
      "Epoch: 35996 \tTraining Loss: 1.198353 \tValidation Loss: 2.720006\n",
      "Epoch: 35997 \tTraining Loss: 1.185600 \tValidation Loss: 2.720494\n",
      "Epoch: 35998 \tTraining Loss: 1.199852 \tValidation Loss: 2.717964\n",
      "Epoch: 35999 \tTraining Loss: 1.263836 \tValidation Loss: 2.717866\n",
      "Epoch: 36000 \tTraining Loss: 1.224132 \tValidation Loss: 2.719837\n",
      "Epoch: 36001 \tTraining Loss: 1.261866 \tValidation Loss: 2.721352\n",
      "Epoch: 36002 \tTraining Loss: 1.200792 \tValidation Loss: 2.718476\n",
      "Epoch: 36003 \tTraining Loss: 1.223577 \tValidation Loss: 2.718488\n",
      "Epoch: 36004 \tTraining Loss: 1.240913 \tValidation Loss: 2.718801\n",
      "Epoch: 36005 \tTraining Loss: 1.214933 \tValidation Loss: 2.719568\n",
      "Epoch: 36006 \tTraining Loss: 1.183547 \tValidation Loss: 2.719671\n",
      "Epoch: 36007 \tTraining Loss: 1.218276 \tValidation Loss: 2.719505\n",
      "Epoch: 36008 \tTraining Loss: 1.222581 \tValidation Loss: 2.720075\n",
      "Epoch: 36009 \tTraining Loss: 1.184527 \tValidation Loss: 2.721998\n",
      "Epoch: 36010 \tTraining Loss: 1.234617 \tValidation Loss: 2.720324\n",
      "Epoch: 36011 \tTraining Loss: 1.182825 \tValidation Loss: 2.720141\n",
      "Epoch: 36012 \tTraining Loss: 1.198451 \tValidation Loss: 2.720024\n",
      "Epoch: 36013 \tTraining Loss: 1.202169 \tValidation Loss: 2.719796\n",
      "Epoch: 36014 \tTraining Loss: 1.187284 \tValidation Loss: 2.721093\n",
      "Epoch: 36015 \tTraining Loss: 1.208919 \tValidation Loss: 2.720737\n",
      "Epoch: 36016 \tTraining Loss: 1.176774 \tValidation Loss: 2.720672\n",
      "Epoch: 36017 \tTraining Loss: 1.190753 \tValidation Loss: 2.720515\n",
      "Epoch: 36018 \tTraining Loss: 1.190280 \tValidation Loss: 2.720073\n",
      "Epoch: 36019 \tTraining Loss: 1.147207 \tValidation Loss: 2.722280\n",
      "Epoch: 36020 \tTraining Loss: 1.162610 \tValidation Loss: 2.721313\n",
      "Epoch: 36021 \tTraining Loss: 1.182265 \tValidation Loss: 2.719272\n",
      "Epoch: 36022 \tTraining Loss: 1.223276 \tValidation Loss: 2.719483\n",
      "Epoch: 36023 \tTraining Loss: 1.195577 \tValidation Loss: 2.720591\n",
      "Epoch: 36024 \tTraining Loss: 1.219090 \tValidation Loss: 2.719276\n",
      "Epoch: 36025 \tTraining Loss: 1.192237 \tValidation Loss: 2.720963\n",
      "Epoch: 36026 \tTraining Loss: 1.216830 \tValidation Loss: 2.719311\n",
      "Epoch: 36027 \tTraining Loss: 1.195503 \tValidation Loss: 2.719665\n",
      "Epoch: 36028 \tTraining Loss: 1.185438 \tValidation Loss: 2.721227\n",
      "Epoch: 36029 \tTraining Loss: 1.147535 \tValidation Loss: 2.722759\n",
      "Epoch: 36030 \tTraining Loss: 1.208498 \tValidation Loss: 2.720556\n",
      "Epoch: 36031 \tTraining Loss: 1.188151 \tValidation Loss: 2.721143\n",
      "Epoch: 36032 \tTraining Loss: 1.225753 \tValidation Loss: 2.721881\n",
      "Epoch: 36033 \tTraining Loss: 1.181936 \tValidation Loss: 2.721666\n",
      "Epoch: 36034 \tTraining Loss: 1.173091 \tValidation Loss: 2.721481\n",
      "Epoch: 36035 \tTraining Loss: 1.195858 \tValidation Loss: 2.720637\n",
      "Epoch: 36036 \tTraining Loss: 1.226264 \tValidation Loss: 2.719383\n",
      "Epoch: 36037 \tTraining Loss: 1.254802 \tValidation Loss: 2.721353\n",
      "Epoch: 36038 \tTraining Loss: 1.197580 \tValidation Loss: 2.719200\n",
      "Epoch: 36039 \tTraining Loss: 1.185888 \tValidation Loss: 2.721238\n",
      "Epoch: 36040 \tTraining Loss: 1.241150 \tValidation Loss: 2.720176\n",
      "Epoch: 36041 \tTraining Loss: 1.193219 \tValidation Loss: 2.721467\n",
      "Epoch: 36042 \tTraining Loss: 1.174184 \tValidation Loss: 2.720899\n",
      "Epoch: 36043 \tTraining Loss: 1.263933 \tValidation Loss: 2.720812\n",
      "Epoch: 36044 \tTraining Loss: 1.235288 \tValidation Loss: 2.721123\n",
      "Epoch: 36045 \tTraining Loss: 1.223324 \tValidation Loss: 2.721408\n",
      "Epoch: 36046 \tTraining Loss: 1.231402 \tValidation Loss: 2.721091\n",
      "Epoch: 36047 \tTraining Loss: 1.144651 \tValidation Loss: 2.722066\n",
      "Epoch: 36048 \tTraining Loss: 1.165494 \tValidation Loss: 2.720585\n",
      "Epoch: 36049 \tTraining Loss: 1.178983 \tValidation Loss: 2.721092\n",
      "Epoch: 36050 \tTraining Loss: 1.182460 \tValidation Loss: 2.721549\n",
      "Epoch: 36051 \tTraining Loss: 1.263377 \tValidation Loss: 2.721791\n",
      "Epoch: 36052 \tTraining Loss: 1.211991 \tValidation Loss: 2.718874\n",
      "Epoch: 36053 \tTraining Loss: 1.197871 \tValidation Loss: 2.721582\n",
      "Epoch: 36054 \tTraining Loss: 1.201491 \tValidation Loss: 2.720394\n",
      "Epoch: 36055 \tTraining Loss: 1.215407 \tValidation Loss: 2.721473\n",
      "Epoch: 36056 \tTraining Loss: 1.192108 \tValidation Loss: 2.720948\n",
      "Epoch: 36057 \tTraining Loss: 1.222869 \tValidation Loss: 2.722805\n",
      "Epoch: 36058 \tTraining Loss: 1.145194 \tValidation Loss: 2.721922\n",
      "Epoch: 36059 \tTraining Loss: 1.193012 \tValidation Loss: 2.722080\n",
      "Epoch: 36060 \tTraining Loss: 1.198400 \tValidation Loss: 2.723030\n",
      "Epoch: 36061 \tTraining Loss: 1.215143 \tValidation Loss: 2.722492\n",
      "Epoch: 36062 \tTraining Loss: 1.194222 \tValidation Loss: 2.721043\n",
      "Epoch: 36063 \tTraining Loss: 1.231867 \tValidation Loss: 2.721373\n",
      "Epoch: 36064 \tTraining Loss: 1.214367 \tValidation Loss: 2.721189\n",
      "Epoch: 36065 \tTraining Loss: 1.185995 \tValidation Loss: 2.721044\n",
      "Epoch: 36066 \tTraining Loss: 1.235847 \tValidation Loss: 2.723158\n",
      "Epoch: 36067 \tTraining Loss: 1.187197 \tValidation Loss: 2.723651\n",
      "Epoch: 36068 \tTraining Loss: 1.208895 \tValidation Loss: 2.722348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36069 \tTraining Loss: 1.207852 \tValidation Loss: 2.720821\n",
      "Epoch: 36070 \tTraining Loss: 1.192664 \tValidation Loss: 2.722034\n",
      "Epoch: 36071 \tTraining Loss: 1.209946 \tValidation Loss: 2.722382\n",
      "Epoch: 36072 \tTraining Loss: 1.225810 \tValidation Loss: 2.722382\n",
      "Epoch: 36073 \tTraining Loss: 1.209986 \tValidation Loss: 2.720561\n",
      "Epoch: 36074 \tTraining Loss: 1.227760 \tValidation Loss: 2.722244\n",
      "Epoch: 36075 \tTraining Loss: 1.200490 \tValidation Loss: 2.723946\n",
      "Epoch: 36076 \tTraining Loss: 1.202110 \tValidation Loss: 2.721275\n",
      "Epoch: 36077 \tTraining Loss: 1.195415 \tValidation Loss: 2.723707\n",
      "Epoch: 36078 \tTraining Loss: 1.283857 \tValidation Loss: 2.720692\n",
      "Epoch: 36079 \tTraining Loss: 1.208616 \tValidation Loss: 2.720392\n",
      "Epoch: 36080 \tTraining Loss: 1.260333 \tValidation Loss: 2.720945\n",
      "Epoch: 36081 \tTraining Loss: 1.202360 \tValidation Loss: 2.720844\n",
      "Epoch: 36082 \tTraining Loss: 1.211257 \tValidation Loss: 2.721681\n",
      "Epoch: 36083 \tTraining Loss: 1.200285 \tValidation Loss: 2.722298\n",
      "Epoch: 36084 \tTraining Loss: 1.202582 \tValidation Loss: 2.720037\n",
      "Epoch: 36085 \tTraining Loss: 1.250440 \tValidation Loss: 2.721297\n",
      "Epoch: 36086 \tTraining Loss: 1.206207 \tValidation Loss: 2.722108\n",
      "Epoch: 36087 \tTraining Loss: 1.261725 \tValidation Loss: 2.721587\n",
      "Epoch: 36088 \tTraining Loss: 1.200837 \tValidation Loss: 2.721339\n",
      "Epoch: 36089 \tTraining Loss: 1.190265 \tValidation Loss: 2.722366\n",
      "Epoch: 36090 \tTraining Loss: 1.219995 \tValidation Loss: 2.721598\n",
      "Epoch: 36091 \tTraining Loss: 1.236012 \tValidation Loss: 2.721127\n",
      "Epoch: 36092 \tTraining Loss: 1.232668 \tValidation Loss: 2.723000\n",
      "Epoch: 36093 \tTraining Loss: 1.200139 \tValidation Loss: 2.722660\n",
      "Epoch: 36094 \tTraining Loss: 1.219357 \tValidation Loss: 2.722006\n",
      "Epoch: 36095 \tTraining Loss: 1.253201 \tValidation Loss: 2.720506\n",
      "Epoch: 36096 \tTraining Loss: 1.158455 \tValidation Loss: 2.722860\n",
      "Epoch: 36097 \tTraining Loss: 1.189677 \tValidation Loss: 2.721671\n",
      "Epoch: 36098 \tTraining Loss: 1.141390 \tValidation Loss: 2.722489\n",
      "Epoch: 36099 \tTraining Loss: 1.177588 \tValidation Loss: 2.720366\n",
      "Epoch: 36100 \tTraining Loss: 1.200373 \tValidation Loss: 2.722644\n",
      "Epoch: 36101 \tTraining Loss: 1.191666 \tValidation Loss: 2.720992\n",
      "Epoch: 36102 \tTraining Loss: 1.228137 \tValidation Loss: 2.723131\n",
      "Epoch: 36103 \tTraining Loss: 1.206002 \tValidation Loss: 2.721655\n",
      "Epoch: 36104 \tTraining Loss: 1.168472 \tValidation Loss: 2.723765\n",
      "Epoch: 36105 \tTraining Loss: 1.169487 \tValidation Loss: 2.722111\n",
      "Epoch: 36106 \tTraining Loss: 1.223815 \tValidation Loss: 2.721508\n",
      "Epoch: 36107 \tTraining Loss: 1.214333 \tValidation Loss: 2.723075\n",
      "Epoch: 36108 \tTraining Loss: 1.185371 \tValidation Loss: 2.722474\n",
      "Epoch: 36109 \tTraining Loss: 1.178862 \tValidation Loss: 2.724197\n",
      "Epoch: 36110 \tTraining Loss: 1.198225 \tValidation Loss: 2.723656\n",
      "Epoch: 36111 \tTraining Loss: 1.219608 \tValidation Loss: 2.723649\n",
      "Epoch: 36112 \tTraining Loss: 1.191174 \tValidation Loss: 2.721236\n",
      "Epoch: 36113 \tTraining Loss: 1.241740 \tValidation Loss: 2.723908\n",
      "Epoch: 36114 \tTraining Loss: 1.213126 \tValidation Loss: 2.722018\n",
      "Epoch: 36115 \tTraining Loss: 1.211073 \tValidation Loss: 2.721565\n",
      "Epoch: 36116 \tTraining Loss: 1.201458 \tValidation Loss: 2.722715\n",
      "Epoch: 36117 \tTraining Loss: 1.192173 \tValidation Loss: 2.723063\n",
      "Epoch: 36118 \tTraining Loss: 1.213421 \tValidation Loss: 2.722616\n",
      "Epoch: 36119 \tTraining Loss: 1.196352 \tValidation Loss: 2.722016\n",
      "Epoch: 36120 \tTraining Loss: 1.168133 \tValidation Loss: 2.722612\n",
      "Epoch: 36121 \tTraining Loss: 1.176897 \tValidation Loss: 2.721415\n",
      "Epoch: 36122 \tTraining Loss: 1.196117 \tValidation Loss: 2.724275\n",
      "Epoch: 36123 \tTraining Loss: 1.225882 \tValidation Loss: 2.725049\n",
      "Epoch: 36124 \tTraining Loss: 1.198396 \tValidation Loss: 2.724096\n",
      "Epoch: 36125 \tTraining Loss: 1.190205 \tValidation Loss: 2.724217\n",
      "Epoch: 36126 \tTraining Loss: 1.215834 \tValidation Loss: 2.723948\n",
      "Epoch: 36127 \tTraining Loss: 1.206207 \tValidation Loss: 2.723918\n",
      "Epoch: 36128 \tTraining Loss: 1.211366 \tValidation Loss: 2.723882\n",
      "Epoch: 36129 \tTraining Loss: 1.217521 \tValidation Loss: 2.724028\n",
      "Epoch: 36130 \tTraining Loss: 1.214932 \tValidation Loss: 2.721575\n",
      "Epoch: 36131 \tTraining Loss: 1.196810 \tValidation Loss: 2.721242\n",
      "Epoch: 36132 \tTraining Loss: 1.164611 \tValidation Loss: 2.722591\n",
      "Epoch: 36133 \tTraining Loss: 1.205858 \tValidation Loss: 2.724126\n",
      "Epoch: 36134 \tTraining Loss: 1.220617 \tValidation Loss: 2.722379\n",
      "Epoch: 36135 \tTraining Loss: 1.168588 \tValidation Loss: 2.725203\n",
      "Epoch: 36136 \tTraining Loss: 1.196492 \tValidation Loss: 2.723378\n",
      "Epoch: 36137 \tTraining Loss: 1.202155 \tValidation Loss: 2.723263\n",
      "Epoch: 36138 \tTraining Loss: 1.254732 \tValidation Loss: 2.720882\n",
      "Epoch: 36139 \tTraining Loss: 1.226678 \tValidation Loss: 2.722157\n",
      "Epoch: 36140 \tTraining Loss: 1.182992 \tValidation Loss: 2.722952\n",
      "Epoch: 36141 \tTraining Loss: 1.207812 \tValidation Loss: 2.723485\n",
      "Epoch: 36142 \tTraining Loss: 1.189526 \tValidation Loss: 2.723042\n",
      "Epoch: 36143 \tTraining Loss: 1.229117 \tValidation Loss: 2.722316\n",
      "Epoch: 36144 \tTraining Loss: 1.210305 \tValidation Loss: 2.723030\n",
      "Epoch: 36145 \tTraining Loss: 1.228187 \tValidation Loss: 2.723149\n",
      "Epoch: 36146 \tTraining Loss: 1.172254 \tValidation Loss: 2.724064\n",
      "Epoch: 36147 \tTraining Loss: 1.201413 \tValidation Loss: 2.724496\n",
      "Epoch: 36148 \tTraining Loss: 1.202839 \tValidation Loss: 2.724032\n",
      "Epoch: 36149 \tTraining Loss: 1.236532 \tValidation Loss: 2.723487\n",
      "Epoch: 36150 \tTraining Loss: 1.216291 \tValidation Loss: 2.723702\n",
      "Epoch: 36151 \tTraining Loss: 1.203346 \tValidation Loss: 2.724149\n",
      "Epoch: 36152 \tTraining Loss: 1.179372 \tValidation Loss: 2.723405\n",
      "Epoch: 36153 \tTraining Loss: 1.205817 \tValidation Loss: 2.723572\n",
      "Epoch: 36154 \tTraining Loss: 1.193052 \tValidation Loss: 2.723591\n",
      "Epoch: 36155 \tTraining Loss: 1.218822 \tValidation Loss: 2.723824\n",
      "Epoch: 36156 \tTraining Loss: 1.210232 \tValidation Loss: 2.724442\n",
      "Epoch: 36157 \tTraining Loss: 1.244291 \tValidation Loss: 2.723632\n",
      "Epoch: 36158 \tTraining Loss: 1.242806 \tValidation Loss: 2.723706\n",
      "Epoch: 36159 \tTraining Loss: 1.229245 \tValidation Loss: 2.724205\n",
      "Epoch: 36160 \tTraining Loss: 1.186991 \tValidation Loss: 2.724098\n",
      "Epoch: 36161 \tTraining Loss: 1.183515 \tValidation Loss: 2.724224\n",
      "Epoch: 36162 \tTraining Loss: 1.210923 \tValidation Loss: 2.722910\n",
      "Epoch: 36163 \tTraining Loss: 1.232468 \tValidation Loss: 2.724490\n",
      "Epoch: 36164 \tTraining Loss: 1.194126 \tValidation Loss: 2.724172\n",
      "Epoch: 36165 \tTraining Loss: 1.228624 \tValidation Loss: 2.721698\n",
      "Epoch: 36166 \tTraining Loss: 1.220732 \tValidation Loss: 2.724696\n",
      "Epoch: 36167 \tTraining Loss: 1.199014 \tValidation Loss: 2.725392\n",
      "Epoch: 36168 \tTraining Loss: 1.204077 \tValidation Loss: 2.723240\n",
      "Epoch: 36169 \tTraining Loss: 1.203327 \tValidation Loss: 2.723613\n",
      "Epoch: 36170 \tTraining Loss: 1.175796 \tValidation Loss: 2.724089\n",
      "Epoch: 36171 \tTraining Loss: 1.238964 \tValidation Loss: 2.722599\n",
      "Epoch: 36172 \tTraining Loss: 1.203653 \tValidation Loss: 2.722554\n",
      "Epoch: 36173 \tTraining Loss: 1.230026 \tValidation Loss: 2.723190\n",
      "Epoch: 36174 \tTraining Loss: 1.232126 \tValidation Loss: 2.724263\n",
      "Epoch: 36175 \tTraining Loss: 1.218921 \tValidation Loss: 2.723776\n",
      "Epoch: 36176 \tTraining Loss: 1.204632 \tValidation Loss: 2.723946\n",
      "Epoch: 36177 \tTraining Loss: 1.180110 \tValidation Loss: 2.725248\n",
      "Epoch: 36178 \tTraining Loss: 1.155293 \tValidation Loss: 2.724751\n",
      "Epoch: 36179 \tTraining Loss: 1.226556 \tValidation Loss: 2.724228\n",
      "Epoch: 36180 \tTraining Loss: 1.177054 \tValidation Loss: 2.724909\n",
      "Epoch: 36181 \tTraining Loss: 1.223413 \tValidation Loss: 2.725499\n",
      "Epoch: 36182 \tTraining Loss: 1.197341 \tValidation Loss: 2.725075\n",
      "Epoch: 36183 \tTraining Loss: 1.183937 \tValidation Loss: 2.725663\n",
      "Epoch: 36184 \tTraining Loss: 1.196636 \tValidation Loss: 2.725179\n",
      "Epoch: 36185 \tTraining Loss: 1.176453 \tValidation Loss: 2.724020\n",
      "Epoch: 36186 \tTraining Loss: 1.202260 \tValidation Loss: 2.724941\n",
      "Epoch: 36187 \tTraining Loss: 1.206545 \tValidation Loss: 2.722783\n",
      "Epoch: 36188 \tTraining Loss: 1.242062 \tValidation Loss: 2.724118\n",
      "Epoch: 36189 \tTraining Loss: 1.220266 \tValidation Loss: 2.723448\n",
      "Epoch: 36190 \tTraining Loss: 1.196130 \tValidation Loss: 2.722449\n",
      "Epoch: 36191 \tTraining Loss: 1.213265 \tValidation Loss: 2.724985\n",
      "Epoch: 36192 \tTraining Loss: 1.247600 \tValidation Loss: 2.723281\n",
      "Epoch: 36193 \tTraining Loss: 1.208145 \tValidation Loss: 2.723018\n",
      "Epoch: 36194 \tTraining Loss: 1.234816 \tValidation Loss: 2.723381\n",
      "Epoch: 36195 \tTraining Loss: 1.177241 \tValidation Loss: 2.725807\n",
      "Epoch: 36196 \tTraining Loss: 1.198466 \tValidation Loss: 2.724969\n",
      "Epoch: 36197 \tTraining Loss: 1.216692 \tValidation Loss: 2.722833\n",
      "Epoch: 36198 \tTraining Loss: 1.241857 \tValidation Loss: 2.723562\n",
      "Epoch: 36199 \tTraining Loss: 1.176026 \tValidation Loss: 2.725180\n",
      "Epoch: 36200 \tTraining Loss: 1.158733 \tValidation Loss: 2.725330\n",
      "Epoch: 36201 \tTraining Loss: 1.244246 \tValidation Loss: 2.723288\n",
      "Epoch: 36202 \tTraining Loss: 1.205337 \tValidation Loss: 2.723636\n",
      "Epoch: 36203 \tTraining Loss: 1.252656 \tValidation Loss: 2.723794\n",
      "Epoch: 36204 \tTraining Loss: 1.205708 \tValidation Loss: 2.724450\n",
      "Epoch: 36205 \tTraining Loss: 1.157968 \tValidation Loss: 2.724517\n",
      "Epoch: 36206 \tTraining Loss: 1.206321 \tValidation Loss: 2.725312\n",
      "Epoch: 36207 \tTraining Loss: 1.215980 \tValidation Loss: 2.724413\n",
      "Epoch: 36208 \tTraining Loss: 1.194530 \tValidation Loss: 2.725412\n",
      "Epoch: 36209 \tTraining Loss: 1.201962 \tValidation Loss: 2.725759\n",
      "Epoch: 36210 \tTraining Loss: 1.192151 \tValidation Loss: 2.725299\n",
      "Epoch: 36211 \tTraining Loss: 1.190115 \tValidation Loss: 2.725277\n",
      "Epoch: 36212 \tTraining Loss: 1.179272 \tValidation Loss: 2.724833\n",
      "Epoch: 36213 \tTraining Loss: 1.196783 \tValidation Loss: 2.723471\n",
      "Epoch: 36214 \tTraining Loss: 1.214503 \tValidation Loss: 2.725369\n",
      "Epoch: 36215 \tTraining Loss: 1.167597 \tValidation Loss: 2.725840\n",
      "Epoch: 36216 \tTraining Loss: 1.166429 \tValidation Loss: 2.725288\n",
      "Epoch: 36217 \tTraining Loss: 1.168659 \tValidation Loss: 2.725291\n",
      "Epoch: 36218 \tTraining Loss: 1.191834 \tValidation Loss: 2.723851\n",
      "Epoch: 36219 \tTraining Loss: 1.176682 \tValidation Loss: 2.724724\n",
      "Epoch: 36220 \tTraining Loss: 1.186773 \tValidation Loss: 2.724524\n",
      "Epoch: 36221 \tTraining Loss: 1.245804 \tValidation Loss: 2.724013\n",
      "Epoch: 36222 \tTraining Loss: 1.234287 \tValidation Loss: 2.724939\n",
      "Epoch: 36223 \tTraining Loss: 1.260583 \tValidation Loss: 2.724025\n",
      "Epoch: 36224 \tTraining Loss: 1.149169 \tValidation Loss: 2.725636\n",
      "Epoch: 36225 \tTraining Loss: 1.228403 \tValidation Loss: 2.724373\n",
      "Epoch: 36226 \tTraining Loss: 1.191798 \tValidation Loss: 2.725404\n",
      "Epoch: 36227 \tTraining Loss: 1.238247 \tValidation Loss: 2.725758\n",
      "Epoch: 36228 \tTraining Loss: 1.200217 \tValidation Loss: 2.725122\n",
      "Epoch: 36229 \tTraining Loss: 1.193339 \tValidation Loss: 2.724855\n",
      "Epoch: 36230 \tTraining Loss: 1.179056 \tValidation Loss: 2.724963\n",
      "Epoch: 36231 \tTraining Loss: 1.226547 \tValidation Loss: 2.724240\n",
      "Epoch: 36232 \tTraining Loss: 1.190925 \tValidation Loss: 2.724431\n",
      "Epoch: 36233 \tTraining Loss: 1.168057 \tValidation Loss: 2.726300\n",
      "Epoch: 36234 \tTraining Loss: 1.165616 \tValidation Loss: 2.726521\n",
      "Epoch: 36235 \tTraining Loss: 1.210983 \tValidation Loss: 2.725640\n",
      "Epoch: 36236 \tTraining Loss: 1.188963 \tValidation Loss: 2.726342\n",
      "Epoch: 36237 \tTraining Loss: 1.154373 \tValidation Loss: 2.724835\n",
      "Epoch: 36238 \tTraining Loss: 1.197516 \tValidation Loss: 2.725477\n",
      "Epoch: 36239 \tTraining Loss: 1.212420 \tValidation Loss: 2.725091\n",
      "Epoch: 36240 \tTraining Loss: 1.218593 \tValidation Loss: 2.724448\n",
      "Epoch: 36241 \tTraining Loss: 1.188921 \tValidation Loss: 2.725238\n",
      "Epoch: 36242 \tTraining Loss: 1.204205 \tValidation Loss: 2.724190\n",
      "Epoch: 36243 \tTraining Loss: 1.192772 \tValidation Loss: 2.725788\n",
      "Epoch: 36244 \tTraining Loss: 1.210358 \tValidation Loss: 2.725483\n",
      "Epoch: 36245 \tTraining Loss: 1.275749 \tValidation Loss: 2.724567\n",
      "Epoch: 36246 \tTraining Loss: 1.189728 \tValidation Loss: 2.726592\n",
      "Epoch: 36247 \tTraining Loss: 1.224930 \tValidation Loss: 2.724375\n",
      "Epoch: 36248 \tTraining Loss: 1.190569 \tValidation Loss: 2.723903\n",
      "Epoch: 36249 \tTraining Loss: 1.165029 \tValidation Loss: 2.725569\n",
      "Epoch: 36250 \tTraining Loss: 1.166201 \tValidation Loss: 2.725070\n",
      "Epoch: 36251 \tTraining Loss: 1.256094 \tValidation Loss: 2.725216\n",
      "Epoch: 36252 \tTraining Loss: 1.174796 \tValidation Loss: 2.724454\n",
      "Epoch: 36253 \tTraining Loss: 1.213143 \tValidation Loss: 2.724317\n",
      "Epoch: 36254 \tTraining Loss: 1.178753 \tValidation Loss: 2.725713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36255 \tTraining Loss: 1.172662 \tValidation Loss: 2.725989\n",
      "Epoch: 36256 \tTraining Loss: 1.213217 \tValidation Loss: 2.724912\n",
      "Epoch: 36257 \tTraining Loss: 1.252269 \tValidation Loss: 2.724126\n",
      "Epoch: 36258 \tTraining Loss: 1.213229 \tValidation Loss: 2.724082\n",
      "Epoch: 36259 \tTraining Loss: 1.189064 \tValidation Loss: 2.725246\n",
      "Epoch: 36260 \tTraining Loss: 1.221110 \tValidation Loss: 2.723971\n",
      "Epoch: 36261 \tTraining Loss: 1.216346 \tValidation Loss: 2.725679\n",
      "Epoch: 36262 \tTraining Loss: 1.187402 \tValidation Loss: 2.725374\n",
      "Epoch: 36263 \tTraining Loss: 1.148336 \tValidation Loss: 2.725127\n",
      "Epoch: 36264 \tTraining Loss: 1.202216 \tValidation Loss: 2.727505\n",
      "Epoch: 36265 \tTraining Loss: 1.182581 \tValidation Loss: 2.727165\n",
      "Epoch: 36266 \tTraining Loss: 1.199053 \tValidation Loss: 2.724391\n",
      "Epoch: 36267 \tTraining Loss: 1.222841 \tValidation Loss: 2.725368\n",
      "Epoch: 36268 \tTraining Loss: 1.189123 \tValidation Loss: 2.726835\n",
      "Epoch: 36269 \tTraining Loss: 1.156805 \tValidation Loss: 2.726446\n",
      "Epoch: 36270 \tTraining Loss: 1.233676 \tValidation Loss: 2.725399\n",
      "Epoch: 36271 \tTraining Loss: 1.197632 \tValidation Loss: 2.726373\n",
      "Epoch: 36272 \tTraining Loss: 1.193223 \tValidation Loss: 2.727349\n",
      "Epoch: 36273 \tTraining Loss: 1.136595 \tValidation Loss: 2.727901\n",
      "Epoch: 36274 \tTraining Loss: 1.249545 \tValidation Loss: 2.725705\n",
      "Epoch: 36275 \tTraining Loss: 1.153755 \tValidation Loss: 2.726701\n",
      "Epoch: 36276 \tTraining Loss: 1.194425 \tValidation Loss: 2.725899\n",
      "Epoch: 36277 \tTraining Loss: 1.200491 \tValidation Loss: 2.725235\n",
      "Epoch: 36278 \tTraining Loss: 1.196156 \tValidation Loss: 2.727516\n",
      "Epoch: 36279 \tTraining Loss: 1.174997 \tValidation Loss: 2.727798\n",
      "Epoch: 36280 \tTraining Loss: 1.202463 \tValidation Loss: 2.727232\n",
      "Epoch: 36281 \tTraining Loss: 1.202121 \tValidation Loss: 2.726002\n",
      "Epoch: 36282 \tTraining Loss: 1.249449 \tValidation Loss: 2.725939\n",
      "Epoch: 36283 \tTraining Loss: 1.231581 \tValidation Loss: 2.725389\n",
      "Epoch: 36284 \tTraining Loss: 1.185292 \tValidation Loss: 2.727051\n",
      "Epoch: 36285 \tTraining Loss: 1.199177 \tValidation Loss: 2.726982\n",
      "Epoch: 36286 \tTraining Loss: 1.178245 \tValidation Loss: 2.724982\n",
      "Epoch: 36287 \tTraining Loss: 1.178220 \tValidation Loss: 2.728438\n",
      "Epoch: 36288 \tTraining Loss: 1.255429 \tValidation Loss: 2.724826\n",
      "Epoch: 36289 \tTraining Loss: 1.235097 \tValidation Loss: 2.724770\n",
      "Epoch: 36290 \tTraining Loss: 1.168963 \tValidation Loss: 2.727121\n",
      "Epoch: 36291 \tTraining Loss: 1.224664 \tValidation Loss: 2.725313\n",
      "Epoch: 36292 \tTraining Loss: 1.191732 \tValidation Loss: 2.726813\n",
      "Epoch: 36293 \tTraining Loss: 1.240547 \tValidation Loss: 2.725132\n",
      "Epoch: 36294 \tTraining Loss: 1.224727 \tValidation Loss: 2.725806\n",
      "Epoch: 36295 \tTraining Loss: 1.204605 \tValidation Loss: 2.728809\n",
      "Epoch: 36296 \tTraining Loss: 1.194300 \tValidation Loss: 2.727107\n",
      "Epoch: 36297 \tTraining Loss: 1.221233 \tValidation Loss: 2.724410\n",
      "Epoch: 36298 \tTraining Loss: 1.175837 \tValidation Loss: 2.724343\n",
      "Epoch: 36299 \tTraining Loss: 1.194015 \tValidation Loss: 2.727168\n",
      "Epoch: 36300 \tTraining Loss: 1.193115 \tValidation Loss: 2.727058\n",
      "Epoch: 36301 \tTraining Loss: 1.152289 \tValidation Loss: 2.729000\n",
      "Epoch: 36302 \tTraining Loss: 1.185714 \tValidation Loss: 2.726403\n",
      "Epoch: 36303 \tTraining Loss: 1.203868 \tValidation Loss: 2.725032\n",
      "Epoch: 36304 \tTraining Loss: 1.232258 \tValidation Loss: 2.727160\n",
      "Epoch: 36305 \tTraining Loss: 1.161627 \tValidation Loss: 2.725996\n",
      "Epoch: 36306 \tTraining Loss: 1.193687 \tValidation Loss: 2.727986\n",
      "Epoch: 36307 \tTraining Loss: 1.194376 \tValidation Loss: 2.727502\n",
      "Epoch: 36308 \tTraining Loss: 1.208662 \tValidation Loss: 2.726660\n",
      "Epoch: 36309 \tTraining Loss: 1.210453 \tValidation Loss: 2.726680\n",
      "Epoch: 36310 \tTraining Loss: 1.204271 \tValidation Loss: 2.726441\n",
      "Epoch: 36311 \tTraining Loss: 1.210171 \tValidation Loss: 2.727647\n",
      "Epoch: 36312 \tTraining Loss: 1.202348 \tValidation Loss: 2.725736\n",
      "Epoch: 36313 \tTraining Loss: 1.226912 \tValidation Loss: 2.726018\n",
      "Epoch: 36314 \tTraining Loss: 1.184124 \tValidation Loss: 2.726554\n",
      "Epoch: 36315 \tTraining Loss: 1.217438 \tValidation Loss: 2.727491\n",
      "Epoch: 36316 \tTraining Loss: 1.136420 \tValidation Loss: 2.727114\n",
      "Epoch: 36317 \tTraining Loss: 1.168842 \tValidation Loss: 2.725269\n",
      "Epoch: 36318 \tTraining Loss: 1.214491 \tValidation Loss: 2.725913\n",
      "Epoch: 36319 \tTraining Loss: 1.185017 \tValidation Loss: 2.726258\n",
      "Epoch: 36320 \tTraining Loss: 1.188278 \tValidation Loss: 2.728018\n",
      "Epoch: 36321 \tTraining Loss: 1.207248 \tValidation Loss: 2.728410\n",
      "Epoch: 36322 \tTraining Loss: 1.176056 \tValidation Loss: 2.727102\n",
      "Epoch: 36323 \tTraining Loss: 1.141016 \tValidation Loss: 2.728477\n",
      "Epoch: 36324 \tTraining Loss: 1.224168 \tValidation Loss: 2.726760\n",
      "Epoch: 36325 \tTraining Loss: 1.275301 \tValidation Loss: 2.726868\n",
      "Epoch: 36326 \tTraining Loss: 1.140354 \tValidation Loss: 2.729167\n",
      "Epoch: 36327 \tTraining Loss: 1.229312 \tValidation Loss: 2.728683\n",
      "Epoch: 36328 \tTraining Loss: 1.226662 \tValidation Loss: 2.725930\n",
      "Epoch: 36329 \tTraining Loss: 1.168619 \tValidation Loss: 2.726618\n",
      "Epoch: 36330 \tTraining Loss: 1.216303 \tValidation Loss: 2.727317\n",
      "Epoch: 36331 \tTraining Loss: 1.202768 \tValidation Loss: 2.727808\n",
      "Epoch: 36332 \tTraining Loss: 1.189744 \tValidation Loss: 2.727945\n",
      "Epoch: 36333 \tTraining Loss: 1.223349 \tValidation Loss: 2.726329\n",
      "Epoch: 36334 \tTraining Loss: 1.170061 \tValidation Loss: 2.726833\n",
      "Epoch: 36335 \tTraining Loss: 1.216432 \tValidation Loss: 2.727492\n",
      "Epoch: 36336 \tTraining Loss: 1.178120 \tValidation Loss: 2.727400\n",
      "Epoch: 36337 \tTraining Loss: 1.221758 \tValidation Loss: 2.726805\n",
      "Epoch: 36338 \tTraining Loss: 1.182971 \tValidation Loss: 2.726455\n",
      "Epoch: 36339 \tTraining Loss: 1.163531 \tValidation Loss: 2.728604\n",
      "Epoch: 36340 \tTraining Loss: 1.219377 \tValidation Loss: 2.728315\n",
      "Epoch: 36341 \tTraining Loss: 1.226992 \tValidation Loss: 2.728350\n",
      "Epoch: 36342 \tTraining Loss: 1.196801 \tValidation Loss: 2.728550\n",
      "Epoch: 36343 \tTraining Loss: 1.217516 \tValidation Loss: 2.727476\n",
      "Epoch: 36344 \tTraining Loss: 1.194499 \tValidation Loss: 2.728726\n",
      "Epoch: 36345 \tTraining Loss: 1.203866 \tValidation Loss: 2.726968\n",
      "Epoch: 36346 \tTraining Loss: 1.228753 \tValidation Loss: 2.726173\n",
      "Epoch: 36347 \tTraining Loss: 1.208836 \tValidation Loss: 2.727594\n",
      "Epoch: 36348 \tTraining Loss: 1.213911 \tValidation Loss: 2.727248\n",
      "Epoch: 36349 \tTraining Loss: 1.209587 \tValidation Loss: 2.727816\n",
      "Epoch: 36350 \tTraining Loss: 1.214922 \tValidation Loss: 2.728323\n",
      "Epoch: 36351 \tTraining Loss: 1.202646 \tValidation Loss: 2.727365\n",
      "Epoch: 36352 \tTraining Loss: 1.215698 \tValidation Loss: 2.726211\n",
      "Epoch: 36353 \tTraining Loss: 1.210579 \tValidation Loss: 2.727399\n",
      "Epoch: 36354 \tTraining Loss: 1.184215 \tValidation Loss: 2.727542\n",
      "Epoch: 36355 \tTraining Loss: 1.204063 \tValidation Loss: 2.727969\n",
      "Epoch: 36356 \tTraining Loss: 1.227566 \tValidation Loss: 2.728647\n",
      "Epoch: 36357 \tTraining Loss: 1.223065 \tValidation Loss: 2.728623\n",
      "Epoch: 36358 \tTraining Loss: 1.209489 \tValidation Loss: 2.727095\n",
      "Epoch: 36359 \tTraining Loss: 1.226126 \tValidation Loss: 2.726676\n",
      "Epoch: 36360 \tTraining Loss: 1.189980 \tValidation Loss: 2.728573\n",
      "Epoch: 36361 \tTraining Loss: 1.151774 \tValidation Loss: 2.727631\n",
      "Epoch: 36362 \tTraining Loss: 1.173408 \tValidation Loss: 2.728536\n",
      "Epoch: 36363 \tTraining Loss: 1.205074 \tValidation Loss: 2.729319\n",
      "Epoch: 36364 \tTraining Loss: 1.170129 \tValidation Loss: 2.728824\n",
      "Epoch: 36365 \tTraining Loss: 1.146035 \tValidation Loss: 2.729349\n",
      "Epoch: 36366 \tTraining Loss: 1.204267 \tValidation Loss: 2.728444\n",
      "Epoch: 36367 \tTraining Loss: 1.196825 \tValidation Loss: 2.728854\n",
      "Epoch: 36368 \tTraining Loss: 1.211693 \tValidation Loss: 2.727305\n",
      "Epoch: 36369 \tTraining Loss: 1.248555 \tValidation Loss: 2.728231\n",
      "Epoch: 36370 \tTraining Loss: 1.235929 \tValidation Loss: 2.728971\n",
      "Epoch: 36371 \tTraining Loss: 1.200689 \tValidation Loss: 2.728321\n",
      "Epoch: 36372 \tTraining Loss: 1.184629 \tValidation Loss: 2.727734\n",
      "Epoch: 36373 \tTraining Loss: 1.181674 \tValidation Loss: 2.728709\n",
      "Epoch: 36374 \tTraining Loss: 1.166277 \tValidation Loss: 2.729289\n",
      "Epoch: 36375 \tTraining Loss: 1.184494 \tValidation Loss: 2.729324\n",
      "Epoch: 36376 \tTraining Loss: 1.201871 \tValidation Loss: 2.727270\n",
      "Epoch: 36377 \tTraining Loss: 1.186761 \tValidation Loss: 2.728640\n",
      "Epoch: 36378 \tTraining Loss: 1.222589 \tValidation Loss: 2.726737\n",
      "Epoch: 36379 \tTraining Loss: 1.197872 \tValidation Loss: 2.729974\n",
      "Epoch: 36380 \tTraining Loss: 1.198217 \tValidation Loss: 2.727572\n",
      "Epoch: 36381 \tTraining Loss: 1.170989 \tValidation Loss: 2.729677\n",
      "Epoch: 36382 \tTraining Loss: 1.206574 \tValidation Loss: 2.729376\n",
      "Epoch: 36383 \tTraining Loss: 1.197131 \tValidation Loss: 2.729764\n",
      "Epoch: 36384 \tTraining Loss: 1.160308 \tValidation Loss: 2.729795\n",
      "Epoch: 36385 \tTraining Loss: 1.153626 \tValidation Loss: 2.730348\n",
      "Epoch: 36386 \tTraining Loss: 1.217175 \tValidation Loss: 2.728654\n",
      "Epoch: 36387 \tTraining Loss: 1.205433 \tValidation Loss: 2.729084\n",
      "Epoch: 36388 \tTraining Loss: 1.174116 \tValidation Loss: 2.728539\n",
      "Epoch: 36389 \tTraining Loss: 1.187412 \tValidation Loss: 2.727859\n",
      "Epoch: 36390 \tTraining Loss: 1.224117 \tValidation Loss: 2.729621\n",
      "Epoch: 36391 \tTraining Loss: 1.140780 \tValidation Loss: 2.729090\n",
      "Epoch: 36392 \tTraining Loss: 1.208261 \tValidation Loss: 2.727600\n",
      "Epoch: 36393 \tTraining Loss: 1.180450 \tValidation Loss: 2.727873\n",
      "Epoch: 36394 \tTraining Loss: 1.201959 \tValidation Loss: 2.728356\n",
      "Epoch: 36395 \tTraining Loss: 1.185062 \tValidation Loss: 2.730399\n",
      "Epoch: 36396 \tTraining Loss: 1.228050 \tValidation Loss: 2.728036\n",
      "Epoch: 36397 \tTraining Loss: 1.206122 \tValidation Loss: 2.728789\n",
      "Epoch: 36398 \tTraining Loss: 1.187503 \tValidation Loss: 2.729082\n",
      "Epoch: 36399 \tTraining Loss: 1.230054 \tValidation Loss: 2.727869\n",
      "Epoch: 36400 \tTraining Loss: 1.165100 \tValidation Loss: 2.730324\n",
      "Epoch: 36401 \tTraining Loss: 1.192623 \tValidation Loss: 2.728739\n",
      "Epoch: 36402 \tTraining Loss: 1.223441 \tValidation Loss: 2.728813\n",
      "Epoch: 36403 \tTraining Loss: 1.257570 \tValidation Loss: 2.728069\n",
      "Epoch: 36404 \tTraining Loss: 1.221886 \tValidation Loss: 2.730339\n",
      "Epoch: 36405 \tTraining Loss: 1.216756 \tValidation Loss: 2.729349\n",
      "Epoch: 36406 \tTraining Loss: 1.223515 \tValidation Loss: 2.728534\n",
      "Epoch: 36407 \tTraining Loss: 1.201645 \tValidation Loss: 2.729519\n",
      "Epoch: 36408 \tTraining Loss: 1.215399 \tValidation Loss: 2.727693\n",
      "Epoch: 36409 \tTraining Loss: 1.204973 \tValidation Loss: 2.729434\n",
      "Epoch: 36410 \tTraining Loss: 1.195638 \tValidation Loss: 2.728837\n",
      "Epoch: 36411 \tTraining Loss: 1.229007 \tValidation Loss: 2.726631\n",
      "Epoch: 36412 \tTraining Loss: 1.241863 \tValidation Loss: 2.728088\n",
      "Epoch: 36413 \tTraining Loss: 1.208145 \tValidation Loss: 2.727958\n",
      "Epoch: 36414 \tTraining Loss: 1.214324 \tValidation Loss: 2.729420\n",
      "Epoch: 36415 \tTraining Loss: 1.179509 \tValidation Loss: 2.728639\n",
      "Epoch: 36416 \tTraining Loss: 1.177298 \tValidation Loss: 2.730120\n",
      "Epoch: 36417 \tTraining Loss: 1.207694 \tValidation Loss: 2.729953\n",
      "Epoch: 36418 \tTraining Loss: 1.222965 \tValidation Loss: 2.728841\n",
      "Epoch: 36419 \tTraining Loss: 1.187767 \tValidation Loss: 2.729045\n",
      "Epoch: 36420 \tTraining Loss: 1.220910 \tValidation Loss: 2.728768\n",
      "Epoch: 36421 \tTraining Loss: 1.164895 \tValidation Loss: 2.728755\n",
      "Epoch: 36422 \tTraining Loss: 1.235605 \tValidation Loss: 2.728930\n",
      "Epoch: 36423 \tTraining Loss: 1.238170 \tValidation Loss: 2.729474\n",
      "Epoch: 36424 \tTraining Loss: 1.244417 \tValidation Loss: 2.727779\n",
      "Epoch: 36425 \tTraining Loss: 1.219148 \tValidation Loss: 2.729349\n",
      "Epoch: 36426 \tTraining Loss: 1.239545 \tValidation Loss: 2.730532\n",
      "Epoch: 36427 \tTraining Loss: 1.229860 \tValidation Loss: 2.728857\n",
      "Epoch: 36428 \tTraining Loss: 1.203917 \tValidation Loss: 2.728958\n",
      "Epoch: 36429 \tTraining Loss: 1.158663 \tValidation Loss: 2.730326\n",
      "Epoch: 36430 \tTraining Loss: 1.198808 \tValidation Loss: 2.727824\n",
      "Epoch: 36431 \tTraining Loss: 1.217052 \tValidation Loss: 2.730338\n",
      "Epoch: 36432 \tTraining Loss: 1.247464 \tValidation Loss: 2.726714\n",
      "Epoch: 36433 \tTraining Loss: 1.177226 \tValidation Loss: 2.730801\n",
      "Epoch: 36434 \tTraining Loss: 1.220429 \tValidation Loss: 2.729383\n",
      "Epoch: 36435 \tTraining Loss: 1.231963 \tValidation Loss: 2.729107\n",
      "Epoch: 36436 \tTraining Loss: 1.223247 \tValidation Loss: 2.728945\n",
      "Epoch: 36437 \tTraining Loss: 1.222154 \tValidation Loss: 2.729765\n",
      "Epoch: 36438 \tTraining Loss: 1.214744 \tValidation Loss: 2.728018\n",
      "Epoch: 36439 \tTraining Loss: 1.200484 \tValidation Loss: 2.729967\n",
      "Epoch: 36440 \tTraining Loss: 1.189708 \tValidation Loss: 2.730176\n",
      "Epoch: 36441 \tTraining Loss: 1.185392 \tValidation Loss: 2.728824\n",
      "Epoch: 36442 \tTraining Loss: 1.171417 \tValidation Loss: 2.731194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36443 \tTraining Loss: 1.216970 \tValidation Loss: 2.728985\n",
      "Epoch: 36444 \tTraining Loss: 1.224121 \tValidation Loss: 2.728798\n",
      "Epoch: 36445 \tTraining Loss: 1.182790 \tValidation Loss: 2.731031\n",
      "Epoch: 36446 \tTraining Loss: 1.217093 \tValidation Loss: 2.730728\n",
      "Epoch: 36447 \tTraining Loss: 1.181441 \tValidation Loss: 2.729980\n",
      "Epoch: 36448 \tTraining Loss: 1.197350 \tValidation Loss: 2.729935\n",
      "Epoch: 36449 \tTraining Loss: 1.207749 \tValidation Loss: 2.728731\n",
      "Epoch: 36450 \tTraining Loss: 1.215415 \tValidation Loss: 2.729612\n",
      "Epoch: 36451 \tTraining Loss: 1.207972 \tValidation Loss: 2.729774\n",
      "Epoch: 36452 \tTraining Loss: 1.238572 \tValidation Loss: 2.729141\n",
      "Epoch: 36453 \tTraining Loss: 1.185835 \tValidation Loss: 2.729328\n",
      "Epoch: 36454 \tTraining Loss: 1.191275 \tValidation Loss: 2.730075\n",
      "Epoch: 36455 \tTraining Loss: 1.154718 \tValidation Loss: 2.731055\n",
      "Epoch: 36456 \tTraining Loss: 1.166329 \tValidation Loss: 2.731001\n",
      "Epoch: 36457 \tTraining Loss: 1.198286 \tValidation Loss: 2.732697\n",
      "Epoch: 36458 \tTraining Loss: 1.262568 \tValidation Loss: 2.728408\n",
      "Epoch: 36459 \tTraining Loss: 1.143553 \tValidation Loss: 2.730885\n",
      "Epoch: 36460 \tTraining Loss: 1.219330 \tValidation Loss: 2.730182\n",
      "Epoch: 36461 \tTraining Loss: 1.198621 \tValidation Loss: 2.731414\n",
      "Epoch: 36462 \tTraining Loss: 1.169162 \tValidation Loss: 2.729980\n",
      "Epoch: 36463 \tTraining Loss: 1.251180 \tValidation Loss: 2.730763\n",
      "Epoch: 36464 \tTraining Loss: 1.150947 \tValidation Loss: 2.731140\n",
      "Epoch: 36465 \tTraining Loss: 1.167834 \tValidation Loss: 2.730692\n",
      "Epoch: 36466 \tTraining Loss: 1.216191 \tValidation Loss: 2.730137\n",
      "Epoch: 36467 \tTraining Loss: 1.228842 \tValidation Loss: 2.730499\n",
      "Epoch: 36468 \tTraining Loss: 1.208804 \tValidation Loss: 2.729975\n",
      "Epoch: 36469 \tTraining Loss: 1.187788 \tValidation Loss: 2.729543\n",
      "Epoch: 36470 \tTraining Loss: 1.228418 \tValidation Loss: 2.728202\n",
      "Epoch: 36471 \tTraining Loss: 1.228539 \tValidation Loss: 2.729210\n",
      "Epoch: 36472 \tTraining Loss: 1.229414 \tValidation Loss: 2.729543\n",
      "Epoch: 36473 \tTraining Loss: 1.205210 \tValidation Loss: 2.730955\n",
      "Epoch: 36474 \tTraining Loss: 1.223641 \tValidation Loss: 2.730999\n",
      "Epoch: 36475 \tTraining Loss: 1.214809 \tValidation Loss: 2.731586\n",
      "Epoch: 36476 \tTraining Loss: 1.190536 \tValidation Loss: 2.730939\n",
      "Epoch: 36477 \tTraining Loss: 1.230717 \tValidation Loss: 2.730173\n",
      "Epoch: 36478 \tTraining Loss: 1.222950 \tValidation Loss: 2.730628\n",
      "Epoch: 36479 \tTraining Loss: 1.203763 \tValidation Loss: 2.731886\n",
      "Epoch: 36480 \tTraining Loss: 1.212178 \tValidation Loss: 2.731738\n",
      "Epoch: 36481 \tTraining Loss: 1.180252 \tValidation Loss: 2.732606\n",
      "Epoch: 36482 \tTraining Loss: 1.212560 \tValidation Loss: 2.730726\n",
      "Epoch: 36483 \tTraining Loss: 1.168184 \tValidation Loss: 2.731960\n",
      "Epoch: 36484 \tTraining Loss: 1.194482 \tValidation Loss: 2.730965\n",
      "Epoch: 36485 \tTraining Loss: 1.236174 \tValidation Loss: 2.731879\n",
      "Epoch: 36486 \tTraining Loss: 1.192546 \tValidation Loss: 2.732674\n",
      "Epoch: 36487 \tTraining Loss: 1.220744 \tValidation Loss: 2.731483\n",
      "Epoch: 36488 \tTraining Loss: 1.166646 \tValidation Loss: 2.732232\n",
      "Epoch: 36489 \tTraining Loss: 1.223691 \tValidation Loss: 2.732074\n",
      "Epoch: 36490 \tTraining Loss: 1.162828 \tValidation Loss: 2.731810\n",
      "Epoch: 36491 \tTraining Loss: 1.208485 \tValidation Loss: 2.729995\n",
      "Epoch: 36492 \tTraining Loss: 1.177737 \tValidation Loss: 2.732367\n",
      "Epoch: 36493 \tTraining Loss: 1.203891 \tValidation Loss: 2.731108\n",
      "Epoch: 36494 \tTraining Loss: 1.230712 \tValidation Loss: 2.731955\n",
      "Epoch: 36495 \tTraining Loss: 1.264637 \tValidation Loss: 2.730889\n",
      "Epoch: 36496 \tTraining Loss: 1.227989 \tValidation Loss: 2.730738\n",
      "Epoch: 36497 \tTraining Loss: 1.237097 \tValidation Loss: 2.730765\n",
      "Epoch: 36498 \tTraining Loss: 1.183478 \tValidation Loss: 2.731218\n",
      "Epoch: 36499 \tTraining Loss: 1.138568 \tValidation Loss: 2.732580\n",
      "Epoch: 36500 \tTraining Loss: 1.195025 \tValidation Loss: 2.730663\n",
      "Epoch: 36501 \tTraining Loss: 1.207011 \tValidation Loss: 2.731437\n",
      "Epoch: 36502 \tTraining Loss: 1.232579 \tValidation Loss: 2.731337\n",
      "Epoch: 36503 \tTraining Loss: 1.182455 \tValidation Loss: 2.731036\n",
      "Epoch: 36504 \tTraining Loss: 1.224234 \tValidation Loss: 2.731112\n",
      "Epoch: 36505 \tTraining Loss: 1.182292 \tValidation Loss: 2.730240\n",
      "Epoch: 36506 \tTraining Loss: 1.177511 \tValidation Loss: 2.731179\n",
      "Epoch: 36507 \tTraining Loss: 1.158306 \tValidation Loss: 2.732751\n",
      "Epoch: 36508 \tTraining Loss: 1.182358 \tValidation Loss: 2.731046\n",
      "Epoch: 36509 \tTraining Loss: 1.240482 \tValidation Loss: 2.728908\n",
      "Epoch: 36510 \tTraining Loss: 1.208106 \tValidation Loss: 2.729724\n",
      "Epoch: 36511 \tTraining Loss: 1.209238 \tValidation Loss: 2.730613\n",
      "Epoch: 36512 \tTraining Loss: 1.212069 \tValidation Loss: 2.730416\n",
      "Epoch: 36513 \tTraining Loss: 1.191087 \tValidation Loss: 2.732574\n",
      "Epoch: 36514 \tTraining Loss: 1.190655 \tValidation Loss: 2.732713\n",
      "Epoch: 36515 \tTraining Loss: 1.223390 \tValidation Loss: 2.730432\n",
      "Epoch: 36516 \tTraining Loss: 1.188080 \tValidation Loss: 2.731312\n",
      "Epoch: 36517 \tTraining Loss: 1.190952 \tValidation Loss: 2.732270\n",
      "Epoch: 36518 \tTraining Loss: 1.205150 \tValidation Loss: 2.731349\n",
      "Epoch: 36519 \tTraining Loss: 1.245425 \tValidation Loss: 2.732126\n",
      "Epoch: 36520 \tTraining Loss: 1.197567 \tValidation Loss: 2.732324\n",
      "Epoch: 36521 \tTraining Loss: 1.219198 \tValidation Loss: 2.732563\n",
      "Epoch: 36522 \tTraining Loss: 1.202225 \tValidation Loss: 2.730859\n",
      "Epoch: 36523 \tTraining Loss: 1.204129 \tValidation Loss: 2.731327\n",
      "Epoch: 36524 \tTraining Loss: 1.205230 \tValidation Loss: 2.732195\n",
      "Epoch: 36525 \tTraining Loss: 1.203635 \tValidation Loss: 2.731327\n",
      "Epoch: 36526 \tTraining Loss: 1.200810 \tValidation Loss: 2.731413\n",
      "Epoch: 36527 \tTraining Loss: 1.254555 \tValidation Loss: 2.730516\n",
      "Epoch: 36528 \tTraining Loss: 1.186838 \tValidation Loss: 2.731297\n",
      "Epoch: 36529 \tTraining Loss: 1.246440 \tValidation Loss: 2.732285\n",
      "Epoch: 36530 \tTraining Loss: 1.195137 \tValidation Loss: 2.732604\n",
      "Epoch: 36531 \tTraining Loss: 1.222228 \tValidation Loss: 2.732097\n",
      "Epoch: 36532 \tTraining Loss: 1.223181 \tValidation Loss: 2.733017\n",
      "Epoch: 36533 \tTraining Loss: 1.212850 \tValidation Loss: 2.731784\n",
      "Epoch: 36534 \tTraining Loss: 1.175695 \tValidation Loss: 2.732187\n",
      "Epoch: 36535 \tTraining Loss: 1.192632 \tValidation Loss: 2.731429\n",
      "Epoch: 36536 \tTraining Loss: 1.187072 \tValidation Loss: 2.731323\n",
      "Epoch: 36537 \tTraining Loss: 1.162314 \tValidation Loss: 2.732372\n",
      "Epoch: 36538 \tTraining Loss: 1.225572 \tValidation Loss: 2.732592\n",
      "Epoch: 36539 \tTraining Loss: 1.220145 \tValidation Loss: 2.733483\n",
      "Epoch: 36540 \tTraining Loss: 1.178596 \tValidation Loss: 2.733793\n",
      "Epoch: 36541 \tTraining Loss: 1.223807 \tValidation Loss: 2.732931\n",
      "Epoch: 36542 \tTraining Loss: 1.233244 \tValidation Loss: 2.731001\n",
      "Epoch: 36543 \tTraining Loss: 1.199894 \tValidation Loss: 2.732142\n",
      "Epoch: 36544 \tTraining Loss: 1.179996 \tValidation Loss: 2.732740\n",
      "Epoch: 36545 \tTraining Loss: 1.222723 \tValidation Loss: 2.731709\n",
      "Epoch: 36546 \tTraining Loss: 1.195408 \tValidation Loss: 2.730848\n",
      "Epoch: 36547 \tTraining Loss: 1.227721 \tValidation Loss: 2.732212\n",
      "Epoch: 36548 \tTraining Loss: 1.224337 \tValidation Loss: 2.733909\n",
      "Epoch: 36549 \tTraining Loss: 1.191023 \tValidation Loss: 2.731235\n",
      "Epoch: 36550 \tTraining Loss: 1.175032 \tValidation Loss: 2.733030\n",
      "Epoch: 36551 \tTraining Loss: 1.191918 \tValidation Loss: 2.731852\n",
      "Epoch: 36552 \tTraining Loss: 1.211208 \tValidation Loss: 2.733080\n",
      "Epoch: 36553 \tTraining Loss: 1.182538 \tValidation Loss: 2.733084\n",
      "Epoch: 36554 \tTraining Loss: 1.207805 \tValidation Loss: 2.732909\n",
      "Epoch: 36555 \tTraining Loss: 1.172363 \tValidation Loss: 2.733866\n",
      "Epoch: 36556 \tTraining Loss: 1.191034 \tValidation Loss: 2.733977\n",
      "Epoch: 36557 \tTraining Loss: 1.258465 \tValidation Loss: 2.733815\n",
      "Epoch: 36558 \tTraining Loss: 1.168183 \tValidation Loss: 2.733980\n",
      "Epoch: 36559 \tTraining Loss: 1.199674 \tValidation Loss: 2.732764\n",
      "Epoch: 36560 \tTraining Loss: 1.211107 \tValidation Loss: 2.734023\n",
      "Epoch: 36561 \tTraining Loss: 1.233983 \tValidation Loss: 2.731653\n",
      "Epoch: 36562 \tTraining Loss: 1.184450 \tValidation Loss: 2.731972\n",
      "Epoch: 36563 \tTraining Loss: 1.185931 \tValidation Loss: 2.732895\n",
      "Epoch: 36564 \tTraining Loss: 1.164662 \tValidation Loss: 2.733358\n",
      "Epoch: 36565 \tTraining Loss: 1.204073 \tValidation Loss: 2.732115\n",
      "Epoch: 36566 \tTraining Loss: 1.215697 \tValidation Loss: 2.733001\n",
      "Epoch: 36567 \tTraining Loss: 1.155560 \tValidation Loss: 2.733085\n",
      "Epoch: 36568 \tTraining Loss: 1.189354 \tValidation Loss: 2.731259\n",
      "Epoch: 36569 \tTraining Loss: 1.204587 \tValidation Loss: 2.733510\n",
      "Epoch: 36570 \tTraining Loss: 1.213356 \tValidation Loss: 2.731843\n",
      "Epoch: 36571 \tTraining Loss: 1.217426 \tValidation Loss: 2.733955\n",
      "Epoch: 36572 \tTraining Loss: 1.215804 \tValidation Loss: 2.732812\n",
      "Epoch: 36573 \tTraining Loss: 1.232176 \tValidation Loss: 2.732727\n",
      "Epoch: 36574 \tTraining Loss: 1.243797 \tValidation Loss: 2.731299\n",
      "Epoch: 36575 \tTraining Loss: 1.167011 \tValidation Loss: 2.732662\n",
      "Epoch: 36576 \tTraining Loss: 1.224492 \tValidation Loss: 2.732856\n",
      "Epoch: 36577 \tTraining Loss: 1.208609 \tValidation Loss: 2.732343\n",
      "Epoch: 36578 \tTraining Loss: 1.198387 \tValidation Loss: 2.731905\n",
      "Epoch: 36579 \tTraining Loss: 1.198906 \tValidation Loss: 2.732825\n",
      "Epoch: 36580 \tTraining Loss: 1.215314 \tValidation Loss: 2.732000\n",
      "Epoch: 36581 \tTraining Loss: 1.159595 \tValidation Loss: 2.732430\n",
      "Epoch: 36582 \tTraining Loss: 1.200359 \tValidation Loss: 2.733494\n",
      "Epoch: 36583 \tTraining Loss: 1.153575 \tValidation Loss: 2.735579\n",
      "Epoch: 36584 \tTraining Loss: 1.205007 \tValidation Loss: 2.733064\n",
      "Epoch: 36585 \tTraining Loss: 1.168243 \tValidation Loss: 2.734171\n",
      "Epoch: 36586 \tTraining Loss: 1.214256 \tValidation Loss: 2.733630\n",
      "Epoch: 36587 \tTraining Loss: 1.161925 \tValidation Loss: 2.733463\n",
      "Epoch: 36588 \tTraining Loss: 1.256520 \tValidation Loss: 2.734233\n",
      "Epoch: 36589 \tTraining Loss: 1.164959 \tValidation Loss: 2.733555\n",
      "Epoch: 36590 \tTraining Loss: 1.237687 \tValidation Loss: 2.733370\n",
      "Epoch: 36591 \tTraining Loss: 1.217758 \tValidation Loss: 2.733964\n",
      "Epoch: 36592 \tTraining Loss: 1.212852 \tValidation Loss: 2.734937\n",
      "Epoch: 36593 \tTraining Loss: 1.181915 \tValidation Loss: 2.735018\n",
      "Epoch: 36594 \tTraining Loss: 1.201835 \tValidation Loss: 2.732319\n",
      "Epoch: 36595 \tTraining Loss: 1.220173 \tValidation Loss: 2.734293\n",
      "Epoch: 36596 \tTraining Loss: 1.190927 \tValidation Loss: 2.734272\n",
      "Epoch: 36597 \tTraining Loss: 1.227203 \tValidation Loss: 2.733658\n",
      "Epoch: 36598 \tTraining Loss: 1.190343 \tValidation Loss: 2.734771\n",
      "Epoch: 36599 \tTraining Loss: 1.227640 \tValidation Loss: 2.733607\n",
      "Epoch: 36600 \tTraining Loss: 1.147844 \tValidation Loss: 2.734884\n",
      "Epoch: 36601 \tTraining Loss: 1.198346 \tValidation Loss: 2.733232\n",
      "Epoch: 36602 \tTraining Loss: 1.199340 \tValidation Loss: 2.734639\n",
      "Epoch: 36603 \tTraining Loss: 1.250308 \tValidation Loss: 2.733555\n",
      "Epoch: 36604 \tTraining Loss: 1.220209 \tValidation Loss: 2.732873\n",
      "Epoch: 36605 \tTraining Loss: 1.188453 \tValidation Loss: 2.734515\n",
      "Epoch: 36606 \tTraining Loss: 1.172098 \tValidation Loss: 2.733706\n",
      "Epoch: 36607 \tTraining Loss: 1.212295 \tValidation Loss: 2.733683\n",
      "Epoch: 36608 \tTraining Loss: 1.194462 \tValidation Loss: 2.732908\n",
      "Epoch: 36609 \tTraining Loss: 1.199588 \tValidation Loss: 2.734436\n",
      "Epoch: 36610 \tTraining Loss: 1.211648 \tValidation Loss: 2.733883\n",
      "Epoch: 36611 \tTraining Loss: 1.223449 \tValidation Loss: 2.735672\n",
      "Epoch: 36612 \tTraining Loss: 1.190112 \tValidation Loss: 2.734157\n",
      "Epoch: 36613 \tTraining Loss: 1.181574 \tValidation Loss: 2.734018\n",
      "Epoch: 36614 \tTraining Loss: 1.170379 \tValidation Loss: 2.734838\n",
      "Epoch: 36615 \tTraining Loss: 1.233496 \tValidation Loss: 2.733950\n",
      "Epoch: 36616 \tTraining Loss: 1.192986 \tValidation Loss: 2.735261\n",
      "Epoch: 36617 \tTraining Loss: 1.219929 \tValidation Loss: 2.733261\n",
      "Epoch: 36618 \tTraining Loss: 1.212135 \tValidation Loss: 2.734330\n",
      "Epoch: 36619 \tTraining Loss: 1.214815 \tValidation Loss: 2.734307\n",
      "Epoch: 36620 \tTraining Loss: 1.174048 \tValidation Loss: 2.735324\n",
      "Epoch: 36621 \tTraining Loss: 1.217646 \tValidation Loss: 2.733494\n",
      "Epoch: 36622 \tTraining Loss: 1.229307 \tValidation Loss: 2.734788\n",
      "Epoch: 36623 \tTraining Loss: 1.223689 \tValidation Loss: 2.734598\n",
      "Epoch: 36624 \tTraining Loss: 1.209559 \tValidation Loss: 2.734302\n",
      "Epoch: 36625 \tTraining Loss: 1.233254 \tValidation Loss: 2.733311\n",
      "Epoch: 36626 \tTraining Loss: 1.245193 \tValidation Loss: 2.733675\n",
      "Epoch: 36627 \tTraining Loss: 1.224047 \tValidation Loss: 2.732220\n",
      "Epoch: 36628 \tTraining Loss: 1.167547 \tValidation Loss: 2.734269\n",
      "Epoch: 36629 \tTraining Loss: 1.177247 \tValidation Loss: 2.734824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36630 \tTraining Loss: 1.190349 \tValidation Loss: 2.736138\n",
      "Epoch: 36631 \tTraining Loss: 1.164256 \tValidation Loss: 2.734356\n",
      "Epoch: 36632 \tTraining Loss: 1.175559 \tValidation Loss: 2.735232\n",
      "Epoch: 36633 \tTraining Loss: 1.245408 \tValidation Loss: 2.734985\n",
      "Epoch: 36634 \tTraining Loss: 1.230880 \tValidation Loss: 2.733613\n",
      "Epoch: 36635 \tTraining Loss: 1.141364 \tValidation Loss: 2.735378\n",
      "Epoch: 36636 \tTraining Loss: 1.206876 \tValidation Loss: 2.733596\n",
      "Epoch: 36637 \tTraining Loss: 1.249301 \tValidation Loss: 2.734594\n",
      "Epoch: 36638 \tTraining Loss: 1.219062 \tValidation Loss: 2.734928\n",
      "Epoch: 36639 \tTraining Loss: 1.190896 \tValidation Loss: 2.734315\n",
      "Epoch: 36640 \tTraining Loss: 1.229525 \tValidation Loss: 2.733070\n",
      "Epoch: 36641 \tTraining Loss: 1.185995 \tValidation Loss: 2.734549\n",
      "Epoch: 36642 \tTraining Loss: 1.234472 \tValidation Loss: 2.735882\n",
      "Epoch: 36643 \tTraining Loss: 1.172374 \tValidation Loss: 2.734883\n",
      "Epoch: 36644 \tTraining Loss: 1.184810 \tValidation Loss: 2.733294\n",
      "Epoch: 36645 \tTraining Loss: 1.198039 \tValidation Loss: 2.733951\n",
      "Epoch: 36646 \tTraining Loss: 1.233417 \tValidation Loss: 2.734927\n",
      "Epoch: 36647 \tTraining Loss: 1.162099 \tValidation Loss: 2.735110\n",
      "Epoch: 36648 \tTraining Loss: 1.234838 \tValidation Loss: 2.735106\n",
      "Epoch: 36649 \tTraining Loss: 1.226585 \tValidation Loss: 2.735901\n",
      "Epoch: 36650 \tTraining Loss: 1.225767 \tValidation Loss: 2.734913\n",
      "Epoch: 36651 \tTraining Loss: 1.224202 \tValidation Loss: 2.734826\n",
      "Epoch: 36652 \tTraining Loss: 1.209588 \tValidation Loss: 2.734154\n",
      "Epoch: 36653 \tTraining Loss: 1.164962 \tValidation Loss: 2.736421\n",
      "Epoch: 36654 \tTraining Loss: 1.225119 \tValidation Loss: 2.734302\n",
      "Epoch: 36655 \tTraining Loss: 1.173356 \tValidation Loss: 2.735786\n",
      "Epoch: 36656 \tTraining Loss: 1.225907 \tValidation Loss: 2.734713\n",
      "Epoch: 36657 \tTraining Loss: 1.189752 \tValidation Loss: 2.735903\n",
      "Epoch: 36658 \tTraining Loss: 1.142873 \tValidation Loss: 2.736110\n",
      "Epoch: 36659 \tTraining Loss: 1.140863 \tValidation Loss: 2.734951\n",
      "Epoch: 36660 \tTraining Loss: 1.221546 \tValidation Loss: 2.735208\n",
      "Epoch: 36661 \tTraining Loss: 1.217306 \tValidation Loss: 2.734310\n",
      "Epoch: 36662 \tTraining Loss: 1.186983 \tValidation Loss: 2.736111\n",
      "Epoch: 36663 \tTraining Loss: 1.225245 \tValidation Loss: 2.733285\n",
      "Epoch: 36664 \tTraining Loss: 1.159255 \tValidation Loss: 2.735020\n",
      "Epoch: 36665 \tTraining Loss: 1.226018 \tValidation Loss: 2.734008\n",
      "Epoch: 36666 \tTraining Loss: 1.193500 \tValidation Loss: 2.734705\n",
      "Epoch: 36667 \tTraining Loss: 1.213575 \tValidation Loss: 2.735313\n",
      "Epoch: 36668 \tTraining Loss: 1.240957 \tValidation Loss: 2.734607\n",
      "Epoch: 36669 \tTraining Loss: 1.166647 \tValidation Loss: 2.735714\n",
      "Epoch: 36670 \tTraining Loss: 1.191194 \tValidation Loss: 2.734762\n",
      "Epoch: 36671 \tTraining Loss: 1.140633 \tValidation Loss: 2.735816\n",
      "Epoch: 36672 \tTraining Loss: 1.265919 \tValidation Loss: 2.734867\n",
      "Epoch: 36673 \tTraining Loss: 1.209230 \tValidation Loss: 2.734922\n",
      "Epoch: 36674 \tTraining Loss: 1.165072 \tValidation Loss: 2.734769\n",
      "Epoch: 36675 \tTraining Loss: 1.166015 \tValidation Loss: 2.736471\n",
      "Epoch: 36676 \tTraining Loss: 1.287353 \tValidation Loss: 2.732460\n",
      "Epoch: 36677 \tTraining Loss: 1.235591 \tValidation Loss: 2.733468\n",
      "Epoch: 36678 \tTraining Loss: 1.190822 \tValidation Loss: 2.734366\n",
      "Epoch: 36679 \tTraining Loss: 1.247634 \tValidation Loss: 2.733158\n",
      "Epoch: 36680 \tTraining Loss: 1.173655 \tValidation Loss: 2.735128\n",
      "Epoch: 36681 \tTraining Loss: 1.198099 \tValidation Loss: 2.735281\n",
      "Epoch: 36682 \tTraining Loss: 1.161779 \tValidation Loss: 2.736563\n",
      "Epoch: 36683 \tTraining Loss: 1.225952 \tValidation Loss: 2.735449\n",
      "Epoch: 36684 \tTraining Loss: 1.194992 \tValidation Loss: 2.734849\n",
      "Epoch: 36685 \tTraining Loss: 1.170263 \tValidation Loss: 2.735522\n",
      "Epoch: 36686 \tTraining Loss: 1.224501 \tValidation Loss: 2.735316\n",
      "Epoch: 36687 \tTraining Loss: 1.176345 \tValidation Loss: 2.735236\n",
      "Epoch: 36688 \tTraining Loss: 1.165965 \tValidation Loss: 2.735579\n",
      "Epoch: 36689 \tTraining Loss: 1.215459 \tValidation Loss: 2.734405\n",
      "Epoch: 36690 \tTraining Loss: 1.192295 \tValidation Loss: 2.736183\n",
      "Epoch: 36691 \tTraining Loss: 1.165067 \tValidation Loss: 2.736299\n",
      "Epoch: 36692 \tTraining Loss: 1.206960 \tValidation Loss: 2.735268\n",
      "Epoch: 36693 \tTraining Loss: 1.238400 \tValidation Loss: 2.734066\n",
      "Epoch: 36694 \tTraining Loss: 1.195054 \tValidation Loss: 2.736474\n",
      "Epoch: 36695 \tTraining Loss: 1.244697 \tValidation Loss: 2.735729\n",
      "Epoch: 36696 \tTraining Loss: 1.207577 \tValidation Loss: 2.734808\n",
      "Epoch: 36697 \tTraining Loss: 1.181676 \tValidation Loss: 2.735814\n",
      "Epoch: 36698 \tTraining Loss: 1.234379 \tValidation Loss: 2.735039\n",
      "Epoch: 36699 \tTraining Loss: 1.154369 \tValidation Loss: 2.735126\n",
      "Epoch: 36700 \tTraining Loss: 1.166166 \tValidation Loss: 2.735805\n",
      "Epoch: 36701 \tTraining Loss: 1.248274 \tValidation Loss: 2.735757\n",
      "Epoch: 36702 \tTraining Loss: 1.163436 \tValidation Loss: 2.735805\n",
      "Epoch: 36703 \tTraining Loss: 1.222566 \tValidation Loss: 2.736479\n",
      "Epoch: 36704 \tTraining Loss: 1.191708 \tValidation Loss: 2.735778\n",
      "Epoch: 36705 \tTraining Loss: 1.213756 \tValidation Loss: 2.736362\n",
      "Epoch: 36706 \tTraining Loss: 1.226663 \tValidation Loss: 2.735017\n",
      "Epoch: 36707 \tTraining Loss: 1.230579 \tValidation Loss: 2.735153\n",
      "Epoch: 36708 \tTraining Loss: 1.192601 \tValidation Loss: 2.735416\n",
      "Epoch: 36709 \tTraining Loss: 1.236162 \tValidation Loss: 2.735349\n",
      "Epoch: 36710 \tTraining Loss: 1.199293 \tValidation Loss: 2.735554\n",
      "Epoch: 36711 \tTraining Loss: 1.237379 \tValidation Loss: 2.734543\n",
      "Epoch: 36712 \tTraining Loss: 1.184077 \tValidation Loss: 2.735977\n",
      "Epoch: 36713 \tTraining Loss: 1.192749 \tValidation Loss: 2.736083\n",
      "Epoch: 36714 \tTraining Loss: 1.197345 \tValidation Loss: 2.735133\n",
      "Epoch: 36715 \tTraining Loss: 1.245771 \tValidation Loss: 2.735358\n",
      "Epoch: 36716 \tTraining Loss: 1.160873 \tValidation Loss: 2.736912\n",
      "Epoch: 36717 \tTraining Loss: 1.228884 \tValidation Loss: 2.736445\n",
      "Epoch: 36718 \tTraining Loss: 1.199544 \tValidation Loss: 2.736125\n",
      "Epoch: 36719 \tTraining Loss: 1.220457 \tValidation Loss: 2.736777\n",
      "Epoch: 36720 \tTraining Loss: 1.170178 \tValidation Loss: 2.736973\n",
      "Epoch: 36721 \tTraining Loss: 1.209289 \tValidation Loss: 2.735939\n",
      "Epoch: 36722 \tTraining Loss: 1.159386 \tValidation Loss: 2.736385\n",
      "Epoch: 36723 \tTraining Loss: 1.211460 \tValidation Loss: 2.734094\n",
      "Epoch: 36724 \tTraining Loss: 1.217733 \tValidation Loss: 2.735207\n",
      "Epoch: 36725 \tTraining Loss: 1.208255 \tValidation Loss: 2.734755\n",
      "Epoch: 36726 \tTraining Loss: 1.184381 \tValidation Loss: 2.736856\n",
      "Epoch: 36727 \tTraining Loss: 1.208053 \tValidation Loss: 2.734872\n",
      "Epoch: 36728 \tTraining Loss: 1.186580 \tValidation Loss: 2.736934\n",
      "Epoch: 36729 \tTraining Loss: 1.231645 \tValidation Loss: 2.735633\n",
      "Epoch: 36730 \tTraining Loss: 1.189175 \tValidation Loss: 2.736245\n",
      "Epoch: 36731 \tTraining Loss: 1.177603 \tValidation Loss: 2.736847\n",
      "Epoch: 36732 \tTraining Loss: 1.163664 \tValidation Loss: 2.736711\n",
      "Epoch: 36733 \tTraining Loss: 1.205412 \tValidation Loss: 2.736220\n",
      "Epoch: 36734 \tTraining Loss: 1.157522 \tValidation Loss: 2.736451\n",
      "Epoch: 36735 \tTraining Loss: 1.181572 \tValidation Loss: 2.734807\n",
      "Epoch: 36736 \tTraining Loss: 1.195277 \tValidation Loss: 2.736946\n",
      "Epoch: 36737 \tTraining Loss: 1.194541 \tValidation Loss: 2.736040\n",
      "Epoch: 36738 \tTraining Loss: 1.165695 \tValidation Loss: 2.736510\n",
      "Epoch: 36739 \tTraining Loss: 1.219551 \tValidation Loss: 2.734142\n",
      "Epoch: 36740 \tTraining Loss: 1.196590 \tValidation Loss: 2.735559\n",
      "Epoch: 36741 \tTraining Loss: 1.166470 \tValidation Loss: 2.735989\n",
      "Epoch: 36742 \tTraining Loss: 1.178984 \tValidation Loss: 2.737100\n",
      "Epoch: 36743 \tTraining Loss: 1.194842 \tValidation Loss: 2.737603\n",
      "Epoch: 36744 \tTraining Loss: 1.137158 \tValidation Loss: 2.737692\n",
      "Epoch: 36745 \tTraining Loss: 1.218343 \tValidation Loss: 2.736354\n",
      "Epoch: 36746 \tTraining Loss: 1.197392 \tValidation Loss: 2.735664\n",
      "Epoch: 36747 \tTraining Loss: 1.254641 \tValidation Loss: 2.736436\n",
      "Epoch: 36748 \tTraining Loss: 1.194989 \tValidation Loss: 2.736431\n",
      "Epoch: 36749 \tTraining Loss: 1.213311 \tValidation Loss: 2.736480\n",
      "Epoch: 36750 \tTraining Loss: 1.196421 \tValidation Loss: 2.736316\n",
      "Epoch: 36751 \tTraining Loss: 1.179696 \tValidation Loss: 2.735853\n",
      "Epoch: 36752 \tTraining Loss: 1.201875 \tValidation Loss: 2.736554\n",
      "Epoch: 36753 \tTraining Loss: 1.196267 \tValidation Loss: 2.737634\n",
      "Epoch: 36754 \tTraining Loss: 1.197607 \tValidation Loss: 2.736111\n",
      "Epoch: 36755 \tTraining Loss: 1.201555 \tValidation Loss: 2.736924\n",
      "Epoch: 36756 \tTraining Loss: 1.184280 \tValidation Loss: 2.736510\n",
      "Epoch: 36757 \tTraining Loss: 1.245355 \tValidation Loss: 2.735885\n",
      "Epoch: 36758 \tTraining Loss: 1.170680 \tValidation Loss: 2.736530\n",
      "Epoch: 36759 \tTraining Loss: 1.200235 \tValidation Loss: 2.736215\n",
      "Epoch: 36760 \tTraining Loss: 1.236137 \tValidation Loss: 2.736830\n",
      "Epoch: 36761 \tTraining Loss: 1.161906 \tValidation Loss: 2.737474\n",
      "Epoch: 36762 \tTraining Loss: 1.247978 \tValidation Loss: 2.736892\n",
      "Epoch: 36763 \tTraining Loss: 1.164788 \tValidation Loss: 2.737553\n",
      "Epoch: 36764 \tTraining Loss: 1.197823 \tValidation Loss: 2.737571\n",
      "Epoch: 36765 \tTraining Loss: 1.213996 \tValidation Loss: 2.736321\n",
      "Epoch: 36766 \tTraining Loss: 1.221582 \tValidation Loss: 2.736615\n",
      "Epoch: 36767 \tTraining Loss: 1.158617 \tValidation Loss: 2.737427\n",
      "Epoch: 36768 \tTraining Loss: 1.203195 \tValidation Loss: 2.737541\n",
      "Epoch: 36769 \tTraining Loss: 1.190890 \tValidation Loss: 2.736873\n",
      "Epoch: 36770 \tTraining Loss: 1.181874 \tValidation Loss: 2.736408\n",
      "Epoch: 36771 \tTraining Loss: 1.189316 \tValidation Loss: 2.736870\n",
      "Epoch: 36772 \tTraining Loss: 1.186605 \tValidation Loss: 2.737588\n",
      "Epoch: 36773 \tTraining Loss: 1.189698 \tValidation Loss: 2.736812\n",
      "Epoch: 36774 \tTraining Loss: 1.163857 \tValidation Loss: 2.736602\n",
      "Epoch: 36775 \tTraining Loss: 1.224248 \tValidation Loss: 2.737705\n",
      "Epoch: 36776 \tTraining Loss: 1.209210 \tValidation Loss: 2.736141\n",
      "Epoch: 36777 \tTraining Loss: 1.169322 \tValidation Loss: 2.738423\n",
      "Epoch: 36778 \tTraining Loss: 1.204558 \tValidation Loss: 2.736326\n",
      "Epoch: 36779 \tTraining Loss: 1.131029 \tValidation Loss: 2.737750\n",
      "Epoch: 36780 \tTraining Loss: 1.175721 \tValidation Loss: 2.739073\n",
      "Epoch: 36781 \tTraining Loss: 1.230718 \tValidation Loss: 2.737284\n",
      "Epoch: 36782 \tTraining Loss: 1.185679 \tValidation Loss: 2.739559\n",
      "Epoch: 36783 \tTraining Loss: 1.172998 \tValidation Loss: 2.737788\n",
      "Epoch: 36784 \tTraining Loss: 1.240366 \tValidation Loss: 2.738688\n",
      "Epoch: 36785 \tTraining Loss: 1.192860 \tValidation Loss: 2.737344\n",
      "Epoch: 36786 \tTraining Loss: 1.171576 \tValidation Loss: 2.737578\n",
      "Epoch: 36787 \tTraining Loss: 1.217308 \tValidation Loss: 2.737448\n",
      "Epoch: 36788 \tTraining Loss: 1.204894 \tValidation Loss: 2.737349\n",
      "Epoch: 36789 \tTraining Loss: 1.209867 \tValidation Loss: 2.738168\n",
      "Epoch: 36790 \tTraining Loss: 1.196867 \tValidation Loss: 2.738127\n",
      "Epoch: 36791 \tTraining Loss: 1.182168 \tValidation Loss: 2.736781\n",
      "Epoch: 36792 \tTraining Loss: 1.163778 \tValidation Loss: 2.737650\n",
      "Epoch: 36793 \tTraining Loss: 1.178590 \tValidation Loss: 2.738525\n",
      "Epoch: 36794 \tTraining Loss: 1.180952 \tValidation Loss: 2.738169\n",
      "Epoch: 36795 \tTraining Loss: 1.229769 \tValidation Loss: 2.737166\n",
      "Epoch: 36796 \tTraining Loss: 1.236840 \tValidation Loss: 2.737705\n",
      "Epoch: 36797 \tTraining Loss: 1.215796 \tValidation Loss: 2.736305\n",
      "Epoch: 36798 \tTraining Loss: 1.202803 \tValidation Loss: 2.736840\n",
      "Epoch: 36799 \tTraining Loss: 1.231846 \tValidation Loss: 2.738880\n",
      "Epoch: 36800 \tTraining Loss: 1.191661 \tValidation Loss: 2.738387\n",
      "Epoch: 36801 \tTraining Loss: 1.265489 \tValidation Loss: 2.736737\n",
      "Epoch: 36802 \tTraining Loss: 1.183261 \tValidation Loss: 2.737835\n",
      "Epoch: 36803 \tTraining Loss: 1.188583 \tValidation Loss: 2.737803\n",
      "Epoch: 36804 \tTraining Loss: 1.199115 \tValidation Loss: 2.738576\n",
      "Epoch: 36805 \tTraining Loss: 1.187984 \tValidation Loss: 2.739665\n",
      "Epoch: 36806 \tTraining Loss: 1.203531 \tValidation Loss: 2.737278\n",
      "Epoch: 36807 \tTraining Loss: 1.190369 \tValidation Loss: 2.737992\n",
      "Epoch: 36808 \tTraining Loss: 1.170539 \tValidation Loss: 2.738286\n",
      "Epoch: 36809 \tTraining Loss: 1.194514 \tValidation Loss: 2.739692\n",
      "Epoch: 36810 \tTraining Loss: 1.170174 \tValidation Loss: 2.738057\n",
      "Epoch: 36811 \tTraining Loss: 1.191413 \tValidation Loss: 2.736030\n",
      "Epoch: 36812 \tTraining Loss: 1.167710 \tValidation Loss: 2.738714\n",
      "Epoch: 36813 \tTraining Loss: 1.186295 \tValidation Loss: 2.739573\n",
      "Epoch: 36814 \tTraining Loss: 1.178418 \tValidation Loss: 2.738228\n",
      "Epoch: 36815 \tTraining Loss: 1.176296 \tValidation Loss: 2.738074\n",
      "Epoch: 36816 \tTraining Loss: 1.215237 \tValidation Loss: 2.738389\n",
      "Epoch: 36817 \tTraining Loss: 1.162653 \tValidation Loss: 2.737245\n",
      "Epoch: 36818 \tTraining Loss: 1.183638 \tValidation Loss: 2.739010\n",
      "Epoch: 36819 \tTraining Loss: 1.201156 \tValidation Loss: 2.737470\n",
      "Epoch: 36820 \tTraining Loss: 1.198427 \tValidation Loss: 2.737661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36821 \tTraining Loss: 1.199269 \tValidation Loss: 2.737580\n",
      "Epoch: 36822 \tTraining Loss: 1.185308 \tValidation Loss: 2.736838\n",
      "Epoch: 36823 \tTraining Loss: 1.222041 \tValidation Loss: 2.738546\n",
      "Epoch: 36824 \tTraining Loss: 1.151772 \tValidation Loss: 2.738251\n",
      "Epoch: 36825 \tTraining Loss: 1.192661 \tValidation Loss: 2.739103\n",
      "Epoch: 36826 \tTraining Loss: 1.183913 \tValidation Loss: 2.738261\n",
      "Epoch: 36827 \tTraining Loss: 1.147356 \tValidation Loss: 2.738726\n",
      "Epoch: 36828 \tTraining Loss: 1.180409 \tValidation Loss: 2.738593\n",
      "Epoch: 36829 \tTraining Loss: 1.215300 \tValidation Loss: 2.738408\n",
      "Epoch: 36830 \tTraining Loss: 1.193555 \tValidation Loss: 2.738561\n",
      "Epoch: 36831 \tTraining Loss: 1.194223 \tValidation Loss: 2.736423\n",
      "Epoch: 36832 \tTraining Loss: 1.189547 \tValidation Loss: 2.737627\n",
      "Epoch: 36833 \tTraining Loss: 1.208506 \tValidation Loss: 2.739076\n",
      "Epoch: 36834 \tTraining Loss: 1.244878 \tValidation Loss: 2.737216\n",
      "Epoch: 36835 \tTraining Loss: 1.196658 \tValidation Loss: 2.738189\n",
      "Epoch: 36836 \tTraining Loss: 1.192257 \tValidation Loss: 2.738528\n",
      "Epoch: 36837 \tTraining Loss: 1.213816 \tValidation Loss: 2.738228\n",
      "Epoch: 36838 \tTraining Loss: 1.230199 \tValidation Loss: 2.737810\n",
      "Epoch: 36839 \tTraining Loss: 1.219860 \tValidation Loss: 2.738504\n",
      "Epoch: 36840 \tTraining Loss: 1.136248 \tValidation Loss: 2.740681\n",
      "Epoch: 36841 \tTraining Loss: 1.188369 \tValidation Loss: 2.738173\n",
      "Epoch: 36842 \tTraining Loss: 1.174113 \tValidation Loss: 2.738770\n",
      "Epoch: 36843 \tTraining Loss: 1.232390 \tValidation Loss: 2.739637\n",
      "Epoch: 36844 \tTraining Loss: 1.163481 \tValidation Loss: 2.739437\n",
      "Epoch: 36845 \tTraining Loss: 1.174891 \tValidation Loss: 2.737718\n",
      "Epoch: 36846 \tTraining Loss: 1.190270 \tValidation Loss: 2.738021\n",
      "Epoch: 36847 \tTraining Loss: 1.187618 \tValidation Loss: 2.738528\n",
      "Epoch: 36848 \tTraining Loss: 1.177524 \tValidation Loss: 2.739155\n",
      "Epoch: 36849 \tTraining Loss: 1.218133 \tValidation Loss: 2.737302\n",
      "Epoch: 36850 \tTraining Loss: 1.183534 \tValidation Loss: 2.739143\n",
      "Epoch: 36851 \tTraining Loss: 1.232740 \tValidation Loss: 2.737626\n",
      "Epoch: 36852 \tTraining Loss: 1.193531 \tValidation Loss: 2.739705\n",
      "Epoch: 36853 \tTraining Loss: 1.183532 \tValidation Loss: 2.738463\n",
      "Epoch: 36854 \tTraining Loss: 1.185015 \tValidation Loss: 2.738367\n",
      "Epoch: 36855 \tTraining Loss: 1.242657 \tValidation Loss: 2.738156\n",
      "Epoch: 36856 \tTraining Loss: 1.215370 \tValidation Loss: 2.739768\n",
      "Epoch: 36857 \tTraining Loss: 1.232271 \tValidation Loss: 2.738869\n",
      "Epoch: 36858 \tTraining Loss: 1.193927 \tValidation Loss: 2.740333\n",
      "Epoch: 36859 \tTraining Loss: 1.202621 \tValidation Loss: 2.737752\n",
      "Epoch: 36860 \tTraining Loss: 1.226240 \tValidation Loss: 2.740603\n",
      "Epoch: 36861 \tTraining Loss: 1.236626 \tValidation Loss: 2.738523\n",
      "Epoch: 36862 \tTraining Loss: 1.190695 \tValidation Loss: 2.739090\n",
      "Epoch: 36863 \tTraining Loss: 1.232864 \tValidation Loss: 2.740056\n",
      "Epoch: 36864 \tTraining Loss: 1.186215 \tValidation Loss: 2.738877\n",
      "Epoch: 36865 \tTraining Loss: 1.222204 \tValidation Loss: 2.739309\n",
      "Epoch: 36866 \tTraining Loss: 1.202744 \tValidation Loss: 2.737867\n",
      "Epoch: 36867 \tTraining Loss: 1.170542 \tValidation Loss: 2.739835\n",
      "Epoch: 36868 \tTraining Loss: 1.209109 \tValidation Loss: 2.740079\n",
      "Epoch: 36869 \tTraining Loss: 1.208662 \tValidation Loss: 2.739495\n",
      "Epoch: 36870 \tTraining Loss: 1.257726 \tValidation Loss: 2.738463\n",
      "Epoch: 36871 \tTraining Loss: 1.217703 \tValidation Loss: 2.736518\n",
      "Epoch: 36872 \tTraining Loss: 1.201208 \tValidation Loss: 2.739697\n",
      "Epoch: 36873 \tTraining Loss: 1.180511 \tValidation Loss: 2.740144\n",
      "Epoch: 36874 \tTraining Loss: 1.229546 \tValidation Loss: 2.740283\n",
      "Epoch: 36875 \tTraining Loss: 1.210767 \tValidation Loss: 2.740865\n",
      "Epoch: 36876 \tTraining Loss: 1.172727 \tValidation Loss: 2.739124\n",
      "Epoch: 36877 \tTraining Loss: 1.183863 \tValidation Loss: 2.741168\n",
      "Epoch: 36878 \tTraining Loss: 1.167806 \tValidation Loss: 2.740233\n",
      "Epoch: 36879 \tTraining Loss: 1.235100 \tValidation Loss: 2.737164\n",
      "Epoch: 36880 \tTraining Loss: 1.135968 \tValidation Loss: 2.738722\n",
      "Epoch: 36881 \tTraining Loss: 1.184176 \tValidation Loss: 2.737560\n",
      "Epoch: 36882 \tTraining Loss: 1.201187 \tValidation Loss: 2.740493\n",
      "Epoch: 36883 \tTraining Loss: 1.202565 \tValidation Loss: 2.740318\n",
      "Epoch: 36884 \tTraining Loss: 1.143023 \tValidation Loss: 2.740031\n",
      "Epoch: 36885 \tTraining Loss: 1.160563 \tValidation Loss: 2.740904\n",
      "Epoch: 36886 \tTraining Loss: 1.183053 \tValidation Loss: 2.740229\n",
      "Epoch: 36887 \tTraining Loss: 1.234120 \tValidation Loss: 2.737572\n",
      "Epoch: 36888 \tTraining Loss: 1.199855 \tValidation Loss: 2.739419\n",
      "Epoch: 36889 \tTraining Loss: 1.196555 \tValidation Loss: 2.740279\n",
      "Epoch: 36890 \tTraining Loss: 1.238223 \tValidation Loss: 2.739700\n",
      "Epoch: 36891 \tTraining Loss: 1.178142 \tValidation Loss: 2.739866\n",
      "Epoch: 36892 \tTraining Loss: 1.221462 \tValidation Loss: 2.738722\n",
      "Epoch: 36893 \tTraining Loss: 1.205037 \tValidation Loss: 2.739897\n",
      "Epoch: 36894 \tTraining Loss: 1.208035 \tValidation Loss: 2.737794\n",
      "Epoch: 36895 \tTraining Loss: 1.175573 \tValidation Loss: 2.740010\n",
      "Epoch: 36896 \tTraining Loss: 1.171255 \tValidation Loss: 2.740931\n",
      "Epoch: 36897 \tTraining Loss: 1.170569 \tValidation Loss: 2.741565\n",
      "Epoch: 36898 \tTraining Loss: 1.169577 \tValidation Loss: 2.739997\n",
      "Epoch: 36899 \tTraining Loss: 1.154566 \tValidation Loss: 2.742058\n",
      "Epoch: 36900 \tTraining Loss: 1.221466 \tValidation Loss: 2.739806\n",
      "Epoch: 36901 \tTraining Loss: 1.163219 \tValidation Loss: 2.740285\n",
      "Epoch: 36902 \tTraining Loss: 1.200404 \tValidation Loss: 2.739048\n",
      "Epoch: 36903 \tTraining Loss: 1.203892 \tValidation Loss: 2.739302\n",
      "Epoch: 36904 \tTraining Loss: 1.199618 \tValidation Loss: 2.740417\n",
      "Epoch: 36905 \tTraining Loss: 1.186699 \tValidation Loss: 2.739071\n",
      "Epoch: 36906 \tTraining Loss: 1.187458 \tValidation Loss: 2.739731\n",
      "Epoch: 36907 \tTraining Loss: 1.209938 \tValidation Loss: 2.739437\n",
      "Epoch: 36908 \tTraining Loss: 1.208041 \tValidation Loss: 2.740609\n",
      "Epoch: 36909 \tTraining Loss: 1.184954 \tValidation Loss: 2.741203\n",
      "Epoch: 36910 \tTraining Loss: 1.163881 \tValidation Loss: 2.741962\n",
      "Epoch: 36911 \tTraining Loss: 1.215664 \tValidation Loss: 2.738520\n",
      "Epoch: 36912 \tTraining Loss: 1.255169 \tValidation Loss: 2.738284\n",
      "Epoch: 36913 \tTraining Loss: 1.163327 \tValidation Loss: 2.740153\n",
      "Epoch: 36914 \tTraining Loss: 1.203191 \tValidation Loss: 2.740654\n",
      "Epoch: 36915 \tTraining Loss: 1.204654 \tValidation Loss: 2.738922\n",
      "Epoch: 36916 \tTraining Loss: 1.246328 \tValidation Loss: 2.740854\n",
      "Epoch: 36917 \tTraining Loss: 1.192161 \tValidation Loss: 2.739625\n",
      "Epoch: 36918 \tTraining Loss: 1.157022 \tValidation Loss: 2.741316\n",
      "Epoch: 36919 \tTraining Loss: 1.177083 \tValidation Loss: 2.739653\n",
      "Epoch: 36920 \tTraining Loss: 1.207438 \tValidation Loss: 2.739946\n",
      "Epoch: 36921 \tTraining Loss: 1.207570 \tValidation Loss: 2.738970\n",
      "Epoch: 36922 \tTraining Loss: 1.163308 \tValidation Loss: 2.741919\n",
      "Epoch: 36923 \tTraining Loss: 1.153571 \tValidation Loss: 2.739965\n",
      "Epoch: 36924 \tTraining Loss: 1.197101 \tValidation Loss: 2.740501\n",
      "Epoch: 36925 \tTraining Loss: 1.196030 \tValidation Loss: 2.740887\n",
      "Epoch: 36926 \tTraining Loss: 1.171200 \tValidation Loss: 2.742488\n",
      "Epoch: 36927 \tTraining Loss: 1.209487 \tValidation Loss: 2.739391\n",
      "Epoch: 36928 \tTraining Loss: 1.190840 \tValidation Loss: 2.740846\n",
      "Epoch: 36929 \tTraining Loss: 1.186724 \tValidation Loss: 2.741889\n",
      "Epoch: 36930 \tTraining Loss: 1.174948 \tValidation Loss: 2.741867\n",
      "Epoch: 36931 \tTraining Loss: 1.183114 \tValidation Loss: 2.741099\n",
      "Epoch: 36932 \tTraining Loss: 1.191518 \tValidation Loss: 2.739960\n",
      "Epoch: 36933 \tTraining Loss: 1.258870 \tValidation Loss: 2.740512\n",
      "Epoch: 36934 \tTraining Loss: 1.140250 \tValidation Loss: 2.739100\n",
      "Epoch: 36935 \tTraining Loss: 1.227945 \tValidation Loss: 2.739639\n",
      "Epoch: 36936 \tTraining Loss: 1.200420 \tValidation Loss: 2.740148\n",
      "Epoch: 36937 \tTraining Loss: 1.208358 \tValidation Loss: 2.739988\n",
      "Epoch: 36938 \tTraining Loss: 1.228634 \tValidation Loss: 2.739142\n",
      "Epoch: 36939 \tTraining Loss: 1.207609 \tValidation Loss: 2.741720\n",
      "Epoch: 36940 \tTraining Loss: 1.190886 \tValidation Loss: 2.739303\n",
      "Epoch: 36941 \tTraining Loss: 1.184079 \tValidation Loss: 2.739927\n",
      "Epoch: 36942 \tTraining Loss: 1.174558 \tValidation Loss: 2.742046\n",
      "Epoch: 36943 \tTraining Loss: 1.176034 \tValidation Loss: 2.740761\n",
      "Epoch: 36944 \tTraining Loss: 1.196141 \tValidation Loss: 2.740864\n",
      "Epoch: 36945 \tTraining Loss: 1.173588 \tValidation Loss: 2.742779\n",
      "Epoch: 36946 \tTraining Loss: 1.177969 \tValidation Loss: 2.740452\n",
      "Epoch: 36947 \tTraining Loss: 1.187638 \tValidation Loss: 2.741685\n",
      "Epoch: 36948 \tTraining Loss: 1.198572 \tValidation Loss: 2.741114\n",
      "Epoch: 36949 \tTraining Loss: 1.167745 \tValidation Loss: 2.741828\n",
      "Epoch: 36950 \tTraining Loss: 1.191275 \tValidation Loss: 2.741593\n",
      "Epoch: 36951 \tTraining Loss: 1.179854 \tValidation Loss: 2.742781\n",
      "Epoch: 36952 \tTraining Loss: 1.190451 \tValidation Loss: 2.742332\n",
      "Epoch: 36953 \tTraining Loss: 1.217562 \tValidation Loss: 2.741073\n",
      "Epoch: 36954 \tTraining Loss: 1.231943 \tValidation Loss: 2.740529\n",
      "Epoch: 36955 \tTraining Loss: 1.211621 \tValidation Loss: 2.739113\n",
      "Epoch: 36956 \tTraining Loss: 1.206723 \tValidation Loss: 2.739022\n",
      "Epoch: 36957 \tTraining Loss: 1.167162 \tValidation Loss: 2.740286\n",
      "Epoch: 36958 \tTraining Loss: 1.248380 \tValidation Loss: 2.741132\n",
      "Epoch: 36959 \tTraining Loss: 1.207226 \tValidation Loss: 2.740624\n",
      "Epoch: 36960 \tTraining Loss: 1.164773 \tValidation Loss: 2.742604\n",
      "Epoch: 36961 \tTraining Loss: 1.191765 \tValidation Loss: 2.741174\n",
      "Epoch: 36962 \tTraining Loss: 1.214725 \tValidation Loss: 2.739313\n",
      "Epoch: 36963 \tTraining Loss: 1.210003 \tValidation Loss: 2.741150\n",
      "Epoch: 36964 \tTraining Loss: 1.198165 \tValidation Loss: 2.740275\n",
      "Epoch: 36965 \tTraining Loss: 1.169960 \tValidation Loss: 2.739208\n",
      "Epoch: 36966 \tTraining Loss: 1.183189 \tValidation Loss: 2.740626\n",
      "Epoch: 36967 \tTraining Loss: 1.180641 \tValidation Loss: 2.742412\n",
      "Epoch: 36968 \tTraining Loss: 1.218397 \tValidation Loss: 2.741027\n",
      "Epoch: 36969 \tTraining Loss: 1.190465 \tValidation Loss: 2.742390\n",
      "Epoch: 36970 \tTraining Loss: 1.198360 \tValidation Loss: 2.741222\n",
      "Epoch: 36971 \tTraining Loss: 1.221992 \tValidation Loss: 2.739333\n",
      "Epoch: 36972 \tTraining Loss: 1.186420 \tValidation Loss: 2.742604\n",
      "Epoch: 36973 \tTraining Loss: 1.176473 \tValidation Loss: 2.740540\n",
      "Epoch: 36974 \tTraining Loss: 1.186835 \tValidation Loss: 2.741349\n",
      "Epoch: 36975 \tTraining Loss: 1.217322 \tValidation Loss: 2.740289\n",
      "Epoch: 36976 \tTraining Loss: 1.162121 \tValidation Loss: 2.742575\n",
      "Epoch: 36977 \tTraining Loss: 1.212814 \tValidation Loss: 2.740724\n",
      "Epoch: 36978 \tTraining Loss: 1.164216 \tValidation Loss: 2.743330\n",
      "Epoch: 36979 \tTraining Loss: 1.172458 \tValidation Loss: 2.743577\n",
      "Epoch: 36980 \tTraining Loss: 1.207428 \tValidation Loss: 2.742855\n",
      "Epoch: 36981 \tTraining Loss: 1.201268 \tValidation Loss: 2.741073\n",
      "Epoch: 36982 \tTraining Loss: 1.172162 \tValidation Loss: 2.743195\n",
      "Epoch: 36983 \tTraining Loss: 1.161869 \tValidation Loss: 2.742055\n",
      "Epoch: 36984 \tTraining Loss: 1.194290 \tValidation Loss: 2.741900\n",
      "Epoch: 36985 \tTraining Loss: 1.202763 \tValidation Loss: 2.741617\n",
      "Epoch: 36986 \tTraining Loss: 1.138011 \tValidation Loss: 2.742455\n",
      "Epoch: 36987 \tTraining Loss: 1.215833 \tValidation Loss: 2.742758\n",
      "Epoch: 36988 \tTraining Loss: 1.156117 \tValidation Loss: 2.741369\n",
      "Epoch: 36989 \tTraining Loss: 1.216587 \tValidation Loss: 2.742082\n",
      "Epoch: 36990 \tTraining Loss: 1.179594 \tValidation Loss: 2.742643\n",
      "Epoch: 36991 \tTraining Loss: 1.162511 \tValidation Loss: 2.742767\n",
      "Epoch: 36992 \tTraining Loss: 1.151528 \tValidation Loss: 2.743179\n",
      "Epoch: 36993 \tTraining Loss: 1.219430 \tValidation Loss: 2.742129\n",
      "Epoch: 36994 \tTraining Loss: 1.123509 \tValidation Loss: 2.742009\n",
      "Epoch: 36995 \tTraining Loss: 1.158616 \tValidation Loss: 2.741531\n",
      "Epoch: 36996 \tTraining Loss: 1.203995 \tValidation Loss: 2.743247\n",
      "Epoch: 36997 \tTraining Loss: 1.148087 \tValidation Loss: 2.743269\n",
      "Epoch: 36998 \tTraining Loss: 1.146412 \tValidation Loss: 2.742077\n",
      "Epoch: 36999 \tTraining Loss: 1.195924 \tValidation Loss: 2.742462\n",
      "Epoch: 37000 \tTraining Loss: 1.232446 \tValidation Loss: 2.741748\n",
      "Epoch: 37001 \tTraining Loss: 1.233795 \tValidation Loss: 2.742369\n",
      "Epoch: 37002 \tTraining Loss: 1.208157 \tValidation Loss: 2.744236\n",
      "Epoch: 37003 \tTraining Loss: 1.218946 \tValidation Loss: 2.741720\n",
      "Epoch: 37004 \tTraining Loss: 1.172561 \tValidation Loss: 2.742532\n",
      "Epoch: 37005 \tTraining Loss: 1.174288 \tValidation Loss: 2.741445\n",
      "Epoch: 37006 \tTraining Loss: 1.140175 \tValidation Loss: 2.742059\n",
      "Epoch: 37007 \tTraining Loss: 1.171844 \tValidation Loss: 2.742072\n",
      "Epoch: 37008 \tTraining Loss: 1.171458 \tValidation Loss: 2.742312\n",
      "Epoch: 37009 \tTraining Loss: 1.208330 \tValidation Loss: 2.742990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37010 \tTraining Loss: 1.205743 \tValidation Loss: 2.740951\n",
      "Epoch: 37011 \tTraining Loss: 1.232511 \tValidation Loss: 2.741247\n",
      "Epoch: 37012 \tTraining Loss: 1.156077 \tValidation Loss: 2.742234\n",
      "Epoch: 37013 \tTraining Loss: 1.201150 \tValidation Loss: 2.741621\n",
      "Epoch: 37014 \tTraining Loss: 1.184165 \tValidation Loss: 2.741700\n",
      "Epoch: 37015 \tTraining Loss: 1.224772 \tValidation Loss: 2.741995\n",
      "Epoch: 37016 \tTraining Loss: 1.196071 \tValidation Loss: 2.742203\n",
      "Epoch: 37017 \tTraining Loss: 1.183116 \tValidation Loss: 2.742565\n",
      "Epoch: 37018 \tTraining Loss: 1.226731 \tValidation Loss: 2.740961\n",
      "Epoch: 37019 \tTraining Loss: 1.226134 \tValidation Loss: 2.742700\n",
      "Epoch: 37020 \tTraining Loss: 1.182374 \tValidation Loss: 2.743154\n",
      "Epoch: 37021 \tTraining Loss: 1.178453 \tValidation Loss: 2.743124\n",
      "Epoch: 37022 \tTraining Loss: 1.209154 \tValidation Loss: 2.743187\n",
      "Epoch: 37023 \tTraining Loss: 1.171095 \tValidation Loss: 2.743143\n",
      "Epoch: 37024 \tTraining Loss: 1.185434 \tValidation Loss: 2.743441\n",
      "Epoch: 37025 \tTraining Loss: 1.190790 \tValidation Loss: 2.743422\n",
      "Epoch: 37026 \tTraining Loss: 1.146854 \tValidation Loss: 2.741992\n",
      "Epoch: 37027 \tTraining Loss: 1.173184 \tValidation Loss: 2.742575\n",
      "Epoch: 37028 \tTraining Loss: 1.195460 \tValidation Loss: 2.742889\n",
      "Epoch: 37029 \tTraining Loss: 1.206203 \tValidation Loss: 2.744129\n",
      "Epoch: 37030 \tTraining Loss: 1.196430 \tValidation Loss: 2.741148\n",
      "Epoch: 37031 \tTraining Loss: 1.223363 \tValidation Loss: 2.742450\n",
      "Epoch: 37032 \tTraining Loss: 1.171114 \tValidation Loss: 2.744003\n",
      "Epoch: 37033 \tTraining Loss: 1.239623 \tValidation Loss: 2.741998\n",
      "Epoch: 37034 \tTraining Loss: 1.213752 \tValidation Loss: 2.741741\n",
      "Epoch: 37035 \tTraining Loss: 1.177712 \tValidation Loss: 2.743423\n",
      "Epoch: 37036 \tTraining Loss: 1.201224 \tValidation Loss: 2.743119\n",
      "Epoch: 37037 \tTraining Loss: 1.194711 \tValidation Loss: 2.743796\n",
      "Epoch: 37038 \tTraining Loss: 1.186615 \tValidation Loss: 2.744777\n",
      "Epoch: 37039 \tTraining Loss: 1.184052 \tValidation Loss: 2.741930\n",
      "Epoch: 37040 \tTraining Loss: 1.171231 \tValidation Loss: 2.743231\n",
      "Epoch: 37041 \tTraining Loss: 1.163270 \tValidation Loss: 2.742199\n",
      "Epoch: 37042 \tTraining Loss: 1.208175 \tValidation Loss: 2.739765\n",
      "Epoch: 37043 \tTraining Loss: 1.148509 \tValidation Loss: 2.742512\n",
      "Epoch: 37044 \tTraining Loss: 1.197782 \tValidation Loss: 2.742865\n",
      "Epoch: 37045 \tTraining Loss: 1.214634 \tValidation Loss: 2.741521\n",
      "Epoch: 37046 \tTraining Loss: 1.202887 \tValidation Loss: 2.743181\n",
      "Epoch: 37047 \tTraining Loss: 1.168664 \tValidation Loss: 2.744568\n",
      "Epoch: 37048 \tTraining Loss: 1.185127 \tValidation Loss: 2.744685\n",
      "Epoch: 37049 \tTraining Loss: 1.170493 \tValidation Loss: 2.744567\n",
      "Epoch: 37050 \tTraining Loss: 1.176822 \tValidation Loss: 2.742697\n",
      "Epoch: 37051 \tTraining Loss: 1.181728 \tValidation Loss: 2.742685\n",
      "Epoch: 37052 \tTraining Loss: 1.135972 \tValidation Loss: 2.744007\n",
      "Epoch: 37053 \tTraining Loss: 1.200549 \tValidation Loss: 2.746137\n",
      "Epoch: 37054 \tTraining Loss: 1.137570 \tValidation Loss: 2.743182\n",
      "Epoch: 37055 \tTraining Loss: 1.241012 \tValidation Loss: 2.742762\n",
      "Epoch: 37056 \tTraining Loss: 1.213492 \tValidation Loss: 2.743303\n",
      "Epoch: 37057 \tTraining Loss: 1.161208 \tValidation Loss: 2.744136\n",
      "Epoch: 37058 \tTraining Loss: 1.180267 \tValidation Loss: 2.744161\n",
      "Epoch: 37059 \tTraining Loss: 1.190538 \tValidation Loss: 2.743864\n",
      "Epoch: 37060 \tTraining Loss: 1.168900 \tValidation Loss: 2.745038\n",
      "Epoch: 37061 \tTraining Loss: 1.239666 \tValidation Loss: 2.742363\n",
      "Epoch: 37062 \tTraining Loss: 1.211666 \tValidation Loss: 2.743559\n",
      "Epoch: 37063 \tTraining Loss: 1.196096 \tValidation Loss: 2.743383\n",
      "Epoch: 37064 \tTraining Loss: 1.186126 \tValidation Loss: 2.744168\n",
      "Epoch: 37065 \tTraining Loss: 1.195460 \tValidation Loss: 2.744162\n",
      "Epoch: 37066 \tTraining Loss: 1.178646 \tValidation Loss: 2.745210\n",
      "Epoch: 37067 \tTraining Loss: 1.173708 \tValidation Loss: 2.742645\n",
      "Epoch: 37068 \tTraining Loss: 1.200591 \tValidation Loss: 2.744308\n",
      "Epoch: 37069 \tTraining Loss: 1.174488 \tValidation Loss: 2.743715\n",
      "Epoch: 37070 \tTraining Loss: 1.209892 \tValidation Loss: 2.741956\n",
      "Epoch: 37071 \tTraining Loss: 1.193501 \tValidation Loss: 2.743762\n",
      "Epoch: 37072 \tTraining Loss: 1.172623 \tValidation Loss: 2.742372\n",
      "Epoch: 37073 \tTraining Loss: 1.193531 \tValidation Loss: 2.743979\n",
      "Epoch: 37074 \tTraining Loss: 1.219668 \tValidation Loss: 2.744706\n",
      "Epoch: 37075 \tTraining Loss: 1.188978 \tValidation Loss: 2.742555\n",
      "Epoch: 37076 \tTraining Loss: 1.196573 \tValidation Loss: 2.743179\n",
      "Epoch: 37077 \tTraining Loss: 1.162937 \tValidation Loss: 2.743923\n",
      "Epoch: 37078 \tTraining Loss: 1.197426 \tValidation Loss: 2.744750\n",
      "Epoch: 37079 \tTraining Loss: 1.240465 \tValidation Loss: 2.741300\n",
      "Epoch: 37080 \tTraining Loss: 1.161680 \tValidation Loss: 2.744022\n",
      "Epoch: 37081 \tTraining Loss: 1.236304 \tValidation Loss: 2.742820\n",
      "Epoch: 37082 \tTraining Loss: 1.209307 \tValidation Loss: 2.743477\n",
      "Epoch: 37083 \tTraining Loss: 1.143345 \tValidation Loss: 2.744659\n",
      "Epoch: 37084 \tTraining Loss: 1.204901 \tValidation Loss: 2.744051\n",
      "Epoch: 37085 \tTraining Loss: 1.175110 \tValidation Loss: 2.743736\n",
      "Epoch: 37086 \tTraining Loss: 1.151195 \tValidation Loss: 2.746927\n",
      "Epoch: 37087 \tTraining Loss: 1.173586 \tValidation Loss: 2.744787\n",
      "Epoch: 37088 \tTraining Loss: 1.180110 \tValidation Loss: 2.744450\n",
      "Epoch: 37089 \tTraining Loss: 1.170492 \tValidation Loss: 2.744223\n",
      "Epoch: 37090 \tTraining Loss: 1.192991 \tValidation Loss: 2.743114\n",
      "Epoch: 37091 \tTraining Loss: 1.210650 \tValidation Loss: 2.745439\n",
      "Epoch: 37092 \tTraining Loss: 1.183114 \tValidation Loss: 2.744507\n",
      "Epoch: 37093 \tTraining Loss: 1.266726 \tValidation Loss: 2.743991\n",
      "Epoch: 37094 \tTraining Loss: 1.167570 \tValidation Loss: 2.743308\n",
      "Epoch: 37095 \tTraining Loss: 1.188280 \tValidation Loss: 2.744641\n",
      "Epoch: 37096 \tTraining Loss: 1.233148 \tValidation Loss: 2.743436\n",
      "Epoch: 37097 \tTraining Loss: 1.209234 \tValidation Loss: 2.744588\n",
      "Epoch: 37098 \tTraining Loss: 1.167793 \tValidation Loss: 2.744669\n",
      "Epoch: 37099 \tTraining Loss: 1.180831 \tValidation Loss: 2.744387\n",
      "Epoch: 37100 \tTraining Loss: 1.200390 \tValidation Loss: 2.743548\n",
      "Epoch: 37101 \tTraining Loss: 1.157383 \tValidation Loss: 2.743697\n",
      "Epoch: 37102 \tTraining Loss: 1.185279 \tValidation Loss: 2.743005\n",
      "Epoch: 37103 \tTraining Loss: 1.178683 \tValidation Loss: 2.745961\n",
      "Epoch: 37104 \tTraining Loss: 1.207862 \tValidation Loss: 2.743258\n",
      "Epoch: 37105 \tTraining Loss: 1.245963 \tValidation Loss: 2.744029\n",
      "Epoch: 37106 \tTraining Loss: 1.175568 \tValidation Loss: 2.744398\n",
      "Epoch: 37107 \tTraining Loss: 1.215269 \tValidation Loss: 2.745013\n",
      "Epoch: 37108 \tTraining Loss: 1.178664 \tValidation Loss: 2.745370\n",
      "Epoch: 37109 \tTraining Loss: 1.195352 \tValidation Loss: 2.744738\n",
      "Epoch: 37110 \tTraining Loss: 1.205828 \tValidation Loss: 2.744121\n",
      "Epoch: 37111 \tTraining Loss: 1.164759 \tValidation Loss: 2.745299\n",
      "Epoch: 37112 \tTraining Loss: 1.207565 \tValidation Loss: 2.744042\n",
      "Epoch: 37113 \tTraining Loss: 1.169272 \tValidation Loss: 2.744853\n",
      "Epoch: 37114 \tTraining Loss: 1.224823 \tValidation Loss: 2.746687\n",
      "Epoch: 37115 \tTraining Loss: 1.225388 \tValidation Loss: 2.744292\n",
      "Epoch: 37116 \tTraining Loss: 1.163524 \tValidation Loss: 2.746037\n",
      "Epoch: 37117 \tTraining Loss: 1.199654 \tValidation Loss: 2.744814\n",
      "Epoch: 37118 \tTraining Loss: 1.194469 \tValidation Loss: 2.745143\n",
      "Epoch: 37119 \tTraining Loss: 1.189782 \tValidation Loss: 2.745013\n",
      "Epoch: 37120 \tTraining Loss: 1.208035 \tValidation Loss: 2.744958\n",
      "Epoch: 37121 \tTraining Loss: 1.232884 \tValidation Loss: 2.743737\n",
      "Epoch: 37122 \tTraining Loss: 1.139398 \tValidation Loss: 2.745889\n",
      "Epoch: 37123 \tTraining Loss: 1.227497 \tValidation Loss: 2.744867\n",
      "Epoch: 37124 \tTraining Loss: 1.170961 \tValidation Loss: 2.745748\n",
      "Epoch: 37125 \tTraining Loss: 1.211740 \tValidation Loss: 2.745456\n",
      "Epoch: 37126 \tTraining Loss: 1.186602 \tValidation Loss: 2.746732\n",
      "Epoch: 37127 \tTraining Loss: 1.212717 \tValidation Loss: 2.743103\n",
      "Epoch: 37128 \tTraining Loss: 1.186050 \tValidation Loss: 2.745484\n",
      "Epoch: 37129 \tTraining Loss: 1.201187 \tValidation Loss: 2.744468\n",
      "Epoch: 37130 \tTraining Loss: 1.235395 \tValidation Loss: 2.743383\n",
      "Epoch: 37131 \tTraining Loss: 1.174359 \tValidation Loss: 2.745477\n",
      "Epoch: 37132 \tTraining Loss: 1.197071 \tValidation Loss: 2.744931\n",
      "Epoch: 37133 \tTraining Loss: 1.197902 \tValidation Loss: 2.745130\n",
      "Epoch: 37134 \tTraining Loss: 1.150515 \tValidation Loss: 2.744941\n",
      "Epoch: 37135 \tTraining Loss: 1.217769 \tValidation Loss: 2.745720\n",
      "Epoch: 37136 \tTraining Loss: 1.218058 \tValidation Loss: 2.744725\n",
      "Epoch: 37137 \tTraining Loss: 1.199475 \tValidation Loss: 2.746365\n",
      "Epoch: 37138 \tTraining Loss: 1.202828 \tValidation Loss: 2.744186\n",
      "Epoch: 37139 \tTraining Loss: 1.237468 \tValidation Loss: 2.745625\n",
      "Epoch: 37140 \tTraining Loss: 1.148500 \tValidation Loss: 2.746667\n",
      "Epoch: 37141 \tTraining Loss: 1.149229 \tValidation Loss: 2.746474\n",
      "Epoch: 37142 \tTraining Loss: 1.168351 \tValidation Loss: 2.745947\n",
      "Epoch: 37143 \tTraining Loss: 1.165648 \tValidation Loss: 2.746855\n",
      "Epoch: 37144 \tTraining Loss: 1.199149 \tValidation Loss: 2.744944\n",
      "Epoch: 37145 \tTraining Loss: 1.199946 \tValidation Loss: 2.746897\n",
      "Epoch: 37146 \tTraining Loss: 1.228207 \tValidation Loss: 2.746250\n",
      "Epoch: 37147 \tTraining Loss: 1.187788 \tValidation Loss: 2.747478\n",
      "Epoch: 37148 \tTraining Loss: 1.170341 \tValidation Loss: 2.746902\n",
      "Epoch: 37149 \tTraining Loss: 1.154394 \tValidation Loss: 2.746391\n",
      "Epoch: 37150 \tTraining Loss: 1.191909 \tValidation Loss: 2.745481\n",
      "Epoch: 37151 \tTraining Loss: 1.155336 \tValidation Loss: 2.746713\n",
      "Epoch: 37152 \tTraining Loss: 1.151435 \tValidation Loss: 2.747049\n",
      "Epoch: 37153 \tTraining Loss: 1.180175 \tValidation Loss: 2.745391\n",
      "Epoch: 37154 \tTraining Loss: 1.218740 \tValidation Loss: 2.745568\n",
      "Epoch: 37155 \tTraining Loss: 1.189096 \tValidation Loss: 2.745195\n",
      "Epoch: 37156 \tTraining Loss: 1.238167 \tValidation Loss: 2.745308\n",
      "Epoch: 37157 \tTraining Loss: 1.228994 \tValidation Loss: 2.745370\n",
      "Epoch: 37158 \tTraining Loss: 1.186221 \tValidation Loss: 2.745538\n",
      "Epoch: 37159 \tTraining Loss: 1.219117 \tValidation Loss: 2.746836\n",
      "Epoch: 37160 \tTraining Loss: 1.210702 \tValidation Loss: 2.746017\n",
      "Epoch: 37161 \tTraining Loss: 1.179520 \tValidation Loss: 2.745373\n",
      "Epoch: 37162 \tTraining Loss: 1.218837 \tValidation Loss: 2.746247\n",
      "Epoch: 37163 \tTraining Loss: 1.196381 \tValidation Loss: 2.746486\n",
      "Epoch: 37164 \tTraining Loss: 1.194080 \tValidation Loss: 2.745798\n",
      "Epoch: 37165 \tTraining Loss: 1.203005 \tValidation Loss: 2.745869\n",
      "Epoch: 37166 \tTraining Loss: 1.169061 \tValidation Loss: 2.745915\n",
      "Epoch: 37167 \tTraining Loss: 1.198730 \tValidation Loss: 2.745785\n",
      "Epoch: 37168 \tTraining Loss: 1.176267 \tValidation Loss: 2.746038\n",
      "Epoch: 37169 \tTraining Loss: 1.213330 \tValidation Loss: 2.746419\n",
      "Epoch: 37170 \tTraining Loss: 1.234921 \tValidation Loss: 2.743379\n",
      "Epoch: 37171 \tTraining Loss: 1.230698 \tValidation Loss: 2.743244\n",
      "Epoch: 37172 \tTraining Loss: 1.202099 \tValidation Loss: 2.745775\n",
      "Epoch: 37173 \tTraining Loss: 1.234227 \tValidation Loss: 2.744728\n",
      "Epoch: 37174 \tTraining Loss: 1.128205 \tValidation Loss: 2.746575\n",
      "Epoch: 37175 \tTraining Loss: 1.188659 \tValidation Loss: 2.747341\n",
      "Epoch: 37176 \tTraining Loss: 1.204015 \tValidation Loss: 2.746145\n",
      "Epoch: 37177 \tTraining Loss: 1.192030 \tValidation Loss: 2.745965\n",
      "Epoch: 37178 \tTraining Loss: 1.141873 \tValidation Loss: 2.747745\n",
      "Epoch: 37179 \tTraining Loss: 1.184610 \tValidation Loss: 2.745377\n",
      "Epoch: 37180 \tTraining Loss: 1.175645 \tValidation Loss: 2.746847\n",
      "Epoch: 37181 \tTraining Loss: 1.228005 \tValidation Loss: 2.745538\n",
      "Epoch: 37182 \tTraining Loss: 1.176587 \tValidation Loss: 2.747316\n",
      "Epoch: 37183 \tTraining Loss: 1.259578 \tValidation Loss: 2.745023\n",
      "Epoch: 37184 \tTraining Loss: 1.194052 \tValidation Loss: 2.747397\n",
      "Epoch: 37185 \tTraining Loss: 1.192457 \tValidation Loss: 2.745286\n",
      "Epoch: 37186 \tTraining Loss: 1.184931 \tValidation Loss: 2.746806\n",
      "Epoch: 37187 \tTraining Loss: 1.209342 \tValidation Loss: 2.748501\n",
      "Epoch: 37188 \tTraining Loss: 1.198136 \tValidation Loss: 2.744368\n",
      "Epoch: 37189 \tTraining Loss: 1.180563 \tValidation Loss: 2.746541\n",
      "Epoch: 37190 \tTraining Loss: 1.182825 \tValidation Loss: 2.746896\n",
      "Epoch: 37191 \tTraining Loss: 1.167184 \tValidation Loss: 2.747612\n",
      "Epoch: 37192 \tTraining Loss: 1.211141 \tValidation Loss: 2.746614\n",
      "Epoch: 37193 \tTraining Loss: 1.184150 \tValidation Loss: 2.747016\n",
      "Epoch: 37194 \tTraining Loss: 1.143041 \tValidation Loss: 2.747204\n",
      "Epoch: 37195 \tTraining Loss: 1.216590 \tValidation Loss: 2.745879\n",
      "Epoch: 37196 \tTraining Loss: 1.167846 \tValidation Loss: 2.748580\n",
      "Epoch: 37197 \tTraining Loss: 1.188654 \tValidation Loss: 2.747209\n",
      "Epoch: 37198 \tTraining Loss: 1.215707 \tValidation Loss: 2.746291\n",
      "Epoch: 37199 \tTraining Loss: 1.202011 \tValidation Loss: 2.746937\n",
      "Epoch: 37200 \tTraining Loss: 1.163296 \tValidation Loss: 2.747198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37201 \tTraining Loss: 1.211741 \tValidation Loss: 2.746875\n",
      "Epoch: 37202 \tTraining Loss: 1.164518 \tValidation Loss: 2.747360\n",
      "Epoch: 37203 \tTraining Loss: 1.175350 \tValidation Loss: 2.747611\n",
      "Epoch: 37204 \tTraining Loss: 1.178616 \tValidation Loss: 2.747911\n",
      "Epoch: 37205 \tTraining Loss: 1.164954 \tValidation Loss: 2.747501\n",
      "Epoch: 37206 \tTraining Loss: 1.168264 \tValidation Loss: 2.747307\n",
      "Epoch: 37207 \tTraining Loss: 1.189235 \tValidation Loss: 2.747579\n",
      "Epoch: 37208 \tTraining Loss: 1.212260 \tValidation Loss: 2.747480\n",
      "Epoch: 37209 \tTraining Loss: 1.236631 \tValidation Loss: 2.746801\n",
      "Epoch: 37210 \tTraining Loss: 1.208210 \tValidation Loss: 2.746150\n",
      "Epoch: 37211 \tTraining Loss: 1.174623 \tValidation Loss: 2.747130\n",
      "Epoch: 37212 \tTraining Loss: 1.167582 \tValidation Loss: 2.747247\n",
      "Epoch: 37213 \tTraining Loss: 1.152176 \tValidation Loss: 2.748546\n",
      "Epoch: 37214 \tTraining Loss: 1.203303 \tValidation Loss: 2.749201\n",
      "Epoch: 37215 \tTraining Loss: 1.192442 \tValidation Loss: 2.748084\n",
      "Epoch: 37216 \tTraining Loss: 1.145001 \tValidation Loss: 2.747907\n",
      "Epoch: 37217 \tTraining Loss: 1.206070 \tValidation Loss: 2.747015\n",
      "Epoch: 37218 \tTraining Loss: 1.222964 \tValidation Loss: 2.747433\n",
      "Epoch: 37219 \tTraining Loss: 1.195347 \tValidation Loss: 2.747285\n",
      "Epoch: 37220 \tTraining Loss: 1.197933 \tValidation Loss: 2.748241\n",
      "Epoch: 37221 \tTraining Loss: 1.226056 \tValidation Loss: 2.746549\n",
      "Epoch: 37222 \tTraining Loss: 1.167897 \tValidation Loss: 2.745651\n",
      "Epoch: 37223 \tTraining Loss: 1.228769 \tValidation Loss: 2.747256\n",
      "Epoch: 37224 \tTraining Loss: 1.228261 \tValidation Loss: 2.746133\n",
      "Epoch: 37225 \tTraining Loss: 1.199732 \tValidation Loss: 2.747163\n",
      "Epoch: 37226 \tTraining Loss: 1.208482 \tValidation Loss: 2.745179\n",
      "Epoch: 37227 \tTraining Loss: 1.216526 \tValidation Loss: 2.746434\n",
      "Epoch: 37228 \tTraining Loss: 1.129034 \tValidation Loss: 2.747226\n",
      "Epoch: 37229 \tTraining Loss: 1.175063 \tValidation Loss: 2.748573\n",
      "Epoch: 37230 \tTraining Loss: 1.180968 \tValidation Loss: 2.747782\n",
      "Epoch: 37231 \tTraining Loss: 1.201549 \tValidation Loss: 2.747740\n",
      "Epoch: 37232 \tTraining Loss: 1.209404 \tValidation Loss: 2.747350\n",
      "Epoch: 37233 \tTraining Loss: 1.194310 \tValidation Loss: 2.747557\n",
      "Epoch: 37234 \tTraining Loss: 1.203736 \tValidation Loss: 2.748036\n",
      "Epoch: 37235 \tTraining Loss: 1.169125 \tValidation Loss: 2.747571\n",
      "Epoch: 37236 \tTraining Loss: 1.124073 \tValidation Loss: 2.748486\n",
      "Epoch: 37237 \tTraining Loss: 1.204027 \tValidation Loss: 2.746615\n",
      "Epoch: 37238 \tTraining Loss: 1.192592 \tValidation Loss: 2.749131\n",
      "Epoch: 37239 \tTraining Loss: 1.199554 \tValidation Loss: 2.747369\n",
      "Epoch: 37240 \tTraining Loss: 1.193315 \tValidation Loss: 2.746167\n",
      "Epoch: 37241 \tTraining Loss: 1.228073 \tValidation Loss: 2.748356\n",
      "Epoch: 37242 \tTraining Loss: 1.158249 \tValidation Loss: 2.747039\n",
      "Epoch: 37243 \tTraining Loss: 1.210608 \tValidation Loss: 2.748261\n",
      "Epoch: 37244 \tTraining Loss: 1.174743 \tValidation Loss: 2.748039\n",
      "Epoch: 37245 \tTraining Loss: 1.181714 \tValidation Loss: 2.748734\n",
      "Epoch: 37246 \tTraining Loss: 1.136935 \tValidation Loss: 2.747737\n",
      "Epoch: 37247 \tTraining Loss: 1.203153 \tValidation Loss: 2.747580\n",
      "Epoch: 37248 \tTraining Loss: 1.171982 \tValidation Loss: 2.749120\n",
      "Epoch: 37249 \tTraining Loss: 1.149395 \tValidation Loss: 2.747689\n",
      "Epoch: 37250 \tTraining Loss: 1.222631 \tValidation Loss: 2.745781\n",
      "Epoch: 37251 \tTraining Loss: 1.183400 \tValidation Loss: 2.746893\n",
      "Epoch: 37252 \tTraining Loss: 1.217707 \tValidation Loss: 2.748579\n",
      "Epoch: 37253 \tTraining Loss: 1.187103 \tValidation Loss: 2.748249\n",
      "Epoch: 37254 \tTraining Loss: 1.227248 \tValidation Loss: 2.748526\n",
      "Epoch: 37255 \tTraining Loss: 1.182096 \tValidation Loss: 2.748220\n",
      "Epoch: 37256 \tTraining Loss: 1.187457 \tValidation Loss: 2.748484\n",
      "Epoch: 37257 \tTraining Loss: 1.229812 \tValidation Loss: 2.746031\n",
      "Epoch: 37258 \tTraining Loss: 1.188766 \tValidation Loss: 2.748060\n",
      "Epoch: 37259 \tTraining Loss: 1.189414 \tValidation Loss: 2.749218\n",
      "Epoch: 37260 \tTraining Loss: 1.241415 \tValidation Loss: 2.747128\n",
      "Epoch: 37261 \tTraining Loss: 1.200730 \tValidation Loss: 2.748612\n",
      "Epoch: 37262 \tTraining Loss: 1.183257 \tValidation Loss: 2.749533\n",
      "Epoch: 37263 \tTraining Loss: 1.175543 \tValidation Loss: 2.748690\n",
      "Epoch: 37264 \tTraining Loss: 1.202732 \tValidation Loss: 2.749096\n",
      "Epoch: 37265 \tTraining Loss: 1.180008 \tValidation Loss: 2.748462\n",
      "Epoch: 37266 \tTraining Loss: 1.153353 \tValidation Loss: 2.749386\n",
      "Epoch: 37267 \tTraining Loss: 1.163268 \tValidation Loss: 2.749370\n",
      "Epoch: 37268 \tTraining Loss: 1.140441 \tValidation Loss: 2.748442\n",
      "Epoch: 37269 \tTraining Loss: 1.186391 \tValidation Loss: 2.749089\n",
      "Epoch: 37270 \tTraining Loss: 1.213157 \tValidation Loss: 2.748985\n",
      "Epoch: 37271 \tTraining Loss: 1.177746 \tValidation Loss: 2.748692\n",
      "Epoch: 37272 \tTraining Loss: 1.174558 \tValidation Loss: 2.748431\n",
      "Epoch: 37273 \tTraining Loss: 1.159944 \tValidation Loss: 2.750675\n",
      "Epoch: 37274 \tTraining Loss: 1.172551 \tValidation Loss: 2.748131\n",
      "Epoch: 37275 \tTraining Loss: 1.155921 \tValidation Loss: 2.749644\n",
      "Epoch: 37276 \tTraining Loss: 1.208468 \tValidation Loss: 2.747814\n",
      "Epoch: 37277 \tTraining Loss: 1.168834 \tValidation Loss: 2.749708\n",
      "Epoch: 37278 \tTraining Loss: 1.208423 \tValidation Loss: 2.748046\n",
      "Epoch: 37279 \tTraining Loss: 1.200308 \tValidation Loss: 2.747370\n",
      "Epoch: 37280 \tTraining Loss: 1.200783 \tValidation Loss: 2.750477\n",
      "Epoch: 37281 \tTraining Loss: 1.216206 \tValidation Loss: 2.747329\n",
      "Epoch: 37282 \tTraining Loss: 1.173181 \tValidation Loss: 2.749348\n",
      "Epoch: 37283 \tTraining Loss: 1.125903 \tValidation Loss: 2.749825\n",
      "Epoch: 37284 \tTraining Loss: 1.183285 \tValidation Loss: 2.748690\n",
      "Epoch: 37285 \tTraining Loss: 1.174056 \tValidation Loss: 2.749101\n",
      "Epoch: 37286 \tTraining Loss: 1.159197 \tValidation Loss: 2.748976\n",
      "Epoch: 37287 \tTraining Loss: 1.105397 \tValidation Loss: 2.748935\n",
      "Epoch: 37288 \tTraining Loss: 1.180667 \tValidation Loss: 2.748854\n",
      "Epoch: 37289 \tTraining Loss: 1.145175 \tValidation Loss: 2.750819\n",
      "Epoch: 37290 \tTraining Loss: 1.231693 \tValidation Loss: 2.750538\n",
      "Epoch: 37291 \tTraining Loss: 1.198157 \tValidation Loss: 2.749552\n",
      "Epoch: 37292 \tTraining Loss: 1.173265 \tValidation Loss: 2.749648\n",
      "Epoch: 37293 \tTraining Loss: 1.199139 \tValidation Loss: 2.749984\n",
      "Epoch: 37294 \tTraining Loss: 1.179375 \tValidation Loss: 2.750998\n",
      "Epoch: 37295 \tTraining Loss: 1.185367 \tValidation Loss: 2.749400\n",
      "Epoch: 37296 \tTraining Loss: 1.193019 \tValidation Loss: 2.749411\n",
      "Epoch: 37297 \tTraining Loss: 1.203698 \tValidation Loss: 2.748973\n",
      "Epoch: 37298 \tTraining Loss: 1.222724 \tValidation Loss: 2.749217\n",
      "Epoch: 37299 \tTraining Loss: 1.209589 \tValidation Loss: 2.749823\n",
      "Epoch: 37300 \tTraining Loss: 1.218739 \tValidation Loss: 2.749302\n",
      "Epoch: 37301 \tTraining Loss: 1.230704 \tValidation Loss: 2.749920\n",
      "Epoch: 37302 \tTraining Loss: 1.191993 \tValidation Loss: 2.748625\n",
      "Epoch: 37303 \tTraining Loss: 1.268325 \tValidation Loss: 2.748420\n",
      "Epoch: 37304 \tTraining Loss: 1.204955 \tValidation Loss: 2.749049\n",
      "Epoch: 37305 \tTraining Loss: 1.157145 \tValidation Loss: 2.750431\n",
      "Epoch: 37306 \tTraining Loss: 1.177335 \tValidation Loss: 2.748698\n",
      "Epoch: 37307 \tTraining Loss: 1.237989 \tValidation Loss: 2.747756\n",
      "Epoch: 37308 \tTraining Loss: 1.183476 \tValidation Loss: 2.750308\n",
      "Epoch: 37309 \tTraining Loss: 1.150876 \tValidation Loss: 2.750589\n",
      "Epoch: 37310 \tTraining Loss: 1.213892 \tValidation Loss: 2.749860\n",
      "Epoch: 37311 \tTraining Loss: 1.172974 \tValidation Loss: 2.750453\n",
      "Epoch: 37312 \tTraining Loss: 1.216412 \tValidation Loss: 2.749871\n",
      "Epoch: 37313 \tTraining Loss: 1.212931 \tValidation Loss: 2.749408\n",
      "Epoch: 37314 \tTraining Loss: 1.180694 \tValidation Loss: 2.748933\n",
      "Epoch: 37315 \tTraining Loss: 1.136905 \tValidation Loss: 2.751800\n",
      "Epoch: 37316 \tTraining Loss: 1.176689 \tValidation Loss: 2.748570\n",
      "Epoch: 37317 \tTraining Loss: 1.152480 \tValidation Loss: 2.750222\n",
      "Epoch: 37318 \tTraining Loss: 1.189520 \tValidation Loss: 2.751945\n",
      "Epoch: 37319 \tTraining Loss: 1.188125 \tValidation Loss: 2.748979\n",
      "Epoch: 37320 \tTraining Loss: 1.230645 \tValidation Loss: 2.749073\n",
      "Epoch: 37321 \tTraining Loss: 1.186724 \tValidation Loss: 2.751506\n",
      "Epoch: 37322 \tTraining Loss: 1.182627 \tValidation Loss: 2.749192\n",
      "Epoch: 37323 \tTraining Loss: 1.183634 \tValidation Loss: 2.749277\n",
      "Epoch: 37324 \tTraining Loss: 1.228238 \tValidation Loss: 2.749921\n",
      "Epoch: 37325 \tTraining Loss: 1.182745 \tValidation Loss: 2.749014\n",
      "Epoch: 37326 \tTraining Loss: 1.180977 \tValidation Loss: 2.748041\n",
      "Epoch: 37327 \tTraining Loss: 1.180118 \tValidation Loss: 2.749617\n",
      "Epoch: 37328 \tTraining Loss: 1.184297 \tValidation Loss: 2.750132\n",
      "Epoch: 37329 \tTraining Loss: 1.187098 \tValidation Loss: 2.750203\n",
      "Epoch: 37330 \tTraining Loss: 1.163383 \tValidation Loss: 2.752652\n",
      "Epoch: 37331 \tTraining Loss: 1.194194 \tValidation Loss: 2.750004\n",
      "Epoch: 37332 \tTraining Loss: 1.213905 \tValidation Loss: 2.748696\n",
      "Epoch: 37333 \tTraining Loss: 1.209092 \tValidation Loss: 2.750271\n",
      "Epoch: 37334 \tTraining Loss: 1.209079 \tValidation Loss: 2.750416\n",
      "Epoch: 37335 \tTraining Loss: 1.172371 \tValidation Loss: 2.750656\n",
      "Epoch: 37336 \tTraining Loss: 1.192502 \tValidation Loss: 2.750290\n",
      "Epoch: 37337 \tTraining Loss: 1.174292 \tValidation Loss: 2.750183\n",
      "Epoch: 37338 \tTraining Loss: 1.160266 \tValidation Loss: 2.751801\n",
      "Epoch: 37339 \tTraining Loss: 1.177958 \tValidation Loss: 2.750918\n",
      "Epoch: 37340 \tTraining Loss: 1.164586 \tValidation Loss: 2.751365\n",
      "Epoch: 37341 \tTraining Loss: 1.192061 \tValidation Loss: 2.749306\n",
      "Epoch: 37342 \tTraining Loss: 1.170548 \tValidation Loss: 2.751961\n",
      "Epoch: 37343 \tTraining Loss: 1.227882 \tValidation Loss: 2.751944\n",
      "Epoch: 37344 \tTraining Loss: 1.204121 \tValidation Loss: 2.750522\n",
      "Epoch: 37345 \tTraining Loss: 1.193222 \tValidation Loss: 2.749294\n",
      "Epoch: 37346 \tTraining Loss: 1.220206 \tValidation Loss: 2.748251\n",
      "Epoch: 37347 \tTraining Loss: 1.189448 \tValidation Loss: 2.750970\n",
      "Epoch: 37348 \tTraining Loss: 1.203228 \tValidation Loss: 2.750520\n",
      "Epoch: 37349 \tTraining Loss: 1.181239 \tValidation Loss: 2.750376\n",
      "Epoch: 37350 \tTraining Loss: 1.171627 \tValidation Loss: 2.749396\n",
      "Epoch: 37351 \tTraining Loss: 1.198860 \tValidation Loss: 2.750129\n",
      "Epoch: 37352 \tTraining Loss: 1.200709 \tValidation Loss: 2.748853\n",
      "Epoch: 37353 \tTraining Loss: 1.173403 \tValidation Loss: 2.749131\n",
      "Epoch: 37354 \tTraining Loss: 1.226242 \tValidation Loss: 2.749326\n",
      "Epoch: 37355 \tTraining Loss: 1.194282 \tValidation Loss: 2.751298\n",
      "Epoch: 37356 \tTraining Loss: 1.195706 \tValidation Loss: 2.751256\n",
      "Epoch: 37357 \tTraining Loss: 1.176993 \tValidation Loss: 2.748889\n",
      "Epoch: 37358 \tTraining Loss: 1.134225 \tValidation Loss: 2.750477\n",
      "Epoch: 37359 \tTraining Loss: 1.188522 \tValidation Loss: 2.751808\n",
      "Epoch: 37360 \tTraining Loss: 1.232779 \tValidation Loss: 2.750604\n",
      "Epoch: 37361 \tTraining Loss: 1.239188 \tValidation Loss: 2.749057\n",
      "Epoch: 37362 \tTraining Loss: 1.268172 \tValidation Loss: 2.750793\n",
      "Epoch: 37363 \tTraining Loss: 1.185983 \tValidation Loss: 2.752128\n",
      "Epoch: 37364 \tTraining Loss: 1.199172 \tValidation Loss: 2.750829\n",
      "Epoch: 37365 \tTraining Loss: 1.166462 \tValidation Loss: 2.750098\n",
      "Epoch: 37366 \tTraining Loss: 1.226385 \tValidation Loss: 2.750108\n",
      "Epoch: 37367 \tTraining Loss: 1.147413 \tValidation Loss: 2.751948\n",
      "Epoch: 37368 \tTraining Loss: 1.202851 \tValidation Loss: 2.749795\n",
      "Epoch: 37369 \tTraining Loss: 1.201813 \tValidation Loss: 2.750371\n",
      "Epoch: 37370 \tTraining Loss: 1.150759 \tValidation Loss: 2.749768\n",
      "Epoch: 37371 \tTraining Loss: 1.171123 \tValidation Loss: 2.752549\n",
      "Epoch: 37372 \tTraining Loss: 1.231265 \tValidation Loss: 2.750611\n",
      "Epoch: 37373 \tTraining Loss: 1.206108 \tValidation Loss: 2.750234\n",
      "Epoch: 37374 \tTraining Loss: 1.210802 \tValidation Loss: 2.752569\n",
      "Epoch: 37375 \tTraining Loss: 1.197514 \tValidation Loss: 2.750270\n",
      "Epoch: 37376 \tTraining Loss: 1.251564 \tValidation Loss: 2.750103\n",
      "Epoch: 37377 \tTraining Loss: 1.193544 \tValidation Loss: 2.750543\n",
      "Epoch: 37378 \tTraining Loss: 1.210523 \tValidation Loss: 2.750426\n",
      "Epoch: 37379 \tTraining Loss: 1.204030 \tValidation Loss: 2.751882\n",
      "Epoch: 37380 \tTraining Loss: 1.206555 \tValidation Loss: 2.749216\n",
      "Epoch: 37381 \tTraining Loss: 1.195012 \tValidation Loss: 2.751515\n",
      "Epoch: 37382 \tTraining Loss: 1.227545 \tValidation Loss: 2.750183\n",
      "Epoch: 37383 \tTraining Loss: 1.222790 \tValidation Loss: 2.751890\n",
      "Epoch: 37384 \tTraining Loss: 1.192869 \tValidation Loss: 2.750080\n",
      "Epoch: 37385 \tTraining Loss: 1.183451 \tValidation Loss: 2.751639\n",
      "Epoch: 37386 \tTraining Loss: 1.156080 \tValidation Loss: 2.751697\n",
      "Epoch: 37387 \tTraining Loss: 1.195177 \tValidation Loss: 2.751401\n",
      "Epoch: 37388 \tTraining Loss: 1.170063 \tValidation Loss: 2.751945\n",
      "Epoch: 37389 \tTraining Loss: 1.153636 \tValidation Loss: 2.752540\n",
      "Epoch: 37390 \tTraining Loss: 1.199325 \tValidation Loss: 2.751368\n",
      "Epoch: 37391 \tTraining Loss: 1.194377 \tValidation Loss: 2.752402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37392 \tTraining Loss: 1.171111 \tValidation Loss: 2.750582\n",
      "Epoch: 37393 \tTraining Loss: 1.165120 \tValidation Loss: 2.751096\n",
      "Epoch: 37394 \tTraining Loss: 1.212430 \tValidation Loss: 2.750772\n",
      "Epoch: 37395 \tTraining Loss: 1.195807 \tValidation Loss: 2.750546\n",
      "Epoch: 37396 \tTraining Loss: 1.200337 \tValidation Loss: 2.752292\n",
      "Epoch: 37397 \tTraining Loss: 1.218110 \tValidation Loss: 2.751243\n",
      "Epoch: 37398 \tTraining Loss: 1.212052 \tValidation Loss: 2.749940\n",
      "Epoch: 37399 \tTraining Loss: 1.171731 \tValidation Loss: 2.752171\n",
      "Epoch: 37400 \tTraining Loss: 1.200867 \tValidation Loss: 2.750842\n",
      "Epoch: 37401 \tTraining Loss: 1.175042 \tValidation Loss: 2.752287\n",
      "Epoch: 37402 \tTraining Loss: 1.213200 \tValidation Loss: 2.750886\n",
      "Epoch: 37403 \tTraining Loss: 1.192558 \tValidation Loss: 2.750216\n",
      "Epoch: 37404 \tTraining Loss: 1.156334 \tValidation Loss: 2.752862\n",
      "Epoch: 37405 \tTraining Loss: 1.209443 \tValidation Loss: 2.752014\n",
      "Epoch: 37406 \tTraining Loss: 1.189556 \tValidation Loss: 2.751787\n",
      "Epoch: 37407 \tTraining Loss: 1.171727 \tValidation Loss: 2.752856\n",
      "Epoch: 37408 \tTraining Loss: 1.189729 \tValidation Loss: 2.751568\n",
      "Epoch: 37409 \tTraining Loss: 1.200351 \tValidation Loss: 2.752180\n",
      "Epoch: 37410 \tTraining Loss: 1.210611 \tValidation Loss: 2.751392\n",
      "Epoch: 37411 \tTraining Loss: 1.172771 \tValidation Loss: 2.751428\n",
      "Epoch: 37412 \tTraining Loss: 1.188393 \tValidation Loss: 2.752649\n",
      "Epoch: 37413 \tTraining Loss: 1.173039 \tValidation Loss: 2.753773\n",
      "Epoch: 37414 \tTraining Loss: 1.201620 \tValidation Loss: 2.751037\n",
      "Epoch: 37415 \tTraining Loss: 1.152949 \tValidation Loss: 2.753272\n",
      "Epoch: 37416 \tTraining Loss: 1.142446 \tValidation Loss: 2.753937\n",
      "Epoch: 37417 \tTraining Loss: 1.130997 \tValidation Loss: 2.753670\n",
      "Epoch: 37418 \tTraining Loss: 1.184923 \tValidation Loss: 2.753323\n",
      "Epoch: 37419 \tTraining Loss: 1.195742 \tValidation Loss: 2.753147\n",
      "Epoch: 37420 \tTraining Loss: 1.176357 \tValidation Loss: 2.751983\n",
      "Epoch: 37421 \tTraining Loss: 1.197656 \tValidation Loss: 2.752268\n",
      "Epoch: 37422 \tTraining Loss: 1.198266 \tValidation Loss: 2.751042\n",
      "Epoch: 37423 \tTraining Loss: 1.153495 \tValidation Loss: 2.751344\n",
      "Epoch: 37424 \tTraining Loss: 1.204563 \tValidation Loss: 2.751893\n",
      "Epoch: 37425 \tTraining Loss: 1.159783 \tValidation Loss: 2.752036\n",
      "Epoch: 37426 \tTraining Loss: 1.155074 \tValidation Loss: 2.751735\n",
      "Epoch: 37427 \tTraining Loss: 1.144764 \tValidation Loss: 2.753370\n",
      "Epoch: 37428 \tTraining Loss: 1.169201 \tValidation Loss: 2.754352\n",
      "Epoch: 37429 \tTraining Loss: 1.183429 \tValidation Loss: 2.753285\n",
      "Epoch: 37430 \tTraining Loss: 1.149709 \tValidation Loss: 2.753888\n",
      "Epoch: 37431 \tTraining Loss: 1.180696 \tValidation Loss: 2.751632\n",
      "Epoch: 37432 \tTraining Loss: 1.158960 \tValidation Loss: 2.753376\n",
      "Epoch: 37433 \tTraining Loss: 1.158750 \tValidation Loss: 2.751041\n",
      "Epoch: 37434 \tTraining Loss: 1.197570 \tValidation Loss: 2.752906\n",
      "Epoch: 37435 \tTraining Loss: 1.152153 \tValidation Loss: 2.751769\n",
      "Epoch: 37436 \tTraining Loss: 1.215540 \tValidation Loss: 2.752249\n",
      "Epoch: 37437 \tTraining Loss: 1.228433 \tValidation Loss: 2.754835\n",
      "Epoch: 37438 \tTraining Loss: 1.173456 \tValidation Loss: 2.753235\n",
      "Epoch: 37439 \tTraining Loss: 1.177778 \tValidation Loss: 2.752081\n",
      "Epoch: 37440 \tTraining Loss: 1.197948 \tValidation Loss: 2.751491\n",
      "Epoch: 37441 \tTraining Loss: 1.182427 \tValidation Loss: 2.752911\n",
      "Epoch: 37442 \tTraining Loss: 1.178882 \tValidation Loss: 2.753924\n",
      "Epoch: 37443 \tTraining Loss: 1.185188 \tValidation Loss: 2.751837\n",
      "Epoch: 37444 \tTraining Loss: 1.174387 \tValidation Loss: 2.752767\n",
      "Epoch: 37445 \tTraining Loss: 1.162922 \tValidation Loss: 2.751567\n",
      "Epoch: 37446 \tTraining Loss: 1.214494 \tValidation Loss: 2.752489\n",
      "Epoch: 37447 \tTraining Loss: 1.163321 \tValidation Loss: 2.754568\n",
      "Epoch: 37448 \tTraining Loss: 1.160929 \tValidation Loss: 2.754091\n",
      "Epoch: 37449 \tTraining Loss: 1.184921 \tValidation Loss: 2.754572\n",
      "Epoch: 37450 \tTraining Loss: 1.170504 \tValidation Loss: 2.755191\n",
      "Epoch: 37451 \tTraining Loss: 1.197944 \tValidation Loss: 2.751877\n",
      "Epoch: 37452 \tTraining Loss: 1.184464 \tValidation Loss: 2.754054\n",
      "Epoch: 37453 \tTraining Loss: 1.128682 \tValidation Loss: 2.753152\n",
      "Epoch: 37454 \tTraining Loss: 1.185511 \tValidation Loss: 2.753636\n",
      "Epoch: 37455 \tTraining Loss: 1.216042 \tValidation Loss: 2.752783\n",
      "Epoch: 37456 \tTraining Loss: 1.178861 \tValidation Loss: 2.753307\n",
      "Epoch: 37457 \tTraining Loss: 1.192494 \tValidation Loss: 2.751733\n",
      "Epoch: 37458 \tTraining Loss: 1.192805 \tValidation Loss: 2.754091\n",
      "Epoch: 37459 \tTraining Loss: 1.176028 \tValidation Loss: 2.753193\n",
      "Epoch: 37460 \tTraining Loss: 1.178584 \tValidation Loss: 2.754445\n",
      "Epoch: 37461 \tTraining Loss: 1.207370 \tValidation Loss: 2.753154\n",
      "Epoch: 37462 \tTraining Loss: 1.188825 \tValidation Loss: 2.753985\n",
      "Epoch: 37463 \tTraining Loss: 1.142432 \tValidation Loss: 2.755054\n",
      "Epoch: 37464 \tTraining Loss: 1.218286 \tValidation Loss: 2.753557\n",
      "Epoch: 37465 \tTraining Loss: 1.177105 \tValidation Loss: 2.753723\n",
      "Epoch: 37466 \tTraining Loss: 1.174659 \tValidation Loss: 2.753062\n",
      "Epoch: 37467 \tTraining Loss: 1.179350 \tValidation Loss: 2.754486\n",
      "Epoch: 37468 \tTraining Loss: 1.173882 \tValidation Loss: 2.755101\n",
      "Epoch: 37469 \tTraining Loss: 1.215770 \tValidation Loss: 2.752786\n",
      "Epoch: 37470 \tTraining Loss: 1.206495 \tValidation Loss: 2.752851\n",
      "Epoch: 37471 \tTraining Loss: 1.195820 \tValidation Loss: 2.753819\n",
      "Epoch: 37472 \tTraining Loss: 1.182607 \tValidation Loss: 2.752284\n",
      "Epoch: 37473 \tTraining Loss: 1.222325 \tValidation Loss: 2.751516\n",
      "Epoch: 37474 \tTraining Loss: 1.174855 \tValidation Loss: 2.752863\n",
      "Epoch: 37475 \tTraining Loss: 1.133985 \tValidation Loss: 2.754434\n",
      "Epoch: 37476 \tTraining Loss: 1.159771 \tValidation Loss: 2.753940\n",
      "Epoch: 37477 \tTraining Loss: 1.156202 \tValidation Loss: 2.753037\n",
      "Epoch: 37478 \tTraining Loss: 1.187480 \tValidation Loss: 2.752936\n",
      "Epoch: 37479 \tTraining Loss: 1.165321 \tValidation Loss: 2.752144\n",
      "Epoch: 37480 \tTraining Loss: 1.163682 \tValidation Loss: 2.753706\n",
      "Epoch: 37481 \tTraining Loss: 1.185040 \tValidation Loss: 2.753095\n",
      "Epoch: 37482 \tTraining Loss: 1.188555 \tValidation Loss: 2.753556\n",
      "Epoch: 37483 \tTraining Loss: 1.133267 \tValidation Loss: 2.753787\n",
      "Epoch: 37484 \tTraining Loss: 1.169393 \tValidation Loss: 2.750816\n",
      "Epoch: 37485 \tTraining Loss: 1.246190 \tValidation Loss: 2.752046\n",
      "Epoch: 37486 \tTraining Loss: 1.204116 \tValidation Loss: 2.752996\n",
      "Epoch: 37487 \tTraining Loss: 1.178633 \tValidation Loss: 2.753433\n",
      "Epoch: 37488 \tTraining Loss: 1.191520 \tValidation Loss: 2.753997\n",
      "Epoch: 37489 \tTraining Loss: 1.125825 \tValidation Loss: 2.753459\n",
      "Epoch: 37490 \tTraining Loss: 1.138289 \tValidation Loss: 2.753156\n",
      "Epoch: 37491 \tTraining Loss: 1.239428 \tValidation Loss: 2.751813\n",
      "Epoch: 37492 \tTraining Loss: 1.187776 \tValidation Loss: 2.752717\n",
      "Epoch: 37493 \tTraining Loss: 1.190387 \tValidation Loss: 2.754204\n",
      "Epoch: 37494 \tTraining Loss: 1.162909 \tValidation Loss: 2.754810\n",
      "Epoch: 37495 \tTraining Loss: 1.229850 \tValidation Loss: 2.754218\n",
      "Epoch: 37496 \tTraining Loss: 1.220311 \tValidation Loss: 2.754084\n",
      "Epoch: 37497 \tTraining Loss: 1.192882 \tValidation Loss: 2.753620\n",
      "Epoch: 37498 \tTraining Loss: 1.200215 \tValidation Loss: 2.752888\n",
      "Epoch: 37499 \tTraining Loss: 1.179059 \tValidation Loss: 2.753255\n",
      "Epoch: 37500 \tTraining Loss: 1.185883 \tValidation Loss: 2.753864\n",
      "Epoch: 37501 \tTraining Loss: 1.128159 \tValidation Loss: 2.755236\n",
      "Epoch: 37502 \tTraining Loss: 1.190155 \tValidation Loss: 2.754183\n",
      "Epoch: 37503 \tTraining Loss: 1.190946 \tValidation Loss: 2.753804\n",
      "Epoch: 37504 \tTraining Loss: 1.189899 \tValidation Loss: 2.753188\n",
      "Epoch: 37505 \tTraining Loss: 1.170667 \tValidation Loss: 2.755795\n",
      "Epoch: 37506 \tTraining Loss: 1.190901 \tValidation Loss: 2.754011\n",
      "Epoch: 37507 \tTraining Loss: 1.179363 \tValidation Loss: 2.755038\n",
      "Epoch: 37508 \tTraining Loss: 1.232613 \tValidation Loss: 2.753806\n",
      "Epoch: 37509 \tTraining Loss: 1.233654 \tValidation Loss: 2.753117\n",
      "Epoch: 37510 \tTraining Loss: 1.144201 \tValidation Loss: 2.754392\n",
      "Epoch: 37511 \tTraining Loss: 1.167044 \tValidation Loss: 2.752996\n",
      "Epoch: 37512 \tTraining Loss: 1.211401 \tValidation Loss: 2.754757\n",
      "Epoch: 37513 \tTraining Loss: 1.180657 \tValidation Loss: 2.753431\n",
      "Epoch: 37514 \tTraining Loss: 1.207730 \tValidation Loss: 2.754138\n",
      "Epoch: 37515 \tTraining Loss: 1.196337 \tValidation Loss: 2.755073\n",
      "Epoch: 37516 \tTraining Loss: 1.200939 \tValidation Loss: 2.754353\n",
      "Epoch: 37517 \tTraining Loss: 1.243983 \tValidation Loss: 2.754988\n",
      "Epoch: 37518 \tTraining Loss: 1.176324 \tValidation Loss: 2.754598\n",
      "Epoch: 37519 \tTraining Loss: 1.193973 \tValidation Loss: 2.754687\n",
      "Epoch: 37520 \tTraining Loss: 1.145491 \tValidation Loss: 2.754658\n",
      "Epoch: 37521 \tTraining Loss: 1.212022 \tValidation Loss: 2.755066\n",
      "Epoch: 37522 \tTraining Loss: 1.161135 \tValidation Loss: 2.754593\n",
      "Epoch: 37523 \tTraining Loss: 1.194318 \tValidation Loss: 2.754622\n",
      "Epoch: 37524 \tTraining Loss: 1.177762 \tValidation Loss: 2.756023\n",
      "Epoch: 37525 \tTraining Loss: 1.205181 \tValidation Loss: 2.756418\n",
      "Epoch: 37526 \tTraining Loss: 1.211052 \tValidation Loss: 2.755337\n",
      "Epoch: 37527 \tTraining Loss: 1.201873 \tValidation Loss: 2.753632\n",
      "Epoch: 37528 \tTraining Loss: 1.176717 \tValidation Loss: 2.755134\n",
      "Epoch: 37529 \tTraining Loss: 1.169537 \tValidation Loss: 2.754385\n",
      "Epoch: 37530 \tTraining Loss: 1.153654 \tValidation Loss: 2.756321\n",
      "Epoch: 37531 \tTraining Loss: 1.226935 \tValidation Loss: 2.753119\n",
      "Epoch: 37532 \tTraining Loss: 1.201028 \tValidation Loss: 2.755570\n",
      "Epoch: 37533 \tTraining Loss: 1.177921 \tValidation Loss: 2.755311\n",
      "Epoch: 37534 \tTraining Loss: 1.154056 \tValidation Loss: 2.755168\n",
      "Epoch: 37535 \tTraining Loss: 1.183693 \tValidation Loss: 2.755837\n",
      "Epoch: 37536 \tTraining Loss: 1.234058 \tValidation Loss: 2.754299\n",
      "Epoch: 37537 \tTraining Loss: 1.148505 \tValidation Loss: 2.755191\n",
      "Epoch: 37538 \tTraining Loss: 1.167760 \tValidation Loss: 2.753924\n",
      "Epoch: 37539 \tTraining Loss: 1.179535 \tValidation Loss: 2.754632\n",
      "Epoch: 37540 \tTraining Loss: 1.147718 \tValidation Loss: 2.754779\n",
      "Epoch: 37541 \tTraining Loss: 1.202732 \tValidation Loss: 2.753433\n",
      "Epoch: 37542 \tTraining Loss: 1.163095 \tValidation Loss: 2.754560\n",
      "Epoch: 37543 \tTraining Loss: 1.183645 \tValidation Loss: 2.755902\n",
      "Epoch: 37544 \tTraining Loss: 1.168514 \tValidation Loss: 2.755488\n",
      "Epoch: 37545 \tTraining Loss: 1.213672 \tValidation Loss: 2.755187\n",
      "Epoch: 37546 \tTraining Loss: 1.169975 \tValidation Loss: 2.755809\n",
      "Epoch: 37547 \tTraining Loss: 1.167046 \tValidation Loss: 2.756371\n",
      "Epoch: 37548 \tTraining Loss: 1.189488 \tValidation Loss: 2.755356\n",
      "Epoch: 37549 \tTraining Loss: 1.156427 \tValidation Loss: 2.756365\n",
      "Epoch: 37550 \tTraining Loss: 1.176233 \tValidation Loss: 2.754663\n",
      "Epoch: 37551 \tTraining Loss: 1.183757 \tValidation Loss: 2.755615\n",
      "Epoch: 37552 \tTraining Loss: 1.131618 \tValidation Loss: 2.755366\n",
      "Epoch: 37553 \tTraining Loss: 1.249052 \tValidation Loss: 2.754537\n",
      "Epoch: 37554 \tTraining Loss: 1.183218 \tValidation Loss: 2.755371\n",
      "Epoch: 37555 \tTraining Loss: 1.200417 \tValidation Loss: 2.755113\n",
      "Epoch: 37556 \tTraining Loss: 1.200535 \tValidation Loss: 2.754598\n",
      "Epoch: 37557 \tTraining Loss: 1.168747 \tValidation Loss: 2.755695\n",
      "Epoch: 37558 \tTraining Loss: 1.156589 \tValidation Loss: 2.756745\n",
      "Epoch: 37559 \tTraining Loss: 1.166346 \tValidation Loss: 2.755064\n",
      "Epoch: 37560 \tTraining Loss: 1.200261 \tValidation Loss: 2.756350\n",
      "Epoch: 37561 \tTraining Loss: 1.174683 \tValidation Loss: 2.754001\n",
      "Epoch: 37562 \tTraining Loss: 1.193414 \tValidation Loss: 2.754818\n",
      "Epoch: 37563 \tTraining Loss: 1.216525 \tValidation Loss: 2.755271\n",
      "Epoch: 37564 \tTraining Loss: 1.191211 \tValidation Loss: 2.756296\n",
      "Epoch: 37565 \tTraining Loss: 1.173498 \tValidation Loss: 2.752850\n",
      "Epoch: 37566 \tTraining Loss: 1.158319 \tValidation Loss: 2.755632\n",
      "Epoch: 37567 \tTraining Loss: 1.190171 \tValidation Loss: 2.755076\n",
      "Epoch: 37568 \tTraining Loss: 1.192877 \tValidation Loss: 2.755455\n",
      "Epoch: 37569 \tTraining Loss: 1.181768 \tValidation Loss: 2.756634\n",
      "Epoch: 37570 \tTraining Loss: 1.196781 \tValidation Loss: 2.756247\n",
      "Epoch: 37571 \tTraining Loss: 1.190183 \tValidation Loss: 2.756647\n",
      "Epoch: 37572 \tTraining Loss: 1.179196 \tValidation Loss: 2.754579\n",
      "Epoch: 37573 \tTraining Loss: 1.158912 \tValidation Loss: 2.756070\n",
      "Epoch: 37574 \tTraining Loss: 1.223358 \tValidation Loss: 2.756339\n",
      "Epoch: 37575 \tTraining Loss: 1.194704 \tValidation Loss: 2.756937\n",
      "Epoch: 37576 \tTraining Loss: 1.218089 \tValidation Loss: 2.754489\n",
      "Epoch: 37577 \tTraining Loss: 1.199534 \tValidation Loss: 2.755726\n",
      "Epoch: 37578 \tTraining Loss: 1.127325 \tValidation Loss: 2.756571\n",
      "Epoch: 37579 \tTraining Loss: 1.198400 \tValidation Loss: 2.755950\n",
      "Epoch: 37580 \tTraining Loss: 1.228586 \tValidation Loss: 2.755841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37581 \tTraining Loss: 1.206072 \tValidation Loss: 2.755540\n",
      "Epoch: 37582 \tTraining Loss: 1.211707 \tValidation Loss: 2.756288\n",
      "Epoch: 37583 \tTraining Loss: 1.170292 \tValidation Loss: 2.756185\n",
      "Epoch: 37584 \tTraining Loss: 1.179708 \tValidation Loss: 2.756085\n",
      "Epoch: 37585 \tTraining Loss: 1.153957 \tValidation Loss: 2.755934\n",
      "Epoch: 37586 \tTraining Loss: 1.183133 \tValidation Loss: 2.756353\n",
      "Epoch: 37587 \tTraining Loss: 1.172513 \tValidation Loss: 2.756202\n",
      "Epoch: 37588 \tTraining Loss: 1.169733 \tValidation Loss: 2.755184\n",
      "Epoch: 37589 \tTraining Loss: 1.172795 \tValidation Loss: 2.755903\n",
      "Epoch: 37590 \tTraining Loss: 1.161435 \tValidation Loss: 2.757531\n",
      "Epoch: 37591 \tTraining Loss: 1.152559 \tValidation Loss: 2.756297\n",
      "Epoch: 37592 \tTraining Loss: 1.145008 \tValidation Loss: 2.754154\n",
      "Epoch: 37593 \tTraining Loss: 1.179367 \tValidation Loss: 2.756551\n",
      "Epoch: 37594 \tTraining Loss: 1.163046 \tValidation Loss: 2.755991\n",
      "Epoch: 37595 \tTraining Loss: 1.168969 \tValidation Loss: 2.754793\n",
      "Epoch: 37596 \tTraining Loss: 1.173776 \tValidation Loss: 2.756598\n",
      "Epoch: 37597 \tTraining Loss: 1.243715 \tValidation Loss: 2.755477\n",
      "Epoch: 37598 \tTraining Loss: 1.135679 \tValidation Loss: 2.758096\n",
      "Epoch: 37599 \tTraining Loss: 1.208752 \tValidation Loss: 2.756127\n",
      "Epoch: 37600 \tTraining Loss: 1.219450 \tValidation Loss: 2.755298\n",
      "Epoch: 37601 \tTraining Loss: 1.147645 \tValidation Loss: 2.757458\n",
      "Epoch: 37602 \tTraining Loss: 1.198068 \tValidation Loss: 2.755936\n",
      "Epoch: 37603 \tTraining Loss: 1.170377 \tValidation Loss: 2.756469\n",
      "Epoch: 37604 \tTraining Loss: 1.209333 \tValidation Loss: 2.753583\n",
      "Epoch: 37605 \tTraining Loss: 1.185426 \tValidation Loss: 2.754780\n",
      "Epoch: 37606 \tTraining Loss: 1.204231 \tValidation Loss: 2.755929\n",
      "Epoch: 37607 \tTraining Loss: 1.208143 \tValidation Loss: 2.754814\n",
      "Epoch: 37608 \tTraining Loss: 1.195892 \tValidation Loss: 2.756660\n",
      "Epoch: 37609 \tTraining Loss: 1.164440 \tValidation Loss: 2.756688\n",
      "Epoch: 37610 \tTraining Loss: 1.171880 \tValidation Loss: 2.757082\n",
      "Epoch: 37611 \tTraining Loss: 1.210280 \tValidation Loss: 2.756629\n",
      "Epoch: 37612 \tTraining Loss: 1.180485 \tValidation Loss: 2.755914\n",
      "Epoch: 37613 \tTraining Loss: 1.233373 \tValidation Loss: 2.755574\n",
      "Epoch: 37614 \tTraining Loss: 1.146320 \tValidation Loss: 2.756870\n",
      "Epoch: 37615 \tTraining Loss: 1.204453 \tValidation Loss: 2.756862\n",
      "Epoch: 37616 \tTraining Loss: 1.163011 \tValidation Loss: 2.755567\n",
      "Epoch: 37617 \tTraining Loss: 1.211125 \tValidation Loss: 2.757562\n",
      "Epoch: 37618 \tTraining Loss: 1.238677 \tValidation Loss: 2.754196\n",
      "Epoch: 37619 \tTraining Loss: 1.177142 \tValidation Loss: 2.757270\n",
      "Epoch: 37620 \tTraining Loss: 1.231871 \tValidation Loss: 2.757038\n",
      "Epoch: 37621 \tTraining Loss: 1.196528 \tValidation Loss: 2.757862\n",
      "Epoch: 37622 \tTraining Loss: 1.145138 \tValidation Loss: 2.758227\n",
      "Epoch: 37623 \tTraining Loss: 1.170483 \tValidation Loss: 2.756873\n",
      "Epoch: 37624 \tTraining Loss: 1.219318 \tValidation Loss: 2.756729\n",
      "Epoch: 37625 \tTraining Loss: 1.164889 \tValidation Loss: 2.757916\n",
      "Epoch: 37626 \tTraining Loss: 1.176048 \tValidation Loss: 2.756297\n",
      "Epoch: 37627 \tTraining Loss: 1.163122 \tValidation Loss: 2.757749\n",
      "Epoch: 37628 \tTraining Loss: 1.163194 \tValidation Loss: 2.756466\n",
      "Epoch: 37629 \tTraining Loss: 1.161237 \tValidation Loss: 2.756603\n",
      "Epoch: 37630 \tTraining Loss: 1.182806 \tValidation Loss: 2.754469\n",
      "Epoch: 37631 \tTraining Loss: 1.180012 \tValidation Loss: 2.756630\n",
      "Epoch: 37632 \tTraining Loss: 1.192981 \tValidation Loss: 2.757989\n",
      "Epoch: 37633 \tTraining Loss: 1.223725 \tValidation Loss: 2.755116\n",
      "Epoch: 37634 \tTraining Loss: 1.175671 \tValidation Loss: 2.758232\n",
      "Epoch: 37635 \tTraining Loss: 1.195128 \tValidation Loss: 2.755452\n",
      "Epoch: 37636 \tTraining Loss: 1.210617 \tValidation Loss: 2.757185\n",
      "Epoch: 37637 \tTraining Loss: 1.181546 \tValidation Loss: 2.756735\n",
      "Epoch: 37638 \tTraining Loss: 1.198212 \tValidation Loss: 2.757152\n",
      "Epoch: 37639 \tTraining Loss: 1.193187 \tValidation Loss: 2.757222\n",
      "Epoch: 37640 \tTraining Loss: 1.135517 \tValidation Loss: 2.757976\n",
      "Epoch: 37641 \tTraining Loss: 1.188888 \tValidation Loss: 2.758803\n",
      "Epoch: 37642 \tTraining Loss: 1.184920 \tValidation Loss: 2.757800\n",
      "Epoch: 37643 \tTraining Loss: 1.165957 \tValidation Loss: 2.757572\n",
      "Epoch: 37644 \tTraining Loss: 1.157473 \tValidation Loss: 2.757054\n",
      "Epoch: 37645 \tTraining Loss: 1.183207 \tValidation Loss: 2.757786\n",
      "Epoch: 37646 \tTraining Loss: 1.165968 \tValidation Loss: 2.760002\n",
      "Epoch: 37647 \tTraining Loss: 1.215713 \tValidation Loss: 2.755306\n",
      "Epoch: 37648 \tTraining Loss: 1.164718 \tValidation Loss: 2.756570\n",
      "Epoch: 37649 \tTraining Loss: 1.173530 \tValidation Loss: 2.757993\n",
      "Epoch: 37650 \tTraining Loss: 1.165923 \tValidation Loss: 2.757841\n",
      "Epoch: 37651 \tTraining Loss: 1.168089 \tValidation Loss: 2.757548\n",
      "Epoch: 37652 \tTraining Loss: 1.157321 \tValidation Loss: 2.757059\n",
      "Epoch: 37653 \tTraining Loss: 1.183572 \tValidation Loss: 2.758813\n",
      "Epoch: 37654 \tTraining Loss: 1.207074 \tValidation Loss: 2.756461\n",
      "Epoch: 37655 \tTraining Loss: 1.138337 \tValidation Loss: 2.758183\n",
      "Epoch: 37656 \tTraining Loss: 1.218769 \tValidation Loss: 2.755218\n",
      "Epoch: 37657 \tTraining Loss: 1.199017 \tValidation Loss: 2.756166\n",
      "Epoch: 37658 \tTraining Loss: 1.173007 \tValidation Loss: 2.757476\n",
      "Epoch: 37659 \tTraining Loss: 1.173695 \tValidation Loss: 2.756683\n",
      "Epoch: 37660 \tTraining Loss: 1.203587 \tValidation Loss: 2.757179\n",
      "Epoch: 37661 \tTraining Loss: 1.212356 \tValidation Loss: 2.756889\n",
      "Epoch: 37662 \tTraining Loss: 1.250784 \tValidation Loss: 2.756944\n",
      "Epoch: 37663 \tTraining Loss: 1.160838 \tValidation Loss: 2.756634\n",
      "Epoch: 37664 \tTraining Loss: 1.166400 \tValidation Loss: 2.756020\n",
      "Epoch: 37665 \tTraining Loss: 1.176701 \tValidation Loss: 2.757122\n",
      "Epoch: 37666 \tTraining Loss: 1.159592 \tValidation Loss: 2.757924\n",
      "Epoch: 37667 \tTraining Loss: 1.146126 \tValidation Loss: 2.758667\n",
      "Epoch: 37668 \tTraining Loss: 1.193362 \tValidation Loss: 2.758304\n",
      "Epoch: 37669 \tTraining Loss: 1.229642 \tValidation Loss: 2.756309\n",
      "Epoch: 37670 \tTraining Loss: 1.211888 \tValidation Loss: 2.758072\n",
      "Epoch: 37671 \tTraining Loss: 1.173980 \tValidation Loss: 2.757622\n",
      "Epoch: 37672 \tTraining Loss: 1.177970 \tValidation Loss: 2.757925\n",
      "Epoch: 37673 \tTraining Loss: 1.183833 \tValidation Loss: 2.758224\n",
      "Epoch: 37674 \tTraining Loss: 1.155676 \tValidation Loss: 2.759666\n",
      "Epoch: 37675 \tTraining Loss: 1.170715 \tValidation Loss: 2.758395\n",
      "Epoch: 37676 \tTraining Loss: 1.191311 \tValidation Loss: 2.758401\n",
      "Epoch: 37677 \tTraining Loss: 1.177515 \tValidation Loss: 2.758002\n",
      "Epoch: 37678 \tTraining Loss: 1.185013 \tValidation Loss: 2.758599\n",
      "Epoch: 37679 \tTraining Loss: 1.190353 \tValidation Loss: 2.758973\n",
      "Epoch: 37680 \tTraining Loss: 1.191972 \tValidation Loss: 2.756751\n",
      "Epoch: 37681 \tTraining Loss: 1.139877 \tValidation Loss: 2.759035\n",
      "Epoch: 37682 \tTraining Loss: 1.156035 \tValidation Loss: 2.757590\n",
      "Epoch: 37683 \tTraining Loss: 1.154821 \tValidation Loss: 2.758142\n",
      "Epoch: 37684 \tTraining Loss: 1.193724 \tValidation Loss: 2.757433\n",
      "Epoch: 37685 \tTraining Loss: 1.205868 \tValidation Loss: 2.758050\n",
      "Epoch: 37686 \tTraining Loss: 1.153482 \tValidation Loss: 2.758124\n",
      "Epoch: 37687 \tTraining Loss: 1.112973 \tValidation Loss: 2.757625\n",
      "Epoch: 37688 \tTraining Loss: 1.212651 \tValidation Loss: 2.758545\n",
      "Epoch: 37689 \tTraining Loss: 1.185156 \tValidation Loss: 2.758981\n",
      "Epoch: 37690 \tTraining Loss: 1.232282 \tValidation Loss: 2.759701\n",
      "Epoch: 37691 \tTraining Loss: 1.217142 \tValidation Loss: 2.758508\n",
      "Epoch: 37692 \tTraining Loss: 1.171937 \tValidation Loss: 2.759145\n",
      "Epoch: 37693 \tTraining Loss: 1.190818 \tValidation Loss: 2.757683\n",
      "Epoch: 37694 \tTraining Loss: 1.176814 \tValidation Loss: 2.758427\n",
      "Epoch: 37695 \tTraining Loss: 1.192932 \tValidation Loss: 2.758627\n",
      "Epoch: 37696 \tTraining Loss: 1.133023 \tValidation Loss: 2.758757\n",
      "Epoch: 37697 \tTraining Loss: 1.192481 \tValidation Loss: 2.757705\n",
      "Epoch: 37698 \tTraining Loss: 1.196695 \tValidation Loss: 2.758868\n",
      "Epoch: 37699 \tTraining Loss: 1.188971 \tValidation Loss: 2.758776\n",
      "Epoch: 37700 \tTraining Loss: 1.170425 \tValidation Loss: 2.758262\n",
      "Epoch: 37701 \tTraining Loss: 1.204761 \tValidation Loss: 2.759839\n",
      "Epoch: 37702 \tTraining Loss: 1.170133 \tValidation Loss: 2.759672\n",
      "Epoch: 37703 \tTraining Loss: 1.186373 \tValidation Loss: 2.758110\n",
      "Epoch: 37704 \tTraining Loss: 1.171203 \tValidation Loss: 2.758597\n",
      "Epoch: 37705 \tTraining Loss: 1.156832 \tValidation Loss: 2.761283\n",
      "Epoch: 37706 \tTraining Loss: 1.219219 \tValidation Loss: 2.757468\n",
      "Epoch: 37707 \tTraining Loss: 1.186230 \tValidation Loss: 2.758371\n",
      "Epoch: 37708 \tTraining Loss: 1.169631 \tValidation Loss: 2.760178\n",
      "Epoch: 37709 \tTraining Loss: 1.157482 \tValidation Loss: 2.758107\n",
      "Epoch: 37710 \tTraining Loss: 1.170828 \tValidation Loss: 2.758263\n",
      "Epoch: 37711 \tTraining Loss: 1.158629 \tValidation Loss: 2.758651\n",
      "Epoch: 37712 \tTraining Loss: 1.148452 \tValidation Loss: 2.758828\n",
      "Epoch: 37713 \tTraining Loss: 1.173877 \tValidation Loss: 2.757735\n",
      "Epoch: 37714 \tTraining Loss: 1.165163 \tValidation Loss: 2.757903\n",
      "Epoch: 37715 \tTraining Loss: 1.190026 \tValidation Loss: 2.759809\n",
      "Epoch: 37716 \tTraining Loss: 1.122005 \tValidation Loss: 2.761263\n",
      "Epoch: 37717 \tTraining Loss: 1.209755 \tValidation Loss: 2.759242\n",
      "Epoch: 37718 \tTraining Loss: 1.152290 \tValidation Loss: 2.758932\n",
      "Epoch: 37719 \tTraining Loss: 1.217162 \tValidation Loss: 2.757913\n",
      "Epoch: 37720 \tTraining Loss: 1.226593 \tValidation Loss: 2.757177\n",
      "Epoch: 37721 \tTraining Loss: 1.142755 \tValidation Loss: 2.759930\n",
      "Epoch: 37722 \tTraining Loss: 1.167503 \tValidation Loss: 2.760719\n",
      "Epoch: 37723 \tTraining Loss: 1.155766 \tValidation Loss: 2.760451\n",
      "Epoch: 37724 \tTraining Loss: 1.184447 \tValidation Loss: 2.758413\n",
      "Epoch: 37725 \tTraining Loss: 1.190458 \tValidation Loss: 2.760202\n",
      "Epoch: 37726 \tTraining Loss: 1.201171 \tValidation Loss: 2.759825\n",
      "Epoch: 37727 \tTraining Loss: 1.152915 \tValidation Loss: 2.757500\n",
      "Epoch: 37728 \tTraining Loss: 1.160293 \tValidation Loss: 2.759442\n",
      "Epoch: 37729 \tTraining Loss: 1.200193 \tValidation Loss: 2.759281\n",
      "Epoch: 37730 \tTraining Loss: 1.227560 \tValidation Loss: 2.757467\n",
      "Epoch: 37731 \tTraining Loss: 1.147036 \tValidation Loss: 2.760103\n",
      "Epoch: 37732 \tTraining Loss: 1.167551 \tValidation Loss: 2.760130\n",
      "Epoch: 37733 \tTraining Loss: 1.189628 \tValidation Loss: 2.760020\n",
      "Epoch: 37734 \tTraining Loss: 1.174570 \tValidation Loss: 2.759295\n",
      "Epoch: 37735 \tTraining Loss: 1.164137 \tValidation Loss: 2.760411\n",
      "Epoch: 37736 \tTraining Loss: 1.201584 \tValidation Loss: 2.758636\n",
      "Epoch: 37737 \tTraining Loss: 1.191421 \tValidation Loss: 2.759397\n",
      "Epoch: 37738 \tTraining Loss: 1.220293 \tValidation Loss: 2.759384\n",
      "Epoch: 37739 \tTraining Loss: 1.216057 \tValidation Loss: 2.757628\n",
      "Epoch: 37740 \tTraining Loss: 1.190201 \tValidation Loss: 2.759361\n",
      "Epoch: 37741 \tTraining Loss: 1.215144 \tValidation Loss: 2.759519\n",
      "Epoch: 37742 \tTraining Loss: 1.204314 \tValidation Loss: 2.760077\n",
      "Epoch: 37743 \tTraining Loss: 1.224447 \tValidation Loss: 2.759913\n",
      "Epoch: 37744 \tTraining Loss: 1.162080 \tValidation Loss: 2.761040\n",
      "Epoch: 37745 \tTraining Loss: 1.187037 \tValidation Loss: 2.759877\n",
      "Epoch: 37746 \tTraining Loss: 1.208413 \tValidation Loss: 2.758878\n",
      "Epoch: 37747 \tTraining Loss: 1.168823 \tValidation Loss: 2.760568\n",
      "Epoch: 37748 \tTraining Loss: 1.155290 \tValidation Loss: 2.759460\n",
      "Epoch: 37749 \tTraining Loss: 1.147050 \tValidation Loss: 2.760448\n",
      "Epoch: 37750 \tTraining Loss: 1.210906 \tValidation Loss: 2.759548\n",
      "Epoch: 37751 \tTraining Loss: 1.195086 \tValidation Loss: 2.759592\n",
      "Epoch: 37752 \tTraining Loss: 1.191735 \tValidation Loss: 2.759085\n",
      "Epoch: 37753 \tTraining Loss: 1.203161 \tValidation Loss: 2.760491\n",
      "Epoch: 37754 \tTraining Loss: 1.198209 \tValidation Loss: 2.759753\n",
      "Epoch: 37755 \tTraining Loss: 1.188730 \tValidation Loss: 2.761450\n",
      "Epoch: 37756 \tTraining Loss: 1.214055 \tValidation Loss: 2.759176\n",
      "Epoch: 37757 \tTraining Loss: 1.211821 \tValidation Loss: 2.758717\n",
      "Epoch: 37758 \tTraining Loss: 1.204882 \tValidation Loss: 2.760534\n",
      "Epoch: 37759 \tTraining Loss: 1.175228 \tValidation Loss: 2.759391\n",
      "Epoch: 37760 \tTraining Loss: 1.160041 \tValidation Loss: 2.760862\n",
      "Epoch: 37761 \tTraining Loss: 1.166619 \tValidation Loss: 2.760859\n",
      "Epoch: 37762 \tTraining Loss: 1.193771 \tValidation Loss: 2.763258\n",
      "Epoch: 37763 \tTraining Loss: 1.143840 \tValidation Loss: 2.759773\n",
      "Epoch: 37764 \tTraining Loss: 1.175702 \tValidation Loss: 2.759866\n",
      "Epoch: 37765 \tTraining Loss: 1.153827 \tValidation Loss: 2.761508\n",
      "Epoch: 37766 \tTraining Loss: 1.188742 \tValidation Loss: 2.762163\n",
      "Epoch: 37767 \tTraining Loss: 1.168571 \tValidation Loss: 2.760801\n",
      "Epoch: 37768 \tTraining Loss: 1.161429 \tValidation Loss: 2.760985\n",
      "Epoch: 37769 \tTraining Loss: 1.159591 \tValidation Loss: 2.760735\n",
      "Epoch: 37770 \tTraining Loss: 1.173519 \tValidation Loss: 2.759894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37771 \tTraining Loss: 1.188994 \tValidation Loss: 2.761461\n",
      "Epoch: 37772 \tTraining Loss: 1.156407 \tValidation Loss: 2.761637\n",
      "Epoch: 37773 \tTraining Loss: 1.178509 \tValidation Loss: 2.760705\n",
      "Epoch: 37774 \tTraining Loss: 1.172104 \tValidation Loss: 2.760810\n",
      "Epoch: 37775 \tTraining Loss: 1.168969 \tValidation Loss: 2.761911\n",
      "Epoch: 37776 \tTraining Loss: 1.139708 \tValidation Loss: 2.760891\n",
      "Epoch: 37777 \tTraining Loss: 1.212953 \tValidation Loss: 2.760988\n",
      "Epoch: 37778 \tTraining Loss: 1.182015 \tValidation Loss: 2.762651\n",
      "Epoch: 37779 \tTraining Loss: 1.178175 \tValidation Loss: 2.761616\n",
      "Epoch: 37780 \tTraining Loss: 1.161261 \tValidation Loss: 2.761083\n",
      "Epoch: 37781 \tTraining Loss: 1.181197 \tValidation Loss: 2.760182\n",
      "Epoch: 37782 \tTraining Loss: 1.206665 \tValidation Loss: 2.760909\n",
      "Epoch: 37783 \tTraining Loss: 1.227909 \tValidation Loss: 2.759865\n",
      "Epoch: 37784 \tTraining Loss: 1.188460 \tValidation Loss: 2.760850\n",
      "Epoch: 37785 \tTraining Loss: 1.149839 \tValidation Loss: 2.759418\n",
      "Epoch: 37786 \tTraining Loss: 1.190643 \tValidation Loss: 2.759745\n",
      "Epoch: 37787 \tTraining Loss: 1.152191 \tValidation Loss: 2.761833\n",
      "Epoch: 37788 \tTraining Loss: 1.209896 \tValidation Loss: 2.762084\n",
      "Epoch: 37789 \tTraining Loss: 1.201485 \tValidation Loss: 2.759895\n",
      "Epoch: 37790 \tTraining Loss: 1.228390 \tValidation Loss: 2.760549\n",
      "Epoch: 37791 \tTraining Loss: 1.191387 \tValidation Loss: 2.761149\n",
      "Epoch: 37792 \tTraining Loss: 1.120517 \tValidation Loss: 2.763415\n",
      "Epoch: 37793 \tTraining Loss: 1.121862 \tValidation Loss: 2.761027\n",
      "Epoch: 37794 \tTraining Loss: 1.167712 \tValidation Loss: 2.762743\n",
      "Epoch: 37795 \tTraining Loss: 1.223829 \tValidation Loss: 2.761266\n",
      "Epoch: 37796 \tTraining Loss: 1.141550 \tValidation Loss: 2.760952\n",
      "Epoch: 37797 \tTraining Loss: 1.162451 \tValidation Loss: 2.761388\n",
      "Epoch: 37798 \tTraining Loss: 1.242048 \tValidation Loss: 2.761726\n",
      "Epoch: 37799 \tTraining Loss: 1.251086 \tValidation Loss: 2.759337\n",
      "Epoch: 37800 \tTraining Loss: 1.196071 \tValidation Loss: 2.761712\n",
      "Epoch: 37801 \tTraining Loss: 1.174621 \tValidation Loss: 2.763751\n",
      "Epoch: 37802 \tTraining Loss: 1.208368 \tValidation Loss: 2.759659\n",
      "Epoch: 37803 \tTraining Loss: 1.169682 \tValidation Loss: 2.761875\n",
      "Epoch: 37804 \tTraining Loss: 1.172351 \tValidation Loss: 2.762217\n",
      "Epoch: 37805 \tTraining Loss: 1.178067 \tValidation Loss: 2.761475\n",
      "Epoch: 37806 \tTraining Loss: 1.169825 \tValidation Loss: 2.762687\n",
      "Epoch: 37807 \tTraining Loss: 1.167191 \tValidation Loss: 2.761198\n",
      "Epoch: 37808 \tTraining Loss: 1.218669 \tValidation Loss: 2.762087\n",
      "Epoch: 37809 \tTraining Loss: 1.210329 \tValidation Loss: 2.761765\n",
      "Epoch: 37810 \tTraining Loss: 1.241539 \tValidation Loss: 2.761928\n",
      "Epoch: 37811 \tTraining Loss: 1.138888 \tValidation Loss: 2.761458\n",
      "Epoch: 37812 \tTraining Loss: 1.177877 \tValidation Loss: 2.760901\n",
      "Epoch: 37813 \tTraining Loss: 1.221348 \tValidation Loss: 2.761582\n",
      "Epoch: 37814 \tTraining Loss: 1.176195 \tValidation Loss: 2.762221\n",
      "Epoch: 37815 \tTraining Loss: 1.178201 \tValidation Loss: 2.761937\n",
      "Epoch: 37816 \tTraining Loss: 1.153762 \tValidation Loss: 2.762398\n",
      "Epoch: 37817 \tTraining Loss: 1.186380 \tValidation Loss: 2.762525\n",
      "Epoch: 37818 \tTraining Loss: 1.204090 \tValidation Loss: 2.762326\n",
      "Epoch: 37819 \tTraining Loss: 1.159117 \tValidation Loss: 2.761426\n",
      "Epoch: 37820 \tTraining Loss: 1.213582 \tValidation Loss: 2.762708\n",
      "Epoch: 37821 \tTraining Loss: 1.197967 \tValidation Loss: 2.761020\n",
      "Epoch: 37822 \tTraining Loss: 1.205029 \tValidation Loss: 2.761657\n",
      "Epoch: 37823 \tTraining Loss: 1.180565 \tValidation Loss: 2.760742\n",
      "Epoch: 37824 \tTraining Loss: 1.200268 \tValidation Loss: 2.762152\n",
      "Epoch: 37825 \tTraining Loss: 1.174690 \tValidation Loss: 2.763235\n",
      "Epoch: 37826 \tTraining Loss: 1.180397 \tValidation Loss: 2.762379\n",
      "Epoch: 37827 \tTraining Loss: 1.165147 \tValidation Loss: 2.760436\n",
      "Epoch: 37828 \tTraining Loss: 1.205164 \tValidation Loss: 2.760345\n",
      "Epoch: 37829 \tTraining Loss: 1.198344 \tValidation Loss: 2.761295\n",
      "Epoch: 37830 \tTraining Loss: 1.155288 \tValidation Loss: 2.763214\n",
      "Epoch: 37831 \tTraining Loss: 1.191155 \tValidation Loss: 2.761761\n",
      "Epoch: 37832 \tTraining Loss: 1.201330 \tValidation Loss: 2.761884\n",
      "Epoch: 37833 \tTraining Loss: 1.183493 \tValidation Loss: 2.762340\n",
      "Epoch: 37834 \tTraining Loss: 1.205603 \tValidation Loss: 2.759952\n",
      "Epoch: 37835 \tTraining Loss: 1.172192 \tValidation Loss: 2.761828\n",
      "Epoch: 37836 \tTraining Loss: 1.193862 \tValidation Loss: 2.762459\n",
      "Epoch: 37837 \tTraining Loss: 1.155537 \tValidation Loss: 2.762317\n",
      "Epoch: 37838 \tTraining Loss: 1.169798 \tValidation Loss: 2.763462\n",
      "Epoch: 37839 \tTraining Loss: 1.163742 \tValidation Loss: 2.763791\n",
      "Epoch: 37840 \tTraining Loss: 1.204301 \tValidation Loss: 2.761775\n",
      "Epoch: 37841 \tTraining Loss: 1.201036 \tValidation Loss: 2.764278\n",
      "Epoch: 37842 \tTraining Loss: 1.147050 \tValidation Loss: 2.762408\n",
      "Epoch: 37843 \tTraining Loss: 1.210194 \tValidation Loss: 2.763183\n",
      "Epoch: 37844 \tTraining Loss: 1.189838 \tValidation Loss: 2.762009\n",
      "Epoch: 37845 \tTraining Loss: 1.196816 \tValidation Loss: 2.762685\n",
      "Epoch: 37846 \tTraining Loss: 1.184815 \tValidation Loss: 2.763793\n",
      "Epoch: 37847 \tTraining Loss: 1.188535 \tValidation Loss: 2.762409\n",
      "Epoch: 37848 \tTraining Loss: 1.187588 \tValidation Loss: 2.763129\n",
      "Epoch: 37849 \tTraining Loss: 1.171131 \tValidation Loss: 2.761762\n",
      "Epoch: 37850 \tTraining Loss: 1.174984 \tValidation Loss: 2.763766\n",
      "Epoch: 37851 \tTraining Loss: 1.185049 \tValidation Loss: 2.764389\n",
      "Epoch: 37852 \tTraining Loss: 1.157469 \tValidation Loss: 2.761617\n",
      "Epoch: 37853 \tTraining Loss: 1.200008 \tValidation Loss: 2.761990\n",
      "Epoch: 37854 \tTraining Loss: 1.173195 \tValidation Loss: 2.764084\n",
      "Epoch: 37855 \tTraining Loss: 1.152530 \tValidation Loss: 2.763189\n",
      "Epoch: 37856 \tTraining Loss: 1.160828 \tValidation Loss: 2.763592\n",
      "Epoch: 37857 \tTraining Loss: 1.170526 \tValidation Loss: 2.764295\n",
      "Epoch: 37858 \tTraining Loss: 1.184124 \tValidation Loss: 2.761621\n",
      "Epoch: 37859 \tTraining Loss: 1.171050 \tValidation Loss: 2.763020\n",
      "Epoch: 37860 \tTraining Loss: 1.211958 \tValidation Loss: 2.761919\n",
      "Epoch: 37861 \tTraining Loss: 1.127376 \tValidation Loss: 2.764910\n",
      "Epoch: 37862 \tTraining Loss: 1.179449 \tValidation Loss: 2.762537\n",
      "Epoch: 37863 \tTraining Loss: 1.181129 \tValidation Loss: 2.763947\n",
      "Epoch: 37864 \tTraining Loss: 1.159715 \tValidation Loss: 2.763846\n",
      "Epoch: 37865 \tTraining Loss: 1.193715 \tValidation Loss: 2.761803\n",
      "Epoch: 37866 \tTraining Loss: 1.163225 \tValidation Loss: 2.763551\n",
      "Epoch: 37867 \tTraining Loss: 1.200913 \tValidation Loss: 2.761350\n",
      "Epoch: 37868 \tTraining Loss: 1.207515 \tValidation Loss: 2.763596\n",
      "Epoch: 37869 \tTraining Loss: 1.204424 \tValidation Loss: 2.763361\n",
      "Epoch: 37870 \tTraining Loss: 1.167303 \tValidation Loss: 2.761772\n",
      "Epoch: 37871 \tTraining Loss: 1.174144 \tValidation Loss: 2.763209\n",
      "Epoch: 37872 \tTraining Loss: 1.236778 \tValidation Loss: 2.764036\n",
      "Epoch: 37873 \tTraining Loss: 1.164987 \tValidation Loss: 2.763161\n",
      "Epoch: 37874 \tTraining Loss: 1.169552 \tValidation Loss: 2.763273\n",
      "Epoch: 37875 \tTraining Loss: 1.159671 \tValidation Loss: 2.762124\n",
      "Epoch: 37876 \tTraining Loss: 1.144843 \tValidation Loss: 2.764059\n",
      "Epoch: 37877 \tTraining Loss: 1.159775 \tValidation Loss: 2.762538\n",
      "Epoch: 37878 \tTraining Loss: 1.174344 \tValidation Loss: 2.764375\n",
      "Epoch: 37879 \tTraining Loss: 1.167407 \tValidation Loss: 2.763600\n",
      "Epoch: 37880 \tTraining Loss: 1.182406 \tValidation Loss: 2.764266\n",
      "Epoch: 37881 \tTraining Loss: 1.179360 \tValidation Loss: 2.762110\n",
      "Epoch: 37882 \tTraining Loss: 1.151715 \tValidation Loss: 2.764075\n",
      "Epoch: 37883 \tTraining Loss: 1.225744 \tValidation Loss: 2.762000\n",
      "Epoch: 37884 \tTraining Loss: 1.162695 \tValidation Loss: 2.763022\n",
      "Epoch: 37885 \tTraining Loss: 1.160758 \tValidation Loss: 2.763608\n",
      "Epoch: 37886 \tTraining Loss: 1.155420 \tValidation Loss: 2.764446\n",
      "Epoch: 37887 \tTraining Loss: 1.154377 \tValidation Loss: 2.763383\n",
      "Epoch: 37888 \tTraining Loss: 1.148386 \tValidation Loss: 2.761901\n",
      "Epoch: 37889 \tTraining Loss: 1.177142 \tValidation Loss: 2.764232\n",
      "Epoch: 37890 \tTraining Loss: 1.171287 \tValidation Loss: 2.763240\n",
      "Epoch: 37891 \tTraining Loss: 1.199717 \tValidation Loss: 2.762635\n",
      "Epoch: 37892 \tTraining Loss: 1.125142 \tValidation Loss: 2.763393\n",
      "Epoch: 37893 \tTraining Loss: 1.137652 \tValidation Loss: 2.764252\n",
      "Epoch: 37894 \tTraining Loss: 1.208467 \tValidation Loss: 2.763390\n",
      "Epoch: 37895 \tTraining Loss: 1.185515 \tValidation Loss: 2.762599\n",
      "Epoch: 37896 \tTraining Loss: 1.174217 \tValidation Loss: 2.763170\n",
      "Epoch: 37897 \tTraining Loss: 1.185860 \tValidation Loss: 2.763898\n",
      "Epoch: 37898 \tTraining Loss: 1.177750 \tValidation Loss: 2.764485\n",
      "Epoch: 37899 \tTraining Loss: 1.196879 \tValidation Loss: 2.762750\n",
      "Epoch: 37900 \tTraining Loss: 1.172210 \tValidation Loss: 2.763788\n",
      "Epoch: 37901 \tTraining Loss: 1.154973 \tValidation Loss: 2.766654\n",
      "Epoch: 37902 \tTraining Loss: 1.170258 \tValidation Loss: 2.764945\n",
      "Epoch: 37903 \tTraining Loss: 1.160208 \tValidation Loss: 2.764851\n",
      "Epoch: 37904 \tTraining Loss: 1.168989 \tValidation Loss: 2.764590\n",
      "Epoch: 37905 \tTraining Loss: 1.184975 \tValidation Loss: 2.765063\n",
      "Epoch: 37906 \tTraining Loss: 1.157523 \tValidation Loss: 2.763137\n",
      "Epoch: 37907 \tTraining Loss: 1.216044 \tValidation Loss: 2.763995\n",
      "Epoch: 37908 \tTraining Loss: 1.188963 \tValidation Loss: 2.764106\n",
      "Epoch: 37909 \tTraining Loss: 1.209496 \tValidation Loss: 2.764075\n",
      "Epoch: 37910 \tTraining Loss: 1.133240 \tValidation Loss: 2.764693\n",
      "Epoch: 37911 \tTraining Loss: 1.197621 \tValidation Loss: 2.764618\n",
      "Epoch: 37912 \tTraining Loss: 1.153364 \tValidation Loss: 2.764015\n",
      "Epoch: 37913 \tTraining Loss: 1.189968 \tValidation Loss: 2.764733\n",
      "Epoch: 37914 \tTraining Loss: 1.202116 \tValidation Loss: 2.764142\n",
      "Epoch: 37915 \tTraining Loss: 1.217191 \tValidation Loss: 2.763284\n",
      "Epoch: 37916 \tTraining Loss: 1.166096 \tValidation Loss: 2.763030\n",
      "Epoch: 37917 \tTraining Loss: 1.215731 \tValidation Loss: 2.763844\n",
      "Epoch: 37918 \tTraining Loss: 1.196192 \tValidation Loss: 2.764589\n",
      "Epoch: 37919 \tTraining Loss: 1.216513 \tValidation Loss: 2.763063\n",
      "Epoch: 37920 \tTraining Loss: 1.175499 \tValidation Loss: 2.763490\n",
      "Epoch: 37921 \tTraining Loss: 1.203949 \tValidation Loss: 2.763053\n",
      "Epoch: 37922 \tTraining Loss: 1.175576 \tValidation Loss: 2.763354\n",
      "Epoch: 37923 \tTraining Loss: 1.202669 \tValidation Loss: 2.763876\n",
      "Epoch: 37924 \tTraining Loss: 1.202677 \tValidation Loss: 2.764345\n",
      "Epoch: 37925 \tTraining Loss: 1.165932 \tValidation Loss: 2.766144\n",
      "Epoch: 37926 \tTraining Loss: 1.198778 \tValidation Loss: 2.764310\n",
      "Epoch: 37927 \tTraining Loss: 1.157730 \tValidation Loss: 2.764805\n",
      "Epoch: 37928 \tTraining Loss: 1.116575 \tValidation Loss: 2.765383\n",
      "Epoch: 37929 \tTraining Loss: 1.130082 \tValidation Loss: 2.765074\n",
      "Epoch: 37930 \tTraining Loss: 1.218207 \tValidation Loss: 2.762581\n",
      "Epoch: 37931 \tTraining Loss: 1.218356 \tValidation Loss: 2.764164\n",
      "Epoch: 37932 \tTraining Loss: 1.206381 \tValidation Loss: 2.763811\n",
      "Epoch: 37933 \tTraining Loss: 1.167297 \tValidation Loss: 2.764530\n",
      "Epoch: 37934 \tTraining Loss: 1.201117 \tValidation Loss: 2.764331\n",
      "Epoch: 37935 \tTraining Loss: 1.155593 \tValidation Loss: 2.765502\n",
      "Epoch: 37936 \tTraining Loss: 1.161183 \tValidation Loss: 2.764246\n",
      "Epoch: 37937 \tTraining Loss: 1.222051 \tValidation Loss: 2.763268\n",
      "Epoch: 37938 \tTraining Loss: 1.194824 \tValidation Loss: 2.763363\n",
      "Epoch: 37939 \tTraining Loss: 1.193115 \tValidation Loss: 2.764062\n",
      "Epoch: 37940 \tTraining Loss: 1.166169 \tValidation Loss: 2.765215\n",
      "Epoch: 37941 \tTraining Loss: 1.196625 \tValidation Loss: 2.762602\n",
      "Epoch: 37942 \tTraining Loss: 1.207622 \tValidation Loss: 2.762554\n",
      "Epoch: 37943 \tTraining Loss: 1.113572 \tValidation Loss: 2.765252\n",
      "Epoch: 37944 \tTraining Loss: 1.203571 \tValidation Loss: 2.764126\n",
      "Epoch: 37945 \tTraining Loss: 1.154499 \tValidation Loss: 2.764452\n",
      "Epoch: 37946 \tTraining Loss: 1.179928 \tValidation Loss: 2.763417\n",
      "Epoch: 37947 \tTraining Loss: 1.142578 \tValidation Loss: 2.763574\n",
      "Epoch: 37948 \tTraining Loss: 1.184978 \tValidation Loss: 2.764624\n",
      "Epoch: 37949 \tTraining Loss: 1.207112 \tValidation Loss: 2.765416\n",
      "Epoch: 37950 \tTraining Loss: 1.176954 \tValidation Loss: 2.763987\n",
      "Epoch: 37951 \tTraining Loss: 1.202228 \tValidation Loss: 2.764932\n",
      "Epoch: 37952 \tTraining Loss: 1.176259 \tValidation Loss: 2.764361\n",
      "Epoch: 37953 \tTraining Loss: 1.137904 \tValidation Loss: 2.764514\n",
      "Epoch: 37954 \tTraining Loss: 1.184693 \tValidation Loss: 2.764821\n",
      "Epoch: 37955 \tTraining Loss: 1.160304 \tValidation Loss: 2.766185\n",
      "Epoch: 37956 \tTraining Loss: 1.192023 \tValidation Loss: 2.765118\n",
      "Epoch: 37957 \tTraining Loss: 1.144323 \tValidation Loss: 2.764593\n",
      "Epoch: 37958 \tTraining Loss: 1.189496 \tValidation Loss: 2.765801\n",
      "Epoch: 37959 \tTraining Loss: 1.162053 \tValidation Loss: 2.766369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37960 \tTraining Loss: 1.179139 \tValidation Loss: 2.766563\n",
      "Epoch: 37961 \tTraining Loss: 1.169120 \tValidation Loss: 2.767257\n",
      "Epoch: 37962 \tTraining Loss: 1.196720 \tValidation Loss: 2.765828\n",
      "Epoch: 37963 \tTraining Loss: 1.170843 \tValidation Loss: 2.765357\n",
      "Epoch: 37964 \tTraining Loss: 1.129201 \tValidation Loss: 2.765460\n",
      "Epoch: 37965 \tTraining Loss: 1.196798 \tValidation Loss: 2.766176\n",
      "Epoch: 37966 \tTraining Loss: 1.167223 \tValidation Loss: 2.765446\n",
      "Epoch: 37967 \tTraining Loss: 1.195065 \tValidation Loss: 2.764970\n",
      "Epoch: 37968 \tTraining Loss: 1.130498 \tValidation Loss: 2.765574\n",
      "Epoch: 37969 \tTraining Loss: 1.159721 \tValidation Loss: 2.767018\n",
      "Epoch: 37970 \tTraining Loss: 1.152364 \tValidation Loss: 2.766108\n",
      "Epoch: 37971 \tTraining Loss: 1.132468 \tValidation Loss: 2.764367\n",
      "Epoch: 37972 \tTraining Loss: 1.194995 \tValidation Loss: 2.764485\n",
      "Epoch: 37973 \tTraining Loss: 1.177526 \tValidation Loss: 2.766073\n",
      "Epoch: 37974 \tTraining Loss: 1.192343 \tValidation Loss: 2.763883\n",
      "Epoch: 37975 \tTraining Loss: 1.174340 \tValidation Loss: 2.765016\n",
      "Epoch: 37976 \tTraining Loss: 1.139199 \tValidation Loss: 2.766593\n",
      "Epoch: 37977 \tTraining Loss: 1.210020 \tValidation Loss: 2.763822\n",
      "Epoch: 37978 \tTraining Loss: 1.153698 \tValidation Loss: 2.766802\n",
      "Epoch: 37979 \tTraining Loss: 1.179729 \tValidation Loss: 2.766249\n",
      "Epoch: 37980 \tTraining Loss: 1.180920 \tValidation Loss: 2.765899\n",
      "Epoch: 37981 \tTraining Loss: 1.185142 \tValidation Loss: 2.765791\n",
      "Epoch: 37982 \tTraining Loss: 1.181725 \tValidation Loss: 2.765437\n",
      "Epoch: 37983 \tTraining Loss: 1.163668 \tValidation Loss: 2.765098\n",
      "Epoch: 37984 \tTraining Loss: 1.146180 \tValidation Loss: 2.764963\n",
      "Epoch: 37985 \tTraining Loss: 1.168152 \tValidation Loss: 2.766333\n",
      "Epoch: 37986 \tTraining Loss: 1.173813 \tValidation Loss: 2.765099\n",
      "Epoch: 37987 \tTraining Loss: 1.175602 \tValidation Loss: 2.764726\n",
      "Epoch: 37988 \tTraining Loss: 1.179929 \tValidation Loss: 2.767780\n",
      "Epoch: 37989 \tTraining Loss: 1.212190 \tValidation Loss: 2.764862\n",
      "Epoch: 37990 \tTraining Loss: 1.176700 \tValidation Loss: 2.764416\n",
      "Epoch: 37991 \tTraining Loss: 1.241651 \tValidation Loss: 2.766145\n",
      "Epoch: 37992 \tTraining Loss: 1.169802 \tValidation Loss: 2.766838\n",
      "Epoch: 37993 \tTraining Loss: 1.195058 \tValidation Loss: 2.765481\n",
      "Epoch: 37994 \tTraining Loss: 1.153142 \tValidation Loss: 2.766638\n",
      "Epoch: 37995 \tTraining Loss: 1.190112 \tValidation Loss: 2.765639\n",
      "Epoch: 37996 \tTraining Loss: 1.145871 \tValidation Loss: 2.767088\n",
      "Epoch: 37997 \tTraining Loss: 1.197913 \tValidation Loss: 2.763390\n",
      "Epoch: 37998 \tTraining Loss: 1.191626 \tValidation Loss: 2.762513\n",
      "Epoch: 37999 \tTraining Loss: 1.184501 \tValidation Loss: 2.765818\n",
      "Epoch: 38000 \tTraining Loss: 1.203612 \tValidation Loss: 2.766798\n",
      "Epoch: 38001 \tTraining Loss: 1.128898 \tValidation Loss: 2.765967\n",
      "Epoch: 38002 \tTraining Loss: 1.189527 \tValidation Loss: 2.765802\n",
      "Epoch: 38003 \tTraining Loss: 1.172247 \tValidation Loss: 2.766451\n",
      "Epoch: 38004 \tTraining Loss: 1.173918 \tValidation Loss: 2.766146\n",
      "Epoch: 38005 \tTraining Loss: 1.176904 \tValidation Loss: 2.765727\n",
      "Epoch: 38006 \tTraining Loss: 1.194231 \tValidation Loss: 2.765373\n",
      "Epoch: 38007 \tTraining Loss: 1.178067 \tValidation Loss: 2.765447\n",
      "Epoch: 38008 \tTraining Loss: 1.210296 \tValidation Loss: 2.766796\n",
      "Epoch: 38009 \tTraining Loss: 1.145500 \tValidation Loss: 2.766518\n",
      "Epoch: 38010 \tTraining Loss: 1.162073 \tValidation Loss: 2.766415\n",
      "Epoch: 38011 \tTraining Loss: 1.172287 \tValidation Loss: 2.765186\n",
      "Epoch: 38012 \tTraining Loss: 1.187090 \tValidation Loss: 2.767267\n",
      "Epoch: 38013 \tTraining Loss: 1.179817 \tValidation Loss: 2.766913\n",
      "Epoch: 38014 \tTraining Loss: 1.192904 \tValidation Loss: 2.767411\n",
      "Epoch: 38015 \tTraining Loss: 1.187180 \tValidation Loss: 2.765983\n",
      "Epoch: 38016 \tTraining Loss: 1.191621 \tValidation Loss: 2.766215\n",
      "Epoch: 38017 \tTraining Loss: 1.150903 \tValidation Loss: 2.766020\n",
      "Epoch: 38018 \tTraining Loss: 1.156814 \tValidation Loss: 2.766604\n",
      "Epoch: 38019 \tTraining Loss: 1.190296 \tValidation Loss: 2.764436\n",
      "Epoch: 38020 \tTraining Loss: 1.167137 \tValidation Loss: 2.767543\n",
      "Epoch: 38021 \tTraining Loss: 1.163119 \tValidation Loss: 2.767118\n",
      "Epoch: 38022 \tTraining Loss: 1.197368 \tValidation Loss: 2.766823\n",
      "Epoch: 38023 \tTraining Loss: 1.158699 \tValidation Loss: 2.766750\n",
      "Epoch: 38024 \tTraining Loss: 1.188352 \tValidation Loss: 2.765477\n",
      "Epoch: 38025 \tTraining Loss: 1.180083 \tValidation Loss: 2.768043\n",
      "Epoch: 38026 \tTraining Loss: 1.154116 \tValidation Loss: 2.766879\n",
      "Epoch: 38027 \tTraining Loss: 1.187368 \tValidation Loss: 2.765790\n",
      "Epoch: 38028 \tTraining Loss: 1.162065 \tValidation Loss: 2.766173\n",
      "Epoch: 38029 \tTraining Loss: 1.168738 \tValidation Loss: 2.767018\n",
      "Epoch: 38030 \tTraining Loss: 1.140930 \tValidation Loss: 2.768380\n",
      "Epoch: 38031 \tTraining Loss: 1.151889 \tValidation Loss: 2.766913\n",
      "Epoch: 38032 \tTraining Loss: 1.210406 \tValidation Loss: 2.767071\n",
      "Epoch: 38033 \tTraining Loss: 1.125057 \tValidation Loss: 2.767344\n",
      "Epoch: 38034 \tTraining Loss: 1.223412 \tValidation Loss: 2.766110\n",
      "Epoch: 38035 \tTraining Loss: 1.206095 \tValidation Loss: 2.765217\n",
      "Epoch: 38036 \tTraining Loss: 1.229752 \tValidation Loss: 2.764899\n",
      "Epoch: 38037 \tTraining Loss: 1.206809 \tValidation Loss: 2.766420\n",
      "Epoch: 38038 \tTraining Loss: 1.134247 \tValidation Loss: 2.767159\n",
      "Epoch: 38039 \tTraining Loss: 1.179611 \tValidation Loss: 2.768392\n",
      "Epoch: 38040 \tTraining Loss: 1.149825 \tValidation Loss: 2.766031\n",
      "Epoch: 38041 \tTraining Loss: 1.190759 \tValidation Loss: 2.765737\n",
      "Epoch: 38042 \tTraining Loss: 1.167188 \tValidation Loss: 2.767210\n",
      "Epoch: 38043 \tTraining Loss: 1.187568 \tValidation Loss: 2.766543\n",
      "Epoch: 38044 \tTraining Loss: 1.159686 \tValidation Loss: 2.766693\n",
      "Epoch: 38045 \tTraining Loss: 1.138255 \tValidation Loss: 2.768132\n",
      "Epoch: 38046 \tTraining Loss: 1.164679 \tValidation Loss: 2.767142\n",
      "Epoch: 38047 \tTraining Loss: 1.166133 \tValidation Loss: 2.768779\n",
      "Epoch: 38048 \tTraining Loss: 1.184001 \tValidation Loss: 2.766661\n",
      "Epoch: 38049 \tTraining Loss: 1.192154 \tValidation Loss: 2.765989\n",
      "Epoch: 38050 \tTraining Loss: 1.196296 \tValidation Loss: 2.766725\n",
      "Epoch: 38051 \tTraining Loss: 1.198184 \tValidation Loss: 2.767040\n",
      "Epoch: 38052 \tTraining Loss: 1.171402 \tValidation Loss: 2.769195\n",
      "Epoch: 38053 \tTraining Loss: 1.175671 \tValidation Loss: 2.765601\n",
      "Epoch: 38054 \tTraining Loss: 1.163869 \tValidation Loss: 2.766571\n",
      "Epoch: 38055 \tTraining Loss: 1.221709 \tValidation Loss: 2.766544\n",
      "Epoch: 38056 \tTraining Loss: 1.182835 \tValidation Loss: 2.768123\n",
      "Epoch: 38057 \tTraining Loss: 1.165798 \tValidation Loss: 2.767545\n",
      "Epoch: 38058 \tTraining Loss: 1.181550 \tValidation Loss: 2.767138\n",
      "Epoch: 38059 \tTraining Loss: 1.178381 \tValidation Loss: 2.767743\n",
      "Epoch: 38060 \tTraining Loss: 1.189769 \tValidation Loss: 2.768574\n",
      "Epoch: 38061 \tTraining Loss: 1.151265 \tValidation Loss: 2.768493\n",
      "Epoch: 38062 \tTraining Loss: 1.203543 \tValidation Loss: 2.766133\n",
      "Epoch: 38063 \tTraining Loss: 1.163041 \tValidation Loss: 2.767701\n",
      "Epoch: 38064 \tTraining Loss: 1.210237 \tValidation Loss: 2.766346\n",
      "Epoch: 38065 \tTraining Loss: 1.198251 \tValidation Loss: 2.765860\n",
      "Epoch: 38066 \tTraining Loss: 1.164735 \tValidation Loss: 2.769210\n",
      "Epoch: 38067 \tTraining Loss: 1.128480 \tValidation Loss: 2.766420\n",
      "Epoch: 38068 \tTraining Loss: 1.110834 \tValidation Loss: 2.767347\n",
      "Epoch: 38069 \tTraining Loss: 1.183075 \tValidation Loss: 2.768912\n",
      "Epoch: 38070 \tTraining Loss: 1.202999 \tValidation Loss: 2.767122\n",
      "Epoch: 38071 \tTraining Loss: 1.186159 \tValidation Loss: 2.766397\n",
      "Epoch: 38072 \tTraining Loss: 1.208932 \tValidation Loss: 2.766028\n",
      "Epoch: 38073 \tTraining Loss: 1.171302 \tValidation Loss: 2.769108\n",
      "Epoch: 38074 \tTraining Loss: 1.178456 \tValidation Loss: 2.766437\n",
      "Epoch: 38075 \tTraining Loss: 1.187915 \tValidation Loss: 2.769679\n",
      "Epoch: 38076 \tTraining Loss: 1.145270 \tValidation Loss: 2.767194\n",
      "Epoch: 38077 \tTraining Loss: 1.138083 \tValidation Loss: 2.767694\n",
      "Epoch: 38078 \tTraining Loss: 1.174967 \tValidation Loss: 2.766476\n",
      "Epoch: 38079 \tTraining Loss: 1.247776 \tValidation Loss: 2.768213\n",
      "Epoch: 38080 \tTraining Loss: 1.172157 \tValidation Loss: 2.768756\n",
      "Epoch: 38081 \tTraining Loss: 1.156159 \tValidation Loss: 2.769791\n",
      "Epoch: 38082 \tTraining Loss: 1.149839 \tValidation Loss: 2.768304\n",
      "Epoch: 38083 \tTraining Loss: 1.166835 \tValidation Loss: 2.769655\n",
      "Epoch: 38084 \tTraining Loss: 1.176979 \tValidation Loss: 2.768915\n",
      "Epoch: 38085 \tTraining Loss: 1.179474 \tValidation Loss: 2.768004\n",
      "Epoch: 38086 \tTraining Loss: 1.177054 \tValidation Loss: 2.766856\n",
      "Epoch: 38087 \tTraining Loss: 1.145520 \tValidation Loss: 2.768055\n",
      "Epoch: 38088 \tTraining Loss: 1.142271 \tValidation Loss: 2.768161\n",
      "Epoch: 38089 \tTraining Loss: 1.151882 \tValidation Loss: 2.769521\n",
      "Epoch: 38090 \tTraining Loss: 1.172986 \tValidation Loss: 2.769408\n",
      "Epoch: 38091 \tTraining Loss: 1.184217 \tValidation Loss: 2.769662\n",
      "Epoch: 38092 \tTraining Loss: 1.153501 \tValidation Loss: 2.768929\n",
      "Epoch: 38093 \tTraining Loss: 1.159269 \tValidation Loss: 2.767570\n",
      "Epoch: 38094 \tTraining Loss: 1.156203 \tValidation Loss: 2.768987\n",
      "Epoch: 38095 \tTraining Loss: 1.141087 \tValidation Loss: 2.767567\n",
      "Epoch: 38096 \tTraining Loss: 1.188076 \tValidation Loss: 2.768771\n",
      "Epoch: 38097 \tTraining Loss: 1.172952 \tValidation Loss: 2.767196\n",
      "Epoch: 38098 \tTraining Loss: 1.185048 \tValidation Loss: 2.767306\n",
      "Epoch: 38099 \tTraining Loss: 1.244396 \tValidation Loss: 2.768087\n",
      "Epoch: 38100 \tTraining Loss: 1.188872 \tValidation Loss: 2.767588\n",
      "Epoch: 38101 \tTraining Loss: 1.154374 \tValidation Loss: 2.769641\n",
      "Epoch: 38102 \tTraining Loss: 1.148228 \tValidation Loss: 2.768829\n",
      "Epoch: 38103 \tTraining Loss: 1.165496 \tValidation Loss: 2.768529\n",
      "Epoch: 38104 \tTraining Loss: 1.137494 \tValidation Loss: 2.770651\n",
      "Epoch: 38105 \tTraining Loss: 1.201130 \tValidation Loss: 2.769531\n",
      "Epoch: 38106 \tTraining Loss: 1.187811 \tValidation Loss: 2.767967\n",
      "Epoch: 38107 \tTraining Loss: 1.182895 \tValidation Loss: 2.768478\n",
      "Epoch: 38108 \tTraining Loss: 1.196675 \tValidation Loss: 2.770350\n",
      "Epoch: 38109 \tTraining Loss: 1.159764 \tValidation Loss: 2.768846\n",
      "Epoch: 38110 \tTraining Loss: 1.163368 \tValidation Loss: 2.768000\n",
      "Epoch: 38111 \tTraining Loss: 1.135838 \tValidation Loss: 2.770289\n",
      "Epoch: 38112 \tTraining Loss: 1.211207 \tValidation Loss: 2.767090\n",
      "Epoch: 38113 \tTraining Loss: 1.165823 \tValidation Loss: 2.769342\n",
      "Epoch: 38114 \tTraining Loss: 1.159404 \tValidation Loss: 2.769146\n",
      "Epoch: 38115 \tTraining Loss: 1.149952 \tValidation Loss: 2.770440\n",
      "Epoch: 38116 \tTraining Loss: 1.179867 \tValidation Loss: 2.769380\n",
      "Epoch: 38117 \tTraining Loss: 1.198582 \tValidation Loss: 2.768898\n",
      "Epoch: 38118 \tTraining Loss: 1.161026 \tValidation Loss: 2.768709\n",
      "Epoch: 38119 \tTraining Loss: 1.217968 \tValidation Loss: 2.768124\n",
      "Epoch: 38120 \tTraining Loss: 1.173376 \tValidation Loss: 2.770188\n",
      "Epoch: 38121 \tTraining Loss: 1.134519 \tValidation Loss: 2.769365\n",
      "Epoch: 38122 \tTraining Loss: 1.161839 \tValidation Loss: 2.768899\n",
      "Epoch: 38123 \tTraining Loss: 1.216794 \tValidation Loss: 2.768172\n",
      "Epoch: 38124 \tTraining Loss: 1.168753 \tValidation Loss: 2.768660\n",
      "Epoch: 38125 \tTraining Loss: 1.192744 \tValidation Loss: 2.768135\n",
      "Epoch: 38126 \tTraining Loss: 1.159817 \tValidation Loss: 2.771031\n",
      "Epoch: 38127 \tTraining Loss: 1.162846 \tValidation Loss: 2.769486\n",
      "Epoch: 38128 \tTraining Loss: 1.187418 \tValidation Loss: 2.768578\n",
      "Epoch: 38129 \tTraining Loss: 1.147968 \tValidation Loss: 2.769131\n",
      "Epoch: 38130 \tTraining Loss: 1.137438 \tValidation Loss: 2.770215\n",
      "Epoch: 38131 \tTraining Loss: 1.147055 \tValidation Loss: 2.768872\n",
      "Epoch: 38132 \tTraining Loss: 1.194925 \tValidation Loss: 2.769652\n",
      "Epoch: 38133 \tTraining Loss: 1.192617 \tValidation Loss: 2.769129\n",
      "Epoch: 38134 \tTraining Loss: 1.182043 \tValidation Loss: 2.767688\n",
      "Epoch: 38135 \tTraining Loss: 1.170426 \tValidation Loss: 2.769023\n",
      "Epoch: 38136 \tTraining Loss: 1.188877 \tValidation Loss: 2.768770\n",
      "Epoch: 38137 \tTraining Loss: 1.172178 \tValidation Loss: 2.768260\n",
      "Epoch: 38138 \tTraining Loss: 1.179199 \tValidation Loss: 2.770250\n",
      "Epoch: 38139 \tTraining Loss: 1.205882 \tValidation Loss: 2.768958\n",
      "Epoch: 38140 \tTraining Loss: 1.166731 \tValidation Loss: 2.770030\n",
      "Epoch: 38141 \tTraining Loss: 1.203396 \tValidation Loss: 2.768164\n",
      "Epoch: 38142 \tTraining Loss: 1.219560 \tValidation Loss: 2.770721\n",
      "Epoch: 38143 \tTraining Loss: 1.195751 \tValidation Loss: 2.769911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38144 \tTraining Loss: 1.198637 \tValidation Loss: 2.767215\n",
      "Epoch: 38145 \tTraining Loss: 1.165789 \tValidation Loss: 2.768585\n",
      "Epoch: 38146 \tTraining Loss: 1.173574 \tValidation Loss: 2.769114\n",
      "Epoch: 38147 \tTraining Loss: 1.174326 \tValidation Loss: 2.769284\n",
      "Epoch: 38148 \tTraining Loss: 1.188609 \tValidation Loss: 2.768822\n",
      "Epoch: 38149 \tTraining Loss: 1.202830 \tValidation Loss: 2.769308\n",
      "Epoch: 38150 \tTraining Loss: 1.133645 \tValidation Loss: 2.769144\n",
      "Epoch: 38151 \tTraining Loss: 1.199136 \tValidation Loss: 2.769589\n",
      "Epoch: 38152 \tTraining Loss: 1.135603 \tValidation Loss: 2.768061\n",
      "Epoch: 38153 \tTraining Loss: 1.137148 \tValidation Loss: 2.769747\n",
      "Epoch: 38154 \tTraining Loss: 1.213225 \tValidation Loss: 2.767745\n",
      "Epoch: 38155 \tTraining Loss: 1.153921 \tValidation Loss: 2.769491\n",
      "Epoch: 38156 \tTraining Loss: 1.208028 \tValidation Loss: 2.769439\n",
      "Epoch: 38157 \tTraining Loss: 1.190657 \tValidation Loss: 2.767851\n",
      "Epoch: 38158 \tTraining Loss: 1.160606 \tValidation Loss: 2.769030\n",
      "Epoch: 38159 \tTraining Loss: 1.204812 \tValidation Loss: 2.767683\n",
      "Epoch: 38160 \tTraining Loss: 1.148778 \tValidation Loss: 2.771042\n",
      "Epoch: 38161 \tTraining Loss: 1.196486 \tValidation Loss: 2.770603\n",
      "Epoch: 38162 \tTraining Loss: 1.192914 \tValidation Loss: 2.770529\n",
      "Epoch: 38163 \tTraining Loss: 1.157811 \tValidation Loss: 2.769029\n",
      "Epoch: 38164 \tTraining Loss: 1.161068 \tValidation Loss: 2.769834\n",
      "Epoch: 38165 \tTraining Loss: 1.161517 \tValidation Loss: 2.769244\n",
      "Epoch: 38166 \tTraining Loss: 1.155681 \tValidation Loss: 2.770770\n",
      "Epoch: 38167 \tTraining Loss: 1.128158 \tValidation Loss: 2.769408\n",
      "Epoch: 38168 \tTraining Loss: 1.160111 \tValidation Loss: 2.770081\n",
      "Epoch: 38169 \tTraining Loss: 1.193405 \tValidation Loss: 2.768445\n",
      "Epoch: 38170 \tTraining Loss: 1.194948 \tValidation Loss: 2.768801\n",
      "Epoch: 38171 \tTraining Loss: 1.156917 \tValidation Loss: 2.768960\n",
      "Epoch: 38172 \tTraining Loss: 1.180771 \tValidation Loss: 2.769679\n",
      "Epoch: 38173 \tTraining Loss: 1.129781 \tValidation Loss: 2.769890\n",
      "Epoch: 38174 \tTraining Loss: 1.185502 \tValidation Loss: 2.770465\n",
      "Epoch: 38175 \tTraining Loss: 1.215192 \tValidation Loss: 2.770578\n",
      "Epoch: 38176 \tTraining Loss: 1.193665 \tValidation Loss: 2.769817\n",
      "Epoch: 38177 \tTraining Loss: 1.183249 \tValidation Loss: 2.769836\n",
      "Epoch: 38178 \tTraining Loss: 1.161848 \tValidation Loss: 2.771476\n",
      "Epoch: 38179 \tTraining Loss: 1.137901 \tValidation Loss: 2.768993\n",
      "Epoch: 38180 \tTraining Loss: 1.155996 \tValidation Loss: 2.769381\n",
      "Epoch: 38181 \tTraining Loss: 1.192971 \tValidation Loss: 2.771057\n",
      "Epoch: 38182 \tTraining Loss: 1.203844 \tValidation Loss: 2.769331\n",
      "Epoch: 38183 \tTraining Loss: 1.135333 \tValidation Loss: 2.769631\n",
      "Epoch: 38184 \tTraining Loss: 1.114416 \tValidation Loss: 2.770740\n",
      "Epoch: 38185 \tTraining Loss: 1.155750 \tValidation Loss: 2.772306\n",
      "Epoch: 38186 \tTraining Loss: 1.119784 \tValidation Loss: 2.771914\n",
      "Epoch: 38187 \tTraining Loss: 1.132848 \tValidation Loss: 2.772195\n",
      "Epoch: 38188 \tTraining Loss: 1.179506 \tValidation Loss: 2.771401\n",
      "Epoch: 38189 \tTraining Loss: 1.170918 \tValidation Loss: 2.769859\n",
      "Epoch: 38190 \tTraining Loss: 1.220109 \tValidation Loss: 2.770953\n",
      "Epoch: 38191 \tTraining Loss: 1.166797 \tValidation Loss: 2.769427\n",
      "Epoch: 38192 \tTraining Loss: 1.173477 \tValidation Loss: 2.769788\n",
      "Epoch: 38193 \tTraining Loss: 1.167782 \tValidation Loss: 2.771841\n",
      "Epoch: 38194 \tTraining Loss: 1.187563 \tValidation Loss: 2.771364\n",
      "Epoch: 38195 \tTraining Loss: 1.168223 \tValidation Loss: 2.771791\n",
      "Epoch: 38196 \tTraining Loss: 1.178508 \tValidation Loss: 2.768926\n",
      "Epoch: 38197 \tTraining Loss: 1.156908 \tValidation Loss: 2.771193\n",
      "Epoch: 38198 \tTraining Loss: 1.139902 \tValidation Loss: 2.771910\n",
      "Epoch: 38199 \tTraining Loss: 1.183177 \tValidation Loss: 2.770428\n",
      "Epoch: 38200 \tTraining Loss: 1.140490 \tValidation Loss: 2.773093\n",
      "Epoch: 38201 \tTraining Loss: 1.218584 \tValidation Loss: 2.769644\n",
      "Epoch: 38202 \tTraining Loss: 1.183303 \tValidation Loss: 2.770515\n",
      "Epoch: 38203 \tTraining Loss: 1.168692 \tValidation Loss: 2.770708\n",
      "Epoch: 38204 \tTraining Loss: 1.210895 \tValidation Loss: 2.771070\n",
      "Epoch: 38205 \tTraining Loss: 1.204344 \tValidation Loss: 2.772142\n",
      "Epoch: 38206 \tTraining Loss: 1.172416 \tValidation Loss: 2.773479\n",
      "Epoch: 38207 \tTraining Loss: 1.191143 \tValidation Loss: 2.769841\n",
      "Epoch: 38208 \tTraining Loss: 1.206848 \tValidation Loss: 2.771200\n",
      "Epoch: 38209 \tTraining Loss: 1.156881 \tValidation Loss: 2.771405\n",
      "Epoch: 38210 \tTraining Loss: 1.186303 \tValidation Loss: 2.772054\n",
      "Epoch: 38211 \tTraining Loss: 1.203836 \tValidation Loss: 2.773154\n",
      "Epoch: 38212 \tTraining Loss: 1.185827 \tValidation Loss: 2.771249\n",
      "Epoch: 38213 \tTraining Loss: 1.182161 \tValidation Loss: 2.772179\n",
      "Epoch: 38214 \tTraining Loss: 1.164775 \tValidation Loss: 2.771538\n",
      "Epoch: 38215 \tTraining Loss: 1.181538 \tValidation Loss: 2.770768\n",
      "Epoch: 38216 \tTraining Loss: 1.145314 \tValidation Loss: 2.770112\n",
      "Epoch: 38217 \tTraining Loss: 1.184638 \tValidation Loss: 2.769840\n",
      "Epoch: 38218 \tTraining Loss: 1.158306 \tValidation Loss: 2.772558\n",
      "Epoch: 38219 \tTraining Loss: 1.193543 \tValidation Loss: 2.772030\n",
      "Epoch: 38220 \tTraining Loss: 1.132214 \tValidation Loss: 2.772976\n",
      "Epoch: 38221 \tTraining Loss: 1.160733 \tValidation Loss: 2.769909\n",
      "Epoch: 38222 \tTraining Loss: 1.202904 \tValidation Loss: 2.770629\n",
      "Epoch: 38223 \tTraining Loss: 1.198826 \tValidation Loss: 2.771848\n",
      "Epoch: 38224 \tTraining Loss: 1.145858 \tValidation Loss: 2.770052\n",
      "Epoch: 38225 \tTraining Loss: 1.132307 \tValidation Loss: 2.769944\n",
      "Epoch: 38226 \tTraining Loss: 1.202356 \tValidation Loss: 2.770701\n",
      "Epoch: 38227 \tTraining Loss: 1.184685 \tValidation Loss: 2.770855\n",
      "Epoch: 38228 \tTraining Loss: 1.171940 \tValidation Loss: 2.771222\n",
      "Epoch: 38229 \tTraining Loss: 1.196750 \tValidation Loss: 2.770167\n",
      "Epoch: 38230 \tTraining Loss: 1.167040 \tValidation Loss: 2.771798\n",
      "Epoch: 38231 \tTraining Loss: 1.150184 \tValidation Loss: 2.771151\n",
      "Epoch: 38232 \tTraining Loss: 1.202138 \tValidation Loss: 2.772880\n",
      "Epoch: 38233 \tTraining Loss: 1.172259 \tValidation Loss: 2.772262\n",
      "Epoch: 38234 \tTraining Loss: 1.195632 \tValidation Loss: 2.772656\n",
      "Epoch: 38235 \tTraining Loss: 1.125808 \tValidation Loss: 2.774495\n",
      "Epoch: 38236 \tTraining Loss: 1.162523 \tValidation Loss: 2.773124\n",
      "Epoch: 38237 \tTraining Loss: 1.187131 \tValidation Loss: 2.771030\n",
      "Epoch: 38238 \tTraining Loss: 1.186095 \tValidation Loss: 2.769959\n",
      "Epoch: 38239 \tTraining Loss: 1.155990 \tValidation Loss: 2.771035\n",
      "Epoch: 38240 \tTraining Loss: 1.201622 \tValidation Loss: 2.771341\n",
      "Epoch: 38241 \tTraining Loss: 1.114408 \tValidation Loss: 2.772324\n",
      "Epoch: 38242 \tTraining Loss: 1.152012 \tValidation Loss: 2.773787\n",
      "Epoch: 38243 \tTraining Loss: 1.123788 \tValidation Loss: 2.771700\n",
      "Epoch: 38244 \tTraining Loss: 1.174795 \tValidation Loss: 2.772353\n",
      "Epoch: 38245 \tTraining Loss: 1.138577 \tValidation Loss: 2.771618\n",
      "Epoch: 38246 \tTraining Loss: 1.161283 \tValidation Loss: 2.770705\n",
      "Epoch: 38247 \tTraining Loss: 1.159237 \tValidation Loss: 2.772786\n",
      "Epoch: 38248 \tTraining Loss: 1.202548 \tValidation Loss: 2.771651\n",
      "Epoch: 38249 \tTraining Loss: 1.197021 \tValidation Loss: 2.772670\n",
      "Epoch: 38250 \tTraining Loss: 1.241885 \tValidation Loss: 2.772205\n",
      "Epoch: 38251 \tTraining Loss: 1.229674 \tValidation Loss: 2.770701\n",
      "Epoch: 38252 \tTraining Loss: 1.151697 \tValidation Loss: 2.772717\n",
      "Epoch: 38253 \tTraining Loss: 1.241986 \tValidation Loss: 2.770590\n",
      "Epoch: 38254 \tTraining Loss: 1.187196 \tValidation Loss: 2.773451\n",
      "Epoch: 38255 \tTraining Loss: 1.191025 \tValidation Loss: 2.772848\n",
      "Epoch: 38256 \tTraining Loss: 1.149949 \tValidation Loss: 2.771848\n",
      "Epoch: 38257 \tTraining Loss: 1.153047 \tValidation Loss: 2.772697\n",
      "Epoch: 38258 \tTraining Loss: 1.204462 \tValidation Loss: 2.771338\n",
      "Epoch: 38259 \tTraining Loss: 1.188191 \tValidation Loss: 2.773572\n",
      "Epoch: 38260 \tTraining Loss: 1.164694 \tValidation Loss: 2.772727\n",
      "Epoch: 38261 \tTraining Loss: 1.143539 \tValidation Loss: 2.773441\n",
      "Epoch: 38262 \tTraining Loss: 1.176783 \tValidation Loss: 2.772788\n",
      "Epoch: 38263 \tTraining Loss: 1.165291 \tValidation Loss: 2.771626\n",
      "Epoch: 38264 \tTraining Loss: 1.133713 \tValidation Loss: 2.772661\n",
      "Epoch: 38265 \tTraining Loss: 1.197155 \tValidation Loss: 2.772962\n",
      "Epoch: 38266 \tTraining Loss: 1.155375 \tValidation Loss: 2.773465\n",
      "Epoch: 38267 \tTraining Loss: 1.158862 \tValidation Loss: 2.773890\n",
      "Epoch: 38268 \tTraining Loss: 1.185979 \tValidation Loss: 2.772624\n",
      "Epoch: 38269 \tTraining Loss: 1.163070 \tValidation Loss: 2.772875\n",
      "Epoch: 38270 \tTraining Loss: 1.196339 \tValidation Loss: 2.771929\n",
      "Epoch: 38271 \tTraining Loss: 1.147788 \tValidation Loss: 2.772429\n",
      "Epoch: 38272 \tTraining Loss: 1.202660 \tValidation Loss: 2.771632\n",
      "Epoch: 38273 \tTraining Loss: 1.160250 \tValidation Loss: 2.773155\n",
      "Epoch: 38274 \tTraining Loss: 1.178522 \tValidation Loss: 2.772116\n",
      "Epoch: 38275 \tTraining Loss: 1.119075 \tValidation Loss: 2.772810\n",
      "Epoch: 38276 \tTraining Loss: 1.197068 \tValidation Loss: 2.771152\n",
      "Epoch: 38277 \tTraining Loss: 1.180363 \tValidation Loss: 2.771485\n",
      "Epoch: 38278 \tTraining Loss: 1.155592 \tValidation Loss: 2.773183\n",
      "Epoch: 38279 \tTraining Loss: 1.153641 \tValidation Loss: 2.774079\n",
      "Epoch: 38280 \tTraining Loss: 1.164515 \tValidation Loss: 2.773777\n",
      "Epoch: 38281 \tTraining Loss: 1.157473 \tValidation Loss: 2.772503\n",
      "Epoch: 38282 \tTraining Loss: 1.152539 \tValidation Loss: 2.773837\n",
      "Epoch: 38283 \tTraining Loss: 1.196905 \tValidation Loss: 2.771665\n",
      "Epoch: 38284 \tTraining Loss: 1.152225 \tValidation Loss: 2.773565\n",
      "Epoch: 38285 \tTraining Loss: 1.164536 \tValidation Loss: 2.772433\n",
      "Epoch: 38286 \tTraining Loss: 1.165727 \tValidation Loss: 2.772302\n",
      "Epoch: 38287 \tTraining Loss: 1.180022 \tValidation Loss: 2.772331\n",
      "Epoch: 38288 \tTraining Loss: 1.181636 \tValidation Loss: 2.773442\n",
      "Epoch: 38289 \tTraining Loss: 1.156942 \tValidation Loss: 2.774090\n",
      "Epoch: 38290 \tTraining Loss: 1.167803 \tValidation Loss: 2.772461\n",
      "Epoch: 38291 \tTraining Loss: 1.197167 \tValidation Loss: 2.773514\n",
      "Epoch: 38292 \tTraining Loss: 1.172992 \tValidation Loss: 2.772450\n",
      "Epoch: 38293 \tTraining Loss: 1.246765 \tValidation Loss: 2.773475\n",
      "Epoch: 38294 \tTraining Loss: 1.173552 \tValidation Loss: 2.772577\n",
      "Epoch: 38295 \tTraining Loss: 1.179181 \tValidation Loss: 2.770610\n",
      "Epoch: 38296 \tTraining Loss: 1.140070 \tValidation Loss: 2.772866\n",
      "Epoch: 38297 \tTraining Loss: 1.134473 \tValidation Loss: 2.773400\n",
      "Epoch: 38298 \tTraining Loss: 1.195746 \tValidation Loss: 2.773328\n",
      "Epoch: 38299 \tTraining Loss: 1.126920 \tValidation Loss: 2.773664\n",
      "Epoch: 38300 \tTraining Loss: 1.155756 \tValidation Loss: 2.772002\n",
      "Epoch: 38301 \tTraining Loss: 1.177976 \tValidation Loss: 2.773599\n",
      "Epoch: 38302 \tTraining Loss: 1.173292 \tValidation Loss: 2.771470\n",
      "Epoch: 38303 \tTraining Loss: 1.195056 \tValidation Loss: 2.773201\n",
      "Epoch: 38304 \tTraining Loss: 1.211488 \tValidation Loss: 2.771724\n",
      "Epoch: 38305 \tTraining Loss: 1.232586 \tValidation Loss: 2.772002\n",
      "Epoch: 38306 \tTraining Loss: 1.196163 \tValidation Loss: 2.773375\n",
      "Epoch: 38307 \tTraining Loss: 1.158471 \tValidation Loss: 2.772835\n",
      "Epoch: 38308 \tTraining Loss: 1.215585 \tValidation Loss: 2.774493\n",
      "Epoch: 38309 \tTraining Loss: 1.186760 \tValidation Loss: 2.772938\n",
      "Epoch: 38310 \tTraining Loss: 1.149835 \tValidation Loss: 2.772786\n",
      "Epoch: 38311 \tTraining Loss: 1.179123 \tValidation Loss: 2.772416\n",
      "Epoch: 38312 \tTraining Loss: 1.163372 \tValidation Loss: 2.773971\n",
      "Epoch: 38313 \tTraining Loss: 1.153806 \tValidation Loss: 2.773498\n",
      "Epoch: 38314 \tTraining Loss: 1.203123 \tValidation Loss: 2.772818\n",
      "Epoch: 38315 \tTraining Loss: 1.209766 \tValidation Loss: 2.773670\n",
      "Epoch: 38316 \tTraining Loss: 1.176616 \tValidation Loss: 2.774296\n",
      "Epoch: 38317 \tTraining Loss: 1.150739 \tValidation Loss: 2.773842\n",
      "Epoch: 38318 \tTraining Loss: 1.144294 \tValidation Loss: 2.772720\n",
      "Epoch: 38319 \tTraining Loss: 1.161017 \tValidation Loss: 2.773775\n",
      "Epoch: 38320 \tTraining Loss: 1.165880 \tValidation Loss: 2.773659\n",
      "Epoch: 38321 \tTraining Loss: 1.203662 \tValidation Loss: 2.774074\n",
      "Epoch: 38322 \tTraining Loss: 1.188120 \tValidation Loss: 2.775008\n",
      "Epoch: 38323 \tTraining Loss: 1.173637 \tValidation Loss: 2.774342\n",
      "Epoch: 38324 \tTraining Loss: 1.157542 \tValidation Loss: 2.773586\n",
      "Epoch: 38325 \tTraining Loss: 1.165635 \tValidation Loss: 2.774297\n",
      "Epoch: 38326 \tTraining Loss: 1.150874 \tValidation Loss: 2.774253\n",
      "Epoch: 38327 \tTraining Loss: 1.148330 \tValidation Loss: 2.774785\n",
      "Epoch: 38328 \tTraining Loss: 1.172591 \tValidation Loss: 2.773176\n",
      "Epoch: 38329 \tTraining Loss: 1.190894 \tValidation Loss: 2.772833\n",
      "Epoch: 38330 \tTraining Loss: 1.160739 \tValidation Loss: 2.774777\n",
      "Epoch: 38331 \tTraining Loss: 1.184440 \tValidation Loss: 2.772865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38332 \tTraining Loss: 1.165332 \tValidation Loss: 2.774496\n",
      "Epoch: 38333 \tTraining Loss: 1.138859 \tValidation Loss: 2.773723\n",
      "Epoch: 38334 \tTraining Loss: 1.222897 \tValidation Loss: 2.774105\n",
      "Epoch: 38335 \tTraining Loss: 1.185534 \tValidation Loss: 2.775007\n",
      "Epoch: 38336 \tTraining Loss: 1.163914 \tValidation Loss: 2.775373\n",
      "Epoch: 38337 \tTraining Loss: 1.180651 \tValidation Loss: 2.775165\n",
      "Epoch: 38338 \tTraining Loss: 1.203195 \tValidation Loss: 2.773414\n",
      "Epoch: 38339 \tTraining Loss: 1.154510 \tValidation Loss: 2.772932\n",
      "Epoch: 38340 \tTraining Loss: 1.138892 \tValidation Loss: 2.774667\n",
      "Epoch: 38341 \tTraining Loss: 1.168832 \tValidation Loss: 2.773719\n",
      "Epoch: 38342 \tTraining Loss: 1.156744 \tValidation Loss: 2.774135\n",
      "Epoch: 38343 \tTraining Loss: 1.184638 \tValidation Loss: 2.774107\n",
      "Epoch: 38344 \tTraining Loss: 1.191741 \tValidation Loss: 2.774221\n",
      "Epoch: 38345 \tTraining Loss: 1.189577 \tValidation Loss: 2.774071\n",
      "Epoch: 38346 \tTraining Loss: 1.152026 \tValidation Loss: 2.773196\n",
      "Epoch: 38347 \tTraining Loss: 1.192260 \tValidation Loss: 2.775424\n",
      "Epoch: 38348 \tTraining Loss: 1.195696 \tValidation Loss: 2.772414\n",
      "Epoch: 38349 \tTraining Loss: 1.154701 \tValidation Loss: 2.774372\n",
      "Epoch: 38350 \tTraining Loss: 1.164192 \tValidation Loss: 2.773942\n",
      "Epoch: 38351 \tTraining Loss: 1.202365 \tValidation Loss: 2.775484\n",
      "Epoch: 38352 \tTraining Loss: 1.189118 \tValidation Loss: 2.772468\n",
      "Epoch: 38353 \tTraining Loss: 1.190129 \tValidation Loss: 2.775691\n",
      "Epoch: 38354 \tTraining Loss: 1.147916 \tValidation Loss: 2.775608\n",
      "Epoch: 38355 \tTraining Loss: 1.171302 \tValidation Loss: 2.772883\n",
      "Epoch: 38356 \tTraining Loss: 1.198313 \tValidation Loss: 2.775002\n",
      "Epoch: 38357 \tTraining Loss: 1.123731 \tValidation Loss: 2.774208\n",
      "Epoch: 38358 \tTraining Loss: 1.148167 \tValidation Loss: 2.776053\n",
      "Epoch: 38359 \tTraining Loss: 1.162748 \tValidation Loss: 2.775520\n",
      "Epoch: 38360 \tTraining Loss: 1.217359 \tValidation Loss: 2.774423\n",
      "Epoch: 38361 \tTraining Loss: 1.172409 \tValidation Loss: 2.776618\n",
      "Epoch: 38362 \tTraining Loss: 1.167791 \tValidation Loss: 2.773835\n",
      "Epoch: 38363 \tTraining Loss: 1.168830 \tValidation Loss: 2.775177\n",
      "Epoch: 38364 \tTraining Loss: 1.195565 \tValidation Loss: 2.776473\n",
      "Epoch: 38365 \tTraining Loss: 1.132581 \tValidation Loss: 2.774910\n",
      "Epoch: 38366 \tTraining Loss: 1.168193 \tValidation Loss: 2.775042\n",
      "Epoch: 38367 \tTraining Loss: 1.185737 \tValidation Loss: 2.774268\n",
      "Epoch: 38368 \tTraining Loss: 1.215213 \tValidation Loss: 2.774901\n",
      "Epoch: 38369 \tTraining Loss: 1.127905 \tValidation Loss: 2.776556\n",
      "Epoch: 38370 \tTraining Loss: 1.131977 \tValidation Loss: 2.774412\n",
      "Epoch: 38371 \tTraining Loss: 1.144445 \tValidation Loss: 2.775657\n",
      "Epoch: 38372 \tTraining Loss: 1.216027 \tValidation Loss: 2.775483\n",
      "Epoch: 38373 \tTraining Loss: 1.115109 \tValidation Loss: 2.777235\n",
      "Epoch: 38374 \tTraining Loss: 1.162404 \tValidation Loss: 2.776058\n",
      "Epoch: 38375 \tTraining Loss: 1.151066 \tValidation Loss: 2.776433\n",
      "Epoch: 38376 \tTraining Loss: 1.163843 \tValidation Loss: 2.774471\n",
      "Epoch: 38377 \tTraining Loss: 1.169159 \tValidation Loss: 2.774344\n",
      "Epoch: 38378 \tTraining Loss: 1.237759 \tValidation Loss: 2.775386\n",
      "Epoch: 38379 \tTraining Loss: 1.161425 \tValidation Loss: 2.776002\n",
      "Epoch: 38380 \tTraining Loss: 1.180487 \tValidation Loss: 2.775966\n",
      "Epoch: 38381 \tTraining Loss: 1.188142 \tValidation Loss: 2.776066\n",
      "Epoch: 38382 \tTraining Loss: 1.178817 \tValidation Loss: 2.775522\n",
      "Epoch: 38383 \tTraining Loss: 1.148583 \tValidation Loss: 2.774732\n",
      "Epoch: 38384 \tTraining Loss: 1.169650 \tValidation Loss: 2.774981\n",
      "Epoch: 38385 \tTraining Loss: 1.212617 \tValidation Loss: 2.776055\n",
      "Epoch: 38386 \tTraining Loss: 1.125793 \tValidation Loss: 2.776885\n",
      "Epoch: 38387 \tTraining Loss: 1.215434 \tValidation Loss: 2.775974\n",
      "Epoch: 38388 \tTraining Loss: 1.154922 \tValidation Loss: 2.775512\n",
      "Epoch: 38389 \tTraining Loss: 1.183783 \tValidation Loss: 2.776310\n",
      "Epoch: 38390 \tTraining Loss: 1.204292 \tValidation Loss: 2.775310\n",
      "Epoch: 38391 \tTraining Loss: 1.142226 \tValidation Loss: 2.777042\n",
      "Epoch: 38392 \tTraining Loss: 1.175496 \tValidation Loss: 2.776755\n",
      "Epoch: 38393 \tTraining Loss: 1.143062 \tValidation Loss: 2.774910\n",
      "Epoch: 38394 \tTraining Loss: 1.145090 \tValidation Loss: 2.775302\n",
      "Epoch: 38395 \tTraining Loss: 1.145330 \tValidation Loss: 2.776179\n",
      "Epoch: 38396 \tTraining Loss: 1.210133 \tValidation Loss: 2.776633\n",
      "Epoch: 38397 \tTraining Loss: 1.167902 \tValidation Loss: 2.776958\n",
      "Epoch: 38398 \tTraining Loss: 1.205880 \tValidation Loss: 2.775179\n",
      "Epoch: 38399 \tTraining Loss: 1.192172 \tValidation Loss: 2.775398\n",
      "Epoch: 38400 \tTraining Loss: 1.162135 \tValidation Loss: 2.775991\n",
      "Epoch: 38401 \tTraining Loss: 1.130451 \tValidation Loss: 2.775399\n",
      "Epoch: 38402 \tTraining Loss: 1.197932 \tValidation Loss: 2.776059\n",
      "Epoch: 38403 \tTraining Loss: 1.159943 \tValidation Loss: 2.776234\n",
      "Epoch: 38404 \tTraining Loss: 1.209109 \tValidation Loss: 2.775578\n",
      "Epoch: 38405 \tTraining Loss: 1.194662 \tValidation Loss: 2.774922\n",
      "Epoch: 38406 \tTraining Loss: 1.182811 \tValidation Loss: 2.776109\n",
      "Epoch: 38407 \tTraining Loss: 1.170743 \tValidation Loss: 2.776350\n",
      "Epoch: 38408 \tTraining Loss: 1.150520 \tValidation Loss: 2.776325\n",
      "Epoch: 38409 \tTraining Loss: 1.147416 \tValidation Loss: 2.778050\n",
      "Epoch: 38410 \tTraining Loss: 1.179165 \tValidation Loss: 2.775635\n",
      "Epoch: 38411 \tTraining Loss: 1.159984 \tValidation Loss: 2.777169\n",
      "Epoch: 38412 \tTraining Loss: 1.244607 \tValidation Loss: 2.776601\n",
      "Epoch: 38413 \tTraining Loss: 1.201978 \tValidation Loss: 2.776670\n",
      "Epoch: 38414 \tTraining Loss: 1.136212 \tValidation Loss: 2.777872\n",
      "Epoch: 38415 \tTraining Loss: 1.163847 \tValidation Loss: 2.777273\n",
      "Epoch: 38416 \tTraining Loss: 1.153690 \tValidation Loss: 2.776211\n",
      "Epoch: 38417 \tTraining Loss: 1.187559 \tValidation Loss: 2.777383\n",
      "Epoch: 38418 \tTraining Loss: 1.103804 \tValidation Loss: 2.777483\n",
      "Epoch: 38419 \tTraining Loss: 1.148026 \tValidation Loss: 2.776781\n",
      "Epoch: 38420 \tTraining Loss: 1.163782 \tValidation Loss: 2.775730\n",
      "Epoch: 38421 \tTraining Loss: 1.159002 \tValidation Loss: 2.776024\n",
      "Epoch: 38422 \tTraining Loss: 1.154814 \tValidation Loss: 2.778274\n",
      "Epoch: 38423 \tTraining Loss: 1.151109 \tValidation Loss: 2.775631\n",
      "Epoch: 38424 \tTraining Loss: 1.139932 \tValidation Loss: 2.777342\n",
      "Epoch: 38425 \tTraining Loss: 1.214691 \tValidation Loss: 2.777397\n",
      "Epoch: 38426 \tTraining Loss: 1.190285 \tValidation Loss: 2.777258\n",
      "Epoch: 38427 \tTraining Loss: 1.164418 \tValidation Loss: 2.775932\n",
      "Epoch: 38428 \tTraining Loss: 1.155801 \tValidation Loss: 2.777431\n",
      "Epoch: 38429 \tTraining Loss: 1.185024 \tValidation Loss: 2.776235\n",
      "Epoch: 38430 \tTraining Loss: 1.192541 \tValidation Loss: 2.776830\n",
      "Epoch: 38431 \tTraining Loss: 1.181059 \tValidation Loss: 2.776379\n",
      "Epoch: 38432 \tTraining Loss: 1.132629 \tValidation Loss: 2.778432\n",
      "Epoch: 38433 \tTraining Loss: 1.190293 \tValidation Loss: 2.776268\n",
      "Epoch: 38434 \tTraining Loss: 1.196349 \tValidation Loss: 2.775279\n",
      "Epoch: 38435 \tTraining Loss: 1.170892 \tValidation Loss: 2.777818\n",
      "Epoch: 38436 \tTraining Loss: 1.176824 \tValidation Loss: 2.776576\n",
      "Epoch: 38437 \tTraining Loss: 1.211019 \tValidation Loss: 2.778282\n",
      "Epoch: 38438 \tTraining Loss: 1.212482 \tValidation Loss: 2.776933\n",
      "Epoch: 38439 \tTraining Loss: 1.217406 \tValidation Loss: 2.775959\n",
      "Epoch: 38440 \tTraining Loss: 1.184640 \tValidation Loss: 2.777562\n",
      "Epoch: 38441 \tTraining Loss: 1.229370 \tValidation Loss: 2.777000\n",
      "Epoch: 38442 \tTraining Loss: 1.197659 \tValidation Loss: 2.775659\n",
      "Epoch: 38443 \tTraining Loss: 1.183995 \tValidation Loss: 2.776896\n",
      "Epoch: 38444 \tTraining Loss: 1.171545 \tValidation Loss: 2.777920\n",
      "Epoch: 38445 \tTraining Loss: 1.207646 \tValidation Loss: 2.778446\n",
      "Epoch: 38446 \tTraining Loss: 1.221059 \tValidation Loss: 2.775433\n",
      "Epoch: 38447 \tTraining Loss: 1.211257 \tValidation Loss: 2.776754\n",
      "Epoch: 38448 \tTraining Loss: 1.163315 \tValidation Loss: 2.776983\n",
      "Epoch: 38449 \tTraining Loss: 1.155818 \tValidation Loss: 2.777331\n",
      "Epoch: 38450 \tTraining Loss: 1.193602 \tValidation Loss: 2.778021\n",
      "Epoch: 38451 \tTraining Loss: 1.161618 \tValidation Loss: 2.776264\n",
      "Epoch: 38452 \tTraining Loss: 1.178857 \tValidation Loss: 2.777796\n",
      "Epoch: 38453 \tTraining Loss: 1.178212 \tValidation Loss: 2.777402\n",
      "Epoch: 38454 \tTraining Loss: 1.170023 \tValidation Loss: 2.778400\n",
      "Epoch: 38455 \tTraining Loss: 1.191144 \tValidation Loss: 2.776957\n",
      "Epoch: 38456 \tTraining Loss: 1.158656 \tValidation Loss: 2.776389\n",
      "Epoch: 38457 \tTraining Loss: 1.123329 \tValidation Loss: 2.776367\n",
      "Epoch: 38458 \tTraining Loss: 1.232491 \tValidation Loss: 2.778060\n",
      "Epoch: 38459 \tTraining Loss: 1.194776 \tValidation Loss: 2.777469\n",
      "Epoch: 38460 \tTraining Loss: 1.196505 \tValidation Loss: 2.777058\n",
      "Epoch: 38461 \tTraining Loss: 1.173558 \tValidation Loss: 2.777652\n",
      "Epoch: 38462 \tTraining Loss: 1.176596 \tValidation Loss: 2.777928\n",
      "Epoch: 38463 \tTraining Loss: 1.187384 \tValidation Loss: 2.776711\n",
      "Epoch: 38464 \tTraining Loss: 1.204145 \tValidation Loss: 2.778150\n",
      "Epoch: 38465 \tTraining Loss: 1.185604 \tValidation Loss: 2.776669\n",
      "Epoch: 38466 \tTraining Loss: 1.209405 \tValidation Loss: 2.777336\n",
      "Epoch: 38467 \tTraining Loss: 1.145268 \tValidation Loss: 2.776407\n",
      "Epoch: 38468 \tTraining Loss: 1.227174 \tValidation Loss: 2.778511\n",
      "Epoch: 38469 \tTraining Loss: 1.195986 \tValidation Loss: 2.776600\n",
      "Epoch: 38470 \tTraining Loss: 1.156501 \tValidation Loss: 2.778665\n",
      "Epoch: 38471 \tTraining Loss: 1.183210 \tValidation Loss: 2.777106\n",
      "Epoch: 38472 \tTraining Loss: 1.188158 \tValidation Loss: 2.776839\n",
      "Epoch: 38473 \tTraining Loss: 1.181944 \tValidation Loss: 2.776608\n",
      "Epoch: 38474 \tTraining Loss: 1.170120 \tValidation Loss: 2.778190\n",
      "Epoch: 38475 \tTraining Loss: 1.193268 \tValidation Loss: 2.778226\n",
      "Epoch: 38476 \tTraining Loss: 1.211817 \tValidation Loss: 2.776432\n",
      "Epoch: 38477 \tTraining Loss: 1.147997 \tValidation Loss: 2.779730\n",
      "Epoch: 38478 \tTraining Loss: 1.157033 \tValidation Loss: 2.776656\n",
      "Epoch: 38479 \tTraining Loss: 1.175692 \tValidation Loss: 2.777503\n",
      "Epoch: 38480 \tTraining Loss: 1.191719 \tValidation Loss: 2.778185\n",
      "Epoch: 38481 \tTraining Loss: 1.144314 \tValidation Loss: 2.777400\n",
      "Epoch: 38482 \tTraining Loss: 1.136863 \tValidation Loss: 2.779367\n",
      "Epoch: 38483 \tTraining Loss: 1.181203 \tValidation Loss: 2.778940\n",
      "Epoch: 38484 \tTraining Loss: 1.181443 \tValidation Loss: 2.778610\n",
      "Epoch: 38485 \tTraining Loss: 1.181305 \tValidation Loss: 2.778719\n",
      "Epoch: 38486 \tTraining Loss: 1.185540 \tValidation Loss: 2.777954\n",
      "Epoch: 38487 \tTraining Loss: 1.190544 \tValidation Loss: 2.778152\n",
      "Epoch: 38488 \tTraining Loss: 1.177060 \tValidation Loss: 2.777897\n",
      "Epoch: 38489 \tTraining Loss: 1.184663 \tValidation Loss: 2.780178\n",
      "Epoch: 38490 \tTraining Loss: 1.200975 \tValidation Loss: 2.777948\n",
      "Epoch: 38491 \tTraining Loss: 1.173249 \tValidation Loss: 2.778360\n",
      "Epoch: 38492 \tTraining Loss: 1.156895 \tValidation Loss: 2.777446\n",
      "Epoch: 38493 \tTraining Loss: 1.144937 \tValidation Loss: 2.778008\n",
      "Epoch: 38494 \tTraining Loss: 1.177971 \tValidation Loss: 2.777788\n",
      "Epoch: 38495 \tTraining Loss: 1.137662 \tValidation Loss: 2.778086\n",
      "Epoch: 38496 \tTraining Loss: 1.180331 \tValidation Loss: 2.777198\n",
      "Epoch: 38497 \tTraining Loss: 1.161245 \tValidation Loss: 2.778536\n",
      "Epoch: 38498 \tTraining Loss: 1.195153 \tValidation Loss: 2.776794\n",
      "Epoch: 38499 \tTraining Loss: 1.176815 \tValidation Loss: 2.778316\n",
      "Epoch: 38500 \tTraining Loss: 1.182005 \tValidation Loss: 2.778227\n",
      "Epoch: 38501 \tTraining Loss: 1.163842 \tValidation Loss: 2.778676\n",
      "Epoch: 38502 \tTraining Loss: 1.153890 \tValidation Loss: 2.776779\n",
      "Epoch: 38503 \tTraining Loss: 1.156760 \tValidation Loss: 2.778785\n",
      "Epoch: 38504 \tTraining Loss: 1.150672 \tValidation Loss: 2.779803\n",
      "Epoch: 38505 \tTraining Loss: 1.184325 \tValidation Loss: 2.778321\n",
      "Epoch: 38506 \tTraining Loss: 1.165608 \tValidation Loss: 2.777858\n",
      "Epoch: 38507 \tTraining Loss: 1.147924 \tValidation Loss: 2.777777\n",
      "Epoch: 38508 \tTraining Loss: 1.155693 \tValidation Loss: 2.779008\n",
      "Epoch: 38509 \tTraining Loss: 1.180578 \tValidation Loss: 2.779192\n",
      "Epoch: 38510 \tTraining Loss: 1.176479 \tValidation Loss: 2.780182\n",
      "Epoch: 38511 \tTraining Loss: 1.173962 \tValidation Loss: 2.778268\n",
      "Epoch: 38512 \tTraining Loss: 1.169801 \tValidation Loss: 2.778737\n",
      "Epoch: 38513 \tTraining Loss: 1.173033 \tValidation Loss: 2.779271\n",
      "Epoch: 38514 \tTraining Loss: 1.120418 \tValidation Loss: 2.780320\n",
      "Epoch: 38515 \tTraining Loss: 1.106894 \tValidation Loss: 2.779617\n",
      "Epoch: 38516 \tTraining Loss: 1.188952 \tValidation Loss: 2.778629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38517 \tTraining Loss: 1.176107 \tValidation Loss: 2.778645\n",
      "Epoch: 38518 \tTraining Loss: 1.150534 \tValidation Loss: 2.778636\n",
      "Epoch: 38519 \tTraining Loss: 1.186978 \tValidation Loss: 2.777440\n",
      "Epoch: 38520 \tTraining Loss: 1.101882 \tValidation Loss: 2.777336\n",
      "Epoch: 38521 \tTraining Loss: 1.153034 \tValidation Loss: 2.779175\n",
      "Epoch: 38522 \tTraining Loss: 1.206384 \tValidation Loss: 2.777874\n",
      "Epoch: 38523 \tTraining Loss: 1.149598 \tValidation Loss: 2.779088\n",
      "Epoch: 38524 \tTraining Loss: 1.156558 \tValidation Loss: 2.780198\n",
      "Epoch: 38525 \tTraining Loss: 1.136580 \tValidation Loss: 2.780332\n",
      "Epoch: 38526 \tTraining Loss: 1.172255 \tValidation Loss: 2.777904\n",
      "Epoch: 38527 \tTraining Loss: 1.181430 \tValidation Loss: 2.779891\n",
      "Epoch: 38528 \tTraining Loss: 1.168879 \tValidation Loss: 2.778848\n",
      "Epoch: 38529 \tTraining Loss: 1.168635 \tValidation Loss: 2.778780\n",
      "Epoch: 38530 \tTraining Loss: 1.190239 \tValidation Loss: 2.778908\n",
      "Epoch: 38531 \tTraining Loss: 1.199175 \tValidation Loss: 2.778839\n",
      "Epoch: 38532 \tTraining Loss: 1.242823 \tValidation Loss: 2.777790\n",
      "Epoch: 38533 \tTraining Loss: 1.169071 \tValidation Loss: 2.780432\n",
      "Epoch: 38534 \tTraining Loss: 1.159190 \tValidation Loss: 2.780301\n",
      "Epoch: 38535 \tTraining Loss: 1.160288 \tValidation Loss: 2.779811\n",
      "Epoch: 38536 \tTraining Loss: 1.138517 \tValidation Loss: 2.779219\n",
      "Epoch: 38537 \tTraining Loss: 1.176166 \tValidation Loss: 2.781007\n",
      "Epoch: 38538 \tTraining Loss: 1.156335 \tValidation Loss: 2.780930\n",
      "Epoch: 38539 \tTraining Loss: 1.149232 \tValidation Loss: 2.779058\n",
      "Epoch: 38540 \tTraining Loss: 1.223679 \tValidation Loss: 2.779389\n",
      "Epoch: 38541 \tTraining Loss: 1.176727 \tValidation Loss: 2.779154\n",
      "Epoch: 38542 \tTraining Loss: 1.176355 \tValidation Loss: 2.780716\n",
      "Epoch: 38543 \tTraining Loss: 1.166505 \tValidation Loss: 2.778695\n",
      "Epoch: 38544 \tTraining Loss: 1.149966 \tValidation Loss: 2.779408\n",
      "Epoch: 38545 \tTraining Loss: 1.227057 \tValidation Loss: 2.778430\n",
      "Epoch: 38546 \tTraining Loss: 1.172257 \tValidation Loss: 2.778610\n",
      "Epoch: 38547 \tTraining Loss: 1.173033 \tValidation Loss: 2.779300\n",
      "Epoch: 38548 \tTraining Loss: 1.181175 \tValidation Loss: 2.779677\n",
      "Epoch: 38549 \tTraining Loss: 1.160703 \tValidation Loss: 2.777197\n",
      "Epoch: 38550 \tTraining Loss: 1.191047 \tValidation Loss: 2.778464\n",
      "Epoch: 38551 \tTraining Loss: 1.156464 \tValidation Loss: 2.779845\n",
      "Epoch: 38552 \tTraining Loss: 1.133474 \tValidation Loss: 2.779547\n",
      "Epoch: 38553 \tTraining Loss: 1.163822 \tValidation Loss: 2.780183\n",
      "Epoch: 38554 \tTraining Loss: 1.147277 \tValidation Loss: 2.781071\n",
      "Epoch: 38555 \tTraining Loss: 1.149059 \tValidation Loss: 2.778811\n",
      "Epoch: 38556 \tTraining Loss: 1.170986 \tValidation Loss: 2.779543\n",
      "Epoch: 38557 \tTraining Loss: 1.138363 \tValidation Loss: 2.780589\n",
      "Epoch: 38558 \tTraining Loss: 1.197275 \tValidation Loss: 2.780012\n",
      "Epoch: 38559 \tTraining Loss: 1.155482 \tValidation Loss: 2.778976\n",
      "Epoch: 38560 \tTraining Loss: 1.177933 \tValidation Loss: 2.778549\n",
      "Epoch: 38561 \tTraining Loss: 1.167948 \tValidation Loss: 2.779159\n",
      "Epoch: 38562 \tTraining Loss: 1.085256 \tValidation Loss: 2.781164\n",
      "Epoch: 38563 \tTraining Loss: 1.234660 \tValidation Loss: 2.778413\n",
      "Epoch: 38564 \tTraining Loss: 1.209517 \tValidation Loss: 2.776748\n",
      "Epoch: 38565 \tTraining Loss: 1.182131 \tValidation Loss: 2.778843\n",
      "Epoch: 38566 \tTraining Loss: 1.154552 \tValidation Loss: 2.779995\n",
      "Epoch: 38567 \tTraining Loss: 1.224868 \tValidation Loss: 2.779891\n",
      "Epoch: 38568 \tTraining Loss: 1.152086 \tValidation Loss: 2.780560\n",
      "Epoch: 38569 \tTraining Loss: 1.174646 \tValidation Loss: 2.780309\n",
      "Epoch: 38570 \tTraining Loss: 1.092935 \tValidation Loss: 2.779295\n",
      "Epoch: 38571 \tTraining Loss: 1.143206 \tValidation Loss: 2.781624\n",
      "Epoch: 38572 \tTraining Loss: 1.194344 \tValidation Loss: 2.778096\n",
      "Epoch: 38573 \tTraining Loss: 1.174973 \tValidation Loss: 2.779604\n",
      "Epoch: 38574 \tTraining Loss: 1.150728 \tValidation Loss: 2.779525\n",
      "Epoch: 38575 \tTraining Loss: 1.145933 \tValidation Loss: 2.780482\n",
      "Epoch: 38576 \tTraining Loss: 1.175117 \tValidation Loss: 2.780566\n",
      "Epoch: 38577 \tTraining Loss: 1.196136 \tValidation Loss: 2.780038\n",
      "Epoch: 38578 \tTraining Loss: 1.138705 \tValidation Loss: 2.779335\n",
      "Epoch: 38579 \tTraining Loss: 1.130942 \tValidation Loss: 2.779852\n",
      "Epoch: 38580 \tTraining Loss: 1.205596 \tValidation Loss: 2.779956\n",
      "Epoch: 38581 \tTraining Loss: 1.126575 \tValidation Loss: 2.780533\n",
      "Epoch: 38582 \tTraining Loss: 1.193140 \tValidation Loss: 2.780564\n",
      "Epoch: 38583 \tTraining Loss: 1.143355 \tValidation Loss: 2.779895\n",
      "Epoch: 38584 \tTraining Loss: 1.168870 \tValidation Loss: 2.780557\n",
      "Epoch: 38585 \tTraining Loss: 1.128346 \tValidation Loss: 2.780526\n",
      "Epoch: 38586 \tTraining Loss: 1.213384 \tValidation Loss: 2.780106\n",
      "Epoch: 38587 \tTraining Loss: 1.197042 \tValidation Loss: 2.778955\n",
      "Epoch: 38588 \tTraining Loss: 1.153757 \tValidation Loss: 2.779733\n",
      "Epoch: 38589 \tTraining Loss: 1.165103 \tValidation Loss: 2.778521\n",
      "Epoch: 38590 \tTraining Loss: 1.159825 \tValidation Loss: 2.780688\n",
      "Epoch: 38591 \tTraining Loss: 1.156396 \tValidation Loss: 2.781789\n",
      "Epoch: 38592 \tTraining Loss: 1.115410 \tValidation Loss: 2.778432\n",
      "Epoch: 38593 \tTraining Loss: 1.135712 \tValidation Loss: 2.780159\n",
      "Epoch: 38594 \tTraining Loss: 1.180338 \tValidation Loss: 2.780197\n",
      "Epoch: 38595 \tTraining Loss: 1.166300 \tValidation Loss: 2.779823\n",
      "Epoch: 38596 \tTraining Loss: 1.184428 \tValidation Loss: 2.782659\n",
      "Epoch: 38597 \tTraining Loss: 1.136111 \tValidation Loss: 2.779817\n",
      "Epoch: 38598 \tTraining Loss: 1.157204 \tValidation Loss: 2.781023\n",
      "Epoch: 38599 \tTraining Loss: 1.134475 \tValidation Loss: 2.780564\n",
      "Epoch: 38600 \tTraining Loss: 1.172634 \tValidation Loss: 2.779345\n",
      "Epoch: 38601 \tTraining Loss: 1.231503 \tValidation Loss: 2.778948\n",
      "Epoch: 38602 \tTraining Loss: 1.197984 \tValidation Loss: 2.780607\n",
      "Epoch: 38603 \tTraining Loss: 1.189994 \tValidation Loss: 2.781967\n",
      "Epoch: 38604 \tTraining Loss: 1.152303 \tValidation Loss: 2.780648\n",
      "Epoch: 38605 \tTraining Loss: 1.142355 \tValidation Loss: 2.779397\n",
      "Epoch: 38606 \tTraining Loss: 1.176749 \tValidation Loss: 2.782230\n",
      "Epoch: 38607 \tTraining Loss: 1.198251 \tValidation Loss: 2.780833\n",
      "Epoch: 38608 \tTraining Loss: 1.154269 \tValidation Loss: 2.780825\n",
      "Epoch: 38609 \tTraining Loss: 1.155501 \tValidation Loss: 2.780279\n",
      "Epoch: 38610 \tTraining Loss: 1.171001 \tValidation Loss: 2.782340\n",
      "Epoch: 38611 \tTraining Loss: 1.140456 \tValidation Loss: 2.779635\n",
      "Epoch: 38612 \tTraining Loss: 1.189174 \tValidation Loss: 2.780945\n",
      "Epoch: 38613 \tTraining Loss: 1.186250 \tValidation Loss: 2.781544\n",
      "Epoch: 38614 \tTraining Loss: 1.207361 \tValidation Loss: 2.781612\n",
      "Epoch: 38615 \tTraining Loss: 1.150587 \tValidation Loss: 2.779889\n",
      "Epoch: 38616 \tTraining Loss: 1.155999 \tValidation Loss: 2.781387\n",
      "Epoch: 38617 \tTraining Loss: 1.138169 \tValidation Loss: 2.781996\n",
      "Epoch: 38618 \tTraining Loss: 1.139639 \tValidation Loss: 2.780574\n",
      "Epoch: 38619 \tTraining Loss: 1.139914 \tValidation Loss: 2.781459\n",
      "Epoch: 38620 \tTraining Loss: 1.156296 \tValidation Loss: 2.781084\n",
      "Epoch: 38621 \tTraining Loss: 1.173956 \tValidation Loss: 2.781598\n",
      "Epoch: 38622 \tTraining Loss: 1.201569 \tValidation Loss: 2.779258\n",
      "Epoch: 38623 \tTraining Loss: 1.180874 \tValidation Loss: 2.781109\n",
      "Epoch: 38624 \tTraining Loss: 1.165919 \tValidation Loss: 2.782167\n",
      "Epoch: 38625 \tTraining Loss: 1.163260 \tValidation Loss: 2.781358\n",
      "Epoch: 38626 \tTraining Loss: 1.152090 \tValidation Loss: 2.782818\n",
      "Epoch: 38627 \tTraining Loss: 1.185676 \tValidation Loss: 2.781064\n",
      "Epoch: 38628 \tTraining Loss: 1.176910 \tValidation Loss: 2.781849\n",
      "Epoch: 38629 \tTraining Loss: 1.190467 \tValidation Loss: 2.781237\n",
      "Epoch: 38630 \tTraining Loss: 1.150937 \tValidation Loss: 2.779341\n",
      "Epoch: 38631 \tTraining Loss: 1.184170 \tValidation Loss: 2.782975\n",
      "Epoch: 38632 \tTraining Loss: 1.173093 \tValidation Loss: 2.781769\n",
      "Epoch: 38633 \tTraining Loss: 1.161256 \tValidation Loss: 2.782026\n",
      "Epoch: 38634 \tTraining Loss: 1.199675 \tValidation Loss: 2.780699\n",
      "Epoch: 38635 \tTraining Loss: 1.240756 \tValidation Loss: 2.780569\n",
      "Epoch: 38636 \tTraining Loss: 1.172698 \tValidation Loss: 2.781564\n",
      "Epoch: 38637 \tTraining Loss: 1.175912 \tValidation Loss: 2.783155\n",
      "Epoch: 38638 \tTraining Loss: 1.143158 \tValidation Loss: 2.780771\n",
      "Epoch: 38639 \tTraining Loss: 1.189923 \tValidation Loss: 2.781905\n",
      "Epoch: 38640 \tTraining Loss: 1.213939 \tValidation Loss: 2.782223\n",
      "Epoch: 38641 \tTraining Loss: 1.149166 \tValidation Loss: 2.781213\n",
      "Epoch: 38642 \tTraining Loss: 1.235643 \tValidation Loss: 2.781286\n",
      "Epoch: 38643 \tTraining Loss: 1.204353 \tValidation Loss: 2.779971\n",
      "Epoch: 38644 \tTraining Loss: 1.107615 \tValidation Loss: 2.782348\n",
      "Epoch: 38645 \tTraining Loss: 1.139982 \tValidation Loss: 2.782995\n",
      "Epoch: 38646 \tTraining Loss: 1.177433 \tValidation Loss: 2.781225\n",
      "Epoch: 38647 \tTraining Loss: 1.173003 \tValidation Loss: 2.781771\n",
      "Epoch: 38648 \tTraining Loss: 1.168101 \tValidation Loss: 2.780881\n",
      "Epoch: 38649 \tTraining Loss: 1.196388 \tValidation Loss: 2.780898\n",
      "Epoch: 38650 \tTraining Loss: 1.139294 \tValidation Loss: 2.783991\n",
      "Epoch: 38651 \tTraining Loss: 1.160730 \tValidation Loss: 2.781272\n",
      "Epoch: 38652 \tTraining Loss: 1.160137 \tValidation Loss: 2.782527\n",
      "Epoch: 38653 \tTraining Loss: 1.190835 \tValidation Loss: 2.780134\n",
      "Epoch: 38654 \tTraining Loss: 1.165148 \tValidation Loss: 2.781991\n",
      "Epoch: 38655 \tTraining Loss: 1.187859 \tValidation Loss: 2.782211\n",
      "Epoch: 38656 \tTraining Loss: 1.172676 \tValidation Loss: 2.780622\n",
      "Epoch: 38657 \tTraining Loss: 1.116944 \tValidation Loss: 2.782178\n",
      "Epoch: 38658 \tTraining Loss: 1.171541 \tValidation Loss: 2.781684\n",
      "Epoch: 38659 \tTraining Loss: 1.149747 \tValidation Loss: 2.782789\n",
      "Epoch: 38660 \tTraining Loss: 1.186923 \tValidation Loss: 2.782728\n",
      "Epoch: 38661 \tTraining Loss: 1.196198 \tValidation Loss: 2.781391\n",
      "Epoch: 38662 \tTraining Loss: 1.119787 \tValidation Loss: 2.781304\n",
      "Epoch: 38663 \tTraining Loss: 1.150194 \tValidation Loss: 2.783416\n",
      "Epoch: 38664 \tTraining Loss: 1.157069 \tValidation Loss: 2.782932\n",
      "Epoch: 38665 \tTraining Loss: 1.169984 \tValidation Loss: 2.782085\n",
      "Epoch: 38666 \tTraining Loss: 1.134879 \tValidation Loss: 2.781858\n",
      "Epoch: 38667 \tTraining Loss: 1.158201 \tValidation Loss: 2.783610\n",
      "Epoch: 38668 \tTraining Loss: 1.151266 \tValidation Loss: 2.783667\n",
      "Epoch: 38669 \tTraining Loss: 1.121070 \tValidation Loss: 2.783768\n",
      "Epoch: 38670 \tTraining Loss: 1.203896 \tValidation Loss: 2.783093\n",
      "Epoch: 38671 \tTraining Loss: 1.210075 \tValidation Loss: 2.783158\n",
      "Epoch: 38672 \tTraining Loss: 1.177360 \tValidation Loss: 2.782696\n",
      "Epoch: 38673 \tTraining Loss: 1.196756 \tValidation Loss: 2.782034\n",
      "Epoch: 38674 \tTraining Loss: 1.190436 \tValidation Loss: 2.782595\n",
      "Epoch: 38675 \tTraining Loss: 1.205561 \tValidation Loss: 2.782259\n",
      "Epoch: 38676 \tTraining Loss: 1.170709 \tValidation Loss: 2.783079\n",
      "Epoch: 38677 \tTraining Loss: 1.179021 \tValidation Loss: 2.782498\n",
      "Epoch: 38678 \tTraining Loss: 1.152779 \tValidation Loss: 2.783198\n",
      "Epoch: 38679 \tTraining Loss: 1.167128 \tValidation Loss: 2.782638\n",
      "Epoch: 38680 \tTraining Loss: 1.173032 \tValidation Loss: 2.784124\n",
      "Epoch: 38681 \tTraining Loss: 1.166263 \tValidation Loss: 2.782683\n",
      "Epoch: 38682 \tTraining Loss: 1.161218 \tValidation Loss: 2.782676\n",
      "Epoch: 38683 \tTraining Loss: 1.124319 \tValidation Loss: 2.782619\n",
      "Epoch: 38684 \tTraining Loss: 1.185005 \tValidation Loss: 2.783062\n",
      "Epoch: 38685 \tTraining Loss: 1.187742 \tValidation Loss: 2.783116\n",
      "Epoch: 38686 \tTraining Loss: 1.139728 \tValidation Loss: 2.781986\n",
      "Epoch: 38687 \tTraining Loss: 1.204038 \tValidation Loss: 2.783111\n",
      "Epoch: 38688 \tTraining Loss: 1.154810 \tValidation Loss: 2.782810\n",
      "Epoch: 38689 \tTraining Loss: 1.168774 \tValidation Loss: 2.783162\n",
      "Epoch: 38690 \tTraining Loss: 1.182595 \tValidation Loss: 2.782951\n",
      "Epoch: 38691 \tTraining Loss: 1.164922 \tValidation Loss: 2.784737\n",
      "Epoch: 38692 \tTraining Loss: 1.137713 \tValidation Loss: 2.783777\n",
      "Epoch: 38693 \tTraining Loss: 1.174738 \tValidation Loss: 2.783846\n",
      "Epoch: 38694 \tTraining Loss: 1.182427 \tValidation Loss: 2.783514\n",
      "Epoch: 38695 \tTraining Loss: 1.164072 \tValidation Loss: 2.783119\n",
      "Epoch: 38696 \tTraining Loss: 1.171164 \tValidation Loss: 2.784507\n",
      "Epoch: 38697 \tTraining Loss: 1.137233 \tValidation Loss: 2.784247\n",
      "Epoch: 38698 \tTraining Loss: 1.140002 \tValidation Loss: 2.785455\n",
      "Epoch: 38699 \tTraining Loss: 1.117420 \tValidation Loss: 2.782372\n",
      "Epoch: 38700 \tTraining Loss: 1.169762 \tValidation Loss: 2.784225\n",
      "Epoch: 38701 \tTraining Loss: 1.158774 \tValidation Loss: 2.784331\n",
      "Epoch: 38702 \tTraining Loss: 1.094015 \tValidation Loss: 2.784136\n",
      "Epoch: 38703 \tTraining Loss: 1.146286 \tValidation Loss: 2.783535\n",
      "Epoch: 38704 \tTraining Loss: 1.125702 \tValidation Loss: 2.785372\n",
      "Epoch: 38705 \tTraining Loss: 1.205570 \tValidation Loss: 2.781465\n",
      "Epoch: 38706 \tTraining Loss: 1.157258 \tValidation Loss: 2.784546\n",
      "Epoch: 38707 \tTraining Loss: 1.186842 \tValidation Loss: 2.782615\n",
      "Epoch: 38708 \tTraining Loss: 1.158408 \tValidation Loss: 2.783485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38709 \tTraining Loss: 1.206547 \tValidation Loss: 2.784175\n",
      "Epoch: 38710 \tTraining Loss: 1.153767 \tValidation Loss: 2.784682\n",
      "Epoch: 38711 \tTraining Loss: 1.179554 \tValidation Loss: 2.781888\n",
      "Epoch: 38712 \tTraining Loss: 1.178179 \tValidation Loss: 2.781792\n",
      "Epoch: 38713 \tTraining Loss: 1.164805 \tValidation Loss: 2.784740\n",
      "Epoch: 38714 \tTraining Loss: 1.177624 \tValidation Loss: 2.782730\n",
      "Epoch: 38715 \tTraining Loss: 1.192518 \tValidation Loss: 2.783324\n",
      "Epoch: 38716 \tTraining Loss: 1.211388 \tValidation Loss: 2.784861\n",
      "Epoch: 38717 \tTraining Loss: 1.186545 \tValidation Loss: 2.782256\n",
      "Epoch: 38718 \tTraining Loss: 1.163147 \tValidation Loss: 2.782874\n",
      "Epoch: 38719 \tTraining Loss: 1.117004 \tValidation Loss: 2.783663\n",
      "Epoch: 38720 \tTraining Loss: 1.133713 \tValidation Loss: 2.783792\n",
      "Epoch: 38721 \tTraining Loss: 1.124277 \tValidation Loss: 2.785011\n",
      "Epoch: 38722 \tTraining Loss: 1.186942 \tValidation Loss: 2.784204\n",
      "Epoch: 38723 \tTraining Loss: 1.188204 \tValidation Loss: 2.782727\n",
      "Epoch: 38724 \tTraining Loss: 1.184921 \tValidation Loss: 2.783628\n",
      "Epoch: 38725 \tTraining Loss: 1.176510 \tValidation Loss: 2.783657\n",
      "Epoch: 38726 \tTraining Loss: 1.153354 \tValidation Loss: 2.782908\n",
      "Epoch: 38727 \tTraining Loss: 1.187542 \tValidation Loss: 2.784011\n",
      "Epoch: 38728 \tTraining Loss: 1.164181 \tValidation Loss: 2.785159\n",
      "Epoch: 38729 \tTraining Loss: 1.148350 \tValidation Loss: 2.783126\n",
      "Epoch: 38730 \tTraining Loss: 1.183389 \tValidation Loss: 2.784626\n",
      "Epoch: 38731 \tTraining Loss: 1.204403 \tValidation Loss: 2.783655\n",
      "Epoch: 38732 \tTraining Loss: 1.146868 \tValidation Loss: 2.784830\n",
      "Epoch: 38733 \tTraining Loss: 1.160734 \tValidation Loss: 2.783716\n",
      "Epoch: 38734 \tTraining Loss: 1.193086 \tValidation Loss: 2.784600\n",
      "Epoch: 38735 \tTraining Loss: 1.147411 \tValidation Loss: 2.783612\n",
      "Epoch: 38736 \tTraining Loss: 1.237720 \tValidation Loss: 2.784259\n",
      "Epoch: 38737 \tTraining Loss: 1.177833 \tValidation Loss: 2.782828\n",
      "Epoch: 38738 \tTraining Loss: 1.134446 \tValidation Loss: 2.785368\n",
      "Epoch: 38739 \tTraining Loss: 1.191232 \tValidation Loss: 2.784612\n",
      "Epoch: 38740 \tTraining Loss: 1.122827 \tValidation Loss: 2.785873\n",
      "Epoch: 38741 \tTraining Loss: 1.157305 \tValidation Loss: 2.784559\n",
      "Epoch: 38742 \tTraining Loss: 1.176971 \tValidation Loss: 2.783734\n",
      "Epoch: 38743 \tTraining Loss: 1.135062 \tValidation Loss: 2.786120\n",
      "Epoch: 38744 \tTraining Loss: 1.123070 \tValidation Loss: 2.785034\n",
      "Epoch: 38745 \tTraining Loss: 1.147841 \tValidation Loss: 2.784434\n",
      "Epoch: 38746 \tTraining Loss: 1.167994 \tValidation Loss: 2.784859\n",
      "Epoch: 38747 \tTraining Loss: 1.155278 \tValidation Loss: 2.785424\n",
      "Epoch: 38748 \tTraining Loss: 1.132741 \tValidation Loss: 2.783798\n",
      "Epoch: 38749 \tTraining Loss: 1.219904 \tValidation Loss: 2.783036\n",
      "Epoch: 38750 \tTraining Loss: 1.146980 \tValidation Loss: 2.784454\n",
      "Epoch: 38751 \tTraining Loss: 1.190276 \tValidation Loss: 2.784690\n",
      "Epoch: 38752 \tTraining Loss: 1.204045 \tValidation Loss: 2.783703\n",
      "Epoch: 38753 \tTraining Loss: 1.178982 \tValidation Loss: 2.783481\n",
      "Epoch: 38754 \tTraining Loss: 1.198046 \tValidation Loss: 2.785227\n",
      "Epoch: 38755 \tTraining Loss: 1.152943 \tValidation Loss: 2.785784\n",
      "Epoch: 38756 \tTraining Loss: 1.177276 \tValidation Loss: 2.784915\n",
      "Epoch: 38757 \tTraining Loss: 1.125784 \tValidation Loss: 2.785854\n",
      "Epoch: 38758 \tTraining Loss: 1.166733 \tValidation Loss: 2.784820\n",
      "Epoch: 38759 \tTraining Loss: 1.166383 \tValidation Loss: 2.784256\n",
      "Epoch: 38760 \tTraining Loss: 1.193636 \tValidation Loss: 2.784739\n",
      "Epoch: 38761 \tTraining Loss: 1.160219 \tValidation Loss: 2.785098\n",
      "Epoch: 38762 \tTraining Loss: 1.151279 \tValidation Loss: 2.786626\n",
      "Epoch: 38763 \tTraining Loss: 1.146272 \tValidation Loss: 2.783870\n",
      "Epoch: 38764 \tTraining Loss: 1.133876 \tValidation Loss: 2.784854\n",
      "Epoch: 38765 \tTraining Loss: 1.172343 \tValidation Loss: 2.786193\n",
      "Epoch: 38766 \tTraining Loss: 1.148120 \tValidation Loss: 2.785106\n",
      "Epoch: 38767 \tTraining Loss: 1.186273 \tValidation Loss: 2.785193\n",
      "Epoch: 38768 \tTraining Loss: 1.160198 \tValidation Loss: 2.784940\n",
      "Epoch: 38769 \tTraining Loss: 1.125876 \tValidation Loss: 2.787541\n",
      "Epoch: 38770 \tTraining Loss: 1.164827 \tValidation Loss: 2.784194\n",
      "Epoch: 38771 \tTraining Loss: 1.138064 \tValidation Loss: 2.784058\n",
      "Epoch: 38772 \tTraining Loss: 1.168683 \tValidation Loss: 2.784993\n",
      "Epoch: 38773 \tTraining Loss: 1.164606 \tValidation Loss: 2.783332\n",
      "Epoch: 38774 \tTraining Loss: 1.165801 \tValidation Loss: 2.784219\n",
      "Epoch: 38775 \tTraining Loss: 1.180990 \tValidation Loss: 2.784146\n",
      "Epoch: 38776 \tTraining Loss: 1.142786 \tValidation Loss: 2.785578\n",
      "Epoch: 38777 \tTraining Loss: 1.172270 \tValidation Loss: 2.786915\n",
      "Epoch: 38778 \tTraining Loss: 1.150552 \tValidation Loss: 2.786212\n",
      "Epoch: 38779 \tTraining Loss: 1.163929 \tValidation Loss: 2.786186\n",
      "Epoch: 38780 \tTraining Loss: 1.142353 \tValidation Loss: 2.784944\n",
      "Epoch: 38781 \tTraining Loss: 1.188980 \tValidation Loss: 2.784654\n",
      "Epoch: 38782 \tTraining Loss: 1.178249 \tValidation Loss: 2.785914\n",
      "Epoch: 38783 \tTraining Loss: 1.170211 \tValidation Loss: 2.784714\n",
      "Epoch: 38784 \tTraining Loss: 1.199606 \tValidation Loss: 2.783811\n",
      "Epoch: 38785 \tTraining Loss: 1.172971 \tValidation Loss: 2.785550\n",
      "Epoch: 38786 \tTraining Loss: 1.173850 \tValidation Loss: 2.784725\n",
      "Epoch: 38787 \tTraining Loss: 1.157654 \tValidation Loss: 2.784833\n",
      "Epoch: 38788 \tTraining Loss: 1.140055 \tValidation Loss: 2.786061\n",
      "Epoch: 38789 \tTraining Loss: 1.222305 \tValidation Loss: 2.784476\n",
      "Epoch: 38790 \tTraining Loss: 1.158740 \tValidation Loss: 2.784344\n",
      "Epoch: 38791 \tTraining Loss: 1.171533 \tValidation Loss: 2.786281\n",
      "Epoch: 38792 \tTraining Loss: 1.209193 \tValidation Loss: 2.785336\n",
      "Epoch: 38793 \tTraining Loss: 1.187406 \tValidation Loss: 2.783315\n",
      "Epoch: 38794 \tTraining Loss: 1.153638 \tValidation Loss: 2.785881\n",
      "Epoch: 38795 \tTraining Loss: 1.216645 \tValidation Loss: 2.786723\n",
      "Epoch: 38796 \tTraining Loss: 1.145215 \tValidation Loss: 2.786002\n",
      "Epoch: 38797 \tTraining Loss: 1.131378 \tValidation Loss: 2.787759\n",
      "Epoch: 38798 \tTraining Loss: 1.142575 \tValidation Loss: 2.785489\n",
      "Epoch: 38799 \tTraining Loss: 1.183047 \tValidation Loss: 2.783670\n",
      "Epoch: 38800 \tTraining Loss: 1.197120 \tValidation Loss: 2.785043\n",
      "Epoch: 38801 \tTraining Loss: 1.196265 \tValidation Loss: 2.785962\n",
      "Epoch: 38802 \tTraining Loss: 1.176144 \tValidation Loss: 2.784456\n",
      "Epoch: 38803 \tTraining Loss: 1.136297 \tValidation Loss: 2.785642\n",
      "Epoch: 38804 \tTraining Loss: 1.152554 \tValidation Loss: 2.785780\n",
      "Epoch: 38805 \tTraining Loss: 1.164708 \tValidation Loss: 2.784999\n",
      "Epoch: 38806 \tTraining Loss: 1.192573 \tValidation Loss: 2.785895\n",
      "Epoch: 38807 \tTraining Loss: 1.142181 \tValidation Loss: 2.787424\n",
      "Epoch: 38808 \tTraining Loss: 1.162747 \tValidation Loss: 2.786160\n",
      "Epoch: 38809 \tTraining Loss: 1.191747 \tValidation Loss: 2.785785\n",
      "Epoch: 38810 \tTraining Loss: 1.197979 \tValidation Loss: 2.786708\n",
      "Epoch: 38811 \tTraining Loss: 1.134700 \tValidation Loss: 2.786438\n",
      "Epoch: 38812 \tTraining Loss: 1.216340 \tValidation Loss: 2.786476\n",
      "Epoch: 38813 \tTraining Loss: 1.164296 \tValidation Loss: 2.785985\n",
      "Epoch: 38814 \tTraining Loss: 1.170879 \tValidation Loss: 2.787253\n",
      "Epoch: 38815 \tTraining Loss: 1.151157 \tValidation Loss: 2.786360\n",
      "Epoch: 38816 \tTraining Loss: 1.184498 \tValidation Loss: 2.785052\n",
      "Epoch: 38817 \tTraining Loss: 1.160983 \tValidation Loss: 2.787047\n",
      "Epoch: 38818 \tTraining Loss: 1.165546 \tValidation Loss: 2.786387\n",
      "Epoch: 38819 \tTraining Loss: 1.145666 \tValidation Loss: 2.786608\n",
      "Epoch: 38820 \tTraining Loss: 1.154427 \tValidation Loss: 2.787941\n",
      "Epoch: 38821 \tTraining Loss: 1.140222 \tValidation Loss: 2.787232\n",
      "Epoch: 38822 \tTraining Loss: 1.179600 \tValidation Loss: 2.785732\n",
      "Epoch: 38823 \tTraining Loss: 1.181363 \tValidation Loss: 2.785640\n",
      "Epoch: 38824 \tTraining Loss: 1.163970 \tValidation Loss: 2.786526\n",
      "Epoch: 38825 \tTraining Loss: 1.144072 \tValidation Loss: 2.784836\n",
      "Epoch: 38826 \tTraining Loss: 1.125764 \tValidation Loss: 2.788471\n",
      "Epoch: 38827 \tTraining Loss: 1.170176 \tValidation Loss: 2.786058\n",
      "Epoch: 38828 \tTraining Loss: 1.164042 \tValidation Loss: 2.786292\n",
      "Epoch: 38829 \tTraining Loss: 1.130699 \tValidation Loss: 2.786457\n",
      "Epoch: 38830 \tTraining Loss: 1.204835 \tValidation Loss: 2.786313\n",
      "Epoch: 38831 \tTraining Loss: 1.208127 \tValidation Loss: 2.785829\n",
      "Epoch: 38832 \tTraining Loss: 1.180762 \tValidation Loss: 2.786788\n",
      "Epoch: 38833 \tTraining Loss: 1.151174 \tValidation Loss: 2.784205\n",
      "Epoch: 38834 \tTraining Loss: 1.163412 \tValidation Loss: 2.786247\n",
      "Epoch: 38835 \tTraining Loss: 1.161811 \tValidation Loss: 2.787558\n",
      "Epoch: 38836 \tTraining Loss: 1.153510 \tValidation Loss: 2.786740\n",
      "Epoch: 38837 \tTraining Loss: 1.178331 \tValidation Loss: 2.784267\n",
      "Epoch: 38838 \tTraining Loss: 1.152630 \tValidation Loss: 2.786326\n",
      "Epoch: 38839 \tTraining Loss: 1.183931 \tValidation Loss: 2.786460\n",
      "Epoch: 38840 \tTraining Loss: 1.179668 \tValidation Loss: 2.785337\n",
      "Epoch: 38841 \tTraining Loss: 1.134817 \tValidation Loss: 2.787221\n",
      "Epoch: 38842 \tTraining Loss: 1.162145 \tValidation Loss: 2.787709\n",
      "Epoch: 38843 \tTraining Loss: 1.141041 \tValidation Loss: 2.786001\n",
      "Epoch: 38844 \tTraining Loss: 1.192895 \tValidation Loss: 2.785805\n",
      "Epoch: 38845 \tTraining Loss: 1.167685 \tValidation Loss: 2.786675\n",
      "Epoch: 38846 \tTraining Loss: 1.205044 \tValidation Loss: 2.786046\n",
      "Epoch: 38847 \tTraining Loss: 1.146594 \tValidation Loss: 2.786925\n",
      "Epoch: 38848 \tTraining Loss: 1.161316 \tValidation Loss: 2.787894\n",
      "Epoch: 38849 \tTraining Loss: 1.140043 \tValidation Loss: 2.787656\n",
      "Epoch: 38850 \tTraining Loss: 1.158866 \tValidation Loss: 2.787157\n",
      "Epoch: 38851 \tTraining Loss: 1.205905 \tValidation Loss: 2.785755\n",
      "Epoch: 38852 \tTraining Loss: 1.172180 \tValidation Loss: 2.785975\n",
      "Epoch: 38853 \tTraining Loss: 1.138461 \tValidation Loss: 2.787412\n",
      "Epoch: 38854 \tTraining Loss: 1.181115 \tValidation Loss: 2.787698\n",
      "Epoch: 38855 \tTraining Loss: 1.119731 \tValidation Loss: 2.786736\n",
      "Epoch: 38856 \tTraining Loss: 1.226445 \tValidation Loss: 2.786806\n",
      "Epoch: 38857 \tTraining Loss: 1.136693 \tValidation Loss: 2.786441\n",
      "Epoch: 38858 \tTraining Loss: 1.122811 \tValidation Loss: 2.787052\n",
      "Epoch: 38859 \tTraining Loss: 1.181246 \tValidation Loss: 2.788709\n",
      "Epoch: 38860 \tTraining Loss: 1.172802 \tValidation Loss: 2.786264\n",
      "Epoch: 38861 \tTraining Loss: 1.132961 \tValidation Loss: 2.787962\n",
      "Epoch: 38862 \tTraining Loss: 1.131157 \tValidation Loss: 2.786503\n",
      "Epoch: 38863 \tTraining Loss: 1.176793 \tValidation Loss: 2.787824\n",
      "Epoch: 38864 \tTraining Loss: 1.134744 \tValidation Loss: 2.787494\n",
      "Epoch: 38865 \tTraining Loss: 1.182474 \tValidation Loss: 2.787979\n",
      "Epoch: 38866 \tTraining Loss: 1.157667 \tValidation Loss: 2.788527\n",
      "Epoch: 38867 \tTraining Loss: 1.143753 \tValidation Loss: 2.787624\n",
      "Epoch: 38868 \tTraining Loss: 1.174639 \tValidation Loss: 2.787219\n",
      "Epoch: 38869 \tTraining Loss: 1.178193 \tValidation Loss: 2.787665\n",
      "Epoch: 38870 \tTraining Loss: 1.147669 \tValidation Loss: 2.787131\n",
      "Epoch: 38871 \tTraining Loss: 1.209589 \tValidation Loss: 2.789020\n",
      "Epoch: 38872 \tTraining Loss: 1.135940 \tValidation Loss: 2.787304\n",
      "Epoch: 38873 \tTraining Loss: 1.139715 \tValidation Loss: 2.788591\n",
      "Epoch: 38874 \tTraining Loss: 1.130374 \tValidation Loss: 2.786854\n",
      "Epoch: 38875 \tTraining Loss: 1.151287 \tValidation Loss: 2.789320\n",
      "Epoch: 38876 \tTraining Loss: 1.140275 \tValidation Loss: 2.786527\n",
      "Epoch: 38877 \tTraining Loss: 1.156794 \tValidation Loss: 2.787644\n",
      "Epoch: 38878 \tTraining Loss: 1.160662 \tValidation Loss: 2.787442\n",
      "Epoch: 38879 \tTraining Loss: 1.184354 \tValidation Loss: 2.787545\n",
      "Epoch: 38880 \tTraining Loss: 1.168074 \tValidation Loss: 2.788244\n",
      "Epoch: 38881 \tTraining Loss: 1.156228 \tValidation Loss: 2.789691\n",
      "Epoch: 38882 \tTraining Loss: 1.096658 \tValidation Loss: 2.788370\n",
      "Epoch: 38883 \tTraining Loss: 1.161009 \tValidation Loss: 2.787411\n",
      "Epoch: 38884 \tTraining Loss: 1.142417 \tValidation Loss: 2.787282\n",
      "Epoch: 38885 \tTraining Loss: 1.120962 \tValidation Loss: 2.790407\n",
      "Epoch: 38886 \tTraining Loss: 1.150658 \tValidation Loss: 2.787958\n",
      "Epoch: 38887 \tTraining Loss: 1.161690 \tValidation Loss: 2.787397\n",
      "Epoch: 38888 \tTraining Loss: 1.167961 \tValidation Loss: 2.788314\n",
      "Epoch: 38889 \tTraining Loss: 1.211067 \tValidation Loss: 2.789036\n",
      "Epoch: 38890 \tTraining Loss: 1.193009 \tValidation Loss: 2.789959\n",
      "Epoch: 38891 \tTraining Loss: 1.123075 \tValidation Loss: 2.788173\n",
      "Epoch: 38892 \tTraining Loss: 1.157018 \tValidation Loss: 2.789754\n",
      "Epoch: 38893 \tTraining Loss: 1.211215 \tValidation Loss: 2.787533\n",
      "Epoch: 38894 \tTraining Loss: 1.170624 \tValidation Loss: 2.788494\n",
      "Epoch: 38895 \tTraining Loss: 1.149502 \tValidation Loss: 2.789779\n",
      "Epoch: 38896 \tTraining Loss: 1.171116 \tValidation Loss: 2.790217\n",
      "Epoch: 38897 \tTraining Loss: 1.131516 \tValidation Loss: 2.789584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38898 \tTraining Loss: 1.146518 \tValidation Loss: 2.789418\n",
      "Epoch: 38899 \tTraining Loss: 1.201258 \tValidation Loss: 2.787399\n",
      "Epoch: 38900 \tTraining Loss: 1.170267 \tValidation Loss: 2.788178\n",
      "Epoch: 38901 \tTraining Loss: 1.186347 \tValidation Loss: 2.786432\n",
      "Epoch: 38902 \tTraining Loss: 1.167486 \tValidation Loss: 2.787300\n",
      "Epoch: 38903 \tTraining Loss: 1.163375 \tValidation Loss: 2.790187\n",
      "Epoch: 38904 \tTraining Loss: 1.141022 \tValidation Loss: 2.788630\n",
      "Epoch: 38905 \tTraining Loss: 1.177475 \tValidation Loss: 2.787422\n",
      "Epoch: 38906 \tTraining Loss: 1.163265 \tValidation Loss: 2.788093\n",
      "Epoch: 38907 \tTraining Loss: 1.147586 \tValidation Loss: 2.786988\n",
      "Epoch: 38908 \tTraining Loss: 1.172923 \tValidation Loss: 2.788531\n",
      "Epoch: 38909 \tTraining Loss: 1.118766 \tValidation Loss: 2.790571\n",
      "Epoch: 38910 \tTraining Loss: 1.173427 \tValidation Loss: 2.789854\n",
      "Epoch: 38911 \tTraining Loss: 1.198871 \tValidation Loss: 2.788487\n",
      "Epoch: 38912 \tTraining Loss: 1.154755 \tValidation Loss: 2.789111\n",
      "Epoch: 38913 \tTraining Loss: 1.162086 \tValidation Loss: 2.789022\n",
      "Epoch: 38914 \tTraining Loss: 1.143822 \tValidation Loss: 2.788692\n",
      "Epoch: 38915 \tTraining Loss: 1.133448 \tValidation Loss: 2.790770\n",
      "Epoch: 38916 \tTraining Loss: 1.181642 \tValidation Loss: 2.788790\n",
      "Epoch: 38917 \tTraining Loss: 1.208655 \tValidation Loss: 2.787221\n",
      "Epoch: 38918 \tTraining Loss: 1.156611 \tValidation Loss: 2.785272\n",
      "Epoch: 38919 \tTraining Loss: 1.150950 \tValidation Loss: 2.788674\n",
      "Epoch: 38920 \tTraining Loss: 1.135855 \tValidation Loss: 2.790469\n",
      "Epoch: 38921 \tTraining Loss: 1.152186 \tValidation Loss: 2.790040\n",
      "Epoch: 38922 \tTraining Loss: 1.164940 \tValidation Loss: 2.788378\n",
      "Epoch: 38923 \tTraining Loss: 1.137315 \tValidation Loss: 2.789280\n",
      "Epoch: 38924 \tTraining Loss: 1.148385 \tValidation Loss: 2.788081\n",
      "Epoch: 38925 \tTraining Loss: 1.184779 \tValidation Loss: 2.787694\n",
      "Epoch: 38926 \tTraining Loss: 1.155373 \tValidation Loss: 2.788245\n",
      "Epoch: 38927 \tTraining Loss: 1.156053 \tValidation Loss: 2.789515\n",
      "Epoch: 38928 \tTraining Loss: 1.185156 \tValidation Loss: 2.788353\n",
      "Epoch: 38929 \tTraining Loss: 1.128615 \tValidation Loss: 2.790887\n",
      "Epoch: 38930 \tTraining Loss: 1.175849 \tValidation Loss: 2.789600\n",
      "Epoch: 38931 \tTraining Loss: 1.131563 \tValidation Loss: 2.790055\n",
      "Epoch: 38932 \tTraining Loss: 1.201677 \tValidation Loss: 2.789004\n",
      "Epoch: 38933 \tTraining Loss: 1.194834 \tValidation Loss: 2.790875\n",
      "Epoch: 38934 \tTraining Loss: 1.190416 \tValidation Loss: 2.789306\n",
      "Epoch: 38935 \tTraining Loss: 1.201010 \tValidation Loss: 2.790987\n",
      "Epoch: 38936 \tTraining Loss: 1.233955 \tValidation Loss: 2.787796\n",
      "Epoch: 38937 \tTraining Loss: 1.150353 \tValidation Loss: 2.789152\n",
      "Epoch: 38938 \tTraining Loss: 1.197320 \tValidation Loss: 2.787983\n",
      "Epoch: 38939 \tTraining Loss: 1.189466 \tValidation Loss: 2.789823\n",
      "Epoch: 38940 \tTraining Loss: 1.161584 \tValidation Loss: 2.787117\n",
      "Epoch: 38941 \tTraining Loss: 1.138952 \tValidation Loss: 2.790075\n",
      "Epoch: 38942 \tTraining Loss: 1.130614 \tValidation Loss: 2.790463\n",
      "Epoch: 38943 \tTraining Loss: 1.185370 \tValidation Loss: 2.790494\n",
      "Epoch: 38944 \tTraining Loss: 1.158246 \tValidation Loss: 2.789919\n",
      "Epoch: 38945 \tTraining Loss: 1.210254 \tValidation Loss: 2.789416\n",
      "Epoch: 38946 \tTraining Loss: 1.202614 \tValidation Loss: 2.789762\n",
      "Epoch: 38947 \tTraining Loss: 1.151964 \tValidation Loss: 2.790646\n",
      "Epoch: 38948 \tTraining Loss: 1.150979 \tValidation Loss: 2.790190\n",
      "Epoch: 38949 \tTraining Loss: 1.150341 \tValidation Loss: 2.790148\n",
      "Epoch: 38950 \tTraining Loss: 1.218704 \tValidation Loss: 2.788678\n",
      "Epoch: 38951 \tTraining Loss: 1.184513 \tValidation Loss: 2.790497\n",
      "Epoch: 38952 \tTraining Loss: 1.179955 \tValidation Loss: 2.789457\n",
      "Epoch: 38953 \tTraining Loss: 1.184911 \tValidation Loss: 2.789669\n",
      "Epoch: 38954 \tTraining Loss: 1.137952 \tValidation Loss: 2.790847\n",
      "Epoch: 38955 \tTraining Loss: 1.187605 \tValidation Loss: 2.788069\n",
      "Epoch: 38956 \tTraining Loss: 1.166699 \tValidation Loss: 2.790309\n",
      "Epoch: 38957 \tTraining Loss: 1.116763 \tValidation Loss: 2.790662\n",
      "Epoch: 38958 \tTraining Loss: 1.155559 \tValidation Loss: 2.790073\n",
      "Epoch: 38959 \tTraining Loss: 1.158544 \tValidation Loss: 2.789632\n",
      "Epoch: 38960 \tTraining Loss: 1.146311 \tValidation Loss: 2.790242\n",
      "Epoch: 38961 \tTraining Loss: 1.167257 \tValidation Loss: 2.790559\n",
      "Epoch: 38962 \tTraining Loss: 1.140324 \tValidation Loss: 2.790814\n",
      "Epoch: 38963 \tTraining Loss: 1.146785 \tValidation Loss: 2.790248\n",
      "Epoch: 38964 \tTraining Loss: 1.158608 \tValidation Loss: 2.789852\n",
      "Epoch: 38965 \tTraining Loss: 1.190908 \tValidation Loss: 2.788765\n",
      "Epoch: 38966 \tTraining Loss: 1.184464 \tValidation Loss: 2.788644\n",
      "Epoch: 38967 \tTraining Loss: 1.109442 \tValidation Loss: 2.790656\n",
      "Epoch: 38968 \tTraining Loss: 1.144329 \tValidation Loss: 2.788339\n",
      "Epoch: 38969 \tTraining Loss: 1.169407 \tValidation Loss: 2.790812\n",
      "Epoch: 38970 \tTraining Loss: 1.188834 \tValidation Loss: 2.791017\n",
      "Epoch: 38971 \tTraining Loss: 1.134505 \tValidation Loss: 2.788223\n",
      "Epoch: 38972 \tTraining Loss: 1.139714 \tValidation Loss: 2.790375\n",
      "Epoch: 38973 \tTraining Loss: 1.172543 \tValidation Loss: 2.790212\n",
      "Epoch: 38974 \tTraining Loss: 1.149037 \tValidation Loss: 2.790650\n",
      "Epoch: 38975 \tTraining Loss: 1.186801 \tValidation Loss: 2.789068\n",
      "Epoch: 38976 \tTraining Loss: 1.159757 \tValidation Loss: 2.790803\n",
      "Epoch: 38977 \tTraining Loss: 1.193220 \tValidation Loss: 2.789698\n",
      "Epoch: 38978 \tTraining Loss: 1.114894 \tValidation Loss: 2.791827\n",
      "Epoch: 38979 \tTraining Loss: 1.163130 \tValidation Loss: 2.790810\n",
      "Epoch: 38980 \tTraining Loss: 1.169163 \tValidation Loss: 2.790859\n",
      "Epoch: 38981 \tTraining Loss: 1.184415 \tValidation Loss: 2.788851\n",
      "Epoch: 38982 \tTraining Loss: 1.135474 \tValidation Loss: 2.791329\n",
      "Epoch: 38983 \tTraining Loss: 1.169682 \tValidation Loss: 2.790274\n",
      "Epoch: 38984 \tTraining Loss: 1.161860 \tValidation Loss: 2.789037\n",
      "Epoch: 38985 \tTraining Loss: 1.183795 \tValidation Loss: 2.789362\n",
      "Epoch: 38986 \tTraining Loss: 1.171774 \tValidation Loss: 2.790041\n",
      "Epoch: 38987 \tTraining Loss: 1.148744 \tValidation Loss: 2.791788\n",
      "Epoch: 38988 \tTraining Loss: 1.167486 \tValidation Loss: 2.794521\n",
      "Epoch: 38989 \tTraining Loss: 1.227853 \tValidation Loss: 2.788842\n",
      "Epoch: 38990 \tTraining Loss: 1.190168 \tValidation Loss: 2.791324\n",
      "Epoch: 38991 \tTraining Loss: 1.139368 \tValidation Loss: 2.790791\n",
      "Epoch: 38992 \tTraining Loss: 1.180040 \tValidation Loss: 2.791913\n",
      "Epoch: 38993 \tTraining Loss: 1.098266 \tValidation Loss: 2.791125\n",
      "Epoch: 38994 \tTraining Loss: 1.186303 \tValidation Loss: 2.791351\n",
      "Epoch: 38995 \tTraining Loss: 1.142781 \tValidation Loss: 2.791379\n",
      "Epoch: 38996 \tTraining Loss: 1.176293 \tValidation Loss: 2.791214\n",
      "Epoch: 38997 \tTraining Loss: 1.138287 \tValidation Loss: 2.791297\n",
      "Epoch: 38998 \tTraining Loss: 1.142802 \tValidation Loss: 2.790016\n",
      "Epoch: 38999 \tTraining Loss: 1.158506 \tValidation Loss: 2.789931\n",
      "Epoch: 39000 \tTraining Loss: 1.191858 \tValidation Loss: 2.790076\n",
      "Epoch: 39001 \tTraining Loss: 1.189156 \tValidation Loss: 2.790213\n",
      "Epoch: 39002 \tTraining Loss: 1.172984 \tValidation Loss: 2.790251\n",
      "Epoch: 39003 \tTraining Loss: 1.187637 \tValidation Loss: 2.789986\n",
      "Epoch: 39004 \tTraining Loss: 1.160583 \tValidation Loss: 2.792111\n",
      "Epoch: 39005 \tTraining Loss: 1.191453 \tValidation Loss: 2.788547\n",
      "Epoch: 39006 \tTraining Loss: 1.224545 \tValidation Loss: 2.791008\n",
      "Epoch: 39007 \tTraining Loss: 1.133341 \tValidation Loss: 2.791944\n",
      "Epoch: 39008 \tTraining Loss: 1.172573 \tValidation Loss: 2.790272\n",
      "Epoch: 39009 \tTraining Loss: 1.125337 \tValidation Loss: 2.790535\n",
      "Epoch: 39010 \tTraining Loss: 1.154436 \tValidation Loss: 2.790505\n",
      "Epoch: 39011 \tTraining Loss: 1.163073 \tValidation Loss: 2.790976\n",
      "Epoch: 39012 \tTraining Loss: 1.162810 \tValidation Loss: 2.791811\n",
      "Epoch: 39013 \tTraining Loss: 1.150810 \tValidation Loss: 2.793722\n",
      "Epoch: 39014 \tTraining Loss: 1.167406 \tValidation Loss: 2.792177\n",
      "Epoch: 39015 \tTraining Loss: 1.138900 \tValidation Loss: 2.793108\n",
      "Epoch: 39016 \tTraining Loss: 1.149940 \tValidation Loss: 2.792485\n",
      "Epoch: 39017 \tTraining Loss: 1.139145 \tValidation Loss: 2.791738\n",
      "Epoch: 39018 \tTraining Loss: 1.169477 \tValidation Loss: 2.790171\n",
      "Epoch: 39019 \tTraining Loss: 1.181587 \tValidation Loss: 2.791096\n",
      "Epoch: 39020 \tTraining Loss: 1.157244 \tValidation Loss: 2.791821\n",
      "Epoch: 39021 \tTraining Loss: 1.141654 \tValidation Loss: 2.792332\n",
      "Epoch: 39022 \tTraining Loss: 1.146621 \tValidation Loss: 2.792649\n",
      "Epoch: 39023 \tTraining Loss: 1.175447 \tValidation Loss: 2.791441\n",
      "Epoch: 39024 \tTraining Loss: 1.140249 \tValidation Loss: 2.791942\n",
      "Epoch: 39025 \tTraining Loss: 1.143110 \tValidation Loss: 2.793554\n",
      "Epoch: 39026 \tTraining Loss: 1.198218 \tValidation Loss: 2.792354\n",
      "Epoch: 39027 \tTraining Loss: 1.152332 \tValidation Loss: 2.793055\n",
      "Epoch: 39028 \tTraining Loss: 1.172962 \tValidation Loss: 2.790892\n",
      "Epoch: 39029 \tTraining Loss: 1.130171 \tValidation Loss: 2.793391\n",
      "Epoch: 39030 \tTraining Loss: 1.123289 \tValidation Loss: 2.794168\n",
      "Epoch: 39031 \tTraining Loss: 1.150056 \tValidation Loss: 2.791700\n",
      "Epoch: 39032 \tTraining Loss: 1.127515 \tValidation Loss: 2.791979\n",
      "Epoch: 39033 \tTraining Loss: 1.163526 \tValidation Loss: 2.793101\n",
      "Epoch: 39034 \tTraining Loss: 1.155905 \tValidation Loss: 2.792694\n",
      "Epoch: 39035 \tTraining Loss: 1.149498 \tValidation Loss: 2.793796\n",
      "Epoch: 39036 \tTraining Loss: 1.201181 \tValidation Loss: 2.790694\n",
      "Epoch: 39037 \tTraining Loss: 1.197641 \tValidation Loss: 2.792757\n",
      "Epoch: 39038 \tTraining Loss: 1.151745 \tValidation Loss: 2.793921\n",
      "Epoch: 39039 \tTraining Loss: 1.153117 \tValidation Loss: 2.791205\n",
      "Epoch: 39040 \tTraining Loss: 1.142743 \tValidation Loss: 2.793564\n",
      "Epoch: 39041 \tTraining Loss: 1.161408 \tValidation Loss: 2.792167\n",
      "Epoch: 39042 \tTraining Loss: 1.171889 \tValidation Loss: 2.791219\n",
      "Epoch: 39043 \tTraining Loss: 1.153999 \tValidation Loss: 2.790768\n",
      "Epoch: 39044 \tTraining Loss: 1.135375 \tValidation Loss: 2.792732\n",
      "Epoch: 39045 \tTraining Loss: 1.187256 \tValidation Loss: 2.792333\n",
      "Epoch: 39046 \tTraining Loss: 1.183233 \tValidation Loss: 2.791229\n",
      "Epoch: 39047 \tTraining Loss: 1.143623 \tValidation Loss: 2.792266\n",
      "Epoch: 39048 \tTraining Loss: 1.202819 \tValidation Loss: 2.792985\n",
      "Epoch: 39049 \tTraining Loss: 1.161120 \tValidation Loss: 2.793201\n",
      "Epoch: 39050 \tTraining Loss: 1.155021 \tValidation Loss: 2.792715\n",
      "Epoch: 39051 \tTraining Loss: 1.140395 \tValidation Loss: 2.791460\n",
      "Epoch: 39052 \tTraining Loss: 1.190847 \tValidation Loss: 2.792506\n",
      "Epoch: 39053 \tTraining Loss: 1.201368 \tValidation Loss: 2.792716\n",
      "Epoch: 39054 \tTraining Loss: 1.130691 \tValidation Loss: 2.793453\n",
      "Epoch: 39055 \tTraining Loss: 1.160361 \tValidation Loss: 2.793034\n",
      "Epoch: 39056 \tTraining Loss: 1.144748 \tValidation Loss: 2.793148\n",
      "Epoch: 39057 \tTraining Loss: 1.145413 \tValidation Loss: 2.793712\n",
      "Epoch: 39058 \tTraining Loss: 1.176160 \tValidation Loss: 2.794555\n",
      "Epoch: 39059 \tTraining Loss: 1.148881 \tValidation Loss: 2.794390\n",
      "Epoch: 39060 \tTraining Loss: 1.158902 \tValidation Loss: 2.791579\n",
      "Epoch: 39061 \tTraining Loss: 1.162494 \tValidation Loss: 2.793284\n",
      "Epoch: 39062 \tTraining Loss: 1.144562 \tValidation Loss: 2.793876\n",
      "Epoch: 39063 \tTraining Loss: 1.129213 \tValidation Loss: 2.793655\n",
      "Epoch: 39064 \tTraining Loss: 1.154020 \tValidation Loss: 2.793187\n",
      "Epoch: 39065 \tTraining Loss: 1.187440 \tValidation Loss: 2.791968\n",
      "Epoch: 39066 \tTraining Loss: 1.095998 \tValidation Loss: 2.793398\n",
      "Epoch: 39067 \tTraining Loss: 1.142818 \tValidation Loss: 2.794271\n",
      "Epoch: 39068 \tTraining Loss: 1.183464 \tValidation Loss: 2.793973\n",
      "Epoch: 39069 \tTraining Loss: 1.140762 \tValidation Loss: 2.792349\n",
      "Epoch: 39070 \tTraining Loss: 1.159310 \tValidation Loss: 2.792336\n",
      "Epoch: 39071 \tTraining Loss: 1.167269 \tValidation Loss: 2.793768\n",
      "Epoch: 39072 \tTraining Loss: 1.151787 \tValidation Loss: 2.793532\n",
      "Epoch: 39073 \tTraining Loss: 1.169008 \tValidation Loss: 2.793503\n",
      "Epoch: 39074 \tTraining Loss: 1.213020 \tValidation Loss: 2.793771\n",
      "Epoch: 39075 \tTraining Loss: 1.199374 \tValidation Loss: 2.792170\n",
      "Epoch: 39076 \tTraining Loss: 1.163688 \tValidation Loss: 2.791671\n",
      "Epoch: 39077 \tTraining Loss: 1.168823 \tValidation Loss: 2.791633\n",
      "Epoch: 39078 \tTraining Loss: 1.171717 \tValidation Loss: 2.793732\n",
      "Epoch: 39079 \tTraining Loss: 1.175033 \tValidation Loss: 2.792902\n",
      "Epoch: 39080 \tTraining Loss: 1.137107 \tValidation Loss: 2.792849\n",
      "Epoch: 39081 \tTraining Loss: 1.209252 \tValidation Loss: 2.793826\n",
      "Epoch: 39082 \tTraining Loss: 1.180482 \tValidation Loss: 2.793240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39083 \tTraining Loss: 1.186714 \tValidation Loss: 2.793896\n",
      "Epoch: 39084 \tTraining Loss: 1.177646 \tValidation Loss: 2.791203\n",
      "Epoch: 39085 \tTraining Loss: 1.143138 \tValidation Loss: 2.793315\n",
      "Epoch: 39086 \tTraining Loss: 1.195775 \tValidation Loss: 2.793023\n",
      "Epoch: 39087 \tTraining Loss: 1.157900 \tValidation Loss: 2.792249\n",
      "Epoch: 39088 \tTraining Loss: 1.180902 \tValidation Loss: 2.792566\n",
      "Epoch: 39089 \tTraining Loss: 1.138248 \tValidation Loss: 2.793565\n",
      "Epoch: 39090 \tTraining Loss: 1.126596 \tValidation Loss: 2.793221\n",
      "Epoch: 39091 \tTraining Loss: 1.131688 \tValidation Loss: 2.795530\n",
      "Epoch: 39092 \tTraining Loss: 1.177232 \tValidation Loss: 2.791186\n",
      "Epoch: 39093 \tTraining Loss: 1.167378 \tValidation Loss: 2.793975\n",
      "Epoch: 39094 \tTraining Loss: 1.154693 \tValidation Loss: 2.794065\n",
      "Epoch: 39095 \tTraining Loss: 1.166273 \tValidation Loss: 2.791561\n",
      "Epoch: 39096 \tTraining Loss: 1.201191 \tValidation Loss: 2.791346\n",
      "Epoch: 39097 \tTraining Loss: 1.142166 \tValidation Loss: 2.792646\n",
      "Epoch: 39098 \tTraining Loss: 1.173486 \tValidation Loss: 2.792764\n",
      "Epoch: 39099 \tTraining Loss: 1.206348 \tValidation Loss: 2.791893\n",
      "Epoch: 39100 \tTraining Loss: 1.206199 \tValidation Loss: 2.794340\n",
      "Epoch: 39101 \tTraining Loss: 1.126628 \tValidation Loss: 2.794304\n",
      "Epoch: 39102 \tTraining Loss: 1.173358 \tValidation Loss: 2.792740\n",
      "Epoch: 39103 \tTraining Loss: 1.172348 \tValidation Loss: 2.794870\n",
      "Epoch: 39104 \tTraining Loss: 1.141115 \tValidation Loss: 2.794776\n",
      "Epoch: 39105 \tTraining Loss: 1.144080 \tValidation Loss: 2.793987\n",
      "Epoch: 39106 \tTraining Loss: 1.183593 \tValidation Loss: 2.794014\n",
      "Epoch: 39107 \tTraining Loss: 1.154842 \tValidation Loss: 2.793792\n",
      "Epoch: 39108 \tTraining Loss: 1.175248 \tValidation Loss: 2.794165\n",
      "Epoch: 39109 \tTraining Loss: 1.151562 \tValidation Loss: 2.793763\n",
      "Epoch: 39110 \tTraining Loss: 1.109708 \tValidation Loss: 2.795908\n",
      "Epoch: 39111 \tTraining Loss: 1.187610 \tValidation Loss: 2.793665\n",
      "Epoch: 39112 \tTraining Loss: 1.183837 \tValidation Loss: 2.793758\n",
      "Epoch: 39113 \tTraining Loss: 1.159424 \tValidation Loss: 2.794493\n",
      "Epoch: 39114 \tTraining Loss: 1.206227 \tValidation Loss: 2.792695\n",
      "Epoch: 39115 \tTraining Loss: 1.151719 \tValidation Loss: 2.795189\n",
      "Epoch: 39116 \tTraining Loss: 1.157459 \tValidation Loss: 2.795169\n",
      "Epoch: 39117 \tTraining Loss: 1.142434 \tValidation Loss: 2.795140\n",
      "Epoch: 39118 \tTraining Loss: 1.163642 \tValidation Loss: 2.794155\n",
      "Epoch: 39119 \tTraining Loss: 1.175081 \tValidation Loss: 2.793480\n",
      "Epoch: 39120 \tTraining Loss: 1.158612 \tValidation Loss: 2.792919\n",
      "Epoch: 39121 \tTraining Loss: 1.157944 \tValidation Loss: 2.795258\n",
      "Epoch: 39122 \tTraining Loss: 1.159891 \tValidation Loss: 2.793950\n",
      "Epoch: 39123 \tTraining Loss: 1.192174 \tValidation Loss: 2.793722\n",
      "Epoch: 39124 \tTraining Loss: 1.212657 \tValidation Loss: 2.795088\n",
      "Epoch: 39125 \tTraining Loss: 1.143797 \tValidation Loss: 2.793703\n",
      "Epoch: 39126 \tTraining Loss: 1.169769 \tValidation Loss: 2.795393\n",
      "Epoch: 39127 \tTraining Loss: 1.158684 \tValidation Loss: 2.793937\n",
      "Epoch: 39128 \tTraining Loss: 1.147874 \tValidation Loss: 2.795151\n",
      "Epoch: 39129 \tTraining Loss: 1.140464 \tValidation Loss: 2.793569\n",
      "Epoch: 39130 \tTraining Loss: 1.136971 \tValidation Loss: 2.793902\n",
      "Epoch: 39131 \tTraining Loss: 1.179230 \tValidation Loss: 2.795564\n",
      "Epoch: 39132 \tTraining Loss: 1.163845 \tValidation Loss: 2.794525\n",
      "Epoch: 39133 \tTraining Loss: 1.215063 \tValidation Loss: 2.796323\n",
      "Epoch: 39134 \tTraining Loss: 1.098118 \tValidation Loss: 2.795909\n",
      "Epoch: 39135 \tTraining Loss: 1.165134 \tValidation Loss: 2.793390\n",
      "Epoch: 39136 \tTraining Loss: 1.123469 \tValidation Loss: 2.796018\n",
      "Epoch: 39137 \tTraining Loss: 1.183007 \tValidation Loss: 2.793572\n",
      "Epoch: 39138 \tTraining Loss: 1.164571 \tValidation Loss: 2.796858\n",
      "Epoch: 39139 \tTraining Loss: 1.152303 \tValidation Loss: 2.793874\n",
      "Epoch: 39140 \tTraining Loss: 1.180144 \tValidation Loss: 2.794104\n",
      "Epoch: 39141 \tTraining Loss: 1.118062 \tValidation Loss: 2.793941\n",
      "Epoch: 39142 \tTraining Loss: 1.147140 \tValidation Loss: 2.795420\n",
      "Epoch: 39143 \tTraining Loss: 1.177276 \tValidation Loss: 2.794200\n",
      "Epoch: 39144 \tTraining Loss: 1.172218 \tValidation Loss: 2.793493\n",
      "Epoch: 39145 \tTraining Loss: 1.211286 \tValidation Loss: 2.795466\n",
      "Epoch: 39146 \tTraining Loss: 1.157881 \tValidation Loss: 2.793958\n",
      "Epoch: 39147 \tTraining Loss: 1.160887 \tValidation Loss: 2.794502\n",
      "Epoch: 39148 \tTraining Loss: 1.105306 \tValidation Loss: 2.796065\n",
      "Epoch: 39149 \tTraining Loss: 1.133100 \tValidation Loss: 2.795072\n",
      "Epoch: 39150 \tTraining Loss: 1.222585 \tValidation Loss: 2.794147\n",
      "Epoch: 39151 \tTraining Loss: 1.171365 \tValidation Loss: 2.795516\n",
      "Epoch: 39152 \tTraining Loss: 1.152495 \tValidation Loss: 2.796579\n",
      "Epoch: 39153 \tTraining Loss: 1.175422 \tValidation Loss: 2.793948\n",
      "Epoch: 39154 \tTraining Loss: 1.153242 \tValidation Loss: 2.793834\n",
      "Epoch: 39155 \tTraining Loss: 1.175444 \tValidation Loss: 2.794509\n",
      "Epoch: 39156 \tTraining Loss: 1.194632 \tValidation Loss: 2.795835\n",
      "Epoch: 39157 \tTraining Loss: 1.174022 \tValidation Loss: 2.795165\n",
      "Epoch: 39158 \tTraining Loss: 1.161165 \tValidation Loss: 2.794610\n",
      "Epoch: 39159 \tTraining Loss: 1.149497 \tValidation Loss: 2.795387\n",
      "Epoch: 39160 \tTraining Loss: 1.165225 \tValidation Loss: 2.795414\n",
      "Epoch: 39161 \tTraining Loss: 1.124053 \tValidation Loss: 2.793736\n",
      "Epoch: 39162 \tTraining Loss: 1.182176 \tValidation Loss: 2.794969\n",
      "Epoch: 39163 \tTraining Loss: 1.169034 \tValidation Loss: 2.795776\n",
      "Epoch: 39164 \tTraining Loss: 1.142618 \tValidation Loss: 2.795296\n",
      "Epoch: 39165 \tTraining Loss: 1.215471 \tValidation Loss: 2.795163\n",
      "Epoch: 39166 \tTraining Loss: 1.131753 \tValidation Loss: 2.794535\n",
      "Epoch: 39167 \tTraining Loss: 1.160780 \tValidation Loss: 2.793486\n",
      "Epoch: 39168 \tTraining Loss: 1.184862 \tValidation Loss: 2.792555\n",
      "Epoch: 39169 \tTraining Loss: 1.169072 \tValidation Loss: 2.795852\n",
      "Epoch: 39170 \tTraining Loss: 1.150561 \tValidation Loss: 2.794856\n",
      "Epoch: 39171 \tTraining Loss: 1.150267 \tValidation Loss: 2.795055\n",
      "Epoch: 39172 \tTraining Loss: 1.158615 \tValidation Loss: 2.795644\n",
      "Epoch: 39173 \tTraining Loss: 1.150792 \tValidation Loss: 2.795630\n",
      "Epoch: 39174 \tTraining Loss: 1.130007 \tValidation Loss: 2.795516\n",
      "Epoch: 39175 \tTraining Loss: 1.136403 \tValidation Loss: 2.797894\n",
      "Epoch: 39176 \tTraining Loss: 1.152104 \tValidation Loss: 2.795083\n",
      "Epoch: 39177 \tTraining Loss: 1.157003 \tValidation Loss: 2.795122\n",
      "Epoch: 39178 \tTraining Loss: 1.136593 \tValidation Loss: 2.796676\n",
      "Epoch: 39179 \tTraining Loss: 1.137209 \tValidation Loss: 2.796441\n",
      "Epoch: 39180 \tTraining Loss: 1.149574 \tValidation Loss: 2.795074\n",
      "Epoch: 39181 \tTraining Loss: 1.166676 \tValidation Loss: 2.794214\n",
      "Epoch: 39182 \tTraining Loss: 1.170582 \tValidation Loss: 2.793994\n",
      "Epoch: 39183 \tTraining Loss: 1.188934 \tValidation Loss: 2.794573\n",
      "Epoch: 39184 \tTraining Loss: 1.153934 \tValidation Loss: 2.794372\n",
      "Epoch: 39185 \tTraining Loss: 1.177431 \tValidation Loss: 2.794235\n",
      "Epoch: 39186 \tTraining Loss: 1.143802 \tValidation Loss: 2.796871\n",
      "Epoch: 39187 \tTraining Loss: 1.154889 \tValidation Loss: 2.796159\n",
      "Epoch: 39188 \tTraining Loss: 1.166797 \tValidation Loss: 2.796875\n",
      "Epoch: 39189 \tTraining Loss: 1.208269 \tValidation Loss: 2.795519\n",
      "Epoch: 39190 \tTraining Loss: 1.140304 \tValidation Loss: 2.794621\n",
      "Epoch: 39191 \tTraining Loss: 1.213056 \tValidation Loss: 2.796448\n",
      "Epoch: 39192 \tTraining Loss: 1.165117 \tValidation Loss: 2.794913\n",
      "Epoch: 39193 \tTraining Loss: 1.180943 \tValidation Loss: 2.795799\n",
      "Epoch: 39194 \tTraining Loss: 1.139158 \tValidation Loss: 2.795102\n",
      "Epoch: 39195 \tTraining Loss: 1.196982 \tValidation Loss: 2.797530\n",
      "Epoch: 39196 \tTraining Loss: 1.155979 \tValidation Loss: 2.795339\n",
      "Epoch: 39197 \tTraining Loss: 1.144867 \tValidation Loss: 2.796493\n",
      "Epoch: 39198 \tTraining Loss: 1.148160 \tValidation Loss: 2.796085\n",
      "Epoch: 39199 \tTraining Loss: 1.184248 \tValidation Loss: 2.795614\n",
      "Epoch: 39200 \tTraining Loss: 1.184865 \tValidation Loss: 2.795434\n",
      "Epoch: 39201 \tTraining Loss: 1.184384 \tValidation Loss: 2.795829\n",
      "Epoch: 39202 \tTraining Loss: 1.159933 \tValidation Loss: 2.795565\n",
      "Epoch: 39203 \tTraining Loss: 1.149954 \tValidation Loss: 2.795411\n",
      "Epoch: 39204 \tTraining Loss: 1.156410 \tValidation Loss: 2.795670\n",
      "Epoch: 39205 \tTraining Loss: 1.128434 \tValidation Loss: 2.798088\n",
      "Epoch: 39206 \tTraining Loss: 1.141214 \tValidation Loss: 2.796259\n",
      "Epoch: 39207 \tTraining Loss: 1.180401 \tValidation Loss: 2.795683\n",
      "Epoch: 39208 \tTraining Loss: 1.129822 \tValidation Loss: 2.795596\n",
      "Epoch: 39209 \tTraining Loss: 1.202027 \tValidation Loss: 2.794946\n",
      "Epoch: 39210 \tTraining Loss: 1.176584 \tValidation Loss: 2.793500\n",
      "Epoch: 39211 \tTraining Loss: 1.131015 \tValidation Loss: 2.796891\n",
      "Epoch: 39212 \tTraining Loss: 1.179646 \tValidation Loss: 2.794866\n",
      "Epoch: 39213 \tTraining Loss: 1.158481 \tValidation Loss: 2.797306\n",
      "Epoch: 39214 \tTraining Loss: 1.163311 \tValidation Loss: 2.795180\n",
      "Epoch: 39215 \tTraining Loss: 1.187948 \tValidation Loss: 2.797000\n",
      "Epoch: 39216 \tTraining Loss: 1.241283 \tValidation Loss: 2.795341\n",
      "Epoch: 39217 \tTraining Loss: 1.174851 \tValidation Loss: 2.795476\n",
      "Epoch: 39218 \tTraining Loss: 1.134276 \tValidation Loss: 2.797915\n",
      "Epoch: 39219 \tTraining Loss: 1.157955 \tValidation Loss: 2.796381\n",
      "Epoch: 39220 \tTraining Loss: 1.159483 \tValidation Loss: 2.796996\n",
      "Epoch: 39221 \tTraining Loss: 1.134229 \tValidation Loss: 2.797394\n",
      "Epoch: 39222 \tTraining Loss: 1.154021 \tValidation Loss: 2.797723\n",
      "Epoch: 39223 \tTraining Loss: 1.146320 \tValidation Loss: 2.798561\n",
      "Epoch: 39224 \tTraining Loss: 1.161403 \tValidation Loss: 2.797859\n",
      "Epoch: 39225 \tTraining Loss: 1.158346 \tValidation Loss: 2.796578\n",
      "Epoch: 39226 \tTraining Loss: 1.100542 \tValidation Loss: 2.798484\n",
      "Epoch: 39227 \tTraining Loss: 1.179939 \tValidation Loss: 2.797564\n",
      "Epoch: 39228 \tTraining Loss: 1.165519 \tValidation Loss: 2.796801\n",
      "Epoch: 39229 \tTraining Loss: 1.188193 \tValidation Loss: 2.795710\n",
      "Epoch: 39230 \tTraining Loss: 1.115266 \tValidation Loss: 2.797895\n",
      "Epoch: 39231 \tTraining Loss: 1.195039 \tValidation Loss: 2.796532\n",
      "Epoch: 39232 \tTraining Loss: 1.168602 \tValidation Loss: 2.795424\n",
      "Epoch: 39233 \tTraining Loss: 1.144032 \tValidation Loss: 2.797642\n",
      "Epoch: 39234 \tTraining Loss: 1.139140 \tValidation Loss: 2.796849\n",
      "Epoch: 39235 \tTraining Loss: 1.193415 \tValidation Loss: 2.797386\n",
      "Epoch: 39236 \tTraining Loss: 1.156577 \tValidation Loss: 2.797404\n",
      "Epoch: 39237 \tTraining Loss: 1.176264 \tValidation Loss: 2.798922\n",
      "Epoch: 39238 \tTraining Loss: 1.151467 \tValidation Loss: 2.795760\n",
      "Epoch: 39239 \tTraining Loss: 1.152131 \tValidation Loss: 2.799395\n",
      "Epoch: 39240 \tTraining Loss: 1.169936 \tValidation Loss: 2.796230\n",
      "Epoch: 39241 \tTraining Loss: 1.172231 \tValidation Loss: 2.797020\n",
      "Epoch: 39242 \tTraining Loss: 1.159624 \tValidation Loss: 2.796149\n",
      "Epoch: 39243 \tTraining Loss: 1.169638 \tValidation Loss: 2.796444\n",
      "Epoch: 39244 \tTraining Loss: 1.149481 \tValidation Loss: 2.797608\n",
      "Epoch: 39245 \tTraining Loss: 1.140769 \tValidation Loss: 2.796560\n",
      "Epoch: 39246 \tTraining Loss: 1.110741 \tValidation Loss: 2.799093\n",
      "Epoch: 39247 \tTraining Loss: 1.111192 \tValidation Loss: 2.797731\n",
      "Epoch: 39248 \tTraining Loss: 1.139416 \tValidation Loss: 2.796282\n",
      "Epoch: 39249 \tTraining Loss: 1.146705 \tValidation Loss: 2.796088\n",
      "Epoch: 39250 \tTraining Loss: 1.165020 \tValidation Loss: 2.797230\n",
      "Epoch: 39251 \tTraining Loss: 1.169723 \tValidation Loss: 2.796771\n",
      "Epoch: 39252 \tTraining Loss: 1.138309 \tValidation Loss: 2.798819\n",
      "Epoch: 39253 \tTraining Loss: 1.139295 \tValidation Loss: 2.798852\n",
      "Epoch: 39254 \tTraining Loss: 1.169161 \tValidation Loss: 2.796508\n",
      "Epoch: 39255 \tTraining Loss: 1.159517 \tValidation Loss: 2.797912\n",
      "Epoch: 39256 \tTraining Loss: 1.197555 \tValidation Loss: 2.798313\n",
      "Epoch: 39257 \tTraining Loss: 1.152602 \tValidation Loss: 2.796749\n",
      "Epoch: 39258 \tTraining Loss: 1.164919 \tValidation Loss: 2.797233\n",
      "Epoch: 39259 \tTraining Loss: 1.171920 \tValidation Loss: 2.798058\n",
      "Epoch: 39260 \tTraining Loss: 1.149812 \tValidation Loss: 2.797946\n",
      "Epoch: 39261 \tTraining Loss: 1.145031 \tValidation Loss: 2.797254\n",
      "Epoch: 39262 \tTraining Loss: 1.187129 \tValidation Loss: 2.797707\n",
      "Epoch: 39263 \tTraining Loss: 1.092175 \tValidation Loss: 2.799294\n",
      "Epoch: 39264 \tTraining Loss: 1.174922 \tValidation Loss: 2.797320\n",
      "Epoch: 39265 \tTraining Loss: 1.164418 \tValidation Loss: 2.798812\n",
      "Epoch: 39266 \tTraining Loss: 1.143670 \tValidation Loss: 2.797067\n",
      "Epoch: 39267 \tTraining Loss: 1.134295 \tValidation Loss: 2.797177\n",
      "Epoch: 39268 \tTraining Loss: 1.184143 \tValidation Loss: 2.796767\n",
      "Epoch: 39269 \tTraining Loss: 1.114918 \tValidation Loss: 2.799437\n",
      "Epoch: 39270 \tTraining Loss: 1.169372 \tValidation Loss: 2.797479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39271 \tTraining Loss: 1.173748 \tValidation Loss: 2.797157\n",
      "Epoch: 39272 \tTraining Loss: 1.127551 \tValidation Loss: 2.799216\n",
      "Epoch: 39273 \tTraining Loss: 1.173881 \tValidation Loss: 2.797899\n",
      "Epoch: 39274 \tTraining Loss: 1.200235 \tValidation Loss: 2.798397\n",
      "Epoch: 39275 \tTraining Loss: 1.155789 \tValidation Loss: 2.797026\n",
      "Epoch: 39276 \tTraining Loss: 1.139185 \tValidation Loss: 2.799006\n",
      "Epoch: 39277 \tTraining Loss: 1.199502 \tValidation Loss: 2.799745\n",
      "Epoch: 39278 \tTraining Loss: 1.198815 \tValidation Loss: 2.799065\n",
      "Epoch: 39279 \tTraining Loss: 1.226618 \tValidation Loss: 2.796523\n",
      "Epoch: 39280 \tTraining Loss: 1.123744 \tValidation Loss: 2.798210\n",
      "Epoch: 39281 \tTraining Loss: 1.140842 \tValidation Loss: 2.799767\n",
      "Epoch: 39282 \tTraining Loss: 1.147035 \tValidation Loss: 2.798141\n",
      "Epoch: 39283 \tTraining Loss: 1.166387 \tValidation Loss: 2.798129\n",
      "Epoch: 39284 \tTraining Loss: 1.154570 \tValidation Loss: 2.798583\n",
      "Epoch: 39285 \tTraining Loss: 1.104364 \tValidation Loss: 2.798046\n",
      "Epoch: 39286 \tTraining Loss: 1.182174 \tValidation Loss: 2.797765\n",
      "Epoch: 39287 \tTraining Loss: 1.183645 \tValidation Loss: 2.798680\n",
      "Epoch: 39288 \tTraining Loss: 1.155572 \tValidation Loss: 2.798444\n",
      "Epoch: 39289 \tTraining Loss: 1.123378 \tValidation Loss: 2.798793\n",
      "Epoch: 39290 \tTraining Loss: 1.184303 \tValidation Loss: 2.798266\n",
      "Epoch: 39291 \tTraining Loss: 1.137906 \tValidation Loss: 2.799272\n",
      "Epoch: 39292 \tTraining Loss: 1.162175 \tValidation Loss: 2.798742\n",
      "Epoch: 39293 \tTraining Loss: 1.129983 \tValidation Loss: 2.799367\n",
      "Epoch: 39294 \tTraining Loss: 1.147778 \tValidation Loss: 2.799932\n",
      "Epoch: 39295 \tTraining Loss: 1.216414 \tValidation Loss: 2.797546\n",
      "Epoch: 39296 \tTraining Loss: 1.174783 \tValidation Loss: 2.797885\n",
      "Epoch: 39297 \tTraining Loss: 1.182393 \tValidation Loss: 2.799485\n",
      "Epoch: 39298 \tTraining Loss: 1.132699 \tValidation Loss: 2.797611\n",
      "Epoch: 39299 \tTraining Loss: 1.166813 \tValidation Loss: 2.797958\n",
      "Epoch: 39300 \tTraining Loss: 1.131563 \tValidation Loss: 2.799955\n",
      "Epoch: 39301 \tTraining Loss: 1.175892 \tValidation Loss: 2.799083\n",
      "Epoch: 39302 \tTraining Loss: 1.144618 \tValidation Loss: 2.798805\n",
      "Epoch: 39303 \tTraining Loss: 1.205698 \tValidation Loss: 2.799439\n",
      "Epoch: 39304 \tTraining Loss: 1.119688 \tValidation Loss: 2.800445\n",
      "Epoch: 39305 \tTraining Loss: 1.162752 \tValidation Loss: 2.799123\n",
      "Epoch: 39306 \tTraining Loss: 1.226902 \tValidation Loss: 2.798088\n",
      "Epoch: 39307 \tTraining Loss: 1.198658 \tValidation Loss: 2.799232\n",
      "Epoch: 39308 \tTraining Loss: 1.120787 \tValidation Loss: 2.800842\n",
      "Epoch: 39309 \tTraining Loss: 1.166892 \tValidation Loss: 2.799228\n",
      "Epoch: 39310 \tTraining Loss: 1.179610 \tValidation Loss: 2.798498\n",
      "Epoch: 39311 \tTraining Loss: 1.156526 \tValidation Loss: 2.800251\n",
      "Epoch: 39312 \tTraining Loss: 1.161787 \tValidation Loss: 2.799440\n",
      "Epoch: 39313 \tTraining Loss: 1.130955 \tValidation Loss: 2.799087\n",
      "Epoch: 39314 \tTraining Loss: 1.146008 \tValidation Loss: 2.799409\n",
      "Epoch: 39315 \tTraining Loss: 1.142532 \tValidation Loss: 2.799362\n",
      "Epoch: 39316 \tTraining Loss: 1.127726 \tValidation Loss: 2.800636\n",
      "Epoch: 39317 \tTraining Loss: 1.161095 \tValidation Loss: 2.799152\n",
      "Epoch: 39318 \tTraining Loss: 1.147868 \tValidation Loss: 2.799358\n",
      "Epoch: 39319 \tTraining Loss: 1.143831 \tValidation Loss: 2.799872\n",
      "Epoch: 39320 \tTraining Loss: 1.189055 \tValidation Loss: 2.801134\n",
      "Epoch: 39321 \tTraining Loss: 1.175517 \tValidation Loss: 2.799950\n",
      "Epoch: 39322 \tTraining Loss: 1.176526 \tValidation Loss: 2.799348\n",
      "Epoch: 39323 \tTraining Loss: 1.172679 \tValidation Loss: 2.799748\n",
      "Epoch: 39324 \tTraining Loss: 1.193069 \tValidation Loss: 2.798070\n",
      "Epoch: 39325 \tTraining Loss: 1.138918 \tValidation Loss: 2.799775\n",
      "Epoch: 39326 \tTraining Loss: 1.181981 \tValidation Loss: 2.796978\n",
      "Epoch: 39327 \tTraining Loss: 1.131671 \tValidation Loss: 2.799398\n",
      "Epoch: 39328 \tTraining Loss: 1.173427 \tValidation Loss: 2.798817\n",
      "Epoch: 39329 \tTraining Loss: 1.157888 \tValidation Loss: 2.799193\n",
      "Epoch: 39330 \tTraining Loss: 1.145520 \tValidation Loss: 2.800199\n",
      "Epoch: 39331 \tTraining Loss: 1.126443 \tValidation Loss: 2.798263\n",
      "Epoch: 39332 \tTraining Loss: 1.178226 \tValidation Loss: 2.799633\n",
      "Epoch: 39333 \tTraining Loss: 1.123668 \tValidation Loss: 2.799854\n",
      "Epoch: 39334 \tTraining Loss: 1.118472 \tValidation Loss: 2.800756\n",
      "Epoch: 39335 \tTraining Loss: 1.140425 \tValidation Loss: 2.797465\n",
      "Epoch: 39336 \tTraining Loss: 1.160455 \tValidation Loss: 2.799732\n",
      "Epoch: 39337 \tTraining Loss: 1.154602 \tValidation Loss: 2.799412\n",
      "Epoch: 39338 \tTraining Loss: 1.144183 \tValidation Loss: 2.801879\n",
      "Epoch: 39339 \tTraining Loss: 1.175373 \tValidation Loss: 2.799791\n",
      "Epoch: 39340 \tTraining Loss: 1.191840 \tValidation Loss: 2.799615\n",
      "Epoch: 39341 \tTraining Loss: 1.204363 \tValidation Loss: 2.798508\n",
      "Epoch: 39342 \tTraining Loss: 1.136368 \tValidation Loss: 2.800625\n",
      "Epoch: 39343 \tTraining Loss: 1.161359 \tValidation Loss: 2.799509\n",
      "Epoch: 39344 \tTraining Loss: 1.154932 \tValidation Loss: 2.800552\n",
      "Epoch: 39345 \tTraining Loss: 1.142747 \tValidation Loss: 2.800742\n",
      "Epoch: 39346 \tTraining Loss: 1.148950 \tValidation Loss: 2.799147\n",
      "Epoch: 39347 \tTraining Loss: 1.175193 \tValidation Loss: 2.800554\n",
      "Epoch: 39348 \tTraining Loss: 1.179801 \tValidation Loss: 2.799966\n",
      "Epoch: 39349 \tTraining Loss: 1.151348 \tValidation Loss: 2.800168\n",
      "Epoch: 39350 \tTraining Loss: 1.198866 \tValidation Loss: 2.801750\n",
      "Epoch: 39351 \tTraining Loss: 1.212908 \tValidation Loss: 2.798200\n",
      "Epoch: 39352 \tTraining Loss: 1.174704 \tValidation Loss: 2.801797\n",
      "Epoch: 39353 \tTraining Loss: 1.182534 \tValidation Loss: 2.801274\n",
      "Epoch: 39354 \tTraining Loss: 1.107762 \tValidation Loss: 2.801270\n",
      "Epoch: 39355 \tTraining Loss: 1.143088 \tValidation Loss: 2.800626\n",
      "Epoch: 39356 \tTraining Loss: 1.141493 \tValidation Loss: 2.800439\n",
      "Epoch: 39357 \tTraining Loss: 1.183698 \tValidation Loss: 2.801192\n",
      "Epoch: 39358 \tTraining Loss: 1.195855 \tValidation Loss: 2.799008\n",
      "Epoch: 39359 \tTraining Loss: 1.184393 \tValidation Loss: 2.799848\n",
      "Epoch: 39360 \tTraining Loss: 1.134597 \tValidation Loss: 2.801213\n",
      "Epoch: 39361 \tTraining Loss: 1.153401 \tValidation Loss: 2.799743\n",
      "Epoch: 39362 \tTraining Loss: 1.151113 \tValidation Loss: 2.799849\n",
      "Epoch: 39363 \tTraining Loss: 1.129049 \tValidation Loss: 2.801641\n",
      "Epoch: 39364 \tTraining Loss: 1.166424 \tValidation Loss: 2.801063\n",
      "Epoch: 39365 \tTraining Loss: 1.152508 \tValidation Loss: 2.801075\n",
      "Epoch: 39366 \tTraining Loss: 1.157264 \tValidation Loss: 2.800364\n",
      "Epoch: 39367 \tTraining Loss: 1.155199 \tValidation Loss: 2.801322\n",
      "Epoch: 39368 \tTraining Loss: 1.194043 \tValidation Loss: 2.801965\n",
      "Epoch: 39369 \tTraining Loss: 1.101011 \tValidation Loss: 2.800077\n",
      "Epoch: 39370 \tTraining Loss: 1.177425 \tValidation Loss: 2.800041\n",
      "Epoch: 39371 \tTraining Loss: 1.126384 \tValidation Loss: 2.801058\n",
      "Epoch: 39372 \tTraining Loss: 1.169603 \tValidation Loss: 2.799862\n",
      "Epoch: 39373 \tTraining Loss: 1.198764 \tValidation Loss: 2.800216\n",
      "Epoch: 39374 \tTraining Loss: 1.215676 \tValidation Loss: 2.801034\n",
      "Epoch: 39375 \tTraining Loss: 1.134156 \tValidation Loss: 2.800616\n",
      "Epoch: 39376 \tTraining Loss: 1.182210 \tValidation Loss: 2.800183\n",
      "Epoch: 39377 \tTraining Loss: 1.178189 \tValidation Loss: 2.800367\n",
      "Epoch: 39378 \tTraining Loss: 1.150498 \tValidation Loss: 2.802084\n",
      "Epoch: 39379 \tTraining Loss: 1.172353 \tValidation Loss: 2.799837\n",
      "Epoch: 39380 \tTraining Loss: 1.176938 \tValidation Loss: 2.800650\n",
      "Epoch: 39381 \tTraining Loss: 1.177876 \tValidation Loss: 2.800093\n",
      "Epoch: 39382 \tTraining Loss: 1.190792 \tValidation Loss: 2.801344\n",
      "Epoch: 39383 \tTraining Loss: 1.110369 \tValidation Loss: 2.801975\n",
      "Epoch: 39384 \tTraining Loss: 1.147519 \tValidation Loss: 2.801247\n",
      "Epoch: 39385 \tTraining Loss: 1.187898 \tValidation Loss: 2.799222\n",
      "Epoch: 39386 \tTraining Loss: 1.147458 \tValidation Loss: 2.799914\n",
      "Epoch: 39387 \tTraining Loss: 1.165202 \tValidation Loss: 2.800755\n",
      "Epoch: 39388 \tTraining Loss: 1.149022 \tValidation Loss: 2.801291\n",
      "Epoch: 39389 \tTraining Loss: 1.161127 \tValidation Loss: 2.801154\n",
      "Epoch: 39390 \tTraining Loss: 1.190745 \tValidation Loss: 2.801454\n",
      "Epoch: 39391 \tTraining Loss: 1.122658 \tValidation Loss: 2.804101\n",
      "Epoch: 39392 \tTraining Loss: 1.181418 \tValidation Loss: 2.800723\n",
      "Epoch: 39393 \tTraining Loss: 1.157370 \tValidation Loss: 2.799126\n",
      "Epoch: 39394 \tTraining Loss: 1.191862 \tValidation Loss: 2.802979\n",
      "Epoch: 39395 \tTraining Loss: 1.135765 \tValidation Loss: 2.802340\n",
      "Epoch: 39396 \tTraining Loss: 1.127994 \tValidation Loss: 2.803734\n",
      "Epoch: 39397 \tTraining Loss: 1.175694 \tValidation Loss: 2.799601\n",
      "Epoch: 39398 \tTraining Loss: 1.187527 \tValidation Loss: 2.800916\n",
      "Epoch: 39399 \tTraining Loss: 1.165962 \tValidation Loss: 2.801033\n",
      "Epoch: 39400 \tTraining Loss: 1.200214 \tValidation Loss: 2.800806\n",
      "Epoch: 39401 \tTraining Loss: 1.117271 \tValidation Loss: 2.801705\n",
      "Epoch: 39402 \tTraining Loss: 1.151024 \tValidation Loss: 2.800978\n",
      "Epoch: 39403 \tTraining Loss: 1.201255 \tValidation Loss: 2.802111\n",
      "Epoch: 39404 \tTraining Loss: 1.162534 \tValidation Loss: 2.801564\n",
      "Epoch: 39405 \tTraining Loss: 1.200094 \tValidation Loss: 2.801053\n",
      "Epoch: 39406 \tTraining Loss: 1.222092 \tValidation Loss: 2.801118\n",
      "Epoch: 39407 \tTraining Loss: 1.097349 \tValidation Loss: 2.802357\n",
      "Epoch: 39408 \tTraining Loss: 1.181782 \tValidation Loss: 2.800708\n",
      "Epoch: 39409 \tTraining Loss: 1.181588 \tValidation Loss: 2.801512\n",
      "Epoch: 39410 \tTraining Loss: 1.155926 \tValidation Loss: 2.802322\n",
      "Epoch: 39411 \tTraining Loss: 1.166693 \tValidation Loss: 2.800212\n",
      "Epoch: 39412 \tTraining Loss: 1.159402 \tValidation Loss: 2.802587\n",
      "Epoch: 39413 \tTraining Loss: 1.184921 \tValidation Loss: 2.800426\n",
      "Epoch: 39414 \tTraining Loss: 1.177771 \tValidation Loss: 2.803517\n",
      "Epoch: 39415 \tTraining Loss: 1.120805 \tValidation Loss: 2.803465\n",
      "Epoch: 39416 \tTraining Loss: 1.131342 \tValidation Loss: 2.802749\n",
      "Epoch: 39417 \tTraining Loss: 1.137087 \tValidation Loss: 2.803043\n",
      "Epoch: 39418 \tTraining Loss: 1.161571 \tValidation Loss: 2.801716\n",
      "Epoch: 39419 \tTraining Loss: 1.169331 \tValidation Loss: 2.803053\n",
      "Epoch: 39420 \tTraining Loss: 1.152743 \tValidation Loss: 2.801769\n",
      "Epoch: 39421 \tTraining Loss: 1.134542 \tValidation Loss: 2.802189\n",
      "Epoch: 39422 \tTraining Loss: 1.181427 \tValidation Loss: 2.799432\n",
      "Epoch: 39423 \tTraining Loss: 1.176474 \tValidation Loss: 2.802496\n",
      "Epoch: 39424 \tTraining Loss: 1.114599 \tValidation Loss: 2.802860\n",
      "Epoch: 39425 \tTraining Loss: 1.096054 \tValidation Loss: 2.802894\n",
      "Epoch: 39426 \tTraining Loss: 1.203305 \tValidation Loss: 2.802363\n",
      "Epoch: 39427 \tTraining Loss: 1.214680 \tValidation Loss: 2.799627\n",
      "Epoch: 39428 \tTraining Loss: 1.185368 \tValidation Loss: 2.801960\n",
      "Epoch: 39429 \tTraining Loss: 1.133427 \tValidation Loss: 2.802987\n",
      "Epoch: 39430 \tTraining Loss: 1.142982 \tValidation Loss: 2.803386\n",
      "Epoch: 39431 \tTraining Loss: 1.230080 \tValidation Loss: 2.802487\n",
      "Epoch: 39432 \tTraining Loss: 1.114918 \tValidation Loss: 2.801486\n",
      "Epoch: 39433 \tTraining Loss: 1.153594 \tValidation Loss: 2.802337\n",
      "Epoch: 39434 \tTraining Loss: 1.153414 \tValidation Loss: 2.802392\n",
      "Epoch: 39435 \tTraining Loss: 1.139709 \tValidation Loss: 2.802198\n",
      "Epoch: 39436 \tTraining Loss: 1.145480 \tValidation Loss: 2.802245\n",
      "Epoch: 39437 \tTraining Loss: 1.103083 \tValidation Loss: 2.803655\n",
      "Epoch: 39438 \tTraining Loss: 1.145921 \tValidation Loss: 2.803462\n",
      "Epoch: 39439 \tTraining Loss: 1.113097 \tValidation Loss: 2.803526\n",
      "Epoch: 39440 \tTraining Loss: 1.121401 \tValidation Loss: 2.803092\n",
      "Epoch: 39441 \tTraining Loss: 1.188762 \tValidation Loss: 2.802499\n",
      "Epoch: 39442 \tTraining Loss: 1.107507 \tValidation Loss: 2.803975\n",
      "Epoch: 39443 \tTraining Loss: 1.158084 \tValidation Loss: 2.803498\n",
      "Epoch: 39444 \tTraining Loss: 1.167602 \tValidation Loss: 2.801749\n",
      "Epoch: 39445 \tTraining Loss: 1.150186 \tValidation Loss: 2.803383\n",
      "Epoch: 39446 \tTraining Loss: 1.124934 \tValidation Loss: 2.804044\n",
      "Epoch: 39447 \tTraining Loss: 1.107819 \tValidation Loss: 2.806063\n",
      "Epoch: 39448 \tTraining Loss: 1.169390 \tValidation Loss: 2.803881\n",
      "Epoch: 39449 \tTraining Loss: 1.174314 \tValidation Loss: 2.802950\n",
      "Epoch: 39450 \tTraining Loss: 1.164176 \tValidation Loss: 2.805114\n",
      "Epoch: 39451 \tTraining Loss: 1.216351 \tValidation Loss: 2.801775\n",
      "Epoch: 39452 \tTraining Loss: 1.171988 \tValidation Loss: 2.801855\n",
      "Epoch: 39453 \tTraining Loss: 1.117327 \tValidation Loss: 2.804536\n",
      "Epoch: 39454 \tTraining Loss: 1.169331 \tValidation Loss: 2.801030\n",
      "Epoch: 39455 \tTraining Loss: 1.151572 \tValidation Loss: 2.801448\n",
      "Epoch: 39456 \tTraining Loss: 1.173287 \tValidation Loss: 2.802145\n",
      "Epoch: 39457 \tTraining Loss: 1.137255 \tValidation Loss: 2.801982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39458 \tTraining Loss: 1.164769 \tValidation Loss: 2.802046\n",
      "Epoch: 39459 \tTraining Loss: 1.178769 \tValidation Loss: 2.802891\n",
      "Epoch: 39460 \tTraining Loss: 1.126950 \tValidation Loss: 2.804179\n",
      "Epoch: 39461 \tTraining Loss: 1.139982 \tValidation Loss: 2.805307\n",
      "Epoch: 39462 \tTraining Loss: 1.201419 \tValidation Loss: 2.803339\n",
      "Epoch: 39463 \tTraining Loss: 1.161378 \tValidation Loss: 2.803447\n",
      "Epoch: 39464 \tTraining Loss: 1.126217 \tValidation Loss: 2.806401\n",
      "Epoch: 39465 \tTraining Loss: 1.134254 \tValidation Loss: 2.803955\n",
      "Epoch: 39466 \tTraining Loss: 1.157597 \tValidation Loss: 2.803561\n",
      "Epoch: 39467 \tTraining Loss: 1.180507 \tValidation Loss: 2.804042\n",
      "Epoch: 39468 \tTraining Loss: 1.141084 \tValidation Loss: 2.803789\n",
      "Epoch: 39469 \tTraining Loss: 1.164996 \tValidation Loss: 2.801445\n",
      "Epoch: 39470 \tTraining Loss: 1.135369 \tValidation Loss: 2.802586\n",
      "Epoch: 39471 \tTraining Loss: 1.200964 \tValidation Loss: 2.802779\n",
      "Epoch: 39472 \tTraining Loss: 1.157417 \tValidation Loss: 2.803109\n",
      "Epoch: 39473 \tTraining Loss: 1.185669 \tValidation Loss: 2.804211\n",
      "Epoch: 39474 \tTraining Loss: 1.165998 \tValidation Loss: 2.803777\n",
      "Epoch: 39475 \tTraining Loss: 1.173901 \tValidation Loss: 2.801684\n",
      "Epoch: 39476 \tTraining Loss: 1.160708 \tValidation Loss: 2.802327\n",
      "Epoch: 39477 \tTraining Loss: 1.148478 \tValidation Loss: 2.804479\n",
      "Epoch: 39478 \tTraining Loss: 1.192220 \tValidation Loss: 2.803607\n",
      "Epoch: 39479 \tTraining Loss: 1.188626 \tValidation Loss: 2.801950\n",
      "Epoch: 39480 \tTraining Loss: 1.130438 \tValidation Loss: 2.803129\n",
      "Epoch: 39481 \tTraining Loss: 1.140257 \tValidation Loss: 2.804040\n",
      "Epoch: 39482 \tTraining Loss: 1.130887 \tValidation Loss: 2.802978\n",
      "Epoch: 39483 \tTraining Loss: 1.123082 \tValidation Loss: 2.802959\n",
      "Epoch: 39484 \tTraining Loss: 1.139936 \tValidation Loss: 2.802769\n",
      "Epoch: 39485 \tTraining Loss: 1.150237 \tValidation Loss: 2.802788\n",
      "Epoch: 39486 \tTraining Loss: 1.116338 \tValidation Loss: 2.805921\n",
      "Epoch: 39487 \tTraining Loss: 1.122200 \tValidation Loss: 2.805258\n",
      "Epoch: 39488 \tTraining Loss: 1.151764 \tValidation Loss: 2.805920\n",
      "Epoch: 39489 \tTraining Loss: 1.156964 \tValidation Loss: 2.803847\n",
      "Epoch: 39490 \tTraining Loss: 1.111444 \tValidation Loss: 2.803261\n",
      "Epoch: 39491 \tTraining Loss: 1.150829 \tValidation Loss: 2.805717\n",
      "Epoch: 39492 \tTraining Loss: 1.168057 \tValidation Loss: 2.803126\n",
      "Epoch: 39493 \tTraining Loss: 1.182145 \tValidation Loss: 2.804291\n",
      "Epoch: 39494 \tTraining Loss: 1.207330 \tValidation Loss: 2.803106\n",
      "Epoch: 39495 \tTraining Loss: 1.134474 \tValidation Loss: 2.804908\n",
      "Epoch: 39496 \tTraining Loss: 1.152868 \tValidation Loss: 2.803238\n",
      "Epoch: 39497 \tTraining Loss: 1.121111 \tValidation Loss: 2.804011\n",
      "Epoch: 39498 \tTraining Loss: 1.168280 \tValidation Loss: 2.802601\n",
      "Epoch: 39499 \tTraining Loss: 1.169943 \tValidation Loss: 2.804749\n",
      "Epoch: 39500 \tTraining Loss: 1.199950 \tValidation Loss: 2.804415\n",
      "Epoch: 39501 \tTraining Loss: 1.124057 \tValidation Loss: 2.804012\n",
      "Epoch: 39502 \tTraining Loss: 1.142705 \tValidation Loss: 2.803917\n",
      "Epoch: 39503 \tTraining Loss: 1.164043 \tValidation Loss: 2.805383\n",
      "Epoch: 39504 \tTraining Loss: 1.161236 \tValidation Loss: 2.803379\n",
      "Epoch: 39505 \tTraining Loss: 1.161721 \tValidation Loss: 2.804651\n",
      "Epoch: 39506 \tTraining Loss: 1.186998 \tValidation Loss: 2.804188\n",
      "Epoch: 39507 \tTraining Loss: 1.168669 \tValidation Loss: 2.804351\n",
      "Epoch: 39508 \tTraining Loss: 1.175341 \tValidation Loss: 2.803622\n",
      "Epoch: 39509 \tTraining Loss: 1.162231 \tValidation Loss: 2.804357\n",
      "Epoch: 39510 \tTraining Loss: 1.142660 \tValidation Loss: 2.805840\n",
      "Epoch: 39511 \tTraining Loss: 1.163880 \tValidation Loss: 2.804929\n",
      "Epoch: 39512 \tTraining Loss: 1.135756 \tValidation Loss: 2.804102\n",
      "Epoch: 39513 \tTraining Loss: 1.139730 \tValidation Loss: 2.803887\n",
      "Epoch: 39514 \tTraining Loss: 1.183432 \tValidation Loss: 2.804049\n",
      "Epoch: 39515 \tTraining Loss: 1.152841 \tValidation Loss: 2.804578\n",
      "Epoch: 39516 \tTraining Loss: 1.132901 \tValidation Loss: 2.805679\n",
      "Epoch: 39517 \tTraining Loss: 1.131896 \tValidation Loss: 2.804006\n",
      "Epoch: 39518 \tTraining Loss: 1.170043 \tValidation Loss: 2.803722\n",
      "Epoch: 39519 \tTraining Loss: 1.136201 \tValidation Loss: 2.804522\n",
      "Epoch: 39520 \tTraining Loss: 1.166061 \tValidation Loss: 2.804975\n",
      "Epoch: 39521 \tTraining Loss: 1.127703 \tValidation Loss: 2.804902\n",
      "Epoch: 39522 \tTraining Loss: 1.169193 \tValidation Loss: 2.802977\n",
      "Epoch: 39523 \tTraining Loss: 1.136331 \tValidation Loss: 2.805421\n",
      "Epoch: 39524 \tTraining Loss: 1.119290 \tValidation Loss: 2.804908\n",
      "Epoch: 39525 \tTraining Loss: 1.157216 \tValidation Loss: 2.805004\n",
      "Epoch: 39526 \tTraining Loss: 1.170597 \tValidation Loss: 2.804354\n",
      "Epoch: 39527 \tTraining Loss: 1.163686 \tValidation Loss: 2.805158\n",
      "Epoch: 39528 \tTraining Loss: 1.205231 \tValidation Loss: 2.803824\n",
      "Epoch: 39529 \tTraining Loss: 1.142026 \tValidation Loss: 2.805694\n",
      "Epoch: 39530 \tTraining Loss: 1.203693 \tValidation Loss: 2.802653\n",
      "Epoch: 39531 \tTraining Loss: 1.149008 \tValidation Loss: 2.806048\n",
      "Epoch: 39532 \tTraining Loss: 1.182349 \tValidation Loss: 2.806785\n",
      "Epoch: 39533 \tTraining Loss: 1.128896 \tValidation Loss: 2.804042\n",
      "Epoch: 39534 \tTraining Loss: 1.154724 \tValidation Loss: 2.805513\n",
      "Epoch: 39535 \tTraining Loss: 1.136782 \tValidation Loss: 2.806691\n",
      "Epoch: 39536 \tTraining Loss: 1.154281 \tValidation Loss: 2.805452\n",
      "Epoch: 39537 \tTraining Loss: 1.159429 \tValidation Loss: 2.806345\n",
      "Epoch: 39538 \tTraining Loss: 1.146279 \tValidation Loss: 2.806254\n",
      "Epoch: 39539 \tTraining Loss: 1.138561 \tValidation Loss: 2.807446\n",
      "Epoch: 39540 \tTraining Loss: 1.159868 \tValidation Loss: 2.805056\n",
      "Epoch: 39541 \tTraining Loss: 1.144274 \tValidation Loss: 2.806151\n",
      "Epoch: 39542 \tTraining Loss: 1.173436 \tValidation Loss: 2.803732\n",
      "Epoch: 39543 \tTraining Loss: 1.129008 \tValidation Loss: 2.806344\n",
      "Epoch: 39544 \tTraining Loss: 1.171118 \tValidation Loss: 2.805371\n",
      "Epoch: 39545 \tTraining Loss: 1.120573 \tValidation Loss: 2.807310\n",
      "Epoch: 39546 \tTraining Loss: 1.135554 \tValidation Loss: 2.807310\n",
      "Epoch: 39547 \tTraining Loss: 1.192004 \tValidation Loss: 2.803848\n",
      "Epoch: 39548 \tTraining Loss: 1.129370 \tValidation Loss: 2.806864\n",
      "Epoch: 39549 \tTraining Loss: 1.130007 \tValidation Loss: 2.805850\n",
      "Epoch: 39550 \tTraining Loss: 1.180255 \tValidation Loss: 2.804869\n",
      "Epoch: 39551 \tTraining Loss: 1.171316 \tValidation Loss: 2.807186\n",
      "Epoch: 39552 \tTraining Loss: 1.154548 \tValidation Loss: 2.804966\n",
      "Epoch: 39553 \tTraining Loss: 1.202548 \tValidation Loss: 2.806894\n",
      "Epoch: 39554 \tTraining Loss: 1.159133 \tValidation Loss: 2.806973\n",
      "Epoch: 39555 \tTraining Loss: 1.171260 \tValidation Loss: 2.804527\n",
      "Epoch: 39556 \tTraining Loss: 1.167295 \tValidation Loss: 2.805413\n",
      "Epoch: 39557 \tTraining Loss: 1.158397 \tValidation Loss: 2.806628\n",
      "Epoch: 39558 \tTraining Loss: 1.147482 \tValidation Loss: 2.806802\n",
      "Epoch: 39559 \tTraining Loss: 1.222374 \tValidation Loss: 2.805121\n",
      "Epoch: 39560 \tTraining Loss: 1.210193 \tValidation Loss: 2.803698\n",
      "Epoch: 39561 \tTraining Loss: 1.135820 \tValidation Loss: 2.805585\n",
      "Epoch: 39562 \tTraining Loss: 1.152232 \tValidation Loss: 2.806740\n",
      "Epoch: 39563 \tTraining Loss: 1.120381 \tValidation Loss: 2.805866\n",
      "Epoch: 39564 \tTraining Loss: 1.202345 \tValidation Loss: 2.803524\n",
      "Epoch: 39565 \tTraining Loss: 1.115019 \tValidation Loss: 2.807271\n",
      "Epoch: 39566 \tTraining Loss: 1.167718 \tValidation Loss: 2.804223\n",
      "Epoch: 39567 \tTraining Loss: 1.195698 \tValidation Loss: 2.805495\n",
      "Epoch: 39568 \tTraining Loss: 1.125591 \tValidation Loss: 2.806760\n",
      "Epoch: 39569 \tTraining Loss: 1.165711 \tValidation Loss: 2.805441\n",
      "Epoch: 39570 \tTraining Loss: 1.165742 \tValidation Loss: 2.805884\n",
      "Epoch: 39571 \tTraining Loss: 1.154998 \tValidation Loss: 2.805084\n",
      "Epoch: 39572 \tTraining Loss: 1.164611 \tValidation Loss: 2.804786\n",
      "Epoch: 39573 \tTraining Loss: 1.087173 \tValidation Loss: 2.807956\n",
      "Epoch: 39574 \tTraining Loss: 1.161613 \tValidation Loss: 2.805444\n",
      "Epoch: 39575 \tTraining Loss: 1.162223 \tValidation Loss: 2.805352\n",
      "Epoch: 39576 \tTraining Loss: 1.196692 \tValidation Loss: 2.806757\n",
      "Epoch: 39577 \tTraining Loss: 1.165340 \tValidation Loss: 2.805910\n",
      "Epoch: 39578 \tTraining Loss: 1.153289 \tValidation Loss: 2.806893\n",
      "Epoch: 39579 \tTraining Loss: 1.180874 \tValidation Loss: 2.804955\n",
      "Epoch: 39580 \tTraining Loss: 1.160494 \tValidation Loss: 2.804847\n",
      "Epoch: 39581 \tTraining Loss: 1.174232 \tValidation Loss: 2.805041\n",
      "Epoch: 39582 \tTraining Loss: 1.166280 \tValidation Loss: 2.806290\n",
      "Epoch: 39583 \tTraining Loss: 1.145206 \tValidation Loss: 2.806589\n",
      "Epoch: 39584 \tTraining Loss: 1.178363 \tValidation Loss: 2.806695\n",
      "Epoch: 39585 \tTraining Loss: 1.196912 \tValidation Loss: 2.805676\n",
      "Epoch: 39586 \tTraining Loss: 1.153048 \tValidation Loss: 2.806736\n",
      "Epoch: 39587 \tTraining Loss: 1.141080 \tValidation Loss: 2.808654\n",
      "Epoch: 39588 \tTraining Loss: 1.137890 \tValidation Loss: 2.807224\n",
      "Epoch: 39589 \tTraining Loss: 1.119768 \tValidation Loss: 2.805832\n",
      "Epoch: 39590 \tTraining Loss: 1.149180 \tValidation Loss: 2.807559\n",
      "Epoch: 39591 \tTraining Loss: 1.160295 \tValidation Loss: 2.805354\n",
      "Epoch: 39592 \tTraining Loss: 1.178818 \tValidation Loss: 2.804320\n",
      "Epoch: 39593 \tTraining Loss: 1.135471 \tValidation Loss: 2.805990\n",
      "Epoch: 39594 \tTraining Loss: 1.174774 \tValidation Loss: 2.806741\n",
      "Epoch: 39595 \tTraining Loss: 1.164818 \tValidation Loss: 2.806214\n",
      "Epoch: 39596 \tTraining Loss: 1.240137 \tValidation Loss: 2.806137\n",
      "Epoch: 39597 \tTraining Loss: 1.163527 \tValidation Loss: 2.806698\n",
      "Epoch: 39598 \tTraining Loss: 1.202805 \tValidation Loss: 2.805757\n",
      "Epoch: 39599 \tTraining Loss: 1.139730 \tValidation Loss: 2.808638\n",
      "Epoch: 39600 \tTraining Loss: 1.169943 \tValidation Loss: 2.806982\n",
      "Epoch: 39601 \tTraining Loss: 1.150489 \tValidation Loss: 2.808308\n",
      "Epoch: 39602 \tTraining Loss: 1.152995 \tValidation Loss: 2.808254\n",
      "Epoch: 39603 \tTraining Loss: 1.170830 \tValidation Loss: 2.806876\n",
      "Epoch: 39604 \tTraining Loss: 1.182377 \tValidation Loss: 2.806758\n",
      "Epoch: 39605 \tTraining Loss: 1.184027 \tValidation Loss: 2.806369\n",
      "Epoch: 39606 \tTraining Loss: 1.170652 \tValidation Loss: 2.807736\n",
      "Epoch: 39607 \tTraining Loss: 1.133678 \tValidation Loss: 2.806888\n",
      "Epoch: 39608 \tTraining Loss: 1.147212 \tValidation Loss: 2.806696\n",
      "Epoch: 39609 \tTraining Loss: 1.143263 \tValidation Loss: 2.807459\n",
      "Epoch: 39610 \tTraining Loss: 1.151647 \tValidation Loss: 2.805778\n",
      "Epoch: 39611 \tTraining Loss: 1.170555 \tValidation Loss: 2.806784\n",
      "Epoch: 39612 \tTraining Loss: 1.105084 \tValidation Loss: 2.809172\n",
      "Epoch: 39613 \tTraining Loss: 1.123103 \tValidation Loss: 2.808635\n",
      "Epoch: 39614 \tTraining Loss: 1.132686 \tValidation Loss: 2.806548\n",
      "Epoch: 39615 \tTraining Loss: 1.195648 \tValidation Loss: 2.805182\n",
      "Epoch: 39616 \tTraining Loss: 1.093547 \tValidation Loss: 2.807174\n",
      "Epoch: 39617 \tTraining Loss: 1.178764 \tValidation Loss: 2.805777\n",
      "Epoch: 39618 \tTraining Loss: 1.127781 \tValidation Loss: 2.807398\n",
      "Epoch: 39619 \tTraining Loss: 1.180435 \tValidation Loss: 2.805394\n",
      "Epoch: 39620 \tTraining Loss: 1.162491 \tValidation Loss: 2.805810\n",
      "Epoch: 39621 \tTraining Loss: 1.211189 \tValidation Loss: 2.807544\n",
      "Epoch: 39622 \tTraining Loss: 1.113668 \tValidation Loss: 2.808987\n",
      "Epoch: 39623 \tTraining Loss: 1.152539 \tValidation Loss: 2.808010\n",
      "Epoch: 39624 \tTraining Loss: 1.139455 \tValidation Loss: 2.805865\n",
      "Epoch: 39625 \tTraining Loss: 1.153504 \tValidation Loss: 2.807400\n",
      "Epoch: 39626 \tTraining Loss: 1.141281 \tValidation Loss: 2.809210\n",
      "Epoch: 39627 \tTraining Loss: 1.183995 \tValidation Loss: 2.806703\n",
      "Epoch: 39628 \tTraining Loss: 1.168719 \tValidation Loss: 2.806381\n",
      "Epoch: 39629 \tTraining Loss: 1.138072 \tValidation Loss: 2.808147\n",
      "Epoch: 39630 \tTraining Loss: 1.148972 \tValidation Loss: 2.806446\n",
      "Epoch: 39631 \tTraining Loss: 1.129439 \tValidation Loss: 2.807936\n",
      "Epoch: 39632 \tTraining Loss: 1.149470 \tValidation Loss: 2.807714\n",
      "Epoch: 39633 \tTraining Loss: 1.179868 \tValidation Loss: 2.807901\n",
      "Epoch: 39634 \tTraining Loss: 1.145406 \tValidation Loss: 2.807611\n",
      "Epoch: 39635 \tTraining Loss: 1.176074 \tValidation Loss: 2.807479\n",
      "Epoch: 39636 \tTraining Loss: 1.199339 \tValidation Loss: 2.806543\n",
      "Epoch: 39637 \tTraining Loss: 1.143362 \tValidation Loss: 2.807103\n",
      "Epoch: 39638 \tTraining Loss: 1.164083 \tValidation Loss: 2.807492\n",
      "Epoch: 39639 \tTraining Loss: 1.140890 \tValidation Loss: 2.806380\n",
      "Epoch: 39640 \tTraining Loss: 1.189282 \tValidation Loss: 2.806594\n",
      "Epoch: 39641 \tTraining Loss: 1.136541 \tValidation Loss: 2.808184\n",
      "Epoch: 39642 \tTraining Loss: 1.131793 \tValidation Loss: 2.808374\n",
      "Epoch: 39643 \tTraining Loss: 1.173099 \tValidation Loss: 2.806303\n",
      "Epoch: 39644 \tTraining Loss: 1.123847 \tValidation Loss: 2.807938\n",
      "Epoch: 39645 \tTraining Loss: 1.130392 \tValidation Loss: 2.808672\n",
      "Epoch: 39646 \tTraining Loss: 1.150708 \tValidation Loss: 2.808548\n",
      "Epoch: 39647 \tTraining Loss: 1.162997 \tValidation Loss: 2.807389\n",
      "Epoch: 39648 \tTraining Loss: 1.128421 \tValidation Loss: 2.808388\n",
      "Epoch: 39649 \tTraining Loss: 1.158797 \tValidation Loss: 2.806854\n",
      "Epoch: 39650 \tTraining Loss: 1.173207 \tValidation Loss: 2.808389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39651 \tTraining Loss: 1.162947 \tValidation Loss: 2.806823\n",
      "Epoch: 39652 \tTraining Loss: 1.192581 \tValidation Loss: 2.806506\n",
      "Epoch: 39653 \tTraining Loss: 1.160281 \tValidation Loss: 2.808060\n",
      "Epoch: 39654 \tTraining Loss: 1.179107 \tValidation Loss: 2.806716\n",
      "Epoch: 39655 \tTraining Loss: 1.151894 \tValidation Loss: 2.807058\n",
      "Epoch: 39656 \tTraining Loss: 1.101861 \tValidation Loss: 2.809405\n",
      "Epoch: 39657 \tTraining Loss: 1.154687 \tValidation Loss: 2.809505\n",
      "Epoch: 39658 \tTraining Loss: 1.170719 \tValidation Loss: 2.808869\n",
      "Epoch: 39659 \tTraining Loss: 1.120526 \tValidation Loss: 2.806264\n",
      "Epoch: 39660 \tTraining Loss: 1.156115 \tValidation Loss: 2.809236\n",
      "Epoch: 39661 \tTraining Loss: 1.158223 \tValidation Loss: 2.810048\n",
      "Epoch: 39662 \tTraining Loss: 1.168598 \tValidation Loss: 2.806538\n",
      "Epoch: 39663 \tTraining Loss: 1.161119 \tValidation Loss: 2.807208\n",
      "Epoch: 39664 \tTraining Loss: 1.148271 \tValidation Loss: 2.808225\n",
      "Epoch: 39665 \tTraining Loss: 1.181065 \tValidation Loss: 2.809152\n",
      "Epoch: 39666 \tTraining Loss: 1.136967 \tValidation Loss: 2.809865\n",
      "Epoch: 39667 \tTraining Loss: 1.173525 \tValidation Loss: 2.808492\n",
      "Epoch: 39668 \tTraining Loss: 1.171306 \tValidation Loss: 2.808471\n",
      "Epoch: 39669 \tTraining Loss: 1.176018 \tValidation Loss: 2.806929\n",
      "Epoch: 39670 \tTraining Loss: 1.189177 \tValidation Loss: 2.809514\n",
      "Epoch: 39671 \tTraining Loss: 1.146162 \tValidation Loss: 2.808531\n",
      "Epoch: 39672 \tTraining Loss: 1.189867 \tValidation Loss: 2.808555\n",
      "Epoch: 39673 \tTraining Loss: 1.167449 \tValidation Loss: 2.807937\n",
      "Epoch: 39674 \tTraining Loss: 1.149706 \tValidation Loss: 2.809664\n",
      "Epoch: 39675 \tTraining Loss: 1.122217 \tValidation Loss: 2.809131\n",
      "Epoch: 39676 \tTraining Loss: 1.161514 \tValidation Loss: 2.808119\n",
      "Epoch: 39677 \tTraining Loss: 1.100739 \tValidation Loss: 2.811450\n",
      "Epoch: 39678 \tTraining Loss: 1.105585 \tValidation Loss: 2.808437\n",
      "Epoch: 39679 \tTraining Loss: 1.179461 \tValidation Loss: 2.806670\n",
      "Epoch: 39680 \tTraining Loss: 1.162638 \tValidation Loss: 2.809785\n",
      "Epoch: 39681 \tTraining Loss: 1.141765 \tValidation Loss: 2.810013\n",
      "Epoch: 39682 \tTraining Loss: 1.081864 \tValidation Loss: 2.807828\n",
      "Epoch: 39683 \tTraining Loss: 1.134153 \tValidation Loss: 2.809815\n",
      "Epoch: 39684 \tTraining Loss: 1.203835 \tValidation Loss: 2.807132\n",
      "Epoch: 39685 \tTraining Loss: 1.177045 \tValidation Loss: 2.809780\n",
      "Epoch: 39686 \tTraining Loss: 1.119841 \tValidation Loss: 2.808679\n",
      "Epoch: 39687 \tTraining Loss: 1.200408 \tValidation Loss: 2.809556\n",
      "Epoch: 39688 \tTraining Loss: 1.157617 \tValidation Loss: 2.809032\n",
      "Epoch: 39689 \tTraining Loss: 1.108913 \tValidation Loss: 2.808546\n",
      "Epoch: 39690 \tTraining Loss: 1.184464 \tValidation Loss: 2.809417\n",
      "Epoch: 39691 \tTraining Loss: 1.152710 \tValidation Loss: 2.807847\n",
      "Epoch: 39692 \tTraining Loss: 1.193821 \tValidation Loss: 2.808904\n",
      "Epoch: 39693 \tTraining Loss: 1.180001 \tValidation Loss: 2.808529\n",
      "Epoch: 39694 \tTraining Loss: 1.132190 \tValidation Loss: 2.809431\n",
      "Epoch: 39695 \tTraining Loss: 1.239392 \tValidation Loss: 2.806999\n",
      "Epoch: 39696 \tTraining Loss: 1.168561 \tValidation Loss: 2.808519\n",
      "Epoch: 39697 \tTraining Loss: 1.178418 \tValidation Loss: 2.808748\n",
      "Epoch: 39698 \tTraining Loss: 1.141365 \tValidation Loss: 2.807966\n",
      "Epoch: 39699 \tTraining Loss: 1.157980 \tValidation Loss: 2.808246\n",
      "Epoch: 39700 \tTraining Loss: 1.149275 \tValidation Loss: 2.808828\n",
      "Epoch: 39701 \tTraining Loss: 1.122684 \tValidation Loss: 2.808929\n",
      "Epoch: 39702 \tTraining Loss: 1.157675 \tValidation Loss: 2.810968\n",
      "Epoch: 39703 \tTraining Loss: 1.169511 \tValidation Loss: 2.808383\n",
      "Epoch: 39704 \tTraining Loss: 1.134508 \tValidation Loss: 2.808328\n",
      "Epoch: 39705 \tTraining Loss: 1.178696 \tValidation Loss: 2.809474\n",
      "Epoch: 39706 \tTraining Loss: 1.121478 \tValidation Loss: 2.808964\n",
      "Epoch: 39707 \tTraining Loss: 1.152938 \tValidation Loss: 2.808580\n",
      "Epoch: 39708 \tTraining Loss: 1.141133 \tValidation Loss: 2.809113\n",
      "Epoch: 39709 \tTraining Loss: 1.147559 \tValidation Loss: 2.809588\n",
      "Epoch: 39710 \tTraining Loss: 1.167425 \tValidation Loss: 2.808353\n",
      "Epoch: 39711 \tTraining Loss: 1.178016 \tValidation Loss: 2.807616\n",
      "Epoch: 39712 \tTraining Loss: 1.129725 \tValidation Loss: 2.810753\n",
      "Epoch: 39713 \tTraining Loss: 1.171719 \tValidation Loss: 2.808994\n",
      "Epoch: 39714 \tTraining Loss: 1.186218 \tValidation Loss: 2.806714\n",
      "Epoch: 39715 \tTraining Loss: 1.132295 \tValidation Loss: 2.808860\n",
      "Epoch: 39716 \tTraining Loss: 1.161923 \tValidation Loss: 2.810538\n",
      "Epoch: 39717 \tTraining Loss: 1.171361 \tValidation Loss: 2.809776\n",
      "Epoch: 39718 \tTraining Loss: 1.135545 \tValidation Loss: 2.811528\n",
      "Epoch: 39719 \tTraining Loss: 1.148849 \tValidation Loss: 2.808457\n",
      "Epoch: 39720 \tTraining Loss: 1.135438 \tValidation Loss: 2.807686\n",
      "Epoch: 39721 \tTraining Loss: 1.154048 \tValidation Loss: 2.810297\n",
      "Epoch: 39722 \tTraining Loss: 1.171815 \tValidation Loss: 2.807884\n",
      "Epoch: 39723 \tTraining Loss: 1.176572 \tValidation Loss: 2.808765\n",
      "Epoch: 39724 \tTraining Loss: 1.162469 \tValidation Loss: 2.809338\n",
      "Epoch: 39725 \tTraining Loss: 1.125951 \tValidation Loss: 2.809534\n",
      "Epoch: 39726 \tTraining Loss: 1.168488 \tValidation Loss: 2.810488\n",
      "Epoch: 39727 \tTraining Loss: 1.152499 \tValidation Loss: 2.810359\n",
      "Epoch: 39728 \tTraining Loss: 1.165919 \tValidation Loss: 2.808134\n",
      "Epoch: 39729 \tTraining Loss: 1.147584 \tValidation Loss: 2.809774\n",
      "Epoch: 39730 \tTraining Loss: 1.133735 \tValidation Loss: 2.810404\n",
      "Epoch: 39731 \tTraining Loss: 1.142947 \tValidation Loss: 2.808978\n",
      "Epoch: 39732 \tTraining Loss: 1.154396 \tValidation Loss: 2.808496\n",
      "Epoch: 39733 \tTraining Loss: 1.128822 \tValidation Loss: 2.810023\n",
      "Epoch: 39734 \tTraining Loss: 1.100712 \tValidation Loss: 2.809812\n",
      "Epoch: 39735 \tTraining Loss: 1.120226 \tValidation Loss: 2.810858\n",
      "Epoch: 39736 \tTraining Loss: 1.175248 \tValidation Loss: 2.809607\n",
      "Epoch: 39737 \tTraining Loss: 1.146176 \tValidation Loss: 2.807965\n",
      "Epoch: 39738 \tTraining Loss: 1.178908 \tValidation Loss: 2.809120\n",
      "Epoch: 39739 \tTraining Loss: 1.168982 \tValidation Loss: 2.810172\n",
      "Epoch: 39740 \tTraining Loss: 1.145902 \tValidation Loss: 2.808363\n",
      "Epoch: 39741 \tTraining Loss: 1.135471 \tValidation Loss: 2.808270\n",
      "Epoch: 39742 \tTraining Loss: 1.165119 \tValidation Loss: 2.809537\n",
      "Epoch: 39743 \tTraining Loss: 1.123720 \tValidation Loss: 2.811278\n",
      "Epoch: 39744 \tTraining Loss: 1.150293 \tValidation Loss: 2.808727\n",
      "Epoch: 39745 \tTraining Loss: 1.187863 \tValidation Loss: 2.809839\n",
      "Epoch: 39746 \tTraining Loss: 1.138513 \tValidation Loss: 2.811147\n",
      "Epoch: 39747 \tTraining Loss: 1.128497 \tValidation Loss: 2.810722\n",
      "Epoch: 39748 \tTraining Loss: 1.102977 \tValidation Loss: 2.810133\n",
      "Epoch: 39749 \tTraining Loss: 1.146054 \tValidation Loss: 2.810444\n",
      "Epoch: 39750 \tTraining Loss: 1.126528 \tValidation Loss: 2.812336\n",
      "Epoch: 39751 \tTraining Loss: 1.155610 \tValidation Loss: 2.810088\n",
      "Epoch: 39752 \tTraining Loss: 1.146729 \tValidation Loss: 2.811682\n",
      "Epoch: 39753 \tTraining Loss: 1.170063 \tValidation Loss: 2.809869\n",
      "Epoch: 39754 \tTraining Loss: 1.137111 \tValidation Loss: 2.810569\n",
      "Epoch: 39755 \tTraining Loss: 1.137698 \tValidation Loss: 2.810207\n",
      "Epoch: 39756 \tTraining Loss: 1.179005 \tValidation Loss: 2.810501\n",
      "Epoch: 39757 \tTraining Loss: 1.151354 \tValidation Loss: 2.810892\n",
      "Epoch: 39758 \tTraining Loss: 1.177684 \tValidation Loss: 2.809632\n",
      "Epoch: 39759 \tTraining Loss: 1.109359 \tValidation Loss: 2.810724\n",
      "Epoch: 39760 \tTraining Loss: 1.155671 \tValidation Loss: 2.811826\n",
      "Epoch: 39761 \tTraining Loss: 1.153010 \tValidation Loss: 2.810373\n",
      "Epoch: 39762 \tTraining Loss: 1.143712 \tValidation Loss: 2.810671\n",
      "Epoch: 39763 \tTraining Loss: 1.179012 \tValidation Loss: 2.810586\n",
      "Epoch: 39764 \tTraining Loss: 1.197105 \tValidation Loss: 2.809964\n",
      "Epoch: 39765 \tTraining Loss: 1.186797 \tValidation Loss: 2.810570\n",
      "Epoch: 39766 \tTraining Loss: 1.140458 \tValidation Loss: 2.810863\n",
      "Epoch: 39767 \tTraining Loss: 1.153448 \tValidation Loss: 2.812013\n",
      "Epoch: 39768 \tTraining Loss: 1.178068 \tValidation Loss: 2.809899\n",
      "Epoch: 39769 \tTraining Loss: 1.116302 \tValidation Loss: 2.811846\n",
      "Epoch: 39770 \tTraining Loss: 1.119247 \tValidation Loss: 2.813444\n",
      "Epoch: 39771 \tTraining Loss: 1.157886 \tValidation Loss: 2.808921\n",
      "Epoch: 39772 \tTraining Loss: 1.158367 \tValidation Loss: 2.811007\n",
      "Epoch: 39773 \tTraining Loss: 1.127090 \tValidation Loss: 2.810703\n",
      "Epoch: 39774 \tTraining Loss: 1.119015 \tValidation Loss: 2.812200\n",
      "Epoch: 39775 \tTraining Loss: 1.155336 \tValidation Loss: 2.809540\n",
      "Epoch: 39776 \tTraining Loss: 1.152332 \tValidation Loss: 2.812034\n",
      "Epoch: 39777 \tTraining Loss: 1.160452 \tValidation Loss: 2.810950\n",
      "Epoch: 39778 \tTraining Loss: 1.151538 \tValidation Loss: 2.811783\n",
      "Epoch: 39779 \tTraining Loss: 1.176158 \tValidation Loss: 2.809028\n",
      "Epoch: 39780 \tTraining Loss: 1.145979 \tValidation Loss: 2.811893\n",
      "Epoch: 39781 \tTraining Loss: 1.137701 \tValidation Loss: 2.811302\n",
      "Epoch: 39782 \tTraining Loss: 1.191871 \tValidation Loss: 2.811101\n",
      "Epoch: 39783 \tTraining Loss: 1.166711 \tValidation Loss: 2.811736\n",
      "Epoch: 39784 \tTraining Loss: 1.120852 \tValidation Loss: 2.812094\n",
      "Epoch: 39785 \tTraining Loss: 1.163197 \tValidation Loss: 2.810658\n",
      "Epoch: 39786 \tTraining Loss: 1.163838 \tValidation Loss: 2.810230\n",
      "Epoch: 39787 \tTraining Loss: 1.095270 \tValidation Loss: 2.812624\n",
      "Epoch: 39788 \tTraining Loss: 1.166382 \tValidation Loss: 2.810911\n",
      "Epoch: 39789 \tTraining Loss: 1.142954 \tValidation Loss: 2.810581\n",
      "Epoch: 39790 \tTraining Loss: 1.179861 \tValidation Loss: 2.810264\n",
      "Epoch: 39791 \tTraining Loss: 1.152792 \tValidation Loss: 2.813070\n",
      "Epoch: 39792 \tTraining Loss: 1.136696 \tValidation Loss: 2.811374\n",
      "Epoch: 39793 \tTraining Loss: 1.197833 \tValidation Loss: 2.809796\n",
      "Epoch: 39794 \tTraining Loss: 1.145414 \tValidation Loss: 2.810881\n",
      "Epoch: 39795 \tTraining Loss: 1.196654 \tValidation Loss: 2.810735\n",
      "Epoch: 39796 \tTraining Loss: 1.171296 \tValidation Loss: 2.810389\n",
      "Epoch: 39797 \tTraining Loss: 1.153164 \tValidation Loss: 2.812722\n",
      "Epoch: 39798 \tTraining Loss: 1.153127 \tValidation Loss: 2.809651\n",
      "Epoch: 39799 \tTraining Loss: 1.147206 \tValidation Loss: 2.810647\n",
      "Epoch: 39800 \tTraining Loss: 1.151354 \tValidation Loss: 2.812277\n",
      "Epoch: 39801 \tTraining Loss: 1.109377 \tValidation Loss: 2.810707\n",
      "Epoch: 39802 \tTraining Loss: 1.124857 \tValidation Loss: 2.810926\n",
      "Epoch: 39803 \tTraining Loss: 1.165442 \tValidation Loss: 2.810543\n",
      "Epoch: 39804 \tTraining Loss: 1.164133 \tValidation Loss: 2.812275\n",
      "Epoch: 39805 \tTraining Loss: 1.132829 \tValidation Loss: 2.814317\n",
      "Epoch: 39806 \tTraining Loss: 1.135979 \tValidation Loss: 2.811124\n",
      "Epoch: 39807 \tTraining Loss: 1.202252 \tValidation Loss: 2.810335\n",
      "Epoch: 39808 \tTraining Loss: 1.184241 \tValidation Loss: 2.812155\n",
      "Epoch: 39809 \tTraining Loss: 1.169263 \tValidation Loss: 2.812397\n",
      "Epoch: 39810 \tTraining Loss: 1.182825 \tValidation Loss: 2.809597\n",
      "Epoch: 39811 \tTraining Loss: 1.147990 \tValidation Loss: 2.811657\n",
      "Epoch: 39812 \tTraining Loss: 1.161682 \tValidation Loss: 2.812106\n",
      "Epoch: 39813 \tTraining Loss: 1.148365 \tValidation Loss: 2.812550\n",
      "Epoch: 39814 \tTraining Loss: 1.161392 \tValidation Loss: 2.812005\n",
      "Epoch: 39815 \tTraining Loss: 1.162537 \tValidation Loss: 2.811913\n",
      "Epoch: 39816 \tTraining Loss: 1.171843 \tValidation Loss: 2.812262\n",
      "Epoch: 39817 \tTraining Loss: 1.137503 \tValidation Loss: 2.812403\n",
      "Epoch: 39818 \tTraining Loss: 1.163062 \tValidation Loss: 2.812001\n",
      "Epoch: 39819 \tTraining Loss: 1.138081 \tValidation Loss: 2.812701\n",
      "Epoch: 39820 \tTraining Loss: 1.156065 \tValidation Loss: 2.812675\n",
      "Epoch: 39821 \tTraining Loss: 1.196320 \tValidation Loss: 2.810614\n",
      "Epoch: 39822 \tTraining Loss: 1.155768 \tValidation Loss: 2.813884\n",
      "Epoch: 39823 \tTraining Loss: 1.137808 \tValidation Loss: 2.812578\n",
      "Epoch: 39824 \tTraining Loss: 1.161048 \tValidation Loss: 2.811541\n",
      "Epoch: 39825 \tTraining Loss: 1.149794 \tValidation Loss: 2.811203\n",
      "Epoch: 39826 \tTraining Loss: 1.139101 \tValidation Loss: 2.812857\n",
      "Epoch: 39827 \tTraining Loss: 1.159296 \tValidation Loss: 2.811355\n",
      "Epoch: 39828 \tTraining Loss: 1.108025 \tValidation Loss: 2.810803\n",
      "Epoch: 39829 \tTraining Loss: 1.129162 \tValidation Loss: 2.813952\n",
      "Epoch: 39830 \tTraining Loss: 1.167074 \tValidation Loss: 2.812194\n",
      "Epoch: 39831 \tTraining Loss: 1.210299 \tValidation Loss: 2.809958\n",
      "Epoch: 39832 \tTraining Loss: 1.168222 \tValidation Loss: 2.810800\n",
      "Epoch: 39833 \tTraining Loss: 1.120108 \tValidation Loss: 2.811780\n",
      "Epoch: 39834 \tTraining Loss: 1.135108 \tValidation Loss: 2.813496\n",
      "Epoch: 39835 \tTraining Loss: 1.169164 \tValidation Loss: 2.811400\n",
      "Epoch: 39836 \tTraining Loss: 1.132181 \tValidation Loss: 2.813502\n",
      "Epoch: 39837 \tTraining Loss: 1.127381 \tValidation Loss: 2.813663\n",
      "Epoch: 39838 \tTraining Loss: 1.125326 \tValidation Loss: 2.813095\n",
      "Epoch: 39839 \tTraining Loss: 1.123712 \tValidation Loss: 2.812722\n",
      "Epoch: 39840 \tTraining Loss: 1.133589 \tValidation Loss: 2.813120\n",
      "Epoch: 39841 \tTraining Loss: 1.196764 \tValidation Loss: 2.812307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39842 \tTraining Loss: 1.135584 \tValidation Loss: 2.812774\n",
      "Epoch: 39843 \tTraining Loss: 1.124559 \tValidation Loss: 2.814501\n",
      "Epoch: 39844 \tTraining Loss: 1.156306 \tValidation Loss: 2.813506\n",
      "Epoch: 39845 \tTraining Loss: 1.133101 \tValidation Loss: 2.813700\n",
      "Epoch: 39846 \tTraining Loss: 1.098853 \tValidation Loss: 2.813977\n",
      "Epoch: 39847 \tTraining Loss: 1.164362 \tValidation Loss: 2.812577\n",
      "Epoch: 39848 \tTraining Loss: 1.147426 \tValidation Loss: 2.813466\n",
      "Epoch: 39849 \tTraining Loss: 1.145138 \tValidation Loss: 2.813085\n",
      "Epoch: 39850 \tTraining Loss: 1.140939 \tValidation Loss: 2.813038\n",
      "Epoch: 39851 \tTraining Loss: 1.143523 \tValidation Loss: 2.813733\n",
      "Epoch: 39852 \tTraining Loss: 1.139521 \tValidation Loss: 2.814725\n",
      "Epoch: 39853 \tTraining Loss: 1.128088 \tValidation Loss: 2.814789\n",
      "Epoch: 39854 \tTraining Loss: 1.131992 \tValidation Loss: 2.813288\n",
      "Epoch: 39855 \tTraining Loss: 1.170687 \tValidation Loss: 2.814518\n",
      "Epoch: 39856 \tTraining Loss: 1.109764 \tValidation Loss: 2.813152\n",
      "Epoch: 39857 \tTraining Loss: 1.167430 \tValidation Loss: 2.812511\n",
      "Epoch: 39858 \tTraining Loss: 1.163051 \tValidation Loss: 2.814242\n",
      "Epoch: 39859 \tTraining Loss: 1.140048 \tValidation Loss: 2.813236\n",
      "Epoch: 39860 \tTraining Loss: 1.114348 \tValidation Loss: 2.814059\n",
      "Epoch: 39861 \tTraining Loss: 1.152552 \tValidation Loss: 2.814263\n",
      "Epoch: 39862 \tTraining Loss: 1.156512 \tValidation Loss: 2.812692\n",
      "Epoch: 39863 \tTraining Loss: 1.102445 \tValidation Loss: 2.813428\n",
      "Epoch: 39864 \tTraining Loss: 1.166723 \tValidation Loss: 2.814223\n",
      "Epoch: 39865 \tTraining Loss: 1.149397 \tValidation Loss: 2.813764\n",
      "Epoch: 39866 \tTraining Loss: 1.139963 \tValidation Loss: 2.814116\n",
      "Epoch: 39867 \tTraining Loss: 1.196408 \tValidation Loss: 2.813361\n",
      "Epoch: 39868 \tTraining Loss: 1.181085 \tValidation Loss: 2.812449\n",
      "Epoch: 39869 \tTraining Loss: 1.165293 \tValidation Loss: 2.813511\n",
      "Epoch: 39870 \tTraining Loss: 1.159299 \tValidation Loss: 2.813236\n",
      "Epoch: 39871 \tTraining Loss: 1.143366 \tValidation Loss: 2.815646\n",
      "Epoch: 39872 \tTraining Loss: 1.196792 \tValidation Loss: 2.813610\n",
      "Epoch: 39873 \tTraining Loss: 1.180277 \tValidation Loss: 2.814088\n",
      "Epoch: 39874 \tTraining Loss: 1.172289 \tValidation Loss: 2.813447\n",
      "Epoch: 39875 \tTraining Loss: 1.153302 \tValidation Loss: 2.814789\n",
      "Epoch: 39876 \tTraining Loss: 1.155738 \tValidation Loss: 2.815536\n",
      "Epoch: 39877 \tTraining Loss: 1.179145 \tValidation Loss: 2.814422\n",
      "Epoch: 39878 \tTraining Loss: 1.163002 \tValidation Loss: 2.813321\n",
      "Epoch: 39879 \tTraining Loss: 1.164497 \tValidation Loss: 2.814922\n",
      "Epoch: 39880 \tTraining Loss: 1.176760 \tValidation Loss: 2.813012\n",
      "Epoch: 39881 \tTraining Loss: 1.142270 \tValidation Loss: 2.814792\n",
      "Epoch: 39882 \tTraining Loss: 1.144848 \tValidation Loss: 2.814284\n",
      "Epoch: 39883 \tTraining Loss: 1.171945 \tValidation Loss: 2.813465\n",
      "Epoch: 39884 \tTraining Loss: 1.140602 \tValidation Loss: 2.812934\n",
      "Epoch: 39885 \tTraining Loss: 1.164699 \tValidation Loss: 2.812948\n",
      "Epoch: 39886 \tTraining Loss: 1.108843 \tValidation Loss: 2.813408\n",
      "Epoch: 39887 \tTraining Loss: 1.192798 \tValidation Loss: 2.812792\n",
      "Epoch: 39888 \tTraining Loss: 1.162389 \tValidation Loss: 2.814979\n",
      "Epoch: 39889 \tTraining Loss: 1.130913 \tValidation Loss: 2.813958\n",
      "Epoch: 39890 \tTraining Loss: 1.160733 \tValidation Loss: 2.814137\n",
      "Epoch: 39891 \tTraining Loss: 1.102561 \tValidation Loss: 2.813792\n",
      "Epoch: 39892 \tTraining Loss: 1.115792 \tValidation Loss: 2.813606\n",
      "Epoch: 39893 \tTraining Loss: 1.169360 \tValidation Loss: 2.813228\n",
      "Epoch: 39894 \tTraining Loss: 1.128832 \tValidation Loss: 2.814075\n",
      "Epoch: 39895 \tTraining Loss: 1.157177 \tValidation Loss: 2.814695\n",
      "Epoch: 39896 \tTraining Loss: 1.195406 \tValidation Loss: 2.813679\n",
      "Epoch: 39897 \tTraining Loss: 1.099079 \tValidation Loss: 2.813989\n",
      "Epoch: 39898 \tTraining Loss: 1.148640 \tValidation Loss: 2.813342\n",
      "Epoch: 39899 \tTraining Loss: 1.083194 \tValidation Loss: 2.814396\n",
      "Epoch: 39900 \tTraining Loss: 1.184547 \tValidation Loss: 2.814672\n",
      "Epoch: 39901 \tTraining Loss: 1.153385 \tValidation Loss: 2.813060\n",
      "Epoch: 39902 \tTraining Loss: 1.176015 \tValidation Loss: 2.814829\n",
      "Epoch: 39903 \tTraining Loss: 1.140280 \tValidation Loss: 2.813851\n",
      "Epoch: 39904 \tTraining Loss: 1.177496 \tValidation Loss: 2.813644\n",
      "Epoch: 39905 \tTraining Loss: 1.164419 \tValidation Loss: 2.814878\n",
      "Epoch: 39906 \tTraining Loss: 1.145374 \tValidation Loss: 2.814649\n",
      "Epoch: 39907 \tTraining Loss: 1.116350 \tValidation Loss: 2.814469\n",
      "Epoch: 39908 \tTraining Loss: 1.151311 \tValidation Loss: 2.815656\n",
      "Epoch: 39909 \tTraining Loss: 1.154010 \tValidation Loss: 2.814879\n",
      "Epoch: 39910 \tTraining Loss: 1.142460 \tValidation Loss: 2.815211\n",
      "Epoch: 39911 \tTraining Loss: 1.127496 \tValidation Loss: 2.813092\n",
      "Epoch: 39912 \tTraining Loss: 1.144241 \tValidation Loss: 2.814378\n",
      "Epoch: 39913 \tTraining Loss: 1.149935 \tValidation Loss: 2.815535\n",
      "Epoch: 39914 \tTraining Loss: 1.146830 \tValidation Loss: 2.814385\n",
      "Epoch: 39915 \tTraining Loss: 1.163731 \tValidation Loss: 2.815709\n",
      "Epoch: 39916 \tTraining Loss: 1.122184 \tValidation Loss: 2.815518\n",
      "Epoch: 39917 \tTraining Loss: 1.176599 \tValidation Loss: 2.816333\n",
      "Epoch: 39918 \tTraining Loss: 1.208766 \tValidation Loss: 2.813619\n",
      "Epoch: 39919 \tTraining Loss: 1.140640 \tValidation Loss: 2.815336\n",
      "Epoch: 39920 \tTraining Loss: 1.146919 \tValidation Loss: 2.813470\n",
      "Epoch: 39921 \tTraining Loss: 1.128704 \tValidation Loss: 2.814417\n",
      "Epoch: 39922 \tTraining Loss: 1.134393 \tValidation Loss: 2.814912\n",
      "Epoch: 39923 \tTraining Loss: 1.109641 \tValidation Loss: 2.814498\n",
      "Epoch: 39924 \tTraining Loss: 1.100349 \tValidation Loss: 2.816263\n",
      "Epoch: 39925 \tTraining Loss: 1.195140 \tValidation Loss: 2.814888\n",
      "Epoch: 39926 \tTraining Loss: 1.178093 \tValidation Loss: 2.814754\n",
      "Epoch: 39927 \tTraining Loss: 1.133182 \tValidation Loss: 2.814931\n",
      "Epoch: 39928 \tTraining Loss: 1.182384 \tValidation Loss: 2.814607\n",
      "Epoch: 39929 \tTraining Loss: 1.135123 \tValidation Loss: 2.815251\n",
      "Epoch: 39930 \tTraining Loss: 1.161002 \tValidation Loss: 2.815692\n",
      "Epoch: 39931 \tTraining Loss: 1.143701 \tValidation Loss: 2.814356\n",
      "Epoch: 39932 \tTraining Loss: 1.143327 \tValidation Loss: 2.815093\n",
      "Epoch: 39933 \tTraining Loss: 1.209221 \tValidation Loss: 2.813244\n",
      "Epoch: 39934 \tTraining Loss: 1.188905 \tValidation Loss: 2.812053\n",
      "Epoch: 39935 \tTraining Loss: 1.095188 \tValidation Loss: 2.816347\n",
      "Epoch: 39936 \tTraining Loss: 1.106079 \tValidation Loss: 2.815065\n",
      "Epoch: 39937 \tTraining Loss: 1.164213 \tValidation Loss: 2.815781\n",
      "Epoch: 39938 \tTraining Loss: 1.173662 \tValidation Loss: 2.815270\n",
      "Epoch: 39939 \tTraining Loss: 1.137412 \tValidation Loss: 2.815895\n",
      "Epoch: 39940 \tTraining Loss: 1.155298 \tValidation Loss: 2.815928\n",
      "Epoch: 39941 \tTraining Loss: 1.155771 \tValidation Loss: 2.814844\n",
      "Epoch: 39942 \tTraining Loss: 1.140687 \tValidation Loss: 2.814932\n",
      "Epoch: 39943 \tTraining Loss: 1.198362 \tValidation Loss: 2.815889\n",
      "Epoch: 39944 \tTraining Loss: 1.110910 \tValidation Loss: 2.815735\n",
      "Epoch: 39945 \tTraining Loss: 1.140193 \tValidation Loss: 2.815332\n",
      "Epoch: 39946 \tTraining Loss: 1.139092 \tValidation Loss: 2.815569\n",
      "Epoch: 39947 \tTraining Loss: 1.149945 \tValidation Loss: 2.816106\n",
      "Epoch: 39948 \tTraining Loss: 1.129511 \tValidation Loss: 2.816067\n",
      "Epoch: 39949 \tTraining Loss: 1.170488 \tValidation Loss: 2.815441\n",
      "Epoch: 39950 \tTraining Loss: 1.132390 \tValidation Loss: 2.818714\n",
      "Epoch: 39951 \tTraining Loss: 1.166545 \tValidation Loss: 2.816998\n",
      "Epoch: 39952 \tTraining Loss: 1.120581 \tValidation Loss: 2.816102\n",
      "Epoch: 39953 \tTraining Loss: 1.139369 \tValidation Loss: 2.817520\n",
      "Epoch: 39954 \tTraining Loss: 1.184132 \tValidation Loss: 2.815568\n",
      "Epoch: 39955 \tTraining Loss: 1.214477 \tValidation Loss: 2.814148\n",
      "Epoch: 39956 \tTraining Loss: 1.171520 \tValidation Loss: 2.816597\n",
      "Epoch: 39957 \tTraining Loss: 1.128838 \tValidation Loss: 2.815134\n",
      "Epoch: 39958 \tTraining Loss: 1.155718 \tValidation Loss: 2.815773\n",
      "Epoch: 39959 \tTraining Loss: 1.186425 \tValidation Loss: 2.816493\n",
      "Epoch: 39960 \tTraining Loss: 1.135387 \tValidation Loss: 2.814388\n",
      "Epoch: 39961 \tTraining Loss: 1.155831 \tValidation Loss: 2.815945\n",
      "Epoch: 39962 \tTraining Loss: 1.153066 \tValidation Loss: 2.814000\n",
      "Epoch: 39963 \tTraining Loss: 1.148191 \tValidation Loss: 2.815574\n",
      "Epoch: 39964 \tTraining Loss: 1.168455 \tValidation Loss: 2.816459\n",
      "Epoch: 39965 \tTraining Loss: 1.160243 \tValidation Loss: 2.814382\n",
      "Epoch: 39966 \tTraining Loss: 1.181888 \tValidation Loss: 2.815994\n",
      "Epoch: 39967 \tTraining Loss: 1.143003 \tValidation Loss: 2.815928\n",
      "Epoch: 39968 \tTraining Loss: 1.101755 \tValidation Loss: 2.816867\n",
      "Epoch: 39969 \tTraining Loss: 1.135416 \tValidation Loss: 2.815098\n",
      "Epoch: 39970 \tTraining Loss: 1.171650 \tValidation Loss: 2.815088\n",
      "Epoch: 39971 \tTraining Loss: 1.161224 \tValidation Loss: 2.814026\n",
      "Epoch: 39972 \tTraining Loss: 1.172928 \tValidation Loss: 2.815448\n",
      "Epoch: 39973 \tTraining Loss: 1.142692 \tValidation Loss: 2.814862\n",
      "Epoch: 39974 \tTraining Loss: 1.179929 \tValidation Loss: 2.816025\n",
      "Epoch: 39975 \tTraining Loss: 1.120504 \tValidation Loss: 2.816002\n",
      "Epoch: 39976 \tTraining Loss: 1.136803 \tValidation Loss: 2.816070\n",
      "Epoch: 39977 \tTraining Loss: 1.134503 \tValidation Loss: 2.819082\n",
      "Epoch: 39978 \tTraining Loss: 1.172351 \tValidation Loss: 2.815228\n",
      "Epoch: 39979 \tTraining Loss: 1.132183 \tValidation Loss: 2.816553\n",
      "Epoch: 39980 \tTraining Loss: 1.178492 \tValidation Loss: 2.814501\n",
      "Epoch: 39981 \tTraining Loss: 1.139280 \tValidation Loss: 2.817765\n",
      "Epoch: 39982 \tTraining Loss: 1.192202 \tValidation Loss: 2.816031\n",
      "Epoch: 39983 \tTraining Loss: 1.129832 \tValidation Loss: 2.816039\n",
      "Epoch: 39984 \tTraining Loss: 1.140619 \tValidation Loss: 2.816556\n",
      "Epoch: 39985 \tTraining Loss: 1.178362 \tValidation Loss: 2.816444\n",
      "Epoch: 39986 \tTraining Loss: 1.128964 \tValidation Loss: 2.817014\n",
      "Epoch: 39987 \tTraining Loss: 1.121278 \tValidation Loss: 2.817967\n",
      "Epoch: 39988 \tTraining Loss: 1.200232 \tValidation Loss: 2.815969\n",
      "Epoch: 39989 \tTraining Loss: 1.184439 \tValidation Loss: 2.816392\n",
      "Epoch: 39990 \tTraining Loss: 1.148793 \tValidation Loss: 2.813984\n",
      "Epoch: 39991 \tTraining Loss: 1.169848 \tValidation Loss: 2.816265\n",
      "Epoch: 39992 \tTraining Loss: 1.160133 \tValidation Loss: 2.817526\n",
      "Epoch: 39993 \tTraining Loss: 1.152243 \tValidation Loss: 2.816793\n",
      "Epoch: 39994 \tTraining Loss: 1.143445 \tValidation Loss: 2.817226\n",
      "Epoch: 39995 \tTraining Loss: 1.133583 \tValidation Loss: 2.818064\n",
      "Epoch: 39996 \tTraining Loss: 1.148355 \tValidation Loss: 2.817817\n",
      "Epoch: 39997 \tTraining Loss: 1.169566 \tValidation Loss: 2.817139\n",
      "Epoch: 39998 \tTraining Loss: 1.080430 \tValidation Loss: 2.818811\n",
      "Epoch: 39999 \tTraining Loss: 1.171218 \tValidation Loss: 2.816647\n",
      "Epoch: 40000 \tTraining Loss: 1.187252 \tValidation Loss: 2.818151\n",
      "Epoch: 40001 \tTraining Loss: 1.163368 \tValidation Loss: 2.817070\n",
      "Epoch: 40002 \tTraining Loss: 1.146691 \tValidation Loss: 2.816802\n",
      "Epoch: 40003 \tTraining Loss: 1.128652 \tValidation Loss: 2.817901\n",
      "Epoch: 40004 \tTraining Loss: 1.148273 \tValidation Loss: 2.818221\n",
      "Epoch: 40005 \tTraining Loss: 1.114828 \tValidation Loss: 2.818017\n",
      "Epoch: 40006 \tTraining Loss: 1.172764 \tValidation Loss: 2.817655\n",
      "Epoch: 40007 \tTraining Loss: 1.161253 \tValidation Loss: 2.817379\n",
      "Epoch: 40008 \tTraining Loss: 1.173395 \tValidation Loss: 2.817698\n",
      "Epoch: 40009 \tTraining Loss: 1.149719 \tValidation Loss: 2.815751\n",
      "Epoch: 40010 \tTraining Loss: 1.140526 \tValidation Loss: 2.817816\n",
      "Epoch: 40011 \tTraining Loss: 1.182000 \tValidation Loss: 2.816160\n",
      "Epoch: 40012 \tTraining Loss: 1.114267 \tValidation Loss: 2.816756\n",
      "Epoch: 40013 \tTraining Loss: 1.086468 \tValidation Loss: 2.818050\n",
      "Epoch: 40014 \tTraining Loss: 1.143789 \tValidation Loss: 2.817386\n",
      "Epoch: 40015 \tTraining Loss: 1.154829 \tValidation Loss: 2.816339\n",
      "Epoch: 40016 \tTraining Loss: 1.128671 \tValidation Loss: 2.818123\n",
      "Epoch: 40017 \tTraining Loss: 1.157949 \tValidation Loss: 2.818238\n",
      "Epoch: 40018 \tTraining Loss: 1.169576 \tValidation Loss: 2.816516\n",
      "Epoch: 40019 \tTraining Loss: 1.128398 \tValidation Loss: 2.817387\n",
      "Epoch: 40020 \tTraining Loss: 1.201521 \tValidation Loss: 2.816071\n",
      "Epoch: 40021 \tTraining Loss: 1.142396 \tValidation Loss: 2.817548\n",
      "Epoch: 40022 \tTraining Loss: 1.145424 \tValidation Loss: 2.817040\n",
      "Epoch: 40023 \tTraining Loss: 1.138053 \tValidation Loss: 2.817992\n",
      "Epoch: 40024 \tTraining Loss: 1.139824 \tValidation Loss: 2.814852\n",
      "Epoch: 40025 \tTraining Loss: 1.150172 \tValidation Loss: 2.817320\n",
      "Epoch: 40026 \tTraining Loss: 1.190611 \tValidation Loss: 2.814224\n",
      "Epoch: 40027 \tTraining Loss: 1.156095 \tValidation Loss: 2.814794\n",
      "Epoch: 40028 \tTraining Loss: 1.136863 \tValidation Loss: 2.817451\n",
      "Epoch: 40029 \tTraining Loss: 1.163733 \tValidation Loss: 2.818352\n",
      "Epoch: 40030 \tTraining Loss: 1.131585 \tValidation Loss: 2.817559\n",
      "Epoch: 40031 \tTraining Loss: 1.122182 \tValidation Loss: 2.818321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40032 \tTraining Loss: 1.142405 \tValidation Loss: 2.817362\n",
      "Epoch: 40033 \tTraining Loss: 1.141334 \tValidation Loss: 2.817688\n",
      "Epoch: 40034 \tTraining Loss: 1.132406 \tValidation Loss: 2.816673\n",
      "Epoch: 40035 \tTraining Loss: 1.151474 \tValidation Loss: 2.818722\n",
      "Epoch: 40036 \tTraining Loss: 1.164125 \tValidation Loss: 2.816067\n",
      "Epoch: 40037 \tTraining Loss: 1.113567 \tValidation Loss: 2.820830\n",
      "Epoch: 40038 \tTraining Loss: 1.134385 \tValidation Loss: 2.817035\n",
      "Epoch: 40039 \tTraining Loss: 1.188778 \tValidation Loss: 2.817503\n",
      "Epoch: 40040 \tTraining Loss: 1.161412 \tValidation Loss: 2.818865\n",
      "Epoch: 40041 \tTraining Loss: 1.172056 \tValidation Loss: 2.817255\n",
      "Epoch: 40042 \tTraining Loss: 1.169657 \tValidation Loss: 2.817635\n",
      "Epoch: 40043 \tTraining Loss: 1.147738 \tValidation Loss: 2.818602\n",
      "Epoch: 40044 \tTraining Loss: 1.160962 \tValidation Loss: 2.817529\n",
      "Epoch: 40045 \tTraining Loss: 1.125515 \tValidation Loss: 2.818431\n",
      "Epoch: 40046 \tTraining Loss: 1.109554 \tValidation Loss: 2.818876\n",
      "Epoch: 40047 \tTraining Loss: 1.178137 \tValidation Loss: 2.817810\n",
      "Epoch: 40048 \tTraining Loss: 1.153534 \tValidation Loss: 2.818091\n",
      "Epoch: 40049 \tTraining Loss: 1.147267 \tValidation Loss: 2.815782\n",
      "Epoch: 40050 \tTraining Loss: 1.167617 \tValidation Loss: 2.817309\n",
      "Epoch: 40051 \tTraining Loss: 1.165400 \tValidation Loss: 2.818224\n",
      "Epoch: 40052 \tTraining Loss: 1.183986 \tValidation Loss: 2.818142\n",
      "Epoch: 40053 \tTraining Loss: 1.217325 \tValidation Loss: 2.817832\n",
      "Epoch: 40054 \tTraining Loss: 1.148522 \tValidation Loss: 2.817054\n",
      "Epoch: 40055 \tTraining Loss: 1.135616 \tValidation Loss: 2.818352\n",
      "Epoch: 40056 \tTraining Loss: 1.191660 \tValidation Loss: 2.816148\n",
      "Epoch: 40057 \tTraining Loss: 1.121680 \tValidation Loss: 2.816488\n",
      "Epoch: 40058 \tTraining Loss: 1.197677 \tValidation Loss: 2.817676\n",
      "Epoch: 40059 \tTraining Loss: 1.170430 \tValidation Loss: 2.817096\n",
      "Epoch: 40060 \tTraining Loss: 1.179627 \tValidation Loss: 2.817575\n",
      "Epoch: 40061 \tTraining Loss: 1.127474 \tValidation Loss: 2.817852\n",
      "Epoch: 40062 \tTraining Loss: 1.155326 \tValidation Loss: 2.818035\n",
      "Epoch: 40063 \tTraining Loss: 1.154073 \tValidation Loss: 2.817559\n",
      "Epoch: 40064 \tTraining Loss: 1.194204 \tValidation Loss: 2.818313\n",
      "Epoch: 40065 \tTraining Loss: 1.144570 \tValidation Loss: 2.817652\n",
      "Epoch: 40066 \tTraining Loss: 1.111760 \tValidation Loss: 2.817247\n",
      "Epoch: 40067 \tTraining Loss: 1.118508 \tValidation Loss: 2.818155\n",
      "Epoch: 40068 \tTraining Loss: 1.137256 \tValidation Loss: 2.817268\n",
      "Epoch: 40069 \tTraining Loss: 1.199490 \tValidation Loss: 2.817058\n",
      "Epoch: 40070 \tTraining Loss: 1.139803 \tValidation Loss: 2.818484\n",
      "Epoch: 40071 \tTraining Loss: 1.156448 \tValidation Loss: 2.818641\n",
      "Epoch: 40072 \tTraining Loss: 1.136597 \tValidation Loss: 2.819279\n",
      "Epoch: 40073 \tTraining Loss: 1.159960 \tValidation Loss: 2.818357\n",
      "Epoch: 40074 \tTraining Loss: 1.159973 \tValidation Loss: 2.818212\n",
      "Epoch: 40075 \tTraining Loss: 1.141758 \tValidation Loss: 2.816990\n",
      "Epoch: 40076 \tTraining Loss: 1.147849 \tValidation Loss: 2.818863\n",
      "Epoch: 40077 \tTraining Loss: 1.118974 \tValidation Loss: 2.819211\n",
      "Epoch: 40078 \tTraining Loss: 1.143838 \tValidation Loss: 2.817296\n",
      "Epoch: 40079 \tTraining Loss: 1.165501 \tValidation Loss: 2.817829\n",
      "Epoch: 40080 \tTraining Loss: 1.155453 \tValidation Loss: 2.817627\n",
      "Epoch: 40081 \tTraining Loss: 1.159752 \tValidation Loss: 2.818682\n",
      "Epoch: 40082 \tTraining Loss: 1.161819 \tValidation Loss: 2.818921\n",
      "Epoch: 40083 \tTraining Loss: 1.162236 \tValidation Loss: 2.816695\n",
      "Epoch: 40084 \tTraining Loss: 1.141767 \tValidation Loss: 2.816952\n",
      "Epoch: 40085 \tTraining Loss: 1.173154 \tValidation Loss: 2.817813\n",
      "Epoch: 40086 \tTraining Loss: 1.132569 \tValidation Loss: 2.818622\n",
      "Epoch: 40087 \tTraining Loss: 1.151649 \tValidation Loss: 2.818349\n",
      "Epoch: 40088 \tTraining Loss: 1.124320 \tValidation Loss: 2.819428\n",
      "Epoch: 40089 \tTraining Loss: 1.167399 \tValidation Loss: 2.819577\n",
      "Epoch: 40090 \tTraining Loss: 1.129196 \tValidation Loss: 2.819621\n",
      "Epoch: 40091 \tTraining Loss: 1.140275 \tValidation Loss: 2.819829\n",
      "Epoch: 40092 \tTraining Loss: 1.150052 \tValidation Loss: 2.819751\n",
      "Epoch: 40093 \tTraining Loss: 1.130373 \tValidation Loss: 2.818233\n",
      "Epoch: 40094 \tTraining Loss: 1.085733 \tValidation Loss: 2.820151\n",
      "Epoch: 40095 \tTraining Loss: 1.174123 \tValidation Loss: 2.818975\n",
      "Epoch: 40096 \tTraining Loss: 1.110004 \tValidation Loss: 2.819077\n",
      "Epoch: 40097 \tTraining Loss: 1.169502 \tValidation Loss: 2.820148\n",
      "Epoch: 40098 \tTraining Loss: 1.179026 \tValidation Loss: 2.818688\n",
      "Epoch: 40099 \tTraining Loss: 1.173293 \tValidation Loss: 2.818574\n",
      "Epoch: 40100 \tTraining Loss: 1.131225 \tValidation Loss: 2.818710\n",
      "Epoch: 40101 \tTraining Loss: 1.191195 \tValidation Loss: 2.819602\n",
      "Epoch: 40102 \tTraining Loss: 1.135313 \tValidation Loss: 2.818980\n",
      "Epoch: 40103 \tTraining Loss: 1.130237 \tValidation Loss: 2.820059\n",
      "Epoch: 40104 \tTraining Loss: 1.145434 \tValidation Loss: 2.820229\n",
      "Epoch: 40105 \tTraining Loss: 1.227638 \tValidation Loss: 2.818105\n",
      "Epoch: 40106 \tTraining Loss: 1.145366 \tValidation Loss: 2.819578\n",
      "Epoch: 40107 \tTraining Loss: 1.140000 \tValidation Loss: 2.819260\n",
      "Epoch: 40108 \tTraining Loss: 1.178906 \tValidation Loss: 2.818142\n",
      "Epoch: 40109 \tTraining Loss: 1.172025 \tValidation Loss: 2.818616\n",
      "Epoch: 40110 \tTraining Loss: 1.154925 \tValidation Loss: 2.820720\n",
      "Epoch: 40111 \tTraining Loss: 1.156858 \tValidation Loss: 2.816218\n",
      "Epoch: 40112 \tTraining Loss: 1.130268 \tValidation Loss: 2.819804\n",
      "Epoch: 40113 \tTraining Loss: 1.180888 \tValidation Loss: 2.818136\n",
      "Epoch: 40114 \tTraining Loss: 1.146069 \tValidation Loss: 2.819866\n",
      "Epoch: 40115 \tTraining Loss: 1.138155 \tValidation Loss: 2.819284\n",
      "Epoch: 40116 \tTraining Loss: 1.130886 \tValidation Loss: 2.819989\n",
      "Epoch: 40117 \tTraining Loss: 1.173572 \tValidation Loss: 2.819126\n",
      "Epoch: 40118 \tTraining Loss: 1.102099 \tValidation Loss: 2.820596\n",
      "Epoch: 40119 \tTraining Loss: 1.133720 \tValidation Loss: 2.820624\n",
      "Epoch: 40120 \tTraining Loss: 1.173041 \tValidation Loss: 2.817868\n",
      "Epoch: 40121 \tTraining Loss: 1.184596 \tValidation Loss: 2.819260\n",
      "Epoch: 40122 \tTraining Loss: 1.123873 \tValidation Loss: 2.818953\n",
      "Epoch: 40123 \tTraining Loss: 1.144519 \tValidation Loss: 2.819480\n",
      "Epoch: 40124 \tTraining Loss: 1.127874 \tValidation Loss: 2.820106\n",
      "Epoch: 40125 \tTraining Loss: 1.145920 \tValidation Loss: 2.820430\n",
      "Epoch: 40126 \tTraining Loss: 1.103220 \tValidation Loss: 2.820691\n",
      "Epoch: 40127 \tTraining Loss: 1.152582 \tValidation Loss: 2.819980\n",
      "Epoch: 40128 \tTraining Loss: 1.169272 \tValidation Loss: 2.818707\n",
      "Epoch: 40129 \tTraining Loss: 1.180163 \tValidation Loss: 2.818434\n",
      "Epoch: 40130 \tTraining Loss: 1.152486 \tValidation Loss: 2.817407\n",
      "Epoch: 40131 \tTraining Loss: 1.169490 \tValidation Loss: 2.819524\n",
      "Epoch: 40132 \tTraining Loss: 1.150690 \tValidation Loss: 2.819448\n",
      "Epoch: 40133 \tTraining Loss: 1.128145 \tValidation Loss: 2.819940\n",
      "Epoch: 40134 \tTraining Loss: 1.112470 \tValidation Loss: 2.821933\n",
      "Epoch: 40135 \tTraining Loss: 1.139762 \tValidation Loss: 2.820843\n",
      "Epoch: 40136 \tTraining Loss: 1.126297 \tValidation Loss: 2.819339\n",
      "Epoch: 40137 \tTraining Loss: 1.176278 \tValidation Loss: 2.818815\n",
      "Epoch: 40138 \tTraining Loss: 1.139109 \tValidation Loss: 2.820775\n",
      "Epoch: 40139 \tTraining Loss: 1.152446 \tValidation Loss: 2.819878\n",
      "Epoch: 40140 \tTraining Loss: 1.151659 \tValidation Loss: 2.819841\n",
      "Epoch: 40141 \tTraining Loss: 1.139728 \tValidation Loss: 2.820335\n",
      "Epoch: 40142 \tTraining Loss: 1.177373 \tValidation Loss: 2.819511\n",
      "Epoch: 40143 \tTraining Loss: 1.156899 \tValidation Loss: 2.817443\n",
      "Epoch: 40144 \tTraining Loss: 1.166820 \tValidation Loss: 2.819257\n",
      "Epoch: 40145 \tTraining Loss: 1.141827 \tValidation Loss: 2.821134\n",
      "Epoch: 40146 \tTraining Loss: 1.145066 \tValidation Loss: 2.819350\n",
      "Epoch: 40147 \tTraining Loss: 1.200106 \tValidation Loss: 2.817847\n",
      "Epoch: 40148 \tTraining Loss: 1.180474 \tValidation Loss: 2.818508\n",
      "Epoch: 40149 \tTraining Loss: 1.131880 \tValidation Loss: 2.819376\n",
      "Epoch: 40150 \tTraining Loss: 1.143341 \tValidation Loss: 2.818393\n",
      "Epoch: 40151 \tTraining Loss: 1.150777 \tValidation Loss: 2.819503\n",
      "Epoch: 40152 \tTraining Loss: 1.164842 \tValidation Loss: 2.819571\n",
      "Epoch: 40153 \tTraining Loss: 1.136673 \tValidation Loss: 2.821139\n",
      "Epoch: 40154 \tTraining Loss: 1.183301 \tValidation Loss: 2.818824\n",
      "Epoch: 40155 \tTraining Loss: 1.153909 \tValidation Loss: 2.819894\n",
      "Epoch: 40156 \tTraining Loss: 1.108864 \tValidation Loss: 2.820550\n",
      "Epoch: 40157 \tTraining Loss: 1.117256 \tValidation Loss: 2.820264\n",
      "Epoch: 40158 \tTraining Loss: 1.130119 \tValidation Loss: 2.818979\n",
      "Epoch: 40159 \tTraining Loss: 1.185338 \tValidation Loss: 2.817817\n",
      "Epoch: 40160 \tTraining Loss: 1.114658 \tValidation Loss: 2.820059\n",
      "Epoch: 40161 \tTraining Loss: 1.142709 \tValidation Loss: 2.820389\n",
      "Epoch: 40162 \tTraining Loss: 1.115433 \tValidation Loss: 2.820830\n",
      "Epoch: 40163 \tTraining Loss: 1.144109 \tValidation Loss: 2.820078\n",
      "Epoch: 40164 \tTraining Loss: 1.137715 \tValidation Loss: 2.820258\n",
      "Epoch: 40165 \tTraining Loss: 1.117525 \tValidation Loss: 2.821000\n",
      "Epoch: 40166 \tTraining Loss: 1.127083 \tValidation Loss: 2.819645\n",
      "Epoch: 40167 \tTraining Loss: 1.116870 \tValidation Loss: 2.819335\n",
      "Epoch: 40168 \tTraining Loss: 1.126506 \tValidation Loss: 2.822948\n",
      "Epoch: 40169 \tTraining Loss: 1.169774 \tValidation Loss: 2.819276\n",
      "Epoch: 40170 \tTraining Loss: 1.141450 \tValidation Loss: 2.821491\n",
      "Epoch: 40171 \tTraining Loss: 1.149088 \tValidation Loss: 2.819646\n",
      "Epoch: 40172 \tTraining Loss: 1.132796 \tValidation Loss: 2.820305\n",
      "Epoch: 40173 \tTraining Loss: 1.113169 \tValidation Loss: 2.822223\n",
      "Epoch: 40174 \tTraining Loss: 1.166846 \tValidation Loss: 2.820242\n",
      "Epoch: 40175 \tTraining Loss: 1.164063 \tValidation Loss: 2.820571\n",
      "Epoch: 40176 \tTraining Loss: 1.132066 \tValidation Loss: 2.821208\n",
      "Epoch: 40177 \tTraining Loss: 1.138662 \tValidation Loss: 2.818902\n",
      "Epoch: 40178 \tTraining Loss: 1.109803 \tValidation Loss: 2.819564\n",
      "Epoch: 40179 \tTraining Loss: 1.106485 \tValidation Loss: 2.820114\n",
      "Epoch: 40180 \tTraining Loss: 1.145014 \tValidation Loss: 2.820015\n",
      "Epoch: 40181 \tTraining Loss: 1.135300 \tValidation Loss: 2.821322\n",
      "Epoch: 40182 \tTraining Loss: 1.153702 \tValidation Loss: 2.820071\n",
      "Epoch: 40183 \tTraining Loss: 1.125303 \tValidation Loss: 2.818868\n",
      "Epoch: 40184 \tTraining Loss: 1.109516 \tValidation Loss: 2.820919\n",
      "Epoch: 40185 \tTraining Loss: 1.133563 \tValidation Loss: 2.820135\n",
      "Epoch: 40186 \tTraining Loss: 1.128350 \tValidation Loss: 2.819530\n",
      "Epoch: 40187 \tTraining Loss: 1.154089 \tValidation Loss: 2.821407\n",
      "Epoch: 40188 \tTraining Loss: 1.109235 \tValidation Loss: 2.821777\n",
      "Epoch: 40189 \tTraining Loss: 1.160440 \tValidation Loss: 2.821188\n",
      "Epoch: 40190 \tTraining Loss: 1.164734 \tValidation Loss: 2.819587\n",
      "Epoch: 40191 \tTraining Loss: 1.117565 \tValidation Loss: 2.821258\n",
      "Epoch: 40192 \tTraining Loss: 1.148492 \tValidation Loss: 2.820643\n",
      "Epoch: 40193 \tTraining Loss: 1.149074 \tValidation Loss: 2.821277\n",
      "Epoch: 40194 \tTraining Loss: 1.099069 \tValidation Loss: 2.820893\n",
      "Epoch: 40195 \tTraining Loss: 1.123321 \tValidation Loss: 2.822721\n",
      "Epoch: 40196 \tTraining Loss: 1.154990 \tValidation Loss: 2.819829\n",
      "Epoch: 40197 \tTraining Loss: 1.156052 \tValidation Loss: 2.819893\n",
      "Epoch: 40198 \tTraining Loss: 1.158524 \tValidation Loss: 2.821234\n",
      "Epoch: 40199 \tTraining Loss: 1.178593 \tValidation Loss: 2.820136\n",
      "Epoch: 40200 \tTraining Loss: 1.131612 \tValidation Loss: 2.821361\n",
      "Epoch: 40201 \tTraining Loss: 1.152861 \tValidation Loss: 2.819854\n",
      "Epoch: 40202 \tTraining Loss: 1.147258 \tValidation Loss: 2.821061\n",
      "Epoch: 40203 \tTraining Loss: 1.196710 \tValidation Loss: 2.818399\n",
      "Epoch: 40204 \tTraining Loss: 1.164319 \tValidation Loss: 2.821270\n",
      "Epoch: 40205 \tTraining Loss: 1.124003 \tValidation Loss: 2.822499\n",
      "Epoch: 40206 \tTraining Loss: 1.139839 \tValidation Loss: 2.821188\n",
      "Epoch: 40207 \tTraining Loss: 1.134636 \tValidation Loss: 2.823794\n",
      "Epoch: 40208 \tTraining Loss: 1.187367 \tValidation Loss: 2.818348\n",
      "Epoch: 40209 \tTraining Loss: 1.148389 \tValidation Loss: 2.819462\n",
      "Epoch: 40210 \tTraining Loss: 1.150806 \tValidation Loss: 2.821631\n",
      "Epoch: 40211 \tTraining Loss: 1.165392 \tValidation Loss: 2.821033\n",
      "Epoch: 40212 \tTraining Loss: 1.178272 \tValidation Loss: 2.822455\n",
      "Epoch: 40213 \tTraining Loss: 1.177114 \tValidation Loss: 2.820990\n",
      "Epoch: 40214 \tTraining Loss: 1.108511 \tValidation Loss: 2.821009\n",
      "Epoch: 40215 \tTraining Loss: 1.158467 \tValidation Loss: 2.820135\n",
      "Epoch: 40216 \tTraining Loss: 1.156690 \tValidation Loss: 2.820991\n",
      "Epoch: 40217 \tTraining Loss: 1.144764 \tValidation Loss: 2.822592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40218 \tTraining Loss: 1.117796 \tValidation Loss: 2.821943\n",
      "Epoch: 40219 \tTraining Loss: 1.165157 \tValidation Loss: 2.821301\n",
      "Epoch: 40220 \tTraining Loss: 1.149635 \tValidation Loss: 2.820315\n",
      "Epoch: 40221 \tTraining Loss: 1.127139 \tValidation Loss: 2.821417\n",
      "Epoch: 40222 \tTraining Loss: 1.149474 \tValidation Loss: 2.819695\n",
      "Epoch: 40223 \tTraining Loss: 1.193632 \tValidation Loss: 2.821290\n",
      "Epoch: 40224 \tTraining Loss: 1.138012 \tValidation Loss: 2.822809\n",
      "Epoch: 40225 \tTraining Loss: 1.153938 \tValidation Loss: 2.820640\n",
      "Epoch: 40226 \tTraining Loss: 1.157643 \tValidation Loss: 2.820779\n",
      "Epoch: 40227 \tTraining Loss: 1.144207 \tValidation Loss: 2.819630\n",
      "Epoch: 40228 \tTraining Loss: 1.131135 \tValidation Loss: 2.820869\n",
      "Epoch: 40229 \tTraining Loss: 1.156960 \tValidation Loss: 2.822004\n",
      "Epoch: 40230 \tTraining Loss: 1.173297 \tValidation Loss: 2.821544\n",
      "Epoch: 40231 \tTraining Loss: 1.134715 \tValidation Loss: 2.820006\n",
      "Epoch: 40232 \tTraining Loss: 1.191756 \tValidation Loss: 2.822119\n",
      "Epoch: 40233 \tTraining Loss: 1.114073 \tValidation Loss: 2.821674\n",
      "Epoch: 40234 \tTraining Loss: 1.155184 \tValidation Loss: 2.821409\n",
      "Epoch: 40235 \tTraining Loss: 1.213199 \tValidation Loss: 2.821585\n",
      "Epoch: 40236 \tTraining Loss: 1.139675 \tValidation Loss: 2.822297\n",
      "Epoch: 40237 \tTraining Loss: 1.161131 \tValidation Loss: 2.821909\n",
      "Epoch: 40238 \tTraining Loss: 1.175518 \tValidation Loss: 2.823335\n",
      "Epoch: 40239 \tTraining Loss: 1.146351 \tValidation Loss: 2.822484\n",
      "Epoch: 40240 \tTraining Loss: 1.152831 \tValidation Loss: 2.820759\n",
      "Epoch: 40241 \tTraining Loss: 1.153771 \tValidation Loss: 2.822959\n",
      "Epoch: 40242 \tTraining Loss: 1.165829 \tValidation Loss: 2.822604\n",
      "Epoch: 40243 \tTraining Loss: 1.161191 \tValidation Loss: 2.822869\n",
      "Epoch: 40244 \tTraining Loss: 1.189052 \tValidation Loss: 2.821602\n",
      "Epoch: 40245 \tTraining Loss: 1.142839 \tValidation Loss: 2.823045\n",
      "Epoch: 40246 \tTraining Loss: 1.163138 \tValidation Loss: 2.823606\n",
      "Epoch: 40247 \tTraining Loss: 1.136276 \tValidation Loss: 2.820799\n",
      "Epoch: 40248 \tTraining Loss: 1.108327 \tValidation Loss: 2.822918\n",
      "Epoch: 40249 \tTraining Loss: 1.159779 \tValidation Loss: 2.822488\n",
      "Epoch: 40250 \tTraining Loss: 1.154219 \tValidation Loss: 2.822749\n",
      "Epoch: 40251 \tTraining Loss: 1.166639 \tValidation Loss: 2.822536\n",
      "Epoch: 40252 \tTraining Loss: 1.165402 \tValidation Loss: 2.823136\n",
      "Epoch: 40253 \tTraining Loss: 1.130637 \tValidation Loss: 2.822382\n",
      "Epoch: 40254 \tTraining Loss: 1.130147 \tValidation Loss: 2.823042\n",
      "Epoch: 40255 \tTraining Loss: 1.176289 \tValidation Loss: 2.823405\n",
      "Epoch: 40256 \tTraining Loss: 1.156014 \tValidation Loss: 2.822253\n",
      "Epoch: 40257 \tTraining Loss: 1.146128 \tValidation Loss: 2.826310\n",
      "Epoch: 40258 \tTraining Loss: 1.137670 \tValidation Loss: 2.822177\n",
      "Epoch: 40259 \tTraining Loss: 1.187135 \tValidation Loss: 2.822585\n",
      "Epoch: 40260 \tTraining Loss: 1.129450 \tValidation Loss: 2.824109\n",
      "Epoch: 40261 \tTraining Loss: 1.083881 \tValidation Loss: 2.824174\n",
      "Epoch: 40262 \tTraining Loss: 1.174790 \tValidation Loss: 2.822476\n",
      "Epoch: 40263 \tTraining Loss: 1.115895 \tValidation Loss: 2.821623\n",
      "Epoch: 40264 \tTraining Loss: 1.116877 \tValidation Loss: 2.824952\n",
      "Epoch: 40265 \tTraining Loss: 1.124854 \tValidation Loss: 2.820646\n",
      "Epoch: 40266 \tTraining Loss: 1.217074 \tValidation Loss: 2.821114\n",
      "Epoch: 40267 \tTraining Loss: 1.167475 \tValidation Loss: 2.821531\n",
      "Epoch: 40268 \tTraining Loss: 1.164514 \tValidation Loss: 2.823181\n",
      "Epoch: 40269 \tTraining Loss: 1.100536 \tValidation Loss: 2.823812\n",
      "Epoch: 40270 \tTraining Loss: 1.145374 \tValidation Loss: 2.823420\n",
      "Epoch: 40271 \tTraining Loss: 1.159626 \tValidation Loss: 2.823956\n",
      "Epoch: 40272 \tTraining Loss: 1.159734 \tValidation Loss: 2.824349\n",
      "Epoch: 40273 \tTraining Loss: 1.145819 \tValidation Loss: 2.824303\n",
      "Epoch: 40274 \tTraining Loss: 1.176441 \tValidation Loss: 2.820740\n",
      "Epoch: 40275 \tTraining Loss: 1.097075 \tValidation Loss: 2.824930\n",
      "Epoch: 40276 \tTraining Loss: 1.207774 \tValidation Loss: 2.822714\n",
      "Epoch: 40277 \tTraining Loss: 1.113205 \tValidation Loss: 2.823487\n",
      "Epoch: 40278 \tTraining Loss: 1.185181 \tValidation Loss: 2.822200\n",
      "Epoch: 40279 \tTraining Loss: 1.137948 \tValidation Loss: 2.820902\n",
      "Epoch: 40280 \tTraining Loss: 1.178362 \tValidation Loss: 2.822503\n",
      "Epoch: 40281 \tTraining Loss: 1.139916 \tValidation Loss: 2.823255\n",
      "Epoch: 40282 \tTraining Loss: 1.190071 \tValidation Loss: 2.821046\n",
      "Epoch: 40283 \tTraining Loss: 1.144723 \tValidation Loss: 2.822765\n",
      "Epoch: 40284 \tTraining Loss: 1.098701 \tValidation Loss: 2.822747\n",
      "Epoch: 40285 \tTraining Loss: 1.194445 \tValidation Loss: 2.823057\n",
      "Epoch: 40286 \tTraining Loss: 1.107089 \tValidation Loss: 2.825027\n",
      "Epoch: 40287 \tTraining Loss: 1.102773 \tValidation Loss: 2.823858\n",
      "Epoch: 40288 \tTraining Loss: 1.133403 \tValidation Loss: 2.824706\n",
      "Epoch: 40289 \tTraining Loss: 1.121398 \tValidation Loss: 2.822137\n",
      "Epoch: 40290 \tTraining Loss: 1.148506 \tValidation Loss: 2.822465\n",
      "Epoch: 40291 \tTraining Loss: 1.100341 \tValidation Loss: 2.824281\n",
      "Epoch: 40292 \tTraining Loss: 1.131283 \tValidation Loss: 2.822330\n",
      "Epoch: 40293 \tTraining Loss: 1.167125 \tValidation Loss: 2.823180\n",
      "Epoch: 40294 \tTraining Loss: 1.140098 \tValidation Loss: 2.824385\n",
      "Epoch: 40295 \tTraining Loss: 1.143777 \tValidation Loss: 2.823269\n",
      "Epoch: 40296 \tTraining Loss: 1.138131 \tValidation Loss: 2.823786\n",
      "Epoch: 40297 \tTraining Loss: 1.107014 \tValidation Loss: 2.825613\n",
      "Epoch: 40298 \tTraining Loss: 1.092237 \tValidation Loss: 2.822378\n",
      "Epoch: 40299 \tTraining Loss: 1.106934 \tValidation Loss: 2.824908\n",
      "Epoch: 40300 \tTraining Loss: 1.133784 \tValidation Loss: 2.822584\n",
      "Epoch: 40301 \tTraining Loss: 1.161231 \tValidation Loss: 2.822412\n",
      "Epoch: 40302 \tTraining Loss: 1.126769 \tValidation Loss: 2.823890\n",
      "Epoch: 40303 \tTraining Loss: 1.178738 \tValidation Loss: 2.823962\n",
      "Epoch: 40304 \tTraining Loss: 1.091592 \tValidation Loss: 2.823550\n",
      "Epoch: 40305 \tTraining Loss: 1.124346 \tValidation Loss: 2.823270\n",
      "Epoch: 40306 \tTraining Loss: 1.146624 \tValidation Loss: 2.823472\n",
      "Epoch: 40307 \tTraining Loss: 1.159492 \tValidation Loss: 2.823807\n",
      "Epoch: 40308 \tTraining Loss: 1.108042 \tValidation Loss: 2.824574\n",
      "Epoch: 40309 \tTraining Loss: 1.165452 \tValidation Loss: 2.822841\n",
      "Epoch: 40310 \tTraining Loss: 1.109879 \tValidation Loss: 2.823861\n",
      "Epoch: 40311 \tTraining Loss: 1.135238 \tValidation Loss: 2.824726\n",
      "Epoch: 40312 \tTraining Loss: 1.131016 \tValidation Loss: 2.824638\n",
      "Epoch: 40313 \tTraining Loss: 1.151811 \tValidation Loss: 2.823711\n",
      "Epoch: 40314 \tTraining Loss: 1.130749 \tValidation Loss: 2.822486\n",
      "Epoch: 40315 \tTraining Loss: 1.123819 \tValidation Loss: 2.823247\n",
      "Epoch: 40316 \tTraining Loss: 1.126610 \tValidation Loss: 2.824146\n",
      "Epoch: 40317 \tTraining Loss: 1.149724 \tValidation Loss: 2.824406\n",
      "Epoch: 40318 \tTraining Loss: 1.191383 \tValidation Loss: 2.822013\n",
      "Epoch: 40319 \tTraining Loss: 1.176246 \tValidation Loss: 2.824093\n",
      "Epoch: 40320 \tTraining Loss: 1.175748 \tValidation Loss: 2.823862\n",
      "Epoch: 40321 \tTraining Loss: 1.131506 \tValidation Loss: 2.824625\n",
      "Epoch: 40322 \tTraining Loss: 1.162652 \tValidation Loss: 2.822369\n",
      "Epoch: 40323 \tTraining Loss: 1.136830 \tValidation Loss: 2.823686\n",
      "Epoch: 40324 \tTraining Loss: 1.107913 \tValidation Loss: 2.824961\n",
      "Epoch: 40325 \tTraining Loss: 1.130792 \tValidation Loss: 2.825223\n",
      "Epoch: 40326 \tTraining Loss: 1.167312 \tValidation Loss: 2.824824\n",
      "Epoch: 40327 \tTraining Loss: 1.179495 \tValidation Loss: 2.824501\n",
      "Epoch: 40328 \tTraining Loss: 1.159140 \tValidation Loss: 2.825265\n",
      "Epoch: 40329 \tTraining Loss: 1.120904 \tValidation Loss: 2.824633\n",
      "Epoch: 40330 \tTraining Loss: 1.103637 \tValidation Loss: 2.824231\n",
      "Epoch: 40331 \tTraining Loss: 1.124752 \tValidation Loss: 2.824023\n",
      "Epoch: 40332 \tTraining Loss: 1.128610 \tValidation Loss: 2.826630\n",
      "Epoch: 40333 \tTraining Loss: 1.196129 \tValidation Loss: 2.822255\n",
      "Epoch: 40334 \tTraining Loss: 1.171333 \tValidation Loss: 2.825289\n",
      "Epoch: 40335 \tTraining Loss: 1.150121 \tValidation Loss: 2.824245\n",
      "Epoch: 40336 \tTraining Loss: 1.090296 \tValidation Loss: 2.825984\n",
      "Epoch: 40337 \tTraining Loss: 1.141158 \tValidation Loss: 2.825437\n",
      "Epoch: 40338 \tTraining Loss: 1.151115 \tValidation Loss: 2.825075\n",
      "Epoch: 40339 \tTraining Loss: 1.171205 \tValidation Loss: 2.825256\n",
      "Epoch: 40340 \tTraining Loss: 1.143264 \tValidation Loss: 2.823371\n",
      "Epoch: 40341 \tTraining Loss: 1.161413 \tValidation Loss: 2.822882\n",
      "Epoch: 40342 \tTraining Loss: 1.157504 \tValidation Loss: 2.824782\n",
      "Epoch: 40343 \tTraining Loss: 1.150010 \tValidation Loss: 2.824121\n",
      "Epoch: 40344 \tTraining Loss: 1.164088 \tValidation Loss: 2.824469\n",
      "Epoch: 40345 \tTraining Loss: 1.169725 \tValidation Loss: 2.823048\n",
      "Epoch: 40346 \tTraining Loss: 1.136724 \tValidation Loss: 2.825311\n",
      "Epoch: 40347 \tTraining Loss: 1.160756 \tValidation Loss: 2.825759\n",
      "Epoch: 40348 \tTraining Loss: 1.206221 \tValidation Loss: 2.826045\n",
      "Epoch: 40349 \tTraining Loss: 1.163819 \tValidation Loss: 2.825495\n",
      "Epoch: 40350 \tTraining Loss: 1.203860 \tValidation Loss: 2.824298\n",
      "Epoch: 40351 \tTraining Loss: 1.159589 \tValidation Loss: 2.824394\n",
      "Epoch: 40352 \tTraining Loss: 1.158660 \tValidation Loss: 2.826180\n",
      "Epoch: 40353 \tTraining Loss: 1.150409 \tValidation Loss: 2.824170\n",
      "Epoch: 40354 \tTraining Loss: 1.137410 \tValidation Loss: 2.826290\n",
      "Epoch: 40355 \tTraining Loss: 1.150810 \tValidation Loss: 2.824808\n",
      "Epoch: 40356 \tTraining Loss: 1.148495 \tValidation Loss: 2.824451\n",
      "Epoch: 40357 \tTraining Loss: 1.152951 \tValidation Loss: 2.823112\n",
      "Epoch: 40358 \tTraining Loss: 1.130431 \tValidation Loss: 2.826104\n",
      "Epoch: 40359 \tTraining Loss: 1.181259 \tValidation Loss: 2.823289\n",
      "Epoch: 40360 \tTraining Loss: 1.115552 \tValidation Loss: 2.825947\n",
      "Epoch: 40361 \tTraining Loss: 1.209168 \tValidation Loss: 2.823082\n",
      "Epoch: 40362 \tTraining Loss: 1.146740 \tValidation Loss: 2.824814\n",
      "Epoch: 40363 \tTraining Loss: 1.118475 \tValidation Loss: 2.825075\n",
      "Epoch: 40364 \tTraining Loss: 1.136460 \tValidation Loss: 2.825006\n",
      "Epoch: 40365 \tTraining Loss: 1.163101 \tValidation Loss: 2.827329\n",
      "Epoch: 40366 \tTraining Loss: 1.141004 \tValidation Loss: 2.828310\n",
      "Epoch: 40367 \tTraining Loss: 1.149554 \tValidation Loss: 2.826823\n",
      "Epoch: 40368 \tTraining Loss: 1.196555 \tValidation Loss: 2.825270\n",
      "Epoch: 40369 \tTraining Loss: 1.150724 \tValidation Loss: 2.825398\n",
      "Epoch: 40370 \tTraining Loss: 1.129096 \tValidation Loss: 2.827279\n",
      "Epoch: 40371 \tTraining Loss: 1.114953 \tValidation Loss: 2.827183\n",
      "Epoch: 40372 \tTraining Loss: 1.140472 \tValidation Loss: 2.828895\n",
      "Epoch: 40373 \tTraining Loss: 1.180991 \tValidation Loss: 2.824260\n",
      "Epoch: 40374 \tTraining Loss: 1.150774 \tValidation Loss: 2.825410\n",
      "Epoch: 40375 \tTraining Loss: 1.130617 \tValidation Loss: 2.824314\n",
      "Epoch: 40376 \tTraining Loss: 1.161930 \tValidation Loss: 2.825719\n",
      "Epoch: 40377 \tTraining Loss: 1.149970 \tValidation Loss: 2.826046\n",
      "Epoch: 40378 \tTraining Loss: 1.140237 \tValidation Loss: 2.826002\n",
      "Epoch: 40379 \tTraining Loss: 1.152157 \tValidation Loss: 2.826253\n",
      "Epoch: 40380 \tTraining Loss: 1.147011 \tValidation Loss: 2.827902\n",
      "Epoch: 40381 \tTraining Loss: 1.169020 \tValidation Loss: 2.824359\n",
      "Epoch: 40382 \tTraining Loss: 1.130740 \tValidation Loss: 2.824935\n",
      "Epoch: 40383 \tTraining Loss: 1.142747 \tValidation Loss: 2.825183\n",
      "Epoch: 40384 \tTraining Loss: 1.108729 \tValidation Loss: 2.826118\n",
      "Epoch: 40385 \tTraining Loss: 1.111330 \tValidation Loss: 2.826716\n",
      "Epoch: 40386 \tTraining Loss: 1.181317 \tValidation Loss: 2.825793\n",
      "Epoch: 40387 \tTraining Loss: 1.148244 \tValidation Loss: 2.826453\n",
      "Epoch: 40388 \tTraining Loss: 1.170283 \tValidation Loss: 2.826715\n",
      "Epoch: 40389 \tTraining Loss: 1.124761 \tValidation Loss: 2.826336\n",
      "Epoch: 40390 \tTraining Loss: 1.110099 \tValidation Loss: 2.827397\n",
      "Epoch: 40391 \tTraining Loss: 1.195914 \tValidation Loss: 2.826926\n",
      "Epoch: 40392 \tTraining Loss: 1.173259 \tValidation Loss: 2.826609\n",
      "Epoch: 40393 \tTraining Loss: 1.137342 \tValidation Loss: 2.825890\n",
      "Epoch: 40394 \tTraining Loss: 1.156084 \tValidation Loss: 2.825624\n",
      "Epoch: 40395 \tTraining Loss: 1.135139 \tValidation Loss: 2.827431\n",
      "Epoch: 40396 \tTraining Loss: 1.167565 \tValidation Loss: 2.825315\n",
      "Epoch: 40397 \tTraining Loss: 1.129518 \tValidation Loss: 2.826386\n",
      "Epoch: 40398 \tTraining Loss: 1.114030 \tValidation Loss: 2.828189\n",
      "Epoch: 40399 \tTraining Loss: 1.107974 \tValidation Loss: 2.826521\n",
      "Epoch: 40400 \tTraining Loss: 1.124843 \tValidation Loss: 2.826048\n",
      "Epoch: 40401 \tTraining Loss: 1.174826 \tValidation Loss: 2.827583\n",
      "Epoch: 40402 \tTraining Loss: 1.125769 \tValidation Loss: 2.826623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40403 \tTraining Loss: 1.104630 \tValidation Loss: 2.827587\n",
      "Epoch: 40404 \tTraining Loss: 1.129580 \tValidation Loss: 2.828444\n",
      "Epoch: 40405 \tTraining Loss: 1.152188 \tValidation Loss: 2.826914\n",
      "Epoch: 40406 \tTraining Loss: 1.137639 \tValidation Loss: 2.824658\n",
      "Epoch: 40407 \tTraining Loss: 1.181665 \tValidation Loss: 2.825226\n",
      "Epoch: 40408 \tTraining Loss: 1.088036 \tValidation Loss: 2.828205\n",
      "Epoch: 40409 \tTraining Loss: 1.125624 \tValidation Loss: 2.828588\n",
      "Epoch: 40410 \tTraining Loss: 1.149676 \tValidation Loss: 2.826000\n",
      "Epoch: 40411 \tTraining Loss: 1.124491 \tValidation Loss: 2.827372\n",
      "Epoch: 40412 \tTraining Loss: 1.163194 \tValidation Loss: 2.827315\n",
      "Epoch: 40413 \tTraining Loss: 1.164988 \tValidation Loss: 2.826756\n",
      "Epoch: 40414 \tTraining Loss: 1.133300 \tValidation Loss: 2.828688\n",
      "Epoch: 40415 \tTraining Loss: 1.150575 \tValidation Loss: 2.828877\n",
      "Epoch: 40416 \tTraining Loss: 1.122659 \tValidation Loss: 2.825877\n",
      "Epoch: 40417 \tTraining Loss: 1.154821 \tValidation Loss: 2.826916\n",
      "Epoch: 40418 \tTraining Loss: 1.157160 \tValidation Loss: 2.827161\n",
      "Epoch: 40419 \tTraining Loss: 1.112080 \tValidation Loss: 2.828363\n",
      "Epoch: 40420 \tTraining Loss: 1.195716 \tValidation Loss: 2.825971\n",
      "Epoch: 40421 \tTraining Loss: 1.111211 \tValidation Loss: 2.828309\n",
      "Epoch: 40422 \tTraining Loss: 1.141292 \tValidation Loss: 2.828257\n",
      "Epoch: 40423 \tTraining Loss: 1.084121 \tValidation Loss: 2.828899\n",
      "Epoch: 40424 \tTraining Loss: 1.199350 \tValidation Loss: 2.825498\n",
      "Epoch: 40425 \tTraining Loss: 1.121263 \tValidation Loss: 2.826781\n",
      "Epoch: 40426 \tTraining Loss: 1.183849 \tValidation Loss: 2.824648\n",
      "Epoch: 40427 \tTraining Loss: 1.160576 \tValidation Loss: 2.826536\n",
      "Epoch: 40428 \tTraining Loss: 1.143371 \tValidation Loss: 2.825652\n",
      "Epoch: 40429 \tTraining Loss: 1.156220 \tValidation Loss: 2.827091\n",
      "Epoch: 40430 \tTraining Loss: 1.156006 \tValidation Loss: 2.827512\n",
      "Epoch: 40431 \tTraining Loss: 1.150603 \tValidation Loss: 2.826562\n",
      "Epoch: 40432 \tTraining Loss: 1.139496 \tValidation Loss: 2.827309\n",
      "Epoch: 40433 \tTraining Loss: 1.110377 \tValidation Loss: 2.828466\n",
      "Epoch: 40434 \tTraining Loss: 1.147947 \tValidation Loss: 2.828688\n",
      "Epoch: 40435 \tTraining Loss: 1.122875 \tValidation Loss: 2.826895\n",
      "Epoch: 40436 \tTraining Loss: 1.146081 \tValidation Loss: 2.826082\n",
      "Epoch: 40437 \tTraining Loss: 1.152417 \tValidation Loss: 2.825153\n",
      "Epoch: 40438 \tTraining Loss: 1.122178 \tValidation Loss: 2.825481\n",
      "Epoch: 40439 \tTraining Loss: 1.176647 \tValidation Loss: 2.826173\n",
      "Epoch: 40440 \tTraining Loss: 1.114906 \tValidation Loss: 2.827276\n",
      "Epoch: 40441 \tTraining Loss: 1.122285 \tValidation Loss: 2.828205\n",
      "Epoch: 40442 \tTraining Loss: 1.161095 \tValidation Loss: 2.828581\n",
      "Epoch: 40443 \tTraining Loss: 1.128517 \tValidation Loss: 2.828997\n",
      "Epoch: 40444 \tTraining Loss: 1.138867 \tValidation Loss: 2.827639\n",
      "Epoch: 40445 \tTraining Loss: 1.148383 \tValidation Loss: 2.827966\n",
      "Epoch: 40446 \tTraining Loss: 1.120029 \tValidation Loss: 2.828613\n",
      "Epoch: 40447 \tTraining Loss: 1.157846 \tValidation Loss: 2.828099\n",
      "Epoch: 40448 \tTraining Loss: 1.145972 \tValidation Loss: 2.828909\n",
      "Epoch: 40449 \tTraining Loss: 1.160427 \tValidation Loss: 2.828058\n",
      "Epoch: 40450 \tTraining Loss: 1.177437 \tValidation Loss: 2.828333\n",
      "Epoch: 40451 \tTraining Loss: 1.086377 \tValidation Loss: 2.829950\n",
      "Epoch: 40452 \tTraining Loss: 1.192516 \tValidation Loss: 2.826265\n",
      "Epoch: 40453 \tTraining Loss: 1.142864 \tValidation Loss: 2.827661\n",
      "Epoch: 40454 \tTraining Loss: 1.146495 \tValidation Loss: 2.827855\n",
      "Epoch: 40455 \tTraining Loss: 1.130540 \tValidation Loss: 2.827752\n",
      "Epoch: 40456 \tTraining Loss: 1.114671 \tValidation Loss: 2.828646\n",
      "Epoch: 40457 \tTraining Loss: 1.138011 \tValidation Loss: 2.827646\n",
      "Epoch: 40458 \tTraining Loss: 1.162955 \tValidation Loss: 2.830305\n",
      "Epoch: 40459 \tTraining Loss: 1.141785 \tValidation Loss: 2.828077\n",
      "Epoch: 40460 \tTraining Loss: 1.169870 \tValidation Loss: 2.826668\n",
      "Epoch: 40461 \tTraining Loss: 1.119305 \tValidation Loss: 2.827183\n",
      "Epoch: 40462 \tTraining Loss: 1.155030 \tValidation Loss: 2.828040\n",
      "Epoch: 40463 \tTraining Loss: 1.147275 \tValidation Loss: 2.827249\n",
      "Epoch: 40464 \tTraining Loss: 1.139691 \tValidation Loss: 2.828417\n",
      "Epoch: 40465 \tTraining Loss: 1.141264 \tValidation Loss: 2.830560\n",
      "Epoch: 40466 \tTraining Loss: 1.139282 \tValidation Loss: 2.829650\n",
      "Epoch: 40467 \tTraining Loss: 1.107169 \tValidation Loss: 2.830213\n",
      "Epoch: 40468 \tTraining Loss: 1.202544 \tValidation Loss: 2.827343\n",
      "Epoch: 40469 \tTraining Loss: 1.103013 \tValidation Loss: 2.829933\n",
      "Epoch: 40470 \tTraining Loss: 1.099508 \tValidation Loss: 2.827847\n",
      "Epoch: 40471 \tTraining Loss: 1.128245 \tValidation Loss: 2.829632\n",
      "Epoch: 40472 \tTraining Loss: 1.145704 \tValidation Loss: 2.829200\n",
      "Epoch: 40473 \tTraining Loss: 1.147338 \tValidation Loss: 2.829352\n",
      "Epoch: 40474 \tTraining Loss: 1.163880 \tValidation Loss: 2.827587\n",
      "Epoch: 40475 \tTraining Loss: 1.184989 \tValidation Loss: 2.827408\n",
      "Epoch: 40476 \tTraining Loss: 1.123133 \tValidation Loss: 2.828317\n",
      "Epoch: 40477 \tTraining Loss: 1.159876 \tValidation Loss: 2.827469\n",
      "Epoch: 40478 \tTraining Loss: 1.134973 \tValidation Loss: 2.827446\n",
      "Epoch: 40479 \tTraining Loss: 1.152543 \tValidation Loss: 2.828242\n",
      "Epoch: 40480 \tTraining Loss: 1.125200 \tValidation Loss: 2.828973\n",
      "Epoch: 40481 \tTraining Loss: 1.153156 \tValidation Loss: 2.829597\n",
      "Epoch: 40482 \tTraining Loss: 1.163181 \tValidation Loss: 2.828363\n",
      "Epoch: 40483 \tTraining Loss: 1.102825 \tValidation Loss: 2.828617\n",
      "Epoch: 40484 \tTraining Loss: 1.156354 \tValidation Loss: 2.828068\n",
      "Epoch: 40485 \tTraining Loss: 1.132092 \tValidation Loss: 2.827520\n",
      "Epoch: 40486 \tTraining Loss: 1.168304 \tValidation Loss: 2.827567\n",
      "Epoch: 40487 \tTraining Loss: 1.146226 \tValidation Loss: 2.828194\n",
      "Epoch: 40488 \tTraining Loss: 1.164917 \tValidation Loss: 2.827768\n",
      "Epoch: 40489 \tTraining Loss: 1.129154 \tValidation Loss: 2.825798\n",
      "Epoch: 40490 \tTraining Loss: 1.140291 \tValidation Loss: 2.827688\n",
      "Epoch: 40491 \tTraining Loss: 1.131791 \tValidation Loss: 2.827775\n",
      "Epoch: 40492 \tTraining Loss: 1.168467 \tValidation Loss: 2.826998\n",
      "Epoch: 40493 \tTraining Loss: 1.155515 \tValidation Loss: 2.828809\n",
      "Epoch: 40494 \tTraining Loss: 1.117726 \tValidation Loss: 2.828745\n",
      "Epoch: 40495 \tTraining Loss: 1.154300 \tValidation Loss: 2.827671\n",
      "Epoch: 40496 \tTraining Loss: 1.162342 \tValidation Loss: 2.828383\n",
      "Epoch: 40497 \tTraining Loss: 1.100915 \tValidation Loss: 2.829774\n",
      "Epoch: 40498 \tTraining Loss: 1.133792 \tValidation Loss: 2.829418\n",
      "Epoch: 40499 \tTraining Loss: 1.143132 \tValidation Loss: 2.829023\n",
      "Epoch: 40500 \tTraining Loss: 1.172433 \tValidation Loss: 2.828407\n",
      "Epoch: 40501 \tTraining Loss: 1.139728 \tValidation Loss: 2.829674\n",
      "Epoch: 40502 \tTraining Loss: 1.156354 \tValidation Loss: 2.830513\n",
      "Epoch: 40503 \tTraining Loss: 1.150441 \tValidation Loss: 2.828800\n",
      "Epoch: 40504 \tTraining Loss: 1.116420 \tValidation Loss: 2.828183\n",
      "Epoch: 40505 \tTraining Loss: 1.149739 \tValidation Loss: 2.829279\n",
      "Epoch: 40506 \tTraining Loss: 1.173424 \tValidation Loss: 2.829446\n",
      "Epoch: 40507 \tTraining Loss: 1.149995 \tValidation Loss: 2.826062\n",
      "Epoch: 40508 \tTraining Loss: 1.140900 \tValidation Loss: 2.830474\n",
      "Epoch: 40509 \tTraining Loss: 1.161933 \tValidation Loss: 2.827305\n",
      "Epoch: 40510 \tTraining Loss: 1.185154 \tValidation Loss: 2.827176\n",
      "Epoch: 40511 \tTraining Loss: 1.189578 \tValidation Loss: 2.827944\n",
      "Epoch: 40512 \tTraining Loss: 1.128040 \tValidation Loss: 2.829217\n",
      "Epoch: 40513 \tTraining Loss: 1.168842 \tValidation Loss: 2.829370\n",
      "Epoch: 40514 \tTraining Loss: 1.130664 \tValidation Loss: 2.830042\n",
      "Epoch: 40515 \tTraining Loss: 1.158934 \tValidation Loss: 2.829070\n",
      "Epoch: 40516 \tTraining Loss: 1.186195 \tValidation Loss: 2.829248\n",
      "Epoch: 40517 \tTraining Loss: 1.102274 \tValidation Loss: 2.828703\n",
      "Epoch: 40518 \tTraining Loss: 1.129426 \tValidation Loss: 2.828762\n",
      "Epoch: 40519 \tTraining Loss: 1.150383 \tValidation Loss: 2.829345\n",
      "Epoch: 40520 \tTraining Loss: 1.164383 \tValidation Loss: 2.829813\n",
      "Epoch: 40521 \tTraining Loss: 1.129089 \tValidation Loss: 2.830618\n",
      "Epoch: 40522 \tTraining Loss: 1.132037 \tValidation Loss: 2.827856\n",
      "Epoch: 40523 \tTraining Loss: 1.150873 \tValidation Loss: 2.828120\n",
      "Epoch: 40524 \tTraining Loss: 1.128633 \tValidation Loss: 2.830425\n",
      "Epoch: 40525 \tTraining Loss: 1.116793 \tValidation Loss: 2.830517\n",
      "Epoch: 40526 \tTraining Loss: 1.143508 \tValidation Loss: 2.830264\n",
      "Epoch: 40527 \tTraining Loss: 1.115833 \tValidation Loss: 2.830430\n",
      "Epoch: 40528 \tTraining Loss: 1.153360 \tValidation Loss: 2.830449\n",
      "Epoch: 40529 \tTraining Loss: 1.086851 \tValidation Loss: 2.830411\n",
      "Epoch: 40530 \tTraining Loss: 1.158150 \tValidation Loss: 2.828652\n",
      "Epoch: 40531 \tTraining Loss: 1.142649 \tValidation Loss: 2.830678\n",
      "Epoch: 40532 \tTraining Loss: 1.096076 \tValidation Loss: 2.829426\n",
      "Epoch: 40533 \tTraining Loss: 1.147885 \tValidation Loss: 2.828513\n",
      "Epoch: 40534 \tTraining Loss: 1.185131 \tValidation Loss: 2.830733\n",
      "Epoch: 40535 \tTraining Loss: 1.098587 \tValidation Loss: 2.828799\n",
      "Epoch: 40536 \tTraining Loss: 1.127444 \tValidation Loss: 2.833016\n",
      "Epoch: 40537 \tTraining Loss: 1.145882 \tValidation Loss: 2.826667\n",
      "Epoch: 40538 \tTraining Loss: 1.148254 \tValidation Loss: 2.829049\n",
      "Epoch: 40539 \tTraining Loss: 1.152580 \tValidation Loss: 2.829036\n",
      "Epoch: 40540 \tTraining Loss: 1.161757 \tValidation Loss: 2.830204\n",
      "Epoch: 40541 \tTraining Loss: 1.085427 \tValidation Loss: 2.829142\n",
      "Epoch: 40542 \tTraining Loss: 1.123020 \tValidation Loss: 2.831316\n",
      "Epoch: 40543 \tTraining Loss: 1.137489 \tValidation Loss: 2.829437\n",
      "Epoch: 40544 \tTraining Loss: 1.139665 \tValidation Loss: 2.827164\n",
      "Epoch: 40545 \tTraining Loss: 1.142100 \tValidation Loss: 2.828801\n",
      "Epoch: 40546 \tTraining Loss: 1.123325 \tValidation Loss: 2.830208\n",
      "Epoch: 40547 \tTraining Loss: 1.110611 \tValidation Loss: 2.830467\n",
      "Epoch: 40548 \tTraining Loss: 1.117177 \tValidation Loss: 2.830505\n",
      "Epoch: 40549 \tTraining Loss: 1.133386 \tValidation Loss: 2.829768\n",
      "Epoch: 40550 \tTraining Loss: 1.146154 \tValidation Loss: 2.830338\n",
      "Epoch: 40551 \tTraining Loss: 1.163719 \tValidation Loss: 2.830110\n",
      "Epoch: 40552 \tTraining Loss: 1.121847 \tValidation Loss: 2.830793\n",
      "Epoch: 40553 \tTraining Loss: 1.130484 \tValidation Loss: 2.830409\n",
      "Epoch: 40554 \tTraining Loss: 1.122520 \tValidation Loss: 2.830752\n",
      "Epoch: 40555 \tTraining Loss: 1.124547 \tValidation Loss: 2.830450\n",
      "Epoch: 40556 \tTraining Loss: 1.157939 \tValidation Loss: 2.828135\n",
      "Epoch: 40557 \tTraining Loss: 1.172091 \tValidation Loss: 2.829254\n",
      "Epoch: 40558 \tTraining Loss: 1.137056 \tValidation Loss: 2.829634\n",
      "Epoch: 40559 \tTraining Loss: 1.095813 \tValidation Loss: 2.829705\n",
      "Epoch: 40560 \tTraining Loss: 1.149616 \tValidation Loss: 2.830151\n",
      "Epoch: 40561 \tTraining Loss: 1.120692 \tValidation Loss: 2.831597\n",
      "Epoch: 40562 \tTraining Loss: 1.139032 \tValidation Loss: 2.831698\n",
      "Epoch: 40563 \tTraining Loss: 1.105708 \tValidation Loss: 2.830639\n",
      "Epoch: 40564 \tTraining Loss: 1.159793 \tValidation Loss: 2.829902\n",
      "Epoch: 40565 \tTraining Loss: 1.153842 \tValidation Loss: 2.830531\n",
      "Epoch: 40566 \tTraining Loss: 1.115478 \tValidation Loss: 2.830012\n",
      "Epoch: 40567 \tTraining Loss: 1.131950 \tValidation Loss: 2.830720\n",
      "Epoch: 40568 \tTraining Loss: 1.147493 \tValidation Loss: 2.831036\n",
      "Epoch: 40569 \tTraining Loss: 1.167455 \tValidation Loss: 2.832659\n",
      "Epoch: 40570 \tTraining Loss: 1.183736 \tValidation Loss: 2.828978\n",
      "Epoch: 40571 \tTraining Loss: 1.127562 \tValidation Loss: 2.831186\n",
      "Epoch: 40572 \tTraining Loss: 1.139804 \tValidation Loss: 2.830874\n",
      "Epoch: 40573 \tTraining Loss: 1.161800 \tValidation Loss: 2.831139\n",
      "Epoch: 40574 \tTraining Loss: 1.148646 \tValidation Loss: 2.829831\n",
      "Epoch: 40575 \tTraining Loss: 1.116761 \tValidation Loss: 2.830979\n",
      "Epoch: 40576 \tTraining Loss: 1.156164 \tValidation Loss: 2.829571\n",
      "Epoch: 40577 \tTraining Loss: 1.124111 \tValidation Loss: 2.830408\n",
      "Epoch: 40578 \tTraining Loss: 1.097403 \tValidation Loss: 2.831148\n",
      "Epoch: 40579 \tTraining Loss: 1.141905 \tValidation Loss: 2.833339\n",
      "Epoch: 40580 \tTraining Loss: 1.191608 \tValidation Loss: 2.829081\n",
      "Epoch: 40581 \tTraining Loss: 1.128865 \tValidation Loss: 2.830726\n",
      "Epoch: 40582 \tTraining Loss: 1.152065 \tValidation Loss: 2.829165\n",
      "Epoch: 40583 \tTraining Loss: 1.130226 \tValidation Loss: 2.830224\n",
      "Epoch: 40584 \tTraining Loss: 1.122631 \tValidation Loss: 2.831023\n",
      "Epoch: 40585 \tTraining Loss: 1.152065 \tValidation Loss: 2.830260\n",
      "Epoch: 40586 \tTraining Loss: 1.107774 \tValidation Loss: 2.830405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40587 \tTraining Loss: 1.130722 \tValidation Loss: 2.833038\n",
      "Epoch: 40588 \tTraining Loss: 1.151999 \tValidation Loss: 2.831813\n",
      "Epoch: 40589 \tTraining Loss: 1.150455 \tValidation Loss: 2.831028\n",
      "Epoch: 40590 \tTraining Loss: 1.188011 \tValidation Loss: 2.830962\n",
      "Epoch: 40591 \tTraining Loss: 1.136318 \tValidation Loss: 2.831897\n",
      "Epoch: 40592 \tTraining Loss: 1.145967 \tValidation Loss: 2.831085\n",
      "Epoch: 40593 \tTraining Loss: 1.170175 \tValidation Loss: 2.830971\n",
      "Epoch: 40594 \tTraining Loss: 1.116556 \tValidation Loss: 2.831951\n",
      "Epoch: 40595 \tTraining Loss: 1.126704 \tValidation Loss: 2.831036\n",
      "Epoch: 40596 \tTraining Loss: 1.114104 \tValidation Loss: 2.832577\n",
      "Epoch: 40597 \tTraining Loss: 1.130705 \tValidation Loss: 2.831790\n",
      "Epoch: 40598 \tTraining Loss: 1.114370 \tValidation Loss: 2.831734\n",
      "Epoch: 40599 \tTraining Loss: 1.145345 \tValidation Loss: 2.830538\n",
      "Epoch: 40600 \tTraining Loss: 1.109709 \tValidation Loss: 2.832284\n",
      "Epoch: 40601 \tTraining Loss: 1.121432 \tValidation Loss: 2.831166\n",
      "Epoch: 40602 \tTraining Loss: 1.162226 \tValidation Loss: 2.832751\n",
      "Epoch: 40603 \tTraining Loss: 1.119877 \tValidation Loss: 2.831905\n",
      "Epoch: 40604 \tTraining Loss: 1.119375 \tValidation Loss: 2.831670\n",
      "Epoch: 40605 \tTraining Loss: 1.182100 \tValidation Loss: 2.831460\n",
      "Epoch: 40606 \tTraining Loss: 1.097595 \tValidation Loss: 2.831715\n",
      "Epoch: 40607 \tTraining Loss: 1.158419 \tValidation Loss: 2.831159\n",
      "Epoch: 40608 \tTraining Loss: 1.136370 \tValidation Loss: 2.829945\n",
      "Epoch: 40609 \tTraining Loss: 1.102188 \tValidation Loss: 2.830795\n",
      "Epoch: 40610 \tTraining Loss: 1.121600 \tValidation Loss: 2.832999\n",
      "Epoch: 40611 \tTraining Loss: 1.148437 \tValidation Loss: 2.831683\n",
      "Epoch: 40612 \tTraining Loss: 1.156031 \tValidation Loss: 2.831455\n",
      "Epoch: 40613 \tTraining Loss: 1.198955 \tValidation Loss: 2.831207\n",
      "Epoch: 40614 \tTraining Loss: 1.117042 \tValidation Loss: 2.832036\n",
      "Epoch: 40615 \tTraining Loss: 1.141354 \tValidation Loss: 2.831091\n",
      "Epoch: 40616 \tTraining Loss: 1.188919 \tValidation Loss: 2.831588\n",
      "Epoch: 40617 \tTraining Loss: 1.137169 \tValidation Loss: 2.831432\n",
      "Epoch: 40618 \tTraining Loss: 1.175408 \tValidation Loss: 2.833070\n",
      "Epoch: 40619 \tTraining Loss: 1.114960 \tValidation Loss: 2.832818\n",
      "Epoch: 40620 \tTraining Loss: 1.133654 \tValidation Loss: 2.831477\n",
      "Epoch: 40621 \tTraining Loss: 1.175692 \tValidation Loss: 2.832711\n",
      "Epoch: 40622 \tTraining Loss: 1.166843 \tValidation Loss: 2.830636\n",
      "Epoch: 40623 \tTraining Loss: 1.166824 \tValidation Loss: 2.831140\n",
      "Epoch: 40624 \tTraining Loss: 1.093794 \tValidation Loss: 2.832342\n",
      "Epoch: 40625 \tTraining Loss: 1.171129 \tValidation Loss: 2.831278\n",
      "Epoch: 40626 \tTraining Loss: 1.166710 \tValidation Loss: 2.832135\n",
      "Epoch: 40627 \tTraining Loss: 1.130965 \tValidation Loss: 2.830374\n",
      "Epoch: 40628 \tTraining Loss: 1.120229 \tValidation Loss: 2.831711\n",
      "Epoch: 40629 \tTraining Loss: 1.081396 \tValidation Loss: 2.832548\n",
      "Epoch: 40630 \tTraining Loss: 1.164413 \tValidation Loss: 2.832255\n",
      "Epoch: 40631 \tTraining Loss: 1.103191 \tValidation Loss: 2.834084\n",
      "Epoch: 40632 \tTraining Loss: 1.110158 \tValidation Loss: 2.832692\n",
      "Epoch: 40633 \tTraining Loss: 1.122583 \tValidation Loss: 2.832568\n",
      "Epoch: 40634 \tTraining Loss: 1.184661 \tValidation Loss: 2.829762\n",
      "Epoch: 40635 \tTraining Loss: 1.138773 \tValidation Loss: 2.831725\n",
      "Epoch: 40636 \tTraining Loss: 1.130485 \tValidation Loss: 2.832702\n",
      "Epoch: 40637 \tTraining Loss: 1.188885 \tValidation Loss: 2.832071\n",
      "Epoch: 40638 \tTraining Loss: 1.123958 \tValidation Loss: 2.833002\n",
      "Epoch: 40639 \tTraining Loss: 1.165563 \tValidation Loss: 2.832045\n",
      "Epoch: 40640 \tTraining Loss: 1.157779 \tValidation Loss: 2.831316\n",
      "Epoch: 40641 \tTraining Loss: 1.111461 \tValidation Loss: 2.831088\n",
      "Epoch: 40642 \tTraining Loss: 1.164160 \tValidation Loss: 2.832169\n",
      "Epoch: 40643 \tTraining Loss: 1.143050 \tValidation Loss: 2.833331\n",
      "Epoch: 40644 \tTraining Loss: 1.169824 \tValidation Loss: 2.832418\n",
      "Epoch: 40645 \tTraining Loss: 1.145835 \tValidation Loss: 2.833384\n",
      "Epoch: 40646 \tTraining Loss: 1.189412 \tValidation Loss: 2.829810\n",
      "Epoch: 40647 \tTraining Loss: 1.103239 \tValidation Loss: 2.831700\n",
      "Epoch: 40648 \tTraining Loss: 1.136738 \tValidation Loss: 2.832425\n",
      "Epoch: 40649 \tTraining Loss: 1.111983 \tValidation Loss: 2.833326\n",
      "Epoch: 40650 \tTraining Loss: 1.148747 \tValidation Loss: 2.832002\n",
      "Epoch: 40651 \tTraining Loss: 1.191346 \tValidation Loss: 2.831046\n",
      "Epoch: 40652 \tTraining Loss: 1.161647 \tValidation Loss: 2.830197\n",
      "Epoch: 40653 \tTraining Loss: 1.151568 \tValidation Loss: 2.831329\n",
      "Epoch: 40654 \tTraining Loss: 1.107214 \tValidation Loss: 2.833775\n",
      "Epoch: 40655 \tTraining Loss: 1.174598 \tValidation Loss: 2.832309\n",
      "Epoch: 40656 \tTraining Loss: 1.143821 \tValidation Loss: 2.833309\n",
      "Epoch: 40657 \tTraining Loss: 1.135173 \tValidation Loss: 2.830435\n",
      "Epoch: 40658 \tTraining Loss: 1.091465 \tValidation Loss: 2.834153\n",
      "Epoch: 40659 \tTraining Loss: 1.118005 \tValidation Loss: 2.832902\n",
      "Epoch: 40660 \tTraining Loss: 1.154862 \tValidation Loss: 2.834210\n",
      "Epoch: 40661 \tTraining Loss: 1.140754 \tValidation Loss: 2.833652\n",
      "Epoch: 40662 \tTraining Loss: 1.114579 \tValidation Loss: 2.832934\n",
      "Epoch: 40663 \tTraining Loss: 1.141818 \tValidation Loss: 2.833241\n",
      "Epoch: 40664 \tTraining Loss: 1.137919 \tValidation Loss: 2.831352\n",
      "Epoch: 40665 \tTraining Loss: 1.140801 \tValidation Loss: 2.833284\n",
      "Epoch: 40666 \tTraining Loss: 1.127857 \tValidation Loss: 2.832595\n",
      "Epoch: 40667 \tTraining Loss: 1.169377 \tValidation Loss: 2.830491\n",
      "Epoch: 40668 \tTraining Loss: 1.157732 \tValidation Loss: 2.832622\n",
      "Epoch: 40669 \tTraining Loss: 1.143087 \tValidation Loss: 2.831053\n",
      "Epoch: 40670 \tTraining Loss: 1.115890 \tValidation Loss: 2.832615\n",
      "Epoch: 40671 \tTraining Loss: 1.165719 \tValidation Loss: 2.832683\n",
      "Epoch: 40672 \tTraining Loss: 1.171596 \tValidation Loss: 2.832772\n",
      "Epoch: 40673 \tTraining Loss: 1.156229 \tValidation Loss: 2.832867\n",
      "Epoch: 40674 \tTraining Loss: 1.169691 \tValidation Loss: 2.834687\n",
      "Epoch: 40675 \tTraining Loss: 1.163201 \tValidation Loss: 2.832298\n",
      "Epoch: 40676 \tTraining Loss: 1.138335 \tValidation Loss: 2.833168\n",
      "Epoch: 40677 \tTraining Loss: 1.148786 \tValidation Loss: 2.833040\n",
      "Epoch: 40678 \tTraining Loss: 1.157641 \tValidation Loss: 2.834032\n",
      "Epoch: 40679 \tTraining Loss: 1.170235 \tValidation Loss: 2.832938\n",
      "Epoch: 40680 \tTraining Loss: 1.080544 \tValidation Loss: 2.832190\n",
      "Epoch: 40681 \tTraining Loss: 1.051519 \tValidation Loss: 2.836183\n",
      "Epoch: 40682 \tTraining Loss: 1.154098 \tValidation Loss: 2.832717\n",
      "Epoch: 40683 \tTraining Loss: 1.119554 \tValidation Loss: 2.834732\n",
      "Epoch: 40684 \tTraining Loss: 1.154188 \tValidation Loss: 2.833798\n",
      "Epoch: 40685 \tTraining Loss: 1.117926 \tValidation Loss: 2.833615\n",
      "Epoch: 40686 \tTraining Loss: 1.116844 \tValidation Loss: 2.835312\n",
      "Epoch: 40687 \tTraining Loss: 1.127352 \tValidation Loss: 2.833419\n",
      "Epoch: 40688 \tTraining Loss: 1.134556 \tValidation Loss: 2.834677\n",
      "Epoch: 40689 \tTraining Loss: 1.144852 \tValidation Loss: 2.830827\n",
      "Epoch: 40690 \tTraining Loss: 1.110638 \tValidation Loss: 2.833968\n",
      "Epoch: 40691 \tTraining Loss: 1.178615 \tValidation Loss: 2.833220\n",
      "Epoch: 40692 \tTraining Loss: 1.156755 \tValidation Loss: 2.833084\n",
      "Epoch: 40693 \tTraining Loss: 1.132794 \tValidation Loss: 2.832500\n",
      "Epoch: 40694 \tTraining Loss: 1.133709 \tValidation Loss: 2.833258\n",
      "Epoch: 40695 \tTraining Loss: 1.170289 \tValidation Loss: 2.832519\n",
      "Epoch: 40696 \tTraining Loss: 1.174868 \tValidation Loss: 2.832401\n",
      "Epoch: 40697 \tTraining Loss: 1.140254 \tValidation Loss: 2.834143\n",
      "Epoch: 40698 \tTraining Loss: 1.084451 \tValidation Loss: 2.835252\n",
      "Epoch: 40699 \tTraining Loss: 1.140861 \tValidation Loss: 2.832731\n",
      "Epoch: 40700 \tTraining Loss: 1.138249 \tValidation Loss: 2.834352\n",
      "Epoch: 40701 \tTraining Loss: 1.158015 \tValidation Loss: 2.833500\n",
      "Epoch: 40702 \tTraining Loss: 1.120129 \tValidation Loss: 2.834408\n",
      "Epoch: 40703 \tTraining Loss: 1.136847 \tValidation Loss: 2.835359\n",
      "Epoch: 40704 \tTraining Loss: 1.116884 \tValidation Loss: 2.833468\n",
      "Epoch: 40705 \tTraining Loss: 1.081972 \tValidation Loss: 2.834686\n",
      "Epoch: 40706 \tTraining Loss: 1.157179 \tValidation Loss: 2.834033\n",
      "Epoch: 40707 \tTraining Loss: 1.126867 \tValidation Loss: 2.833625\n",
      "Epoch: 40708 \tTraining Loss: 1.126799 \tValidation Loss: 2.835404\n",
      "Epoch: 40709 \tTraining Loss: 1.145124 \tValidation Loss: 2.833155\n",
      "Epoch: 40710 \tTraining Loss: 1.130951 \tValidation Loss: 2.833822\n",
      "Epoch: 40711 \tTraining Loss: 1.119007 \tValidation Loss: 2.834006\n",
      "Epoch: 40712 \tTraining Loss: 1.148060 \tValidation Loss: 2.834644\n",
      "Epoch: 40713 \tTraining Loss: 1.132768 \tValidation Loss: 2.832824\n",
      "Epoch: 40714 \tTraining Loss: 1.134527 \tValidation Loss: 2.834788\n",
      "Epoch: 40715 \tTraining Loss: 1.146036 \tValidation Loss: 2.835667\n",
      "Epoch: 40716 \tTraining Loss: 1.106633 \tValidation Loss: 2.835730\n",
      "Epoch: 40717 \tTraining Loss: 1.097766 \tValidation Loss: 2.834703\n",
      "Epoch: 40718 \tTraining Loss: 1.107452 \tValidation Loss: 2.834465\n",
      "Epoch: 40719 \tTraining Loss: 1.132679 \tValidation Loss: 2.835635\n",
      "Epoch: 40720 \tTraining Loss: 1.125424 \tValidation Loss: 2.837042\n",
      "Epoch: 40721 \tTraining Loss: 1.116083 \tValidation Loss: 2.833561\n",
      "Epoch: 40722 \tTraining Loss: 1.141663 \tValidation Loss: 2.834321\n",
      "Epoch: 40723 \tTraining Loss: 1.159712 \tValidation Loss: 2.833782\n",
      "Epoch: 40724 \tTraining Loss: 1.127363 \tValidation Loss: 2.836322\n",
      "Epoch: 40725 \tTraining Loss: 1.152501 \tValidation Loss: 2.833611\n",
      "Epoch: 40726 \tTraining Loss: 1.187581 \tValidation Loss: 2.834291\n",
      "Epoch: 40727 \tTraining Loss: 1.166867 \tValidation Loss: 2.834684\n",
      "Epoch: 40728 \tTraining Loss: 1.113210 \tValidation Loss: 2.834147\n",
      "Epoch: 40729 \tTraining Loss: 1.135958 \tValidation Loss: 2.834270\n",
      "Epoch: 40730 \tTraining Loss: 1.116648 \tValidation Loss: 2.836127\n",
      "Epoch: 40731 \tTraining Loss: 1.124520 \tValidation Loss: 2.834881\n",
      "Epoch: 40732 \tTraining Loss: 1.123824 \tValidation Loss: 2.832580\n",
      "Epoch: 40733 \tTraining Loss: 1.120190 \tValidation Loss: 2.834143\n",
      "Epoch: 40734 \tTraining Loss: 1.152038 \tValidation Loss: 2.833370\n",
      "Epoch: 40735 \tTraining Loss: 1.136209 \tValidation Loss: 2.835837\n",
      "Epoch: 40736 \tTraining Loss: 1.108730 \tValidation Loss: 2.835664\n",
      "Epoch: 40737 \tTraining Loss: 1.127087 \tValidation Loss: 2.834267\n",
      "Epoch: 40738 \tTraining Loss: 1.150846 \tValidation Loss: 2.834945\n",
      "Epoch: 40739 \tTraining Loss: 1.113068 \tValidation Loss: 2.837254\n",
      "Epoch: 40740 \tTraining Loss: 1.096902 \tValidation Loss: 2.835394\n",
      "Epoch: 40741 \tTraining Loss: 1.087082 \tValidation Loss: 2.837995\n",
      "Epoch: 40742 \tTraining Loss: 1.165264 \tValidation Loss: 2.835618\n",
      "Epoch: 40743 \tTraining Loss: 1.116028 \tValidation Loss: 2.834242\n",
      "Epoch: 40744 \tTraining Loss: 1.127068 \tValidation Loss: 2.833772\n",
      "Epoch: 40745 \tTraining Loss: 1.169109 \tValidation Loss: 2.834022\n",
      "Epoch: 40746 \tTraining Loss: 1.178837 \tValidation Loss: 2.834270\n",
      "Epoch: 40747 \tTraining Loss: 1.150768 \tValidation Loss: 2.834447\n",
      "Epoch: 40748 \tTraining Loss: 1.143335 \tValidation Loss: 2.835245\n",
      "Epoch: 40749 \tTraining Loss: 1.158102 \tValidation Loss: 2.834889\n",
      "Epoch: 40750 \tTraining Loss: 1.134943 \tValidation Loss: 2.835863\n",
      "Epoch: 40751 \tTraining Loss: 1.138093 \tValidation Loss: 2.834666\n",
      "Epoch: 40752 \tTraining Loss: 1.156252 \tValidation Loss: 2.834596\n",
      "Epoch: 40753 \tTraining Loss: 1.151768 \tValidation Loss: 2.835260\n",
      "Epoch: 40754 \tTraining Loss: 1.131523 \tValidation Loss: 2.834908\n",
      "Epoch: 40755 \tTraining Loss: 1.157745 \tValidation Loss: 2.834184\n",
      "Epoch: 40756 \tTraining Loss: 1.154044 \tValidation Loss: 2.834935\n",
      "Epoch: 40757 \tTraining Loss: 1.145363 \tValidation Loss: 2.835335\n",
      "Epoch: 40758 \tTraining Loss: 1.160603 \tValidation Loss: 2.836975\n",
      "Epoch: 40759 \tTraining Loss: 1.147155 \tValidation Loss: 2.836097\n",
      "Epoch: 40760 \tTraining Loss: 1.137668 \tValidation Loss: 2.836766\n",
      "Epoch: 40761 \tTraining Loss: 1.141650 \tValidation Loss: 2.835612\n",
      "Epoch: 40762 \tTraining Loss: 1.087598 \tValidation Loss: 2.837326\n",
      "Epoch: 40763 \tTraining Loss: 1.188931 \tValidation Loss: 2.835286\n",
      "Epoch: 40764 \tTraining Loss: 1.112176 \tValidation Loss: 2.836890\n",
      "Epoch: 40765 \tTraining Loss: 1.169022 \tValidation Loss: 2.834766\n",
      "Epoch: 40766 \tTraining Loss: 1.098364 \tValidation Loss: 2.834618\n",
      "Epoch: 40767 \tTraining Loss: 1.141219 \tValidation Loss: 2.837844\n",
      "Epoch: 40768 \tTraining Loss: 1.125487 \tValidation Loss: 2.836647\n",
      "Epoch: 40769 \tTraining Loss: 1.139410 \tValidation Loss: 2.834211\n",
      "Epoch: 40770 \tTraining Loss: 1.103177 \tValidation Loss: 2.835463\n",
      "Epoch: 40771 \tTraining Loss: 1.124015 \tValidation Loss: 2.834040\n",
      "Epoch: 40772 \tTraining Loss: 1.149867 \tValidation Loss: 2.834719\n",
      "Epoch: 40773 \tTraining Loss: 1.123417 \tValidation Loss: 2.835230\n",
      "Epoch: 40774 \tTraining Loss: 1.141472 \tValidation Loss: 2.835148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40775 \tTraining Loss: 1.147343 \tValidation Loss: 2.834902\n",
      "Epoch: 40776 \tTraining Loss: 1.139922 \tValidation Loss: 2.837315\n",
      "Epoch: 40777 \tTraining Loss: 1.157999 \tValidation Loss: 2.834359\n",
      "Epoch: 40778 \tTraining Loss: 1.105872 \tValidation Loss: 2.834709\n",
      "Epoch: 40779 \tTraining Loss: 1.099703 \tValidation Loss: 2.836003\n",
      "Epoch: 40780 \tTraining Loss: 1.108933 \tValidation Loss: 2.835798\n",
      "Epoch: 40781 \tTraining Loss: 1.124225 \tValidation Loss: 2.836642\n",
      "Epoch: 40782 \tTraining Loss: 1.154536 \tValidation Loss: 2.836948\n",
      "Epoch: 40783 \tTraining Loss: 1.145365 \tValidation Loss: 2.835232\n",
      "Epoch: 40784 \tTraining Loss: 1.105227 \tValidation Loss: 2.837221\n",
      "Epoch: 40785 \tTraining Loss: 1.120767 \tValidation Loss: 2.836527\n",
      "Epoch: 40786 \tTraining Loss: 1.122500 \tValidation Loss: 2.837299\n",
      "Epoch: 40787 \tTraining Loss: 1.144269 \tValidation Loss: 2.835695\n",
      "Epoch: 40788 \tTraining Loss: 1.121211 \tValidation Loss: 2.836073\n",
      "Epoch: 40789 \tTraining Loss: 1.150987 \tValidation Loss: 2.835565\n",
      "Epoch: 40790 \tTraining Loss: 1.179816 \tValidation Loss: 2.834339\n",
      "Epoch: 40791 \tTraining Loss: 1.116979 \tValidation Loss: 2.836724\n",
      "Epoch: 40792 \tTraining Loss: 1.102830 \tValidation Loss: 2.835742\n",
      "Epoch: 40793 \tTraining Loss: 1.164149 \tValidation Loss: 2.835998\n",
      "Epoch: 40794 \tTraining Loss: 1.130803 \tValidation Loss: 2.835218\n",
      "Epoch: 40795 \tTraining Loss: 1.108334 \tValidation Loss: 2.837886\n",
      "Epoch: 40796 \tTraining Loss: 1.143085 \tValidation Loss: 2.836576\n",
      "Epoch: 40797 \tTraining Loss: 1.133766 \tValidation Loss: 2.836452\n",
      "Epoch: 40798 \tTraining Loss: 1.079587 \tValidation Loss: 2.835916\n",
      "Epoch: 40799 \tTraining Loss: 1.149813 \tValidation Loss: 2.835689\n",
      "Epoch: 40800 \tTraining Loss: 1.127257 \tValidation Loss: 2.836305\n",
      "Epoch: 40801 \tTraining Loss: 1.094429 \tValidation Loss: 2.835940\n",
      "Epoch: 40802 \tTraining Loss: 1.088625 \tValidation Loss: 2.837776\n",
      "Epoch: 40803 \tTraining Loss: 1.129465 \tValidation Loss: 2.836660\n",
      "Epoch: 40804 \tTraining Loss: 1.171504 \tValidation Loss: 2.834028\n",
      "Epoch: 40805 \tTraining Loss: 1.139498 \tValidation Loss: 2.837532\n",
      "Epoch: 40806 \tTraining Loss: 1.099208 \tValidation Loss: 2.836863\n",
      "Epoch: 40807 \tTraining Loss: 1.111849 \tValidation Loss: 2.835424\n",
      "Epoch: 40808 \tTraining Loss: 1.130990 \tValidation Loss: 2.836354\n",
      "Epoch: 40809 \tTraining Loss: 1.145278 \tValidation Loss: 2.835549\n",
      "Epoch: 40810 \tTraining Loss: 1.100297 \tValidation Loss: 2.837631\n",
      "Epoch: 40811 \tTraining Loss: 1.158005 \tValidation Loss: 2.836775\n",
      "Epoch: 40812 \tTraining Loss: 1.116893 \tValidation Loss: 2.838420\n",
      "Epoch: 40813 \tTraining Loss: 1.133841 \tValidation Loss: 2.835578\n",
      "Epoch: 40814 \tTraining Loss: 1.124205 \tValidation Loss: 2.836085\n",
      "Epoch: 40815 \tTraining Loss: 1.117618 \tValidation Loss: 2.837804\n",
      "Epoch: 40816 \tTraining Loss: 1.173460 \tValidation Loss: 2.834295\n",
      "Epoch: 40817 \tTraining Loss: 1.122475 \tValidation Loss: 2.835423\n",
      "Epoch: 40818 \tTraining Loss: 1.136204 \tValidation Loss: 2.836953\n",
      "Epoch: 40819 \tTraining Loss: 1.108802 \tValidation Loss: 2.837891\n",
      "Epoch: 40820 \tTraining Loss: 1.139104 \tValidation Loss: 2.836710\n",
      "Epoch: 40821 \tTraining Loss: 1.168963 \tValidation Loss: 2.836804\n",
      "Epoch: 40822 \tTraining Loss: 1.207896 \tValidation Loss: 2.834300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-9a2933d12a3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#     print('XXXXXXXXXXXXXX',output[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_set=torch.from_numpy(feature_set).float()\n",
    "targets=torch.from_numpy(targets).long()\n",
    "\n",
    "validation_feature_set=torch.from_numpy(validation_feature_set).float()\n",
    "validation_targets=torch.from_numpy(validation_targets).long()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100000\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "  \n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(feature_set)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, targets)\n",
    "#     print('XXXXXXXXXXXXXX',output[0])\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer.step()\n",
    "    # update running training loss\n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(validation_feature_set)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, validation_targets)\n",
    "    # update running validation loss \n",
    "    valid_loss = loss.item()\n",
    "        \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Model with the Lowest Validation Loss\n",
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(torch.from_numpy(X_test).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, target):\n",
    "    t = 0\n",
    "    for o in range(len(out)):\n",
    "        if int((output[o] == output[o].max()).nonzero()[0][0]) == int(target[o]):\n",
    "            t+=1\n",
    "#         print(target[o])\n",
    "    print(100*t/len(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.607223476297968\n"
     ]
    }
   ],
   "source": [
    "accuracy(output.argmax(1),y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.308885\n",
      "\n",
      "Test Accuracy of     0: 30% (11/36)\n",
      "Test Accuracy of     1: 15% ( 7/44)\n",
      "Test Accuracy of     2: 41% (14/34)\n",
      "Test Accuracy of     3: 17% ( 7/40)\n",
      "Test Accuracy of     4: 19% ( 8/41)\n",
      "Test Accuracy of     5:  0% ( 0/41)\n",
      "Test Accuracy of     6:  5% ( 2/34)\n",
      "Test Accuracy of     7:  2% ( 1/47)\n",
      "Test Accuracy of     8: 15% ( 5/33)\n",
      "Test Accuracy of     9: 12% ( 5/40)\n",
      "Test Accuracy of    10: 41% (18/43)\n",
      "Test Accuracy of    11:  0% ( 0/10)\n",
      "\n",
      "Test Accuracy (Overall): 17% (78/443)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(12))\n",
    "class_total = list(0. for i in range(12))\n",
    "\n",
    "test_feature_set=torch.from_numpy(X_test).float()\n",
    "test_targets=torch.from_numpy(y_test).long()\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "\n",
    "# forward pass: compute predicted outputs by passing inputs to the model \n",
    "output = model(test_feature_set)\n",
    "# calculate the loss\n",
    "loss = criterion(output, test_targets)\n",
    "# update test loss \n",
    "test_loss = loss.item()\n",
    "# convert output probabilities to predicted class\n",
    "_, pred = torch.max(output, 1)\n",
    "# compare predictions to true label\n",
    "correct = np.squeeze(pred.eq(test_targets.data.view_as(pred)))\n",
    "# calculate test accuracy for each object class\n",
    "for i in range(len(test_targets)):\n",
    "    label = test_targets.data[i]\n",
    "    class_correct[label] += correct[i].item()\n",
    "    class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(12):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(output,test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-829f1d20305b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##cross validation - Doesn't work HEHEHEHE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "##cross validation - Doesn't work HEHEHEHE\n",
    "\n",
    "n = len(X)\n",
    "kf = KFold(n_splits=5)\n",
    "fold = 0\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "#     clf = LogisticRegression().fit(X_train, y_train)\n",
    "#     score = clf.score(X_test, y_test)\n",
    "    m = trainModel(X_train,y_train,10000)\n",
    "    m.eval() # prep model for evaluation\n",
    "    y_pred = m(X_test)\n",
    "    score=f1_score(y_test, y_pred.argmax(1), average='macro') \n",
    "    print(\"Score for fold %d: %.3f\" % (fold, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
